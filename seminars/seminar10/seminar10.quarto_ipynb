{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Neural Networks\n",
        "jupyter: python3\n",
        "format:\n",
        "  live-html:\n",
        "    toc: true\n",
        "    toc-location: right\n",
        "pyodide:\n",
        "  autorun: false\n",
        "  packages:\n",
        "    - matplotlib\n",
        "    - numpy\n",
        "    - scipy\n",
        "    - pandas\n",
        "---\n",
        "\n",
        "```{pyodide}\n",
        "#| edit: false\n",
        "#| echo: false\n",
        "#| execute: true\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.integrate import odeint\n",
        "import pandas as pd\n",
        "\n",
        "# Set default plotting parameters\n",
        "plt.rcParams.update({\n",
        "    'font.size': 12,\n",
        "    'lines.linewidth': 1,\n",
        "    'lines.markersize': 5,\n",
        "    'axes.labelsize': 11,\n",
        "    'xtick.labelsize': 10,\n",
        "    'ytick.labelsize': 10,\n",
        "    'xtick.top': True,\n",
        "    'xtick.direction': 'in',\n",
        "    'ytick.right': True,\n",
        "    'ytick.direction': 'in',\n",
        "})\n",
        "\n",
        "def get_size(w, h):\n",
        "    return (w/2.54, h/2.54)\n",
        "```\n",
        "\n",
        "# Introduction to Neural Networks in Physics\n",
        "\n",
        "## What are Neural Networks?\n",
        "Neural networks are computational models inspired by how our brains process information. Just like our brain consists of interconnected neurons that process and transmit signals, artificial neural networks consist of mathematical \"neurons\" that process numerical information. They're particularly powerful for:\n",
        "\n",
        "- Recognizing patterns in data\n",
        "- Making predictions\n",
        "- Classifying information\n",
        "- Solving complex problems\n",
        "\n",
        "## Why Neural Networks in Physics?\n",
        "\n",
        "In physics, we often encounter situations that push the boundaries of traditional approaches. Sometimes the mathematical models grow too complex to solve directly, while in other cases we face the challenge of analyzing vast amounts of experimental data. We frequently need to make predictions even when we have incomplete information about a system. These scenarios represent key areas where neural networks can provide valuable solutions.\n",
        "\n",
        "Neural networks help us with these challenges! Some real-world applications include:\n",
        "\n",
        "- Particle physics: Identifying particles in detector data\n",
        "- Astronomy: Classifying galaxies\n",
        "- Materials science: Predicting material properties\n",
        "- Quantum mechanics: Solving many-body problems\n",
        "- Biological physics: Modeling neural activity\n",
        "- Active matter: Predicting collective behavior\n",
        "\n",
        "\n",
        "## Data for Neural Networks: Teaching Computers to Read Numbers\n",
        "\n",
        "Let's start our journey into neural networks with an exciting challenge: teaching a computer to read handwritten numbers! We'll build this step by step using Python, starting with the basics and working our way up to something quite impressive.\n",
        "\n",
        "Think of this like teaching a child to recognize numbers - we'll start by teaching our computer to recognize just one number (zero), and then build up to recognizing all digits from 0 to 9.\n",
        "\n",
        "![MNIS](MNIST.png)\n",
        "\n",
        "We'll use a famous collection of handwritten numbers called the MNIST dataset. Imagine asking 70,000 different people to write down numbers - that's what this dataset is! Each number is written on a small 28 x 28 grid (like graph paper), where each square (or pixel) is shaded in grayscale from white (0) to black (255).\n",
        "\n",
        "## Getting Started with MNIST\n",
        "\n",
        "Just like we need data from experiments in physics, we need data to train our neural network. Fortunately, other scientists have already collected this data for us:\n",
        "\n",
        "### Loading Our Training Data\n",
        "\n",
        "```{pyodide}\n",
        "#| autorun: false\n",
        "from sklearn.datasets import fetch_openml\n",
        "X, y = fetch_openml('mnist_784', version=1, return_X_y=True,as_frame=False)\n",
        "```\n",
        "\n",
        "Here, `X` contains all our images, and `y` contains the correct answer for each image (which number it is). Let's look at one:\n",
        "\n",
        "```{pyodide}\n",
        "#| autorun: false\n",
        "i = 33419\n",
        "plt.figure(figsize=get_size(4,4))\n",
        "plt.imshow(np.array(X)[i].reshape(28, 28), cmap='gray')\n",
        "plt.colorbar()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print('label: ',y[i])\n",
        "```\n",
        "\n",
        "### Making the Data Easier to Work With\n",
        "\n",
        "Just like we often normalize measurements in physics experiments (like dividing by the maximum value), we'll normalize our image data to be between 0 and 1:\n",
        "\n",
        "```{pyodide}\n",
        "#| autorun: false\n",
        "X = X/255  # Dividing by maximum pixel value\n",
        "```\n",
        "\n",
        "### Preparing Our Training and Testing Sets\n",
        "\n",
        "For now, we'll start simple: we'll just teach our network to recognize zeros. We'll mark zeros with a 1 and all other numbers with a 0:\n",
        "\n",
        "```{pyodide}\n",
        "#| autorun: false\n",
        "y_new = np.zeros(y.shape)\n",
        "y_new[np.where(y == '0')[0]] = 1\n",
        "y = y_new\n",
        "```\n",
        "\n",
        "Like any good scientific experiment, we need both training data (to teach the network) and testing data (to check how well it learned). We'll use:\n",
        "- 60,000 images for training\n",
        "- 10,000 images for testing\n",
        "\n",
        "```{pyodide}\n",
        "#| autorun: false\n",
        "m = 60000\n",
        "m_test = X.shape[0] - m\n",
        "\n",
        "X_train, X_test = X[:m].T, X[m:].T\n",
        "y_train, y_test = y[:m].reshape(1,m), y[m:].reshape(1, m_test)\n",
        "```\n",
        "\n",
        "Finally, we shuffle our training data (like shuffling flashcards when studying):\n",
        "\n",
        "```{pyodide}\n",
        "#| autorun: false\n",
        "np.random.seed(1)\n",
        "shuffle_index = np.random.permutation(m)\n",
        "X_train, y_train = X_train[:,shuffle_index], y_train[:,shuffle_index]\n",
        "```\n",
        "\n",
        "Let's check our work by looking at one of our training images:\n",
        "\n",
        "```{pyodide}\n",
        "#| autorun: false\n",
        "i = 39\n",
        "plt.figure(figsize=get_size(4,4))\n",
        "plt.imshow(X_train[:,i].reshape(28, 28), cmap='gray')\n",
        "plt.colorbar()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(y_train[:,i])\n",
        "```\n",
        "\n",
        "Try looking at different images (change the number 39 above) until you find a zero - its label should be 1!\n",
        "\n",
        "\n",
        "## A Single Neuron\n",
        "\n",
        "The basic building block of any neural network is an artificial neuron. Similar to neurons in the human brain that process incoming signals and decide whether to fire or not, an artificial neuron processes numerical inputs through mathematical operations to produce a single output value. The following diagram shows a simple artificial neuron with two input values:\n",
        "\n",
        "![](img/neuron.png)\n",
        "\n",
        "### Understanding Forward Propagation\n",
        "\n",
        "An artificial neuron processes information in three distinct steps that together form what is called forward propagation:\n",
        "\n",
        "1. Input Weighting\n",
        "Each input value gets multiplied by a weight parameter. These weights determine how much influence each input has on the final output, similar to how synapses in biological neurons can be stronger or weaker. For two inputs, this operation looks like:\n",
        "\n",
        "\\begin{eqnarray}\n",
        "x_{1}\\rightarrow x_{1} w_{1}\\\\\n",
        "x_{2}\\rightarrow x_{2} w_{2}\n",
        "\\end{eqnarray}\n",
        "\n",
        "2. Bias Addition\n",
        "After weighting the inputs, a bias value $b$ is added to the sum. The bias helps the neuron learn by shifting the weighted sum up or down, making it easier or harder for the neuron to produce a strong output signal:\n",
        "\n",
        "\\begin{equation}\n",
        "x_{1} w_{1}+ x_{2} w_{2}+b\n",
        "\\end{equation}\n",
        "\n",
        "3. Activation Function\n",
        "The final step applies an activation function $\\sigma()$ to the weighted sum plus bias. This function introduces non-linearity into the network, allowing it to learn complex patterns:\n",
        "\n",
        "\\begin{equation}\n",
        "y=\\sigma( x_{1} w_{1}+ x_{2} w_{2}+b)\n",
        "\\end{equation}\n",
        "\n",
        "The activation function used in this example is called the sigmoid function. This function is particularly useful because it takes any input number (positive or negative, large or small) and transforms it into an output between 0 and 1. This property makes the sigmoid function ideal for tasks where the output should represent a probability or a binary decision.\n",
        "\n",
        "For mathematical convenience, the above steps can be written more compactly using vector notation:\n",
        "\n",
        "\\begin{equation*}\n",
        "\\hat{y} = \\sigma(w^{\\rm T} x + b)\\ .\n",
        "\\end{equation*}\n",
        "\n",
        "The sigmoid function has the following mathematical form:\n",
        "\\begin{equation*}\n",
        "\\sigma(z) = \\frac{1}{1+{\\rm e}^{-z}}\\ .\n",
        "\\end{equation*}\n",
        "\n",
        "Here is a Python implementation of the sigmoid function:\n",
        "\n",
        "```{pyodide}\n",
        "#| autorun: false\n",
        "def sigmoid(z):\n",
        "    return 1/(1 + np.exp(-z))\n",
        "```\n",
        "\n",
        "The following code visualizes how the sigmoid function transforms input values:\n",
        "\n",
        "```{pyodide}\n",
        "#| autorun: false\n",
        "x=np.linspace(-5,5,100)\n",
        "plt.figure(figsize=get_size(8,6))\n",
        "plt.plot(x,sigmoid(x))\n",
        "plt.xlabel('input')\n",
        "plt.ylabel('output')\n",
        "plt.grid()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "To understand how these components work together, consider a simple example with two inputs. Given:\n",
        "\n",
        "\\begin{eqnarray}\n",
        "w=[0,1]\\\\\n",
        "b=4\n",
        "\\end{eqnarray}\n",
        "\n",
        "And input values:\n",
        "\n",
        "\\begin{eqnarray}\n",
        "x=[2,3]\n",
        "\\end{eqnarray}\n",
        "\n",
        "The computation becomes:\n",
        "\n",
        "\\begin{equation}\n",
        "y=f(w\\cdot x+b)=f(7)=0.999\n",
        "\\end{equation}\n",
        "\n",
        "This process of moving from inputs to output through these mathematical operations is called forward propagation. The goal is to extend this concept to create a network capable of processing images, which requires 784 inputs (one for each pixel in a 28 x 28 image) and producing meaningful outputs.\n",
        "\n",
        "For computational efficiency, these calculations can be performed on multiple inputs simultaneously using matrix operations. The forward pass equation becomes:\n",
        "\n",
        "\\begin{equation*}\n",
        "\\hat{y} = \\sigma(w^{\\rm T} X + b)\\ .\n",
        "\\end{equation*}\n",
        "\n",
        "In this matrix form, $\\hat{y}$ represents a vector of outputs rather than a single value. The implementation splits this computation into two parts:\n",
        "\n",
        "1. Calculate the weighted sum: `Z = np.matmul(W.T, X) + b`\n",
        "2. Apply the activation function: `A = sigmoid(Z)`\n",
        "\n",
        "This separation into distinct steps makes the code clearer and prepares for the more complex calculations needed in the backward propagation phase.\n",
        "\n",
        "### Loss Function: Measuring How Wrong We Are\n",
        "\n",
        "Now that our network can make predictions, we need a way to measure how accurate those predictions are. Just like we measure error in physics experiments, we need to measure the error (or \"loss\") in our neural network's predictions.\n",
        "\n",
        "The simplest way would be to use mean squared error, which you've seen before in data fitting:\n",
        "\n",
        "\\begin{equation}\n",
        "MSE(y,\\hat{y})=\\frac{1}{n}\\sum_{i=1}^{n}(y-\\hat{y})^2\n",
        "\\end{equation}\n",
        "\n",
        "Here, $y$ is the true value (what we know is correct) and $\\hat{y}$ is our network's prediction.\n",
        "\n",
        "However, for this type of classification problem, we'll use a different measure called `cross-entropy`. For a single training example, it looks like this:\n",
        "\n",
        "\\begin{equation*}\n",
        "L(y,\\hat{y}) = -y\\log(\\hat{y})-(1-y)\\log(1-\\hat{y})\\ .\n",
        "\\end{equation*}\n",
        "\n",
        "When we have many training examples ($m$ of them), we take the average:\n",
        "\n",
        "\\begin{equation*}\n",
        "L(Y,\\hat{Y}) = -\\frac{1}{m}\\sum_{i = 0}^{m}y^{(i)}\\log(\\hat{y}^{(i)})-(1-y^{(i)})\\log(1-\\hat{y}^{(i)})\\ .\n",
        "\\end{equation*}\n",
        "\n",
        "Here's how we implement this in code:\n",
        "\n",
        "```{pyodide}\n",
        "#| autorun: false\n",
        "def compute_loss(Y, Y_hat):\n",
        "    m = Y.shape[1]\n",
        "    L = -(1./m)*(np.sum(np.multiply(np.log(Y_hat), Y)) + np.sum(np.multiply(np.log(1 - Y_hat), (1 - Y))))\n",
        "    return L\n",
        "```\n",
        "\n",
        "## Training the Network: Making Our Network Learn\n",
        "\n",
        "Think of training a neural network like teaching a student - we need to:\n",
        "1. See how well they're doing (measure the loss)\n",
        "2. Give feedback on what to improve\n",
        "3. Let them practice and improve\n",
        "\n",
        "### Backward Propagation: Learning from Mistakes\n",
        "\n",
        "Just like we adjust our aim when throwing a ball based on how far we missed, our network needs to adjust its weights and biases based on its errors. The loss function depends on all our weights and biases:\n",
        "\n",
        "$$\n",
        "L(w_{1},w_{2},w_{3},\\ldots ,b_{1},b_{2},b_{3},\\ldots)\n",
        "$$\n",
        "\n",
        "To improve, we need to know how changing each weight affects our error. We can find this using partial derivatives:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_j}\n",
        "$$\n",
        "\n",
        "This tells us \"if we change weight $w_j$ a little bit, how much will our error change?\" This process of calculating how to adjust weights based on errors is called **back propagation**.\n",
        "\n",
        "**Calculating How to Improve**\n",
        "\n",
        "Let's break this down into steps. For a single image, we can follow how changes flow through the network:\n",
        "\\begin{align*}\n",
        "z &= w^{\\rm T} x + b\\ , \\\\\n",
        "\\hat{y} &= \\sigma(z)\\ , \\\\\n",
        "L(y,\\hat{y}) &= -y\\log(\\hat{y})-(1-y)\\log(1-\\hat{y})\\ .\n",
        "\\end{align*}\n",
        "\n",
        "Using the chain rule from calculus:\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\partial L}{\\partial w_j} = \\frac{\\partial L}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial z}\\frac{\\partial z}{\\partial w_j}\n",
        "\\end{align*} <br>\n",
        "\n",
        "After working through the calculus (which we won't detail here), we get three parts, each representing how different components of our network affect the final loss:\n",
        "\n",
        "First, we calculate how the loss changes with respect to our prediction ($\\hat{y}$):\n",
        "\n",
        "$\\partial L/\\partial\\hat{y}$:\n",
        "\\begin{align*}\n",
        "\\frac{\\partial L}{\\partial\\hat{y}} &= \\frac{\\hat{y} - y}{\\hat{y}(1-\\hat{y})}\n",
        "\\end{align*}\n",
        "\n",
        "Next, we find how our prediction changes with respect to the weighted input ($z$). This is just the derivative of the sigmoid function:\n",
        "\n",
        "$\\partial \\hat{y}/\\partial z$:\n",
        "\\begin{align*}\n",
        "\\frac{\\partial }{\\partial z}\\sigma(z) &= \\hat{y}(1-\\hat{y})\n",
        "\\end{align*}\n",
        "\n",
        "Finally, we calculate how the weighted input changes with respect to each weight. This is simply the corresponding input value:\n",
        "\n",
        "$\\partial z/\\partial w_j$:\n",
        "\\begin{align*}\n",
        "\\frac{\\partial }{\\partial w_j}(w^{\\rm T} x + b) &= x_j\n",
        "\\end{align*}\n",
        "\n",
        "When we multiply these three components together using the chain rule, something remarkable happens - most terms cancel out, leaving us with this elegantly simple result:\n",
        "\\begin{align*}\n",
        "\\frac{\\partial L}{\\partial w_j} = (\\hat{y} - y)x_j\\ .\n",
        "\\end{align*}\n",
        "\n",
        "This tells us that the adjustment to each weight should be proportional to both the prediction error ($\\hat{y} - y$) and the input value ($x_j$).\n",
        "\n",
        "When dealing with multiple training examples, we need to average these gradients:\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\partial L}{\\partial w} = \\frac{1}{m} X(\\hat{y} - y)^{\\rm T}\\ .\n",
        "\\end{align*}\n",
        "\n",
        "The bias term follows a similar pattern. For a single example:\n",
        "\\begin{align*}\n",
        "\\frac{\\partial L}{\\partial b} = (\\hat{y} - y)\\ .\n",
        "\\end{align*}\n",
        "\n",
        "And for multiple training examples, we average the gradients:\n",
        "\\begin{align*}\n",
        "\\frac{\\partial L}{\\partial b} = \\frac{1}{m}\\sum_{i=1}^{m}{(\\hat{y}^{(i)} - y^{(i)})}\\ .\n",
        "\\end{align*}\n",
        "\n",
        "In our code, these mathematical formulas translate directly into matrix operations:\n",
        "`dW = (1/m) * np.matmul(X, (A-Y).T)` and `db = (1/m)*np.sum(A-Y, axis=1, keepdims=True)`.\n",
        "\n",
        "### Stochastic Gradient Descent: Teaching Our Network to Learn\n",
        "\n",
        "Now comes the exciting part - making our network learn! Just like how you adjust your throw when playing catch based on whether you threw too far or too short, our network needs to adjust its weights and biases based on its mistakes.\n",
        "\n",
        "We'll use a learning method called stochastic gradient descent (SGD). Don't let the fancy name scare you - it's actually quite simple! Think of it like walking down a hill:\n",
        "\n",
        "1. Look around to see which way is steepest downhill (that's the gradient)\n",
        "2. Take a small step in that direction\n",
        "3. Repeat until you reach the bottom\n",
        "\n",
        "Mathematically, we update each weight using this formula:\n",
        "\n",
        "$$\n",
        "w\\leftarrow w-\\eta\\frac{\\partial L}{\\partial w}\n",
        "$$\n",
        "\n",
        "Here, $\\eta$ (eta) is called the learning rate - it controls how big our steps are:\n",
        "\n",
        "- Too large: We might overshoot the bottom\n",
        "- Too small: Learning will take forever\n",
        "\n",
        "The term $\\frac{\\partial L}{\\partial w}$ tells us which direction to step:\n",
        "\n",
        "- If positive: The weight is too large, so decrease it\n",
        "- If negative: The weight is too small, so increase it\n",
        "\n",
        "We do the same thing for the bias $b$. Each complete pass through all our training data is called an \"epoch\".\n",
        "\n",
        "**Physics Connection:** This is similar to finding the minimum of a potential well - we follow the direction where the potential decreases most rapidly!\n",
        "\n",
        "### Building and Training Our First Network\n",
        "\n",
        "Let's put everything together to create a network that can recognize handwritten numbers. We'll train it for 200 epochs (learning cycles) and watch how the loss decreases:\n",
        "\n",
        "```{pyodide}\n",
        "#| autorun: false\n",
        "learning_rate = 1\n",
        "\n",
        "X = np.array(X_train)\n",
        "Y = np.array(y_train)\n",
        "\n",
        "n_x = X.shape[0]\n",
        "m = X.shape[1]\n",
        "\n",
        "# Initialize weights and bias with small random values\n",
        "W = np.random.randn(n_x, 1) * 0.01\n",
        "b = np.zeros((1, 1))\n",
        "\n",
        "# Training loop\n",
        "for i in range(200):\n",
        "    # Forward pass\n",
        "    Z = np.matmul(W.T, X) + b\n",
        "    A = sigmoid(Z)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = compute_loss(Y, A)\n",
        "\n",
        "    # Backward pass - compute gradients\n",
        "    dW = (1/m)*np.matmul(X, (A-Y).T)\n",
        "    db = (1/m)*np.sum(A-Y, axis=1, keepdims=True)\n",
        "\n",
        "    # Update parameters\n",
        "    W = W - learning_rate * dW\n",
        "    b = b - learning_rate * db\n",
        "\n",
        "    # Print progress every 10 epochs\n",
        "    if i % 10 == 0:\n",
        "        print(f\"Epoch {i:3d}, Loss: {loss:.6f}\")\n",
        "\n",
        "print(f\"Final loss: {loss:.6f}\")\n",
        "```\n",
        "\n",
        "### Evaluating Our Network: How Well Did We Do?\n",
        "\n",
        "Just like in physics experiments, we need ways to measure how well our model performs. One powerful tool is the **confusion matrix**. Think of it as a report card for our network:\n",
        "\n",
        "![confusion_matrix](confusion_matrix.png)\n",
        "\n",
        "The confusion matrix shows:\n",
        "- True Positives (TP): We predicted \"yes\" and were right\n",
        "- False Positives (FP): We predicted \"yes\" but were wrong\n",
        "- True Negatives (TN): We predicted \"no\" and were right\n",
        "- False Negatives (FN): We predicted \"no\" but were wrong\n",
        "\n",
        "Let's calculate this for our network:\n",
        "\n",
        "```{pyodide}\n",
        "#| autorun: false\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Make predictions on test data\n",
        "Z = np.matmul(W.T,X_test) + b\n",
        "A = sigmoid(Z)\n",
        "\n",
        "# Convert to binary predictions (0 or 1)\n",
        "predictions = (A>.5)[0,:]\n",
        "labels = (y_test == 1)[0,:]\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(predictions, labels))\n",
        "print(\"\\nDetailed Performance Report:\")\n",
        "print(classification_report(predictions, labels))\n",
        "```\n",
        "\n",
        "### Testing Individual Images\n",
        "\n",
        "Let's see our network in action! We can test it on individual images:\n",
        "\n",
        "```{pyodide}\n",
        "#| autorun: false\n",
        "# Pick a test image\n",
        "i=200\n",
        "prediction = bool(sigmoid(np.matmul(W.T, np.array(X_test)[:,i])+b)>0.5)\n",
        "print(f\"Network prediction: {'Zero' if prediction else 'Not Zero'}\")\n",
        "\n",
        "# Display the image\n",
        "plt.figure(figsize=get_size(6,6))\n",
        "plt.imshow(np.array(X_test)[:,i].reshape(28,28), cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "## Network with Hidden Layers\n",
        "\n",
        "In our example above, we just had an input layer and a single output neuron. More complex neural networks are containing many layers between the input layer and the output layer. These inbetween layers are called hidden layers. Here is a simple example of a neural network with a single hidden layer.\n",
        "\n",
        "![hidden](image.png)\n",
        "\n",
        "So we have now and input layer with 784 inputs that are connected to 64 units in the hidden layer and 1 neuron in the output layer. We will not go through the derivations of all the formulas for the forward and backward passes this time. The code is a simple extension of what we did before and I hope easy to read.\n",
        "\n",
        "```{pyodide}\n",
        "#| autorun: false\n",
        "X = X_train\n",
        "Y = y_train\n",
        "\n",
        "n_x = X.shape[0]\n",
        "n_h = 64\n",
        "learning_rate = 1\n",
        "\n",
        "W1 = np.random.randn(n_h, n_x)\n",
        "b1 = np.zeros((n_h, 1))\n",
        "W2 = np.random.randn(1, n_h)\n",
        "b2 = np.zeros((1, 1))\n",
        "\n",
        "for i in range(100):\n",
        "\n",
        "    Z1 = np.matmul(W1, X) + b1\n",
        "    A1 = sigmoid(Z1)\n",
        "    Z2 = np.matmul(W2, A1) + b2\n",
        "    A2 = sigmoid(Z2)\n",
        "\n",
        "    loss = compute_loss(Y, A2)\n",
        "\n",
        "    dZ2 = A2-Y\n",
        "    dW2 = (1./m) * np.matmul(dZ2, A1.T)\n",
        "    db2 = (1./m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "\n",
        "    dA1 = np.matmul(W2.T, dZ2)\n",
        "    dZ1 = dA1 * sigmoid(Z1) * (1 - sigmoid(Z1))\n",
        "    dW1 = (1./m) * np.matmul(dZ1, X.T)\n",
        "    db1 = (1./m) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "\n",
        "    W2 = W2 - learning_rate * dW2\n",
        "    b2 = b2 - learning_rate * db2\n",
        "    W1 = W1 - learning_rate * dW1\n",
        "    b1 = b1 - learning_rate * db1\n",
        "\n",
        "    if i % 10 == 0:\n",
        "        print(\"Epoch\", i, \"loss: \", loss)\n",
        "\n",
        "print(\"Final loss:\", loss)\n",
        "```\n",
        "\n",
        "To judge the newtork quality we do use again the confusion matrix.\n",
        "\n",
        "```{pyodide}\n",
        "#| autorun: false\n",
        "Z1 = np.matmul(W1, X_test) + b1\n",
        "A1 = sigmoid(Z1)\n",
        "Z2 = np.matmul(W2, A1) + b2\n",
        "A2 = sigmoid(Z2)\n",
        "\n",
        "predictions = (A2>.5)[0,:]\n",
        "labels = (y_test == 1)[0,:]\n",
        "\n",
        "print(confusion_matrix(predictions, labels))\n",
        "print(classification_report(predictions, labels))\n",
        "```\n",
        "\n",
        "## Multiclass Network\n",
        "\n",
        "So far we did only classify if the number we feed to the network is just a 0 or not. We would like to recognize the different number now and therefore need a multiclass network. Each number is then a class and per class, we have multiple realizations of handwritten numbers. We therefore have to create an output layer, which is not only containing a single neuron, but 10 neurons. Each of these neuron can output a value between 0 and 1. Whenever the output is 1, the index of the neuron represents the number predicted.\n",
        "\n",
        "The output array\n",
        "\n",
        "~~~\n",
        "[0,1,0,0,0,0,0,0,0,0]\n",
        "~~~\n",
        "\n",
        "would therefore correspond to the value 1.\n",
        "\n",
        "For this purpose, we need to reload the right labels.\n",
        "\n",
        "```{pyodide}\n",
        "#| autorun: false\n",
        "from sklearn.datasets import fetch_openml\n",
        "X, y = fetch_openml('mnist_784', version=1, return_X_y=True,as_frame=False)\n",
        "\n",
        "X = X / 255\n",
        "```\n",
        "\n",
        "Then we'll one-hot encode MNIST's labels, to get a 10 x 70,000 array.\n",
        "\n",
        "```{pyodide}\n",
        "#| autorun: false\n",
        "digits = 10\n",
        "examples = y.shape[0]\n",
        "\n",
        "y = y.reshape(1, examples)\n",
        "\n",
        "Y_new = np.eye(digits)[y.astype('int32')]\n",
        "Y_new = Y_new.T.reshape(digits, examples)\n",
        "```\n",
        "\n",
        "```{pyodide}\n",
        "#| autorun: false\n",
        "\n",
        "Y_new.shape\n",
        "```\n",
        "\n",
        "We also seperate into trainging and testing data\n",
        "\n",
        "```{pyodide}\n",
        "#| autorun: false\n",
        "m = 60000\n",
        "m_test = X.shape[0] - m\n",
        "\n",
        "X_train, X_test = X[:m].T, X[m:].T\n",
        "Y_train, Y_test = Y_new[:,:m], Y_new[:,m:]\n",
        "\n",
        "shuffle_index = np.random.permutation(m)\n",
        "X_train, Y_train = X_train[:, shuffle_index], Y_train[:, shuffle_index]\n",
        "```\n",
        "\n",
        "```{pyodide}\n",
        "#| autorun: false\n",
        "i = 58\n",
        "plt.imshow(X_train[:,i].reshape(28,28), cmap='gray')\n",
        "plt.colorbar()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "Y_train[:,i]\n",
        "```\n",
        "\n",
        "### Changes to the model\n",
        "\n",
        "OK, so let's consider what changes we need to make to the model itself.\n",
        "\n",
        "#### Forward Pass\n",
        "Only the last layer of our network is changing. To add the softmax, we have to replace our lone, final node with a 10 unit layer. Its final activations are the exponentials of its z-values, normalized across all ten such exponentials. So instead of just computing $\\sigma(z)$, we compute the activation for each unit $i$ using the softmax function:\n",
        "\\begin{align*}\n",
        "\\sigma(z)_i = \\frac{{\\rm e}^{z_i}}{\\sum_{j=0}^9{\\rm e}^{z_i}}\\ .\n",
        "\\end{align*}\n",
        "\n",
        "So, in our vectorized code, the last line of forward propagation will be `A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0)`.\n",
        "\n",
        "#### Loss Function\n",
        "\n",
        "Our loss  function now has to generalize to more than two classes. The general formula for $n$ classes is:\n",
        "\\begin{align*}\n",
        "L(y,\\hat{y}) = -\\sum_{i=0}^n y_i\\log(\\hat{y}_i)\\ .\n",
        "\\end{align*}\n",
        "Averaging over $m$ training examples this becomes:\n",
        "\\begin{align*}\n",
        "L(y,\\hat{y}) = -\\frac{1}{m}\\sum_{j=0}^m\\sum_{i=0}^n y_i^{(i)}\\log(\\hat{y}_i^{(i)})\\ .\n",
        "\\end{align*}\n",
        "\n",
        "So let's define:\n",
        "\n",
        "```{pyodide}\n",
        "#| autorun: false\n",
        "def compute_multiclass_loss(Y, Y_hat):\n",
        "    L_sum = np.sum(np.multiply(Y, np.log(Y_hat)))\n",
        "    m = Y.shape[1]\n",
        "    L = -(1/m) * L_sum\n",
        "    return L\n",
        "```\n",
        "\n",
        "#### Back Propagation\n",
        "\n",
        "Luckily it turns out that back propagation isn't really affected by the switch to a softmax. A softmax generalizes the sigmoid activiation we've been using, and in such a way that the code we wrote earlier still works. We could verify this by deriving:\n",
        "\\begin{align*}\n",
        "\\frac{\\partial L}{\\partial z_i} = \\hat{y}_i - y_i\\ .\n",
        "\\end{align*}\n",
        "\n",
        "But we won't walk through the steps here. Let's just go ahead and build our final network.\n",
        "\n",
        "### Build and Train\n",
        "\n",
        "As we have now more weights and classes, the training takes longer and we actually need also more episodes to achieve a good accuracy.\n",
        "\n",
        "```{pyodide}\n",
        "#| autorun: false\n",
        "n_x = X_train.shape[0]\n",
        "n_h = 64\n",
        "learning_rate = 1\n",
        "\n",
        "W1 = np.random.randn(n_h, n_x)\n",
        "b1 = np.zeros((n_h, 1))\n",
        "W2 = np.random.randn(digits, n_h)\n",
        "b2 = np.zeros((digits, 1))\n",
        "\n",
        "X = X_train\n",
        "Y = Y_train\n",
        "\n",
        "for i in range(200):\n",
        "\n",
        "    Z1 = np.matmul(W1,X) + b1\n",
        "    A1 = sigmoid(Z1)\n",
        "    Z2 = np.matmul(W2,A1) + b2\n",
        "    A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0)\n",
        "\n",
        "    loss = compute_multiclass_loss(Y, A2)\n",
        "\n",
        "    dZ2 = A2-Y\n",
        "    dW2 = (1./m) * np.matmul(dZ2, A1.T)\n",
        "    db2 = (1./m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "\n",
        "    dA1 = np.matmul(W2.T, dZ2)\n",
        "    dZ1 = dA1 * sigmoid(Z1) * (1 - sigmoid(Z1))\n",
        "    dW1 = (1./m) * np.matmul(dZ1, X.T)\n",
        "    db1 = (1./m) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "\n",
        "    W2 = W2 - learning_rate * dW2\n",
        "    b2 = b2 - learning_rate * db2\n",
        "    W1 = W1 - learning_rate * dW1\n",
        "    b1 = b1 - learning_rate * db1\n",
        "\n",
        "    if (i % 10 == 0):\n",
        "        print(\"Epoch\", i, \"loss: \", loss)\n",
        "\n",
        "print(\"Final loss:\", loss)\n",
        "```\n",
        "\n",
        "Let's see how we did:\n",
        "\n",
        "```{pyodide}\n",
        "#| autorun: false\n",
        "Z1 = np.matmul(W1, X_test) + b1\n",
        "A1 = sigmoid(Z1)\n",
        "Z2 = np.matmul(W2, A1) + b2\n",
        "A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0)\n",
        "\n",
        "predictions = np.argmax(A2, axis=0)\n",
        "labels = np.argmax(Y_test, axis=0)\n",
        "```\n",
        "\n",
        "#### Model performance\n",
        "\n",
        "```{pyodide}\n",
        "#| autorun: false\n",
        "print(confusion_matrix(predictions, labels))\n",
        "print(classification_report(predictions, labels))\n",
        "```\n",
        "\n",
        "We are at 84% accuray across all digits, which could be of course better. We may now plot image and the corresponding prediction.\n",
        "\n",
        "## Test the model\n",
        "\n",
        "```{pyodide}\n",
        "#| autorun: false\n",
        "i=2003\n",
        "plt.imshow(X_test[:,i].reshape(28,28), cmap='gray')\n",
        "predictions[i]\n",
        "```"
      ],
      "id": "9cd94e94"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/fci/Library/Jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}