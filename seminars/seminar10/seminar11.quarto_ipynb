{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Neural Networks\n",
        "jupyter: python3\n",
        "format:\n",
        "  live-html:\n",
        "    toc: true\n",
        "    toc-location: right\n",
        "pyodide:\n",
        "  autorun: false\n",
        "  packages:\n",
        "    - matplotlib\n",
        "    - numpy\n",
        "    - scipy\n",
        "---\n",
        "\n",
        "```{pyodide}\n",
        "#| edit: false\n",
        "#| echo: false\n",
        "#| execute: true\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.integrate import odeint\n",
        "\n",
        "# Set default plotting parameters\n",
        "plt.rcParams.update({\n",
        "    'font.size': 12,\n",
        "    'lines.linewidth': 1,\n",
        "    'lines.markersize': 5,\n",
        "    'axes.labelsize': 11,\n",
        "    'xtick.labelsize': 10,\n",
        "    'ytick.labelsize': 10,\n",
        "    'xtick.top': True,\n",
        "    'xtick.direction': 'in',\n",
        "    'ytick.right': True,\n",
        "    'ytick.direction': 'in',\n",
        "})\n",
        "\n",
        "def get_size(w, h):\n",
        "    return (w/2.54, h/2.54)\n",
        "```\n",
        "\n",
        "# Introduction to Neural Networks in Physics\n",
        "\n",
        "## What are Neural Networks?\n",
        "Neural networks are computational models inspired by how our brains process information. Just like our brain consists of interconnected neurons that process and transmit signals, artificial neural networks consist of mathematical \"neurons\" that process numerical information. They're particularly powerful for:\n",
        "\n",
        "- Recognizing patterns in data\n",
        "- Making predictions\n",
        "- Classifying information\n",
        "- Solving complex problems\n",
        "\n",
        "## Why Neural Networks in Physics?\n",
        "\n",
        "In physics, we often encounter problems where:\n",
        "\n",
        "1. Traditional mathematical models become too complex\n",
        "2. We need to analyze large amounts of experimental data\n",
        "3. We want to make predictions based on incomplete information\n",
        "\n",
        "Neural networks help us with these challenges! Some real-world applications include:\n",
        "\n",
        "- Particle physics: Identifying particles in detector data\n",
        "- Astronomy: Classifying galaxies\n",
        "- Materials science: Predicting material properties\n",
        "- Quantum mechanics: Solving many-body problems\n",
        "- Biological physics: Modeling neural activity\n",
        "- Active matter: Predicting collective behavior\n",
        "\n",
        "\n",
        "## A Single Neuron: Building Our First AI Unit\n",
        "\n",
        "### The Big Picture\n",
        "\n",
        "Before diving into the details, let's understand what we're trying to build. Imagine you're creating a smart device that can recognize handwritten numbers. The most basic version of this device would be a single artificial neuron - think of it as an electronic version of a brain cell that can make simple yes/no decisions.\n",
        "\n",
        "### From Biology to Mathematics\n",
        "Just like a biological neuron receives signals from other neurons, our artificial neuron processes numerical inputs through mathematical operations to produce a single output value.\n",
        "\n",
        "The neuron performs three distinct steps:\n",
        "\n",
        "1. **Input Weighting**\n",
        "Each input value gets multiplied by a weight parameter:\n",
        "\n",
        "\\begin{eqnarray}\n",
        "x_{1}\\rightarrow x_{1} w_{1}\\\\\n",
        "x_{2}\\rightarrow x_{2} w_{2}\n",
        "\\end{eqnarray}\n",
        "\n",
        "2. **Bias Addition**\n",
        "A bias value $b$ is added to the weighted sum:\n",
        "\n",
        "\\begin{equation}\n",
        "x_{1} w_{1}+ x_{2} w_{2}+b\n",
        "\\end{equation}\n",
        "\n",
        "3. **Activation Function**\n",
        "The final step applies an activation function $\\sigma()$:\n",
        "\n",
        "\\begin{equation}\n",
        "y=\\sigma( x_{1} w_{1}+ x_{2} w_{2}+b)\n",
        "\\end{equation}\n",
        "\n",
        "For mathematical convenience, we can write this more compactly using vector notation:\n",
        "\n",
        "\\begin{equation*}\n",
        "\\hat{y} = \\sigma(w^{\\rm T} x + b)\n",
        "\\end{equation*}\n",
        "\n",
        "The sigmoid function has the following mathematical form:\n",
        "\\begin{equation*}\n",
        "\\sigma(z) = \\frac{1}{1+{\\rm e}^{-z}}\n",
        "\\end{equation*}\n",
        "\n",
        "Let's implement the sigmoid function and visualize how it transforms inputs:\n",
        "\n",
        "```{pyodide}\n",
        "def sigmoid(z):\n",
        "    return 1/(1 + np.exp(-z))\n",
        "```\n",
        "\n",
        "```{pyodide}\n",
        "x=np.linspace(-5,5,100)\n",
        "plt.figure(figsize=(5,3))\n",
        "plt.plot(x,sigmoid(x))\n",
        "plt.xlabel('input')\n",
        "plt.ylabel('output')\n",
        "plt.grid()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Now let's see how a neuron processes inputs through these operations. Given:\n",
        "\n",
        "\\begin{eqnarray}\n",
        "w=[0,1]\\\\\n",
        "b=4\n",
        "\\end{eqnarray}\n",
        "\n",
        "And input values:\n",
        "\n",
        "\\begin{eqnarray}\n",
        "x=[2,3]\n",
        "\\end{eqnarray}\n",
        "\n",
        "The computation becomes:\n",
        "\n",
        "```{pyodide}\n",
        "# Example neuron computation\n",
        "w = np.array([0, 1])\n",
        "x = np.array([2, 3])\n",
        "b = 4\n",
        "\n",
        "output = sigmoid(np.dot(w, x) + b)\n",
        "print(f\"Neuron output: {output:.3f}\")\n",
        "```\n",
        "\n",
        "For computational efficiency, these calculations can be performed on multiple inputs simultaneously using matrix operations:\n",
        "\n",
        "\\begin{equation*}\n",
        "\\hat{y} = \\sigma(w^{\\rm T} X + b)\n",
        "\\end{equation*}\n",
        "\n",
        "\n",
        "## Loss Function: Measuring Our Network's Mistakes\n",
        "\n",
        "### Why Do We Need a Loss Function?\n",
        "Just like we need a way to measure error in physics experiments, we need a way to measure how wrong our neural network's predictions are. The loss function serves this purpose - it tells us how far our predictions are from the true values.\n",
        "\n",
        "### Understanding Cross-Entropy Loss\n",
        "While we could use simpler measures like mean squared error:\n",
        "\n",
        "\\begin{equation}\n",
        "MSE(y,\\hat{y})=\\frac{1}{n}\\sum_{i=1}^{n}(y-\\hat{y})^2\n",
        "\\end{equation}\n",
        "\n",
        "We'll use a more sophisticated measure called cross-entropy loss. For a single training example, the formula is:\n",
        "\n",
        "\\begin{equation*}\n",
        "L(y,\\hat{y}) = -y\\log(\\hat{y})-(1-y)\\log(1-\\hat{y})\n",
        "\\end{equation*}\n",
        "\n",
        "**Physics Analogy:** This is similar to entropy in thermodynamics - it measures the disorder or uncertainty in our predictions.\n",
        "\n",
        "When dealing with multiple training examples ($m$ of them), we average the loss:\n",
        "\n",
        "\\begin{equation*}\n",
        "L(Y,\\hat{Y}) = -\\frac{1}{m}\\sum_{i = 0}^{m}y^{(i)}\\log(\\hat{y}^{(i)})-(1-y^{(i)})\\log(1-\\hat{y}^{(i)})\n",
        "\\end{equation*}\n",
        "\n",
        "Let's implement this in code:\n",
        "\n",
        "```{pyodide}\n",
        "def compute_loss(Y, Y_hat):\n",
        "    m = Y.shape[1]\n",
        "    L = -(1./m)*(np.sum(np.multiply(np.log(Y_hat), Y)) +\n",
        "                 np.sum(np.multiply(np.log(1 - Y_hat), (1 - Y))))\n",
        "    return L\n",
        "\n",
        "# Let's test our loss function with some example predictions\n",
        "Y_true = np.array([[1, 0, 1, 0]])  # True values\n",
        "Y_pred = np.array([[0.9, 0.1, 0.8, 0.2]])  # Network predictions\n",
        "\n",
        "loss = compute_loss(Y_true, Y_pred)\n",
        "print(f\"Loss for these predictions: {loss:.4f}\")\n",
        "```\n",
        "\n",
        "### Training the Network: The Big Picture\n",
        "The goal of training is to minimize this loss function. We do this by adjusting the weights and biases of our network. Let's see how this works:\n",
        "\n",
        "```{pyodide}\n",
        "# Visualize how different predictions affect loss\n",
        "def plot_loss_curve():\n",
        "    y_true = 1  # True value\n",
        "    y_pred = np.linspace(0.01, 0.99, 100)  # Range of predictions\n",
        "    loss = -y_true * np.log(y_pred) - (1-y_true) * np.log(1-y_pred)\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(y_pred, loss)\n",
        "    plt.xlabel('Prediction')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Cross-Entropy Loss for True Value = 1')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "plot_loss_curve()\n",
        "```\n",
        "\n",
        "### Backward Propagation: Finding the Path to Improvement\n",
        "\n",
        "The loss function $L$ depends on all our weights and biases:\n",
        "\n",
        "$$\n",
        "L(w_{1},w_{2},w_{3},\\ldots ,b_{1},b_{2},b_{3},\\ldots)\n",
        "$$\n",
        "\n",
        "To minimize the loss, we need to know how it changes when we adjust each weight. This is where partial derivatives come in:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_j}\n",
        "$$\n",
        "\n",
        "Breaking this down step by step, we use the chain rule:\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\partial L}{\\partial w_j} = \\frac{\\partial L}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial z}\\frac{\\partial z}{\\partial w_j}\n",
        "\\end{align*}\n",
        "\n",
        "Let's calculate each term:\n",
        "\n",
        "1. $\\partial L/\\partial\\hat{y}$:\n",
        "\\begin{align*}\n",
        "\\frac{\\partial L}{\\partial\\hat{y}} &= \\frac{\\partial}{\\partial\\hat{y}}\\left(-y\\log(\\hat{y})-(1-y)\\log(1-\\hat{y})\\right) \\\\\n",
        "&= -\\frac{y}{\\hat{y}} +\\frac{(1 - y)}{1-\\hat{y}} \\\\\n",
        "&= \\frac{\\hat{y} - y}{\\hat{y}(1-\\hat{y})}\n",
        "\\end{align*}\n",
        "\n",
        "2. $\\partial \\hat{y}/\\partial z$:\n",
        "\\begin{align*}\n",
        "\\frac{\\partial }{\\partial z}\\sigma(z) &= \\sigma(z)(1-\\sigma(z)) \\\\\n",
        "&= \\hat{y}(1-\\hat{y})\n",
        "\\end{align*}\n",
        "\n",
        "3. $\\partial z/\\partial w_j$:\n",
        "\\begin{align*}\n",
        "\\frac{\\partial }{\\partial w_j}(w^{\\rm T} x + b) = x_j\n",
        "\\end{align*}\n",
        "\n",
        "Putting it all together:\n",
        "\\begin{align*}\n",
        "\\frac{\\partial L}{\\partial w_j} = (\\hat{y} - y)x_j\n",
        "\\end{align*}\n",
        "\n",
        "For multiple training examples, in vector form:\n",
        "\\begin{align*}\n",
        "\\frac{\\partial L}{\\partial w} = \\frac{1}{m} X(\\hat{y} - y)^{\\rm T}\n",
        "\\end{align*}\n",
        "\n",
        "Similarly for the bias:\n",
        "\\begin{align*}\n",
        "\\frac{\\partial L}{\\partial b} = \\frac{1}{m}\\sum_{i=1}^{m}{(\\hat{y}^{(i)} - y^{(i)})}\n",
        "\\end{align*}\n",
        "\n",
        "### Interactive Example: Watching Gradients\n",
        "\n",
        "```{pyodide}\n",
        "# Demonstrate how gradients change with different predictions\n",
        "def compute_gradient_example():\n",
        "    x = np.array([1, 2])  # Sample input\n",
        "    y = 1  # True value\n",
        "    w = np.array([0.1, 0.2])  # Initial weights\n",
        "    y_hat = sigmoid(np.dot(w, x))\n",
        "\n",
        "    # Compute gradient\n",
        "    dw = x * (y_hat - y)\n",
        "\n",
        "    print(f\"Prediction (y_hat): {y_hat:.4f}\")\n",
        "    print(f\"Gradient (dw): {dw}\")\n",
        "\n",
        "compute_gradient_example()\n",
        "```\n",
        "\n",
        "### Key Points to Remember:\n",
        "- The loss function measures prediction errors\n",
        "- Cross-entropy loss is particularly suitable for classification problems\n",
        "- Gradients tell us how to adjust weights and biases\n",
        "- The chain rule helps us compute these gradients efficiently\n",
        "\n",
        "\n",
        "## Training the Network: Putting It All Together\n",
        "\n",
        "### Stochastic Gradient Descent (SGD)\n",
        "Now that we understand how to compute gradients, we can use them to train our network. The basic idea is simple:\n",
        "1. Calculate how wrong we are (loss)\n",
        "2. Calculate how to improve (gradients)\n",
        "3. Take a small step in the right direction\n",
        "\n",
        "This process is called Stochastic Gradient Descent (SGD). The mathematical update rule is:\n",
        "\n",
        "$$\n",
        "w\\leftarrow w-\\eta\\frac{\\partial L}{\\partial w}\n",
        "$$\n",
        "\n",
        "where $\\eta$ is the learning rate - a small number that controls how big our improvement steps are.\n",
        "\n",
        "**Physics Analogy:** This is similar to finding the minimum of a potential well. The gradient tells us which way is \"downhill\", and we take small steps in that direction.\n",
        "\n",
        "### Building Our First Complete Network\n",
        "\n",
        "Let's implement a complete training loop. We'll use:\n",
        "- Learning rate $\\eta = 1$\n",
        "- 200 training epochs (complete passes through the data)\n",
        "\n",
        "```{pyodide}\n",
        "learning_rate = 1\n",
        "\n",
        "X = np.array(X_train)\n",
        "Y = np.array(y_train)\n",
        "\n",
        "n_x = X.shape[0]  # number of input features\n",
        "m = X.shape[1]    # number of training examples\n",
        "\n",
        "# Initialize weights and bias\n",
        "W = np.random.randn(n_x, 1) * 0.01  # small random weights\n",
        "b = np.zeros((1, 1))\n",
        "\n",
        "# Training loop\n",
        "for i in range(200):\n",
        "    # Forward propagation\n",
        "    Z = np.matmul(W.T, X) + b\n",
        "    A = sigmoid(Z)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = compute_loss(Y, A)\n",
        "\n",
        "    # Backward propagation\n",
        "    dW = (1/m)*np.matmul(X, (A-Y).T)\n",
        "    db = (1/m)*np.sum(A-Y, axis=1, keepdims=True)\n",
        "\n",
        "    # Update parameters\n",
        "    W = W - learning_rate * dW\n",
        "    b = b - learning_rate * db\n",
        "\n",
        "    # Print progress every 10 epochs\n",
        "    if i % 10 == 0:\n",
        "        print(f\"Epoch {i:3d}, Loss: {loss:.6f}\")\n",
        "\n",
        "print(f\"Final loss: {loss:.6f}\")\n",
        "```\n",
        "\n",
        "### Evaluating Our Network: The Confusion Matrix\n",
        "\n",
        "To understand how well our network performs, we use a confusion matrix. This shows:\n",
        "- True Positives (TP): Correctly predicted positive cases\n",
        "- False Positives (FP): Incorrectly predicted positive cases\n",
        "- True Negatives (TN): Correctly predicted negative cases\n",
        "- False Negatives (FN): Incorrectly predicted negative cases\n",
        "\n",
        "![confusion_matrix](confusion_matrix.png)\n",
        "\n",
        "Let's evaluate our model on the test data:\n",
        "\n",
        "```{pyodide}\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Generate predictions on test data\n",
        "Z = np.matmul(W.T,X_test) + b\n",
        "A = sigmoid(Z)\n",
        "\n",
        "# Convert probabilities to binary predictions\n",
        "predictions = (A > 0.5)[0,:]\n",
        "labels = (y_test == 1)[0,:]\n",
        "\n",
        "# Compute and display confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(predictions, labels))\n",
        "\n",
        "# Display detailed classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(predictions, labels))\n",
        "```\n",
        "\n",
        "### Testing Individual Predictions\n",
        "\n",
        "Let's visualize how our network performs on a single test image:\n",
        "\n",
        "```{pyodide}\n",
        "def test_single_image(index):\n",
        "    # Get prediction for single image\n",
        "    prediction = bool(sigmoid(np.matmul(W.T, np.array(X_test)[:,index])+b) > 0.5)\n",
        "\n",
        "    # Display image and prediction\n",
        "    plt.figure(figsize=(4,4))\n",
        "    plt.imshow(np.array(X_test)[:,index].reshape(28,28), cmap='gray')\n",
        "    plt.title(f'Prediction: {prediction}')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    return prediction\n",
        "\n",
        "# Test an example image\n",
        "test_index = 200\n",
        "result = test_single_image(test_index)\n",
        "print(f\"Predicted class: {result}\")\n",
        "```\n",
        "\n",
        "### Understanding the Results:\n",
        "- The confusion matrix shows us where our model makes mistakes\n",
        "- The classification report gives us metrics like:\n",
        "  - Precision: How many of our positive predictions were correct\n",
        "  - Recall: How many actual positive cases did we catch\n",
        "  - F1-score: A balanced measure of precision and recall\n",
        "\n",
        "### Key Points to Remember:\n",
        "1. Training is an iterative process of:\n",
        "   - Forward propagation\n",
        "   - Loss calculation\n",
        "   - Backward propagation\n",
        "   - Parameter updates\n",
        "2. The learning rate controls how quickly we update our parameters\n",
        "3. We evaluate performance using confusion matrices and classification metrics"
      ],
      "id": "29551985"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/fci/Library/Jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}