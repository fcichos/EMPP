<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.34">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Neural Networks ‚Äì Einf√ºhrung in die Modellierung Physikalischer Prozesse</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../seminars/seminar01/2_reinforcement_learning.html" rel="prev">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-c8ad9e5dbd60b7b70b38521ab19b7da4.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-9344fb9851b4da3cab95007ef9d770dc.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="../../site_libs/quarto-contrib/live-runtime/live-runtime.js" type="module"></script>
<link href="../../site_libs/quarto-contrib/live-runtime/live-runtime.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script type="module" src="../../site_libs/quarto-ojs/quarto-ojs-runtime.js"></script>
<link href="../../site_libs/quarto-ojs/quarto-ojs.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-sidebar floating nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Einf√ºhrung in die Modellierung Physikalischer Prozesse</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">üõ†Ô∏è Hands-On Seminars</li><li class="breadcrumb-item"><a href="../../seminars/seminar01/2_reinforcement_learning.html">Advanced Topics</a></li><li class="breadcrumb-item"><a href="../../seminars/seminar01/1_deep_learning.html">Advanced: Deep Learning</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">üìã Course Info</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course-info/intructors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Vorlesender</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course-info/resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ressourcen</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course-info/assignments.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">√úbungsaufgaben</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course-info/exam.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Pr√ºfungen</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">üöÄ Week 1: Your First Physics Code</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lectures/lecture01/01-lecture01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup: Jupyter Notebooks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lectures/lecture01/01-plotting-basics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quick Win: Plotting Your First Graph</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">üéØ Week 2: Python Building Blocks</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lectures/lecture01/02-lecture01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Variables &amp; Numbers (What You Just Used)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lectures/lecture02/3_datatypes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Datatypes for Physics</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">üåü Week 3: Modeling Motion</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lectures/lecture02/01-lecture02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Functions: Reusable Physics Equations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lectures/lecture02/4_brownian_motion_simple.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Application: Brownian Motion (Simple)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">‚ö° Week 4: Classical Mechanics 1</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lectures/lecture02/4_brownian_motion_simple.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Application: Brownian Motion (Simple)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lectures/lecture03/1_numpy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Arrays with Numpy</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">‚ö° Week 5: Classical Mechanics 2</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lectures/lecture07/1_differentiation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Numerical Differentiation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lectures/lecture08/3_solving_ODEs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Tool: Solving ODEs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lectures/lecture08/2_integration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Tool: Numerical Integration</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">üõ†Ô∏è Hands-On Seminars</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Advanced Topics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth2 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../seminars/seminar01/2_reinforcement_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced: Reinforcement Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../seminars/seminar01/1_deep_learning.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Advanced: Deep Learning</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#overview-building-intelligence-from-scratch" id="toc-overview-building-intelligence-from-scratch" class="nav-link active" data-scroll-target="#overview-building-intelligence-from-scratch">Overview: Building Intelligence from Scratch üß†</a></li>
  <li><a href="#the-mnist-data-set" id="toc-the-mnist-data-set" class="nav-link" data-scroll-target="#the-mnist-data-set">The MNIST Data Set üî¢</a>
  <ul class="collapse">
  <li><a href="#load-the-data" id="toc-load-the-data" class="nav-link" data-scroll-target="#load-the-data">Load the data</a></li>
  <li><a href="#normalize-the-data" id="toc-normalize-the-data" class="nav-link" data-scroll-target="#normalize-the-data">Normalize the data</a></li>
  <li><a href="#preparing-training-and-testing-data" id="toc-preparing-training-and-testing-data" class="nav-link" data-scroll-target="#preparing-training-and-testing-data">Preparing training and testing data</a></li>
  </ul></li>
  <li><a href="#a-single-neuron-the-fundamental-building-block" id="toc-a-single-neuron-the-fundamental-building-block" class="nav-link" data-scroll-target="#a-single-neuron-the-fundamental-building-block">A Single Neuron: The Fundamental Building Block ‚öõÔ∏è</a>
  <ul class="collapse">
  <li><a href="#forward-propagation-from-input-to-output" id="toc-forward-propagation-from-input-to-output" class="nav-link" data-scroll-target="#forward-propagation-from-input-to-output">Forward Propagation: From Input to Output</a></li>
  <li><a href="#loss-function-quantifying-prediction-error" id="toc-loss-function-quantifying-prediction-error" class="nav-link" data-scroll-target="#loss-function-quantifying-prediction-error">Loss Function: Quantifying Prediction Error üìâ</a></li>
  </ul></li>
  <li><a href="#training-the-network-learning-from-data" id="toc-training-the-network-learning-from-data" class="nav-link" data-scroll-target="#training-the-network-learning-from-data">Training the Network: Learning from Data üéì</a>
  <ul class="collapse">
  <li><a href="#backward-propagation-the-chain-rule-in-action" id="toc-backward-propagation-the-chain-rule-in-action" class="nav-link" data-scroll-target="#backward-propagation-the-chain-rule-in-action">Backward Propagation: The Chain Rule in Action üîó</a></li>
  <li><a href="#gradient-descent-following-the-downhill-path" id="toc-gradient-descent-following-the-downhill-path" class="nav-link" data-scroll-target="#gradient-descent-following-the-downhill-path">Gradient Descent: Following the Downhill Path ‚õ∞Ô∏è</a></li>
  <li><a href="#build-and-train-putting-it-all-together" id="toc-build-and-train-putting-it-all-together" class="nav-link" data-scroll-target="#build-and-train-putting-it-all-together">Build and Train: Putting It All Together üî®</a></li>
  <li><a href="#testing-our-model-individual-predictions" id="toc-testing-our-model-individual-predictions" class="nav-link" data-scroll-target="#testing-our-model-individual-predictions">Testing Our Model: Individual Predictions üî¨</a></li>
  </ul></li>
  <li><a href="#network-with-hidden-layers-increasing-representational-power" id="toc-network-with-hidden-layers-increasing-representational-power" class="nav-link" data-scroll-target="#network-with-hidden-layers-increasing-representational-power">Network with Hidden Layers: Increasing Representational Power üß©</a></li>
  <li><a href="#multiclass-network-recognizing-all-digits" id="toc-multiclass-network-recognizing-all-digits" class="nav-link" data-scroll-target="#multiclass-network-recognizing-all-digits">Multiclass Network: Recognizing All Digits üî¢</a>
  <ul class="collapse">
  <li><a href="#changes-to-the-model-architecture" id="toc-changes-to-the-model-architecture" class="nav-link" data-scroll-target="#changes-to-the-model-architecture">Changes to the Model Architecture üîß</a></li>
  <li><a href="#build-and-train-the-complete-network" id="toc-build-and-train-the-complete-network" class="nav-link" data-scroll-target="#build-and-train-the-complete-network">Build and Train: The Complete Network üöÄ</a></li>
  </ul></li>
  <li><a href="#testing-the-model-interactive-exploration" id="toc-testing-the-model-interactive-exploration" class="nav-link" data-scroll-target="#testing-the-model-interactive-exploration">Testing the Model: Interactive Exploration üîç</a></li>
  <li><a href="#reflection-and-next-steps" id="toc-reflection-and-next-steps" class="nav-link" data-scroll-target="#reflection-and-next-steps">Reflection and Next Steps üéØ</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">üõ†Ô∏è Hands-On Seminars</li><li class="breadcrumb-item"><a href="../../seminars/seminar01/2_reinforcement_learning.html">Advanced Topics</a></li><li class="breadcrumb-item"><a href="../../seminars/seminar01/1_deep_learning.html">Advanced: Deep Learning</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Neural Networks</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Neural networks have become one of the most powerful tools in modern computational science, with applications ranging from particle physics to materials science. The term <strong>deep neural networks</strong> refers to architectures with many layers of interconnected neurons, where ‚Äúdeep‚Äù simply indicates the number of these layers. But what makes these systems particularly fascinating for physicists is their ability to learn complex patterns from data‚Äîmuch like how we extract physical laws from experimental observations.</p>
<p>In this lecture, we‚Äôll build neural networks from scratch, gaining deep insight into their inner workings rather than treating them as black boxes. We won‚Äôt rely on high-level libraries like TensorFlow or Keras just yet; instead, we‚Äôll construct everything using NumPy, giving you the same kind of fundamental understanding you‚Äôd gain from deriving equations of motion from first principles rather than just applying them. This hands-on approach will help you understand what‚Äôs really happening under the hood when you train a neural network.</p>
<p>Think of neural networks as complex function approximators. Just as you might fit a polynomial to experimental data, neural networks can approximate incredibly complicated functions‚Äîfunctions that might describe the relationship between input images and their classifications, or between system parameters and physical outcomes. The difference is that neural networks can handle much higher-dimensional spaces and more complex patterns than traditional fitting methods.</p>
<p>Our goal today is to build a neural network that can recognize handwritten digits‚Äîa classic problem that will teach us all the essential concepts. We‚Äôll start simple with a single neuron (essentially logistic regression), then add hidden layers to increase the network‚Äôs representational power. By the end, you‚Äôll have a working digit classifier and a solid foundation for understanding deep learning. This notebook has been largely developed by <strong>Martin Fr√§nzl</strong> and adapted for physics students.</p>
<div id="be3feea5" class="cell" data-tags="[]" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>config InlineBackend.figure_format <span class="op">=</span> <span class="st">'retina'</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>plt.rcParams.update({<span class="st">'font.size'</span>: <span class="dv">18</span>,</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>                     <span class="st">'axes.titlesize'</span>: <span class="dv">20</span>,</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>                     <span class="st">'axes.labelsize'</span>: <span class="dv">20</span>,</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>                     <span class="st">'axes.labelpad'</span>: <span class="dv">1</span>,</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>                     <span class="st">'lines.linewidth'</span>: <span class="dv">2</span>,</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>                     <span class="st">'lines.markersize'</span>: <span class="dv">10</span>,</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>                     <span class="st">'xtick.labelsize'</span> : <span class="dv">18</span>,</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>                     <span class="st">'ytick.labelsize'</span> : <span class="dv">18</span>,</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>                     <span class="st">'xtick.top'</span> : <span class="va">True</span>,</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>                     <span class="st">'xtick.direction'</span> : <span class="st">'in'</span>,</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>                     <span class="st">'ytick.right'</span> : <span class="va">True</span>,</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>                     <span class="st">'ytick.direction'</span> : <span class="st">'in'</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>                    })</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="overview-building-intelligence-from-scratch" class="level2">
<h2 class="anchored" data-anchor-id="overview-building-intelligence-from-scratch">Overview: Building Intelligence from Scratch üß†</h2>
<p>We‚Äôre going to build a neural network from scratch using Python and NumPy, without relying on high-level libraries like Keras or TensorFlow (those will come in Part 2). Our task is to recognize handwritten digits using the famous MNIST dataset‚Äîa benchmark problem in machine learning that has driven much of the field‚Äôs development.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="MNIST.png" class="img-fluid figure-img"></p>
<figcaption>MNIS</figcaption>
</figure>
</div>
<p>Our learning journey proceeds in three stages, each building on the previous one. We‚Äôll start with the simplest possible ‚Äúnetwork‚Äù: a single neuron that learns to recognize just the digit 0. This is actually just logistic regression in disguise, but it will teach us the fundamental concepts of forward propagation, loss functions, and gradient descent. Think of this as the ‚Äúharmonic oscillator‚Äù of neural networks‚Äîthe simplest non-trivial system that captures the essential physics.</p>
<p>Next, we‚Äôll add a hidden layer to our network, still focusing on recognizing zeros. This is where things get interesting: the hidden layer allows the network to learn intermediate representations, much like how eigenstates provide a natural basis for describing quantum systems. The network can now discover its own ‚Äúfeatures‚Äù rather than working directly with raw pixel values.</p>
<p>Finally, we‚Äôll extend our network to recognize all digits from 0 through 9, creating a true multiclass classifier. This requires changing our output layer and loss function, but the core principles remain the same. By the end, we‚Äôll have a network that achieves around 84-92% accuracy‚Äînot state-of-the-art by modern standards, but impressive for a network we built entirely from scratch!</p>
</section>
<section id="the-mnist-data-set" class="level2">
<h2 class="anchored" data-anchor-id="the-mnist-data-set">The MNIST Data Set üî¢</h2>
<p>The MNIST (Modified National Institute of Standards and Technology) dataset is to machine learning what the hydrogen atom is to quantum mechanics‚Äîa fundamental system that everyone studies first. It contains 70,000 images of handwritten digits, each 28 √ó 28 pixels in grayscale with pixel values ranging from 0 (black) to 255 (white). This gives us 784 input features per image (28 √ó 28 = 784), each representing the intensity of one pixel.</p>
<p>From a physicist‚Äôs perspective, each image is a point in a 784-dimensional space. Our task is to partition this space into 10 regions, one for each digit. This is a classification problem, and neural networks provide a flexible way to learn these complex decision boundaries. We could download and preprocess the data ourselves, but the <code>sklearn</code> module has already done the heavy lifting for us:</p>
<section id="load-the-data" class="level3">
<h3 class="anchored" data-anchor-id="load-the-data">Load the data</h3>
<div id="6f885a8f" class="cell" data-tags="[]" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> fetch_openml</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> fetch_openml(<span class="st">'mnist_784'</span>, version<span class="op">=</span><span class="dv">1</span>, return_X_y<span class="op">=</span><span class="va">True</span>,as_frame<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The images are now contained in the array <code>X</code>, while the labels (so which number it is) are contained in <code>y</code>. Let‚Äôs have a look at a random image and label.</p>
<div id="e0fcf8bf" class="cell" data-tags="[]" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> <span class="dv">33419</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>plt.imshow(np.array(X)[i].reshape(<span class="dv">28</span>, <span class="dv">28</span>), cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'label: '</span>,y[i])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="1_deep_learning_files/figure-html/cell-4-output-1.png" width="519" height="420" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>label:  8</code></pre>
</div>
</div>
</section>
<section id="normalize-the-data" class="level3">
<h3 class="anchored" data-anchor-id="normalize-the-data">Normalize the data</h3>
<p>Normalization is a crucial preprocessing step in machine learning. By dividing by 255, we scale all pixel values to the interval [0, 1]. This normalization serves several purposes: it prevents numerical overflow issues during computation, helps the gradient descent algorithm converge faster (since all features are on the same scale), and makes the learning rate easier to tune. Think of it as choosing appropriate units in physics‚Äîworking in natural units (‚Ñè = c = 1) often simplifies calculations, and similarly, normalized data simplifies neural network training.</p>
<div id="f933c83b" class="cell" data-tags="[]" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X<span class="op">/</span><span class="dv">255</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="preparing-training-and-testing-data" class="level3">
<h3 class="anchored" data-anchor-id="preparing-training-and-testing-data">Preparing training and testing data</h3>
<p>For our first network, we‚Äôre building a binary classifier‚Äîa system that answers a yes/no question: ‚ÄúIs this digit a zero?‚Äù The default MNIST labels indicate which digit each image represents (‚Äò0‚Äô, ‚Äò1‚Äô, ‚Äò2‚Äô, etc.), but we need to convert this to binary labels: 1 if the image shows a zero, and 0 otherwise. This is analogous to simplifying a complex quantum system by projecting it onto a two-level system‚Äîwe‚Äôre reducing a 10-class problem to a binary one.</p>
<div id="f81c8882" class="cell" data-tags="[]" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>y_new <span class="op">=</span> np.zeros(y.shape)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>y_new[np.where(y <span class="op">==</span> <span class="st">'0'</span>)[<span class="dv">0</span>]] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> y_new</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We now split the data into training and testing sets. This separation is crucial: we train on one set and evaluate on a completely different set to test our network‚Äôs ability to generalize. This is similar to how you might measure a physical quantity multiple times‚Äîsome measurements are used to calibrate your instrument (training), while others test its accuracy (testing). The MNIST images are pre-arranged so that the first 60,000 form the training set and the last 10,000 serve as the test set. We‚Äôll also transpose the data so that each example becomes a column rather than a row, which makes our matrix operations more intuitive:</p>
<div id="33136a33" class="cell" data-tags="[]" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="dv">60000</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>m_test <span class="op">=</span> X.shape[<span class="dv">0</span>] <span class="op">-</span> m</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>X_train, X_test <span class="op">=</span> X[:m].T, X[m:].T</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>y_train, y_test <span class="op">=</span> y[:m].reshape(<span class="dv">1</span>,m), y[m:].reshape(<span class="dv">1</span>, m_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finally, we shuffle the training set. Shuffling ensures that our network doesn‚Äôt learn any spurious patterns related to the order of examples. If all the zeros came first, followed by all the ones, the network might learn temporal patterns that don‚Äôt actually exist in the data:</p>
<div id="69a9f417" class="cell" data-tags="[]" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">1</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>shuffle_index <span class="op">=</span> np.random.permutation(m)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>X_train, y_train <span class="op">=</span> X_train[:,shuffle_index], y_train[:,shuffle_index]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let‚Äôs verify our preprocessing by examining a random image and its label:</p>
<div id="8704686f" class="cell" data-tags="[]" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> <span class="dv">39</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>plt.imshow(X_train[:,i].reshape(<span class="dv">28</span>, <span class="dv">28</span>), cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(y_train[:,i])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="1_deep_learning_files/figure-html/cell-9-output-1.png" width="511" height="426" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[0.]</code></pre>
</div>
</div>
<p>Try changing the index <code>i</code> to explore different images. When you find a zero, verify that the corresponding label is 1, and when you find any other digit, check that the label is 0. This sanity check ensures our binary labeling worked correctly.</p>
</section>
</section>
<section id="a-single-neuron-the-fundamental-building-block" class="level2">
<h2 class="anchored" data-anchor-id="a-single-neuron-the-fundamental-building-block">A Single Neuron: The Fundamental Building Block ‚öõÔ∏è</h2>
<p>The basic unit of a neural network is an artificial neuron, inspired by biological neurons but mathematically much simpler. A neuron takes multiple inputs, performs a weighted sum (plus a bias), and passes the result through an activation function to produce a single output. The diagram below shows a simple neuron with two inputs:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/neuron.png" class="img-fluid figure-img"></p>
<figcaption>image</figcaption>
</figure>
</div>
<p>While the biological inspiration is interesting, it‚Äôs more useful for us as physicists to think of a neuron as a parametrized nonlinear function. The weights and bias are parameters we‚Äôll adjust during training, and the activation function introduces nonlinearity‚Äîcrucial because linear combinations of linear functions are still linear, but we need nonlinearity to approximate complex decision boundaries.</p>
<section id="forward-propagation-from-input-to-output" class="level3">
<h3 class="anchored" data-anchor-id="forward-propagation-from-input-to-output">Forward Propagation: From Input to Output</h3>
<p>The term ‚Äúforward propagation‚Äù describes how information flows through the network from inputs to outputs. A neuron performs three sequential operations on its inputs.</p>
<p><strong>First</strong>, each input value <span class="math inline">\(x_i\)</span> is multiplied by its corresponding weight <span class="math inline">\(w_i\)</span>: <span class="math display">\[\begin{eqnarray}
x_{1}\rightarrow x_{1} w_{1}\\
x_{2}\rightarrow x_{2} w_{2}
\end{eqnarray}\]</span></p>
<p>These weights determine how much influence each input has on the output. In physics terms, think of the weights as coupling constants that determine the strength of interaction between input features and the output.</p>
<p><strong>Second</strong>, all weighted inputs are summed and a bias term <span class="math inline">\(b\)</span> is added: <span class="math display">\[\begin{equation}
z = x_{1} w_{1}+ x_{2} w_{2}+b
\end{equation}\]</span></p>
<p>The bias allows the neuron to shift its activation threshold, similar to how an external potential can shift energy levels in quantum mechanics. It‚Äôs crucial for fitting data that doesn‚Äôt pass through the origin.</p>
<p><strong>Third</strong>, this weighted sum <span class="math inline">\(z\)</span> is passed through an activation function <span class="math inline">\(\sigma(\cdot)\)</span> to produce the final output: <span class="math display">\[\begin{equation}
y=\sigma(z) = \sigma( x_{1} w_{1}+ x_{2} w_{2}+b)
\end{equation}\]</span></p>
<p>The activation function serves a critical purpose: it introduces nonlinearity into the network. Without it, stacking multiple layers would be pointless‚Äîany composition of linear transformations is itself linear. The activation function allows neural networks to approximate arbitrary nonlinear functions, much like how Fourier series can approximate arbitrary periodic functions through combinations of sines and cosines.</p>
<p>A commonly used activation function is the <strong>sigmoid function</strong>, which smoothly maps any real number to the interval (0, 1).</p>
<p>For a single training example <span class="math inline">\(x\)</span> (which is itself a vector of 784 pixel values), we can write this more compactly using vector notation:</p>
<p><span class="math display">\[\begin{equation*}
\hat{y} = \sigma(w^{\rm T} x + b)\ .
\end{equation*}\]</span></p>
<p>Here <span class="math inline">\(w^{\rm T} x\)</span> is the dot product of the weight vector with the input vector, <span class="math inline">\(b\)</span> is a scalar bias, and <span class="math inline">\(\sigma\)</span> is the sigmoid activation function: <span class="math display">\[\begin{equation*}
\sigma(z) = \frac{1}{1+{\rm e}^{-z}}\ .
\end{equation*}\]</span></p>
<p>The sigmoid function has a beautiful S-shaped curve. For large positive inputs it approaches 1, for large negative inputs it approaches 0, and it transitions smoothly in between. This makes it perfect for binary classification‚Äîwe can interpret the output as a probability. Interestingly, the sigmoid function is related to the Fermi-Dirac distribution in statistical mechanics (without the temperature parameter), which describes the occupation probability of quantum states.</p>
<p>Let‚Äôs define and visualize the sigmoid function:</p>
<div id="e23b9380" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(z):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>z))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="b27cb9a7" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>x<span class="op">=</span>np.linspace(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">100</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">5</span>,<span class="dv">3</span>))</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>plt.plot(x,sigmoid(x))</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'input'</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'output'</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="1_deep_learning_files/figure-html/cell-11-output-1.png" width="464" height="297" class="figure-img"></p>
<figcaption>Sigmoid function</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>Example calculation:</strong> Consider a simple two-input neuron with weights <span class="math inline">\(w=[0,1]\)</span> and bias <span class="math inline">\(b=4\)</span>. If we provide the input <span class="math inline">\(x=[2,3]\)</span>, the neuron computes:</p>
<p><span class="math display">\[\begin{equation}
z = w\cdot x+b = 0 \cdot 2 + 1 \cdot 3 + 4 = 7
\end{equation}\]</span></p>
<p>Passing this through the sigmoid gives: <span class="math display">\[\begin{equation}
y=\sigma(7) = \frac{1}{1+e^{-7}} \approx 0.999
\end{equation}\]</span></p>
<p>The output is very close to 1, indicating high confidence. This entire procedure‚Äîpropagating input values forward through the network to obtain an output‚Äîis called <strong>feedforward</strong> or <strong>forward propagation</strong>.</p>
<p>Our immediate goal is to scale this up: we‚Äôll create a network with a single neuron that has 784 inputs (one for each pixel in our 28 √ó 28 images) and produces a single sigmoid output indicating whether the image is a zero.</p>
<p><strong>Vectorization for efficiency:</strong> The real power of NumPy comes from vectorization‚Äîprocessing multiple examples simultaneously. Instead of computing predictions one image at a time, we‚Äôll stack all training examples side-by-side in a matrix <span class="math inline">\(X\)</span>, where each column is one example. The forward pass then becomes:</p>
<p><span class="math display">\[\begin{equation*}
\hat{Y} = \sigma(w^{\rm T} X + b)\ .
\end{equation*}\]</span></p>
<p>Note that <span class="math inline">\(\hat{Y}\)</span> is now a vector (one prediction per training example), not a scalar. This vectorized computation is orders of magnitude faster than looping over examples individually‚ÄîNumPy uses optimized linear algebra libraries (BLAS/LAPACK) under the hood, similar to how you‚Äôd use optimized FFT routines rather than implementing Fourier transforms naively.</p>
<p>In our code, we‚Äôll compute this in two stages: first <code>Z = np.matmul(W.T, X) + b</code> (the weighted sum), then <code>A = sigmoid(Z)</code> (the activation). We use <code>A</code> for ‚ÄúActivation‚Äù to match standard deep learning notation. This two-stage breakdown might seem unnecessary now, but it will make our backward propagation code clearer‚Äîeach stage needs its own gradient computation.</p>
</section>
<section id="loss-function-quantifying-prediction-error" class="level3">
<h3 class="anchored" data-anchor-id="loss-function-quantifying-prediction-error">Loss Function: Quantifying Prediction Error üìâ</h3>
<p>Now that we can make predictions, we need to quantify how wrong those predictions are. This quantification is called the <strong>loss</strong> (or cost) function. It‚Äôs analogous to defining an energy functional in variational methods‚Äîwe need a scalar quantity to minimize.</p>
<p>You might think to use <strong>mean squared error</strong> (MSE) from your curve-fitting experience:</p>
<p><span class="math display">\[\begin{equation}
MSE(y,\hat{y})=\frac{1}{n}\sum_{i=1}^{n}(y-\hat{y})^2
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\hat{y}\)</span> represents our predictions and <span class="math inline">\(y\)</span> represents the ground truth (actual labels from the training set). MSE works, but for classification problems, there‚Äôs a more principled choice.</p>
<p>We‚Äôll use <strong>binary cross-entropy loss</strong>, which comes from information theory. For a single training example:</p>
<p><span class="math display">\[\begin{equation*}
L(y,\hat{y}) = -y\log(\hat{y})-(1-y)\log(1-\hat{y})\ .
\end{equation*}\]</span></p>
<p>This formula might look strange at first, but it has a beautiful interpretation. When <span class="math inline">\(y=1\)</span> (the image is a zero), only the first term matters, and we‚Äôre penalized more heavily the further <span class="math inline">\(\hat{y}\)</span> is from 1. When <span class="math inline">\(y=0\)</span> (not a zero), only the second term matters, penalizing predictions far from 0. The logarithm ensures that the penalty grows rapidly as predictions become more confident but wrong‚Äîbeing confidently wrong is much worse than being tentatively wrong.</p>
<p>This loss function derives from maximum likelihood estimation and is related to the Kullback-Leibler divergence between probability distributions. It‚Äôs the natural choice for binary classification with sigmoid outputs. For physicists, you might recognize similar logarithmic terms in the partition function of statistical mechanics.</p>
<p>Averaging over a training set of <span class="math inline">\(m\)</span> examples:</p>
<p><span class="math display">\[\begin{equation*}
L(Y,\hat{Y}) = -\frac{1}{m}\sum_{i = 0}^{m}y^{(i)}\log(\hat{y}^{(i)})-(1-y^{(i)})\log(1-\hat{y}^{(i)})\ .
\end{equation*}\]</span></p>
<p>In Python code, this looks like</p>
<div id="b360ea37" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_loss(Y, Y_hat):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> Y.shape[<span class="dv">1</span>]</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    L <span class="op">=</span> <span class="op">-</span>(<span class="fl">1.</span><span class="op">/</span>m)<span class="op">*</span>(np.<span class="bu">sum</span>(np.multiply(np.log(Y_hat), Y)) <span class="op">+</span> np.<span class="bu">sum</span>(np.multiply(np.log(<span class="dv">1</span> <span class="op">-</span> Y_hat), (<span class="dv">1</span> <span class="op">-</span> Y))))</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> L</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="training-the-network-learning-from-data" class="level2">
<h2 class="anchored" data-anchor-id="training-the-network-learning-from-data">Training the Network: Learning from Data üéì</h2>
<p>Training a neural network means adjusting its parameters (weights and biases) to minimize the loss function. This is fundamentally an optimization problem, not unlike minimizing the action in Lagrangian mechanics or finding ground state energies in quantum systems. The key difference is that our loss function lives in a very high-dimensional space‚Äîwith 784 inputs, we have 784 weights plus 1 bias, giving us a 785-dimensional parameter space to navigate.</p>
<p>The optimization technique we‚Äôll use is called <strong>gradient descent</strong>‚Äîwe follow the downhill direction of the loss function, taking small steps toward the minimum. But how do we find this direction? We need gradients, which means we need derivatives of the loss with respect to all parameters. This is where <strong>backpropagation</strong> comes in.</p>
<section id="backward-propagation-the-chain-rule-in-action" class="level3">
<h3 class="anchored" data-anchor-id="backward-propagation-the-chain-rule-in-action">Backward Propagation: The Chain Rule in Action üîó</h3>
<p>The output of our network depends entirely on the inputs (which are fixed for a given training example) and on the parameters we choose: the weights <span class="math inline">\(w\)</span> and biases <span class="math inline">\(b\)</span>. We can therefore view the loss as a function of these parameters:</p>
<p><span class="math display">\[
L(w_{1},w_{2},w_{3},\ldots ,b_{1},b_{2},b_{3},\ldots)
\]</span></p>
<p>To train the network through gradient descent, we need to know how the loss changes when we adjust each weight. This is captured by the partial derivative:</p>
<p><span class="math display">\[
\frac{\partial L}{\partial w_j}
\]</span></p>
<p>This derivative tells us the direction and magnitude of steepest increase in the loss. By moving in the opposite direction (negative gradient), we can reduce the loss. The process of computing these gradients by working backward from the loss through the network is called <strong>backpropagation</strong>‚Äîit‚Äôs just a clever application of the chain rule from calculus, applied systematically to a computational graph.</p>
<p>Backpropagation is often presented as mysterious or complicated, but it‚Äôs really just careful bookkeeping with derivatives. As physicists, you‚Äôve used the chain rule countless times‚Äîbackpropagation is the same principle, just applied to a more complex composite function.</p>
<p><strong>Deriving the gradients</strong></p>
<p>Let‚Äôs focus on a single training example to keep the math clear. We can think of computing the loss in three stages: <span class="math inline">\(w_j\rightarrow z \rightarrow \hat{y} \rightarrow L\)</span>. The formulas for these stages are: <span class="math display">\[\begin{align*}
z &amp;= w^{\rm T} x + b\ , \\
\hat{y} &amp;= \sigma(z)\ , \\
L(y,\hat{y}) &amp;= -y\log(\hat{y})-(1-y)\log(1-\hat{y})\ .
\end{align*}\]</span></p>
<p>By the chain rule from calculus, the gradient of the loss with respect to weight <span class="math inline">\(w_j\)</span> factors into three partial derivatives:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial L}{\partial w_j} = \frac{\partial L}{\partial \hat{y}}\frac{\partial \hat{y}}{\partial z}\frac{\partial z}{\partial w_j}
\end{align*}\]</span></p>
<p>This factorization is the essence of backpropagation. Each partial derivative corresponds to one ‚Äúlink‚Äù in our computational chain. Let‚Äôs compute each factor systematically. The derivatives are a bit tedious to write out, but none of them are particularly complicated‚Äîthey‚Äôre all applications of basic calculus rules you already know.</p>
<p><span class="math inline">\(\partial L/\partial\hat{y}\)</span>: <span class="math display">\[\begin{align*}
\frac{\partial L}{\partial\hat{y}} &amp;= \frac{\partial}{\partial\hat{y}}\left(-y\log(\hat{y})-(1-y)\log(1-\hat{y})\right) \\
&amp;= -y\frac{\partial}{\partial\hat{y}}\log(\hat{y})-(1-y)\frac{\partial}{\partial\hat{y}}\log(1-\hat{y}) \\
&amp;= -\frac{y}{\hat{y}} +\frac{(1 - y)}{1-\hat{y}} \\
&amp;= \frac{\hat{y} - y}{\hat{y}(1-\hat{y})}
\end{align*}\]</span></p>
<p><span class="math inline">\(\partial \hat{y}/\partial z\)</span>: <span class="math display">\[\begin{align*}
\frac{\partial }{\partial z}\sigma(z)
&amp;= \frac{\partial }{\partial z}\left(\frac{1}{1 + {\rm e}^{-z}}\right) \\
&amp;= \frac{1}{(1 + {\rm e}^{-z})^2}\frac{\partial }{\partial z}(1 + {\rm e}^{-z}) \\
&amp;= \frac{{\rm e}^{-z}}{(1 + {\rm e}^{-z})^2} \\
&amp;= \frac{1}{1 + {\rm e}^{-z}}\frac{{\rm e}^{-z}}{1 + {\rm e}^{-z}} \\
&amp;= \frac{1}{1 + {\rm e}^{-z}}\left(1 - \frac{1}{1 + {\rm e}^{-z}}\right) \\
&amp;= \sigma(z)(1-\sigma(z)) \\
&amp;= \hat{y}(1-\hat{y})
\end{align*}\]</span></p>
<p><span class="math inline">\(\partial z/\partial w_j\)</span>: <span class="math display">\[\begin{align*}
\frac{\partial }{\partial w_j}(w^{\rm T} x + b) &amp;= \frac{\partial }{\partial w_j}(w_0x_0 + \dots + w_nx_n + b) \\
&amp;= x_j
\end{align*}\]</span></p>
<p>Substituting back into the chain rule yields: <span class="math display">\[\begin{align*}
\frac{\partial L}{\partial w_j}
&amp;= \frac{\partial L}{\partial \hat{y}}\frac{\partial \hat{y}}{\partial z}\frac{\partial z}{\partial w_j} \\
&amp;= \frac{\hat{y} - y}{\hat{y}(1-\hat{y})}\hat{y}(1-\hat{y}) x_j \\
&amp;= (\hat{y} - y)x_j\ .
\end{align*}\]</span></p>
<p>which does not look that unfriendly anymore.</p>
<p><strong>Vectorizing the gradients:</strong> When we have <span class="math inline">\(m\)</span> training examples processed simultaneously, the weight gradient becomes: <span class="math display">\[\begin{align*}
\frac{\partial L}{\partial w} = \frac{1}{m} X(\hat{Y} - Y)^{\rm T}\ .
\end{align*}\]</span></p>
<p>The factor of <span class="math inline">\(1/m\)</span> comes from averaging the loss over all examples. Notice how clean this is: the gradient is simply the input matrix multiplied by the error vector <span class="math inline">\((\hat{Y} - Y)\)</span>.</p>
<p>A similar derivation for the bias gradient yields, for a single example: <span class="math display">\[\begin{align*}
\frac{\partial L}{\partial b} = (\hat{y} - y)\ .
\end{align*}\]</span></p>
<p>Vectorized over <span class="math inline">\(m\)</span> examples: <span class="math display">\[\begin{align*}
\frac{\partial L}{\partial b} = \frac{1}{m}\sum_{i=1}^{m}{(\hat{y}^{(i)} - y^{(i)})}\ .
\end{align*}\]</span></p>
<p>In our code, we‚Äôll follow the convention of labeling gradients with a <code>d</code> prefix: <code>dW</code> for <span class="math inline">\(\partial L/\partial W\)</span> and <code>db</code> for <span class="math inline">\(\partial L/\partial b\)</span>. This notation reminds us that these are derivatives. The backpropagation step then consists of just two lines: <code>dW = (1/m) * np.matmul(X, (A-Y).T)</code> and <code>db = (1/m)*np.sum(A-Y, axis=1, keepdims=True)</code>.</p>
</section>
<section id="gradient-descent-following-the-downhill-path" class="level3">
<h3 class="anchored" data-anchor-id="gradient-descent-following-the-downhill-path">Gradient Descent: Following the Downhill Path ‚õ∞Ô∏è</h3>
<p>We now have all the ingredients needed to train our neural network! The optimization algorithm we‚Äôll use is called <strong>gradient descent</strong>‚Äîconceptually simple but remarkably effective. The update rule for weights is:</p>
<p><span class="math display">\[
w\leftarrow w-\eta\frac{\partial L}{\partial w}
\]</span></p>
<p>Here <span class="math inline">\(\eta\)</span> (Greek letter eta) is the <strong>learning rate</strong>, a hyperparameter that controls the step size. Think of gradient descent as navigating down a mountain in fog: the gradient tells you which direction is downhill, and the learning rate determines how big a step you take.</p>
<p>The logic is straightforward. If <span class="math inline">\(\partial L/\partial w\)</span> is positive, the loss increases as <span class="math inline">\(w\)</span> increases, so we should decrease <span class="math inline">\(w\)</span> to reduce the loss. If <span class="math inline">\(\partial L/\partial w\)</span> is negative, the loss decreases as <span class="math inline">\(w\)</span> increases, so we should increase <span class="math inline">\(w\)</span>. The gradient descent update automatically does the right thing in both cases.</p>
<p>The identical update rule applies to the bias: <span class="math inline">\(b\leftarrow b-\eta\frac{\partial L}{\partial b}\)</span>.</p>
<p>We repeat this update process many times‚Äîeach complete pass through all training data is called an <strong>epoch</strong>. This is analogous to iterative methods you might have encountered in computational physics, like the relaxation method for solving Laplace‚Äôs equation or the power method for finding eigenvectors. We start with a random guess and iteratively improve it until convergence.</p>
<p>The term ‚Äústochastic‚Äù gradient descent (SGD) actually refers to a variant where we don‚Äôt use all training examples at once but rather small random batches. For our purposes, we‚Äôll compute gradients using the full training set each iteration, which is sometimes called batch gradient descent. This is more stable but slower for large datasets.</p>
</section>
<section id="build-and-train-putting-it-all-together" class="level3">
<h3 class="anchored" data-anchor-id="build-and-train-putting-it-all-together">Build and Train: Putting It All Together üî®</h3>
<p>Now comes the exciting moment‚Äîassembling all our components into a working neural network! Let‚Äôs review what we have: a forward propagation function (computing predictions), a loss function (measuring error), and backpropagation (computing gradients). The training loop combines these pieces iteratively to learn optimal weights.</p>
<p>Here‚Äôs our complete training procedure for a single-neuron network recognizing zeros:</p>
<p>This architecture is actually equivalent to <strong>logistic regression</strong>‚Äîthe sigmoid function is called a ‚Äúlogistic‚Äù function, hence the name. Despite its simplicity, logistic regression is quite powerful and forms the foundation for more complex networks.</p>
<div id="2d11c75e" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array(X_train)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> np.array(y_train)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>n_x <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> np.random.randn(n_x, <span class="dv">1</span>) <span class="op">*</span> <span class="fl">0.01</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.zeros((<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">200</span>):</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> np.matmul(W.T, X) <span class="op">+</span> b</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> sigmoid(Z)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> compute_loss(Y, A)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    dW <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>m)<span class="op">*</span>np.matmul(X, (A<span class="op">-</span>Y).T)</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>    db <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>m)<span class="op">*</span>np.<span class="bu">sum</span>(A<span class="op">-</span>Y, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>    W <span class="op">=</span> W <span class="op">-</span> learning_rate <span class="op">*</span> dW</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> b <span class="op">-</span> learning_rate <span class="op">*</span> db</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Epoch"</span>, i, <span class="st">" loss: "</span>, loss)</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Final loss:"</span>, loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0  loss:  0.7471125121616977
Epoch 10  loss:  0.07308269582929021
Epoch 20  loss:  0.06131832354627721
Epoch 30  loss:  0.05523011981200572
Epoch 40  loss:  0.0513243202361425
Epoch 50  loss:  0.04854004196371184
Epoch 60  loss:  0.04642485272904433
Epoch 70  loss:  0.04474722082574825
Epoch 80  loss:  0.043374333931969114
Epoch 90  loss:  0.04222371551840797
Epoch 100  loss:  0.04124104599832092
Epoch 110  loss:  0.04038888651110667
Epoch 120  loss:  0.03964048057966332
Epoch 130  loss:  0.0389761317586134
Epoch 140  loss:  0.038380979206568466
Epoch 150  loss:  0.03784357458020544
Epoch 160  loss:  0.03735493964481513
Epoch 170  loss:  0.03690792357698068
Epoch 180  loss:  0.03649675336766015
Epoch 190  loss:  0.036116712255247
Final loss: 0.03579805734492837</code></pre>
</div>
</div>
<p>Watching the loss decrease is encouraging‚Äîit means our network is learning! But how do we judge its actual performance? The loss function tells us about training progress, but what we really care about is classification accuracy on unseen data.</p>
<p>To evaluate our network properly, we‚Äôll use the <strong>confusion matrix</strong>‚Äîa fundamental tool in classification that shows not just whether predictions are right or wrong, but specifically how they‚Äôre wrong. The confusion matrix displays actual labels in the rows and predicted labels in the columns, giving us a complete picture of our classifier‚Äôs behavior.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="confusion_matrix.png" class="img-fluid figure-img"></p>
<figcaption>confusion_matrix</figcaption>
</figure>
</div>
<p>Understanding the confusion matrix is crucial. <strong>True positives</strong> (TP) are zeros correctly identified as zeros. <strong>False positives</strong> (FP) are other digits incorrectly classified as zeros. <strong>False negatives</strong> (FN) are zeros that we missed (classified as ‚Äúnot zero‚Äù). <strong>True negatives</strong> (TN) are other digits correctly identified as not-zeros.</p>
<p>From these four quantities, we can derive useful metrics. <strong>Precision</strong> = TP/(TP+FP) tells us what fraction of predicted zeros are actually zeros. <strong>Recall</strong> = TP/(TP+FN) tells us what fraction of actual zeros we successfully identified. <strong>Accuracy</strong> = (TP+TN)/(TP+TN+FP+FN) tells us the overall fraction of correct predictions.</p>
<p>Fortunately, <code>sklearn</code> provides functions to compute all of this automatically. We just need to supply predictions and actual labels. Crucially, we evaluate on the test set <code>X_test</code>‚Äîdata the network has never seen during training. This tests generalization, not memorization.</p>
<div id="d0e438e9" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix, classification_report</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> np.matmul(W.T,X_test) <span class="op">+</span> b</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> sigmoid(Z)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> (A<span class="op">&gt;</span><span class="fl">.5</span>)[<span class="dv">0</span>,:]</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> (y_test <span class="op">==</span> <span class="dv">1</span>)[<span class="dv">0</span>,:]</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(predictions, labels))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[8973   33]
 [  47  947]]</code></pre>
</div>
</div>
<div id="f9c8fd86" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(predictions, labels))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              precision    recall  f1-score   support

       False       0.99      1.00      1.00      9006
        True       0.97      0.95      0.96       994

    accuracy                           0.99     10000
   macro avg       0.98      0.97      0.98     10000
weighted avg       0.99      0.99      0.99     10000
</code></pre>
</div>
</div>
</section>
<section id="testing-our-model-individual-predictions" class="level3">
<h3 class="anchored" data-anchor-id="testing-our-model-individual-predictions">Testing Our Model: Individual Predictions üî¨</h3>
<p>Beyond aggregate statistics, it‚Äôs instructive to examine individual predictions. We can test our network on a single image and see what it predicts. Since sigmoid outputs range from 0 to 1, we use 0.5 as our decision threshold‚Äîoutputs above 0.5 are classified as zeros, below 0.5 as not-zeros. This threshold could be adjusted to trade off precision versus recall, but 0.5 is the natural choice for balanced classes.</p>
<div id="559e4c66" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>i<span class="op">=</span><span class="dv">200</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="bu">bool</span>(sigmoid(np.matmul(W.T, np.array(X_test)[:,i])<span class="op">+</span>b)<span class="op">&gt;</span><span class="fl">0.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>False</code></pre>
</div>
</div>
<div id="071ac298" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>plt.imshow(np.array(X_test)[:,i].reshape(<span class="dv">28</span>,<span class="dv">28</span>),cmap<span class="op">=</span><span class="st">'gray'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="1_deep_learning_files/figure-html/cell-17-output-1.png" width="424" height="420" class="figure-img"></p>
<figcaption>Example Image</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="network-with-hidden-layers-increasing-representational-power" class="level2">
<h2 class="anchored" data-anchor-id="network-with-hidden-layers-increasing-representational-power">Network with Hidden Layers: Increasing Representational Power üß©</h2>
<p>So far, our network consists of just an input layer (784 pixels) connected directly to a single output neuron. This is the simplest possible neural network architecture. To handle more complex patterns, we need to add <strong>hidden layers</strong>‚Äîintermediate layers between input and output.</p>
<p>Hidden layers are called ‚Äúhidden‚Äù because they‚Äôre internal to the network; we don‚Äôt directly observe their outputs during normal operation. But these hidden units are where the magic happens! They allow the network to learn intermediate representations‚Äîabstract features derived from the raw input pixels.</p>
<p>Think of it hierarchically: the first layer might learn to detect edges and curves, while deeper layers combine these into higher-level features like loops and intersections. This compositional structure is what makes deep learning so powerful‚Äîcomplex concepts are built from simpler ones, much like how physical theories build complexity from fundamental principles.</p>
<p>Here‚Äôs the architecture we‚Äôll build: 784 inputs ‚Üí 64 hidden units ‚Üí 1 output neuron.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="image.png" class="img-fluid figure-img"></p>
<figcaption>hidden</figcaption>
</figure>
</div>
<p>Our network now has two sets of parameters: weights <span class="math inline">\(W_1\)</span> and biases <span class="math inline">\(b_1\)</span> connecting the input to the hidden layer (shape 64 √ó 784), and weights <span class="math inline">\(W_2\)</span> and biases <span class="math inline">\(b_2\)</span> connecting the hidden layer to the output (shape 1 √ó 64). This gives us roughly 50,000 trainable parameters‚Äîa significant increase from 785 in the single-neuron network!</p>
<p>The forward pass now has two stages: input ‚Üí hidden layer (apply <span class="math inline">\(W_1\)</span>, <span class="math inline">\(b_1\)</span>, then sigmoid), then hidden layer ‚Üí output (apply <span class="math inline">\(W_2\)</span>, <span class="math inline">\(b_2\)</span>, then sigmoid). Backpropagation also has two stages, working backward from the output error through each layer using the chain rule.</p>
<p>We won‚Äôt derive all the equations again‚Äîthey follow the same principles as before, just applied to each layer. The code extends naturally from our single-neuron implementation:</p>
<div id="117c826e" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X_train</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> y_train</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>n_x <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>n_h <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>W1 <span class="op">=</span> np.random.randn(n_h, n_x)</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>b1 <span class="op">=</span> np.zeros((n_h, <span class="dv">1</span>))</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>W2 <span class="op">=</span> np.random.randn(<span class="dv">1</span>, n_h)</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>b2 <span class="op">=</span> np.zeros((<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>    Z1 <span class="op">=</span> np.matmul(W1, X) <span class="op">+</span> b1</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>    A1 <span class="op">=</span> sigmoid(Z1)</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>    Z2 <span class="op">=</span> np.matmul(W2, A1) <span class="op">+</span> b2</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>    A2 <span class="op">=</span> sigmoid(Z2)</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> compute_loss(Y, A2)</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>    dZ2 <span class="op">=</span> A2<span class="op">-</span>Y</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>    dW2 <span class="op">=</span> (<span class="fl">1.</span><span class="op">/</span>m) <span class="op">*</span> np.matmul(dZ2, A1.T)</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>    db2 <span class="op">=</span> (<span class="fl">1.</span><span class="op">/</span>m) <span class="op">*</span> np.<span class="bu">sum</span>(dZ2, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>    dA1 <span class="op">=</span> np.matmul(W2.T, dZ2)</span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a>    dZ1 <span class="op">=</span> dA1 <span class="op">*</span> sigmoid(Z1) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> sigmoid(Z1))</span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>    dW1 <span class="op">=</span> (<span class="fl">1.</span><span class="op">/</span>m) <span class="op">*</span> np.matmul(dZ1, X.T)</span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a>    db1 <span class="op">=</span> (<span class="fl">1.</span><span class="op">/</span>m) <span class="op">*</span> np.<span class="bu">sum</span>(dZ1, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a>    W2 <span class="op">=</span> W2 <span class="op">-</span> learning_rate <span class="op">*</span> dW2</span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a>    b2 <span class="op">=</span> b2 <span class="op">-</span> learning_rate <span class="op">*</span> db2</span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a>    W1 <span class="op">=</span> W1 <span class="op">-</span> learning_rate <span class="op">*</span> dW1</span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a>    b1 <span class="op">=</span> b1 <span class="op">-</span> learning_rate <span class="op">*</span> db1</span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Epoch"</span>, i, <span class="st">"loss: "</span>, loss)</span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-39"><a href="#cb23-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Final loss:"</span>, loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0 loss:  2.395166635058746
Epoch 10 loss:  0.22074168759268953
Epoch 20 loss:  0.1660154822272753
Epoch 30 loss:  0.13990677867922952
Epoch 40 loss:  0.12390102523919129
Epoch 50 loss:  0.11269161497108851
Epoch 60 loss:  0.10421329497723456
Epoch 70 loss:  0.09747959072905935
Epoch 80 loss:  0.09194898313097832
Epoch 90 loss:  0.0872943606401609
Final loss: 0.08367740628296327</code></pre>
</div>
</div>
<p>To evaluate our improved network, we again use the confusion matrix on the test set. You should notice better performance compared to the single-neuron network‚Äîthe hidden layer provides additional representational power.</p>
<div id="27a2aa49" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>Z1 <span class="op">=</span> np.matmul(W1, X_test) <span class="op">+</span> b1</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>A1 <span class="op">=</span> sigmoid(Z1)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>Z2 <span class="op">=</span> np.matmul(W2, A1) <span class="op">+</span> b2</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>A2 <span class="op">=</span> sigmoid(Z2)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> (A2<span class="op">&gt;</span><span class="fl">.5</span>)[<span class="dv">0</span>,:]</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> (y_test <span class="op">==</span> <span class="dv">1</span>)[<span class="dv">0</span>,:]</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(predictions, labels))</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(predictions, labels))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[8905  178]
 [ 115  802]]
              precision    recall  f1-score   support

       False       0.99      0.98      0.98      9083
        True       0.82      0.87      0.85       917

    accuracy                           0.97     10000
   macro avg       0.90      0.93      0.91     10000
weighted avg       0.97      0.97      0.97     10000
</code></pre>
</div>
</div>
</section>
<section id="multiclass-network-recognizing-all-digits" class="level2">
<h2 class="anchored" data-anchor-id="multiclass-network-recognizing-all-digits">Multiclass Network: Recognizing All Digits üî¢</h2>
<p>Our binary classifier successfully distinguishes zeros from non-zeros, but the real challenge is recognizing all ten digits (0-9). This requires extending to <strong>multiclass classification</strong>‚Äîinstead of a yes/no question, we‚Äôre asking ‚Äúwhich of these 10 categories?‚Äù</p>
<p>The key architectural change is replacing our single output neuron with 10 output neurons, one per digit class. Each neuron produces a score for its corresponding digit. We then interpret these scores as probabilities‚Äîthe neuron with the highest score determines our prediction.</p>
<p>This is analogous to expanding from a two-state quantum system (like spin-¬Ω) to a ten-state system‚Äîwe need ten basis states to represent all possibilities. The output layer produces a probability distribution over these ten states.</p>
<p>For example, the output array <code>[0,1,0,0,0,0,0,0,0,0]</code> would indicate the network predicts digit 1 (the second position, using zero-indexing). In practice, the outputs won‚Äôt be exactly 0 and 1, but rather probabilities that sum to 1. The predicted class is simply the position of the maximum value.</p>
<p>To train a multiclass network, we need to reload the original MNIST labels (not our binary 0/1 encoding):</p>
<div id="1da65619" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> fetch_openml</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> fetch_openml(<span class="st">'mnist_784'</span>, version<span class="op">=</span><span class="dv">1</span>, return_X_y<span class="op">=</span><span class="va">True</span>,as_frame<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X <span class="op">/</span> <span class="dv">255</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We‚Äôll convert these labels to <strong>one-hot encoding</strong>‚Äîa representation where each label becomes a vector of length 10 with a single 1 and nine 0s. The digit ‚Äú3‚Äù becomes <code>[0,0,0,1,0,0,0,0,0,0]</code>, for instance. This encoding matches our network‚Äôs output format and makes the loss calculation straightforward. The result is a 10 √ó 70,000 array where each column is one training example‚Äôs label:</p>
<div id="9399f341" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>digits <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>examples <span class="op">=</span> y.shape[<span class="dv">0</span>]</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> y.reshape(<span class="dv">1</span>, examples)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>Y_new <span class="op">=</span> np.eye(digits)[y.astype(<span class="st">'int32'</span>)]</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>Y_new <span class="op">=</span> Y_new.T.reshape(digits, examples)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="bcbf5e2b" class="cell" data-tags="[]" data-execution_count="21">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>Y_new.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>(10, 70000)</code></pre>
</div>
</div>
<p>We also separate into training and testing sets, maintaining the same 60,000/10,000 split:</p>
<div id="ee3a8478" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="dv">60000</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>m_test <span class="op">=</span> X.shape[<span class="dv">0</span>] <span class="op">-</span> m</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>X_train, X_test <span class="op">=</span> X[:m].T, X[m:].T</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>Y_train, Y_test <span class="op">=</span> Y_new[:,:m], Y_new[:,m:]</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>shuffle_index <span class="op">=</span> np.random.permutation(m)</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>X_train, Y_train <span class="op">=</span> X_train[:, shuffle_index], Y_train[:, shuffle_index]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="4d49b9cb" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> <span class="dv">58</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>plt.imshow(X_train[:,i].reshape(<span class="dv">28</span>,<span class="dv">28</span>), cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>Y_train[:,i]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="1_deep_learning_files/figure-html/cell-24-output-1.png" width="511" height="426" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])</code></pre>
</div>
</div>
<section id="changes-to-the-model-architecture" class="level3">
<h3 class="anchored" data-anchor-id="changes-to-the-model-architecture">Changes to the Model Architecture üîß</h3>
<p>Let‚Äôs systematically examine what changes are required to extend our binary classifier to multiclass. The input and hidden layers remain unchanged‚Äîthey still extract useful features. Only the output layer and loss function need modification.</p>
<section id="forward-pass-the-softmax-function" class="level4">
<h4 class="anchored" data-anchor-id="forward-pass-the-softmax-function">Forward Pass: The Softmax Function</h4>
<p>The critical change is in the output layer. Instead of a single sigmoid unit, we now have 10 units with a <strong>softmax</strong> activation function. Softmax generalizes sigmoid to multiple classes‚Äîit converts a vector of arbitrary real numbers (the logits <span class="math inline">\(z\)</span>) into a probability distribution that sums to 1.</p>
<p>The softmax function computes the activation for each output unit <span class="math inline">\(i\)</span> as: <span class="math display">\[\begin{align*}
\sigma(z)_i = \frac{{\rm e}^{z_i}}{\sum_{j=0}^9{\rm e}^{z_i}}\ .
\end{align*}\]</span></p>
<p>The exponential ensures all outputs are positive, and the normalization ensures they sum to 1, giving us a valid probability distribution. Units with larger logits <span class="math inline">\(z_i\)</span> get exponentially larger probabilities‚Äîthe softmax is ‚Äúsoft‚Äù because it doesn‚Äôt simply pick the maximum but gives a smooth distribution.</p>
<p>In vectorized NumPy code: <code>A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0)</code>. The <code>axis=0</code> parameter ensures we normalize each example independently (summing down columns, not across the entire matrix).</p>
</section>
<section id="loss-function-categorical-cross-entropy" class="level4">
<h4 class="anchored" data-anchor-id="loss-function-categorical-cross-entropy">Loss Function: Categorical Cross-Entropy</h4>
<p>With multiple classes, we need to generalize our loss function. The <strong>categorical cross-entropy</strong> extends binary cross-entropy to <span class="math inline">\(n\)</span> classes. For a single training example: <span class="math display">\[\begin{align*}
L(y,\hat{y}) = -\sum_{i=0}^n y_i\log(\hat{y}_i)\ .
\end{align*}\]</span> This is the negative log-probability assigned to the correct class. Since the labels are one-hot encoded, only one term in the sum is nonzero (the term corresponding to the true class). This means we‚Äôre minimizing the negative log-likelihood‚Äîmaximizing the probability assigned to the correct class.</p>
<p>From a physics perspective, this is deeply connected to statistical mechanics. The cross-entropy loss is equivalent to minimizing the Kullback-Leibler divergence between the true distribution (the one-hot labels) and the predicted distribution (softmax outputs). It‚Äôs the information-theoretic measure of how well our predictions match reality.</p>
<p>Averaging over <span class="math inline">\(m\)</span> training examples: <span class="math display">\[\begin{align*}
L(y,\hat{y}) = -\frac{1}{m}\sum_{j=0}^m\sum_{i=0}^n y_i^{(i)}\log(\hat{y}_i^{(i)})\ .
\end{align*}\]</span></p>
<p>So let‚Äôs define:</p>
<div id="d9104b9b" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_multiclass_loss(Y, Y_hat):</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    L_sum <span class="op">=</span> np.<span class="bu">sum</span>(np.multiply(Y, np.log(Y_hat)))</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> Y.shape[<span class="dv">1</span>]</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>    L <span class="op">=</span> <span class="op">-</span>(<span class="dv">1</span><span class="op">/</span>m) <span class="op">*</span> L_sum</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> L</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="backpropagation-a-fortunate-simplification" class="level4">
<h4 class="anchored" data-anchor-id="backpropagation-a-fortunate-simplification">Backpropagation: A Fortunate Simplification</h4>
<p>Here‚Äôs a beautiful mathematical result: despite the complexity of the softmax function, the gradient simplifies dramatically. When we combine softmax activation with cross-entropy loss, the derivative of the loss with respect to the logits (the <span class="math inline">\(z\)</span> values) is simply: <span class="math display">\[\begin{align*}
\frac{\partial L}{\partial z_i} = \hat{y}_i - y_i\ .
\end{align*}\]</span></p>
<p>This is exactly the same form we had with sigmoid and binary cross-entropy! The predicted probabilities minus the true labels. This isn‚Äôt a coincidence‚Äîit reflects the principled connection between exponential family distributions and their conjugate loss functions. The mathematical elegance here is reminiscent of how the Hamiltonian and Lagrangian formulations of mechanics lead to equivalent equations of motion.</p>
<p>We won‚Äôt walk through the full derivation (it requires careful application of the chain rule with matrix derivatives), but trust that this simple gradient makes implementation straightforward.</p>
</section>
</section>
<section id="build-and-train-the-complete-network" class="level3">
<h3 class="anchored" data-anchor-id="build-and-train-the-complete-network">Build and Train: The Complete Network üöÄ</h3>
<p>Now we assemble our final, full-featured digit classifier. With more parameters (now around 50,000) and a more complex task (10 classes instead of 2), training will take longer. We‚Äôll run 200 epochs to give the network sufficient time to learn the patterns distinguishing all ten digits.</p>
<p>The architecture is: 784 inputs ‚Üí 64 hidden units (sigmoid) ‚Üí 10 outputs (softmax). Watch how the loss decreases as training progresses‚Äîyou‚Äôre witnessing the network discover representations of handwritten digits!</p>
<div id="d9c6e2a5" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>n_x <span class="op">=</span> X_train.shape[<span class="dv">0</span>]</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>n_h <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>W1 <span class="op">=</span> np.random.randn(n_h, n_x)</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>b1 <span class="op">=</span> np.zeros((n_h, <span class="dv">1</span>))</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>W2 <span class="op">=</span> np.random.randn(digits, n_h)</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>b2 <span class="op">=</span> np.zeros((digits, <span class="dv">1</span>))</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X_train</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> Y_train</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">200</span>):</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>    Z1 <span class="op">=</span> np.matmul(W1,X) <span class="op">+</span> b1</span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>    A1 <span class="op">=</span> sigmoid(Z1)</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>    Z2 <span class="op">=</span> np.matmul(W2,A1) <span class="op">+</span> b2</span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a>    A2 <span class="op">=</span> np.exp(Z2) <span class="op">/</span> np.<span class="bu">sum</span>(np.exp(Z2), axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> compute_multiclass_loss(Y, A2)</span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a>    dZ2 <span class="op">=</span> A2<span class="op">-</span>Y</span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a>    dW2 <span class="op">=</span> (<span class="fl">1.</span><span class="op">/</span>m) <span class="op">*</span> np.matmul(dZ2, A1.T)</span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a>    db2 <span class="op">=</span> (<span class="fl">1.</span><span class="op">/</span>m) <span class="op">*</span> np.<span class="bu">sum</span>(dZ2, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a>    dA1 <span class="op">=</span> np.matmul(W2.T, dZ2)</span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a>    dZ1 <span class="op">=</span> dA1 <span class="op">*</span> sigmoid(Z1) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> sigmoid(Z1))</span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a>    dW1 <span class="op">=</span> (<span class="fl">1.</span><span class="op">/</span>m) <span class="op">*</span> np.matmul(dZ1, X.T)</span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a>    db1 <span class="op">=</span> (<span class="fl">1.</span><span class="op">/</span>m) <span class="op">*</span> np.<span class="bu">sum</span>(dZ1, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a>    W2 <span class="op">=</span> W2 <span class="op">-</span> learning_rate <span class="op">*</span> dW2</span>
<span id="cb35-32"><a href="#cb35-32" aria-hidden="true" tabindex="-1"></a>    b2 <span class="op">=</span> b2 <span class="op">-</span> learning_rate <span class="op">*</span> db2</span>
<span id="cb35-33"><a href="#cb35-33" aria-hidden="true" tabindex="-1"></a>    W1 <span class="op">=</span> W1 <span class="op">-</span> learning_rate <span class="op">*</span> dW1</span>
<span id="cb35-34"><a href="#cb35-34" aria-hidden="true" tabindex="-1"></a>    b1 <span class="op">=</span> b1 <span class="op">-</span> learning_rate <span class="op">*</span> db1</span>
<span id="cb35-35"><a href="#cb35-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-36"><a href="#cb35-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>):</span>
<span id="cb35-37"><a href="#cb35-37" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Epoch"</span>, i, <span class="st">"loss: "</span>, loss)</span>
<span id="cb35-38"><a href="#cb35-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-39"><a href="#cb35-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Final loss:"</span>, loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0 loss:  9.359409945262723
Epoch 10 loss:  2.480915410750769
Epoch 20 loss:  1.674432764227767
Epoch 30 loss:  1.3330104308788548
Epoch 40 loss:  1.1447842302497118
Epoch 50 loss:  1.0230964725181804
Epoch 60 loss:  0.9368747323694273
Epoch 70 loss:  0.871957389404843
Epoch 80 loss:  0.8208795576102073
Epoch 90 loss:  0.7793325725168161
Epoch 100 loss:  0.7446649543545801
Epoch 110 loss:  0.7151537041535516
Epoch 120 loss:  0.6896258244540621
Epoch 130 loss:  0.6672519100025255
Epoch 140 loss:  0.6474268213495038
Epoch 150 loss:  0.6296970416913454
Epoch 160 loss:  0.6137147676333653
Epoch 170 loss:  0.5992079750548168
Epoch 180 loss:  0.5859603076597457
Epoch 190 loss:  0.5737971945414019
Final loss: 0.5636592880338959</code></pre>
</div>
</div>
<p>Let‚Äôs see how we did:</p>
<div id="7e0be3e6" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>Z1 <span class="op">=</span> np.matmul(W1, X_test) <span class="op">+</span> b1</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>A1 <span class="op">=</span> sigmoid(Z1)</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>Z2 <span class="op">=</span> np.matmul(W2, A1) <span class="op">+</span> b2</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>A2 <span class="op">=</span> np.exp(Z2) <span class="op">/</span> np.<span class="bu">sum</span>(np.exp(Z2), axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> np.argmax(A2, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> np.argmax(Y_test, axis<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="model-performance-confusion-matrix-analysis" class="level4">
<h4 class="anchored" data-anchor-id="model-performance-confusion-matrix-analysis">Model Performance: Confusion Matrix Analysis üìä</h4>
<p>Now for the moment of truth‚Äîhow well does our network perform on unseen test data?</p>
<div id="780c5d1e" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(predictions, labels))</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(predictions, labels))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[ 896    0   26    8    8   22   30    4   14   10]
 [   0 1076   15    6    2    7    3    6   14    2]
 [  14   13  815   30   10   13   23   33   24   13]
 [  10   12   47  820    4   60    5   18   40   17]
 [   0    1   17    0  790   22   30   17   14  106]
 [  27    4    7   56    5  669   23    6   58   16]
 [  13    5   30    7   23   28  822    0   21    1]
 [   6    2   18   25   12   10    3  866   18   37]
 [  12   22   47   42   19   44   16   15  739   28]
 [   2    0   10   16  109   17    3   63   32  779]]
              precision    recall  f1-score   support

           0       0.91      0.88      0.90      1018
           1       0.95      0.95      0.95      1131
           2       0.79      0.82      0.81       988
           3       0.81      0.79      0.80      1033
           4       0.80      0.79      0.80       997
           5       0.75      0.77      0.76       871
           6       0.86      0.87      0.86       950
           7       0.84      0.87      0.86       997
           8       0.76      0.75      0.75       984
           9       0.77      0.76      0.76      1031

    accuracy                           0.83     10000
   macro avg       0.82      0.83      0.82     10000
weighted avg       0.83      0.83      0.83     10000
</code></pre>
</div>
</div>
<p>We achieve approximately 84% accuracy across all ten digits‚Äînot bad for a network we built entirely from scratch! Modern state-of-the-art methods using convolutional neural networks reach 99%+ accuracy, but our simple fully-connected network demonstrates the fundamental principles.</p>
<p>Looking at the confusion matrix and classification report, you can see which digits are hardest to distinguish. Often, 4s and 9s are confused, or 3s and 5s‚Äîeven humans sometimes struggle with these pairs in badly written examples. The confusion matrix gives insight into the network‚Äôs systematic errors, which could guide improvements.</p>
</section>
</section>
</section>
<section id="testing-the-model-interactive-exploration" class="level2">
<h2 class="anchored" data-anchor-id="testing-the-model-interactive-exploration">Testing the Model: Interactive Exploration üîç</h2>
<p>Let‚Äôs test our trained network on individual examples to get an intuitive feel for its predictions:</p>
<div id="5c6e2ce9" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>i<span class="op">=</span><span class="dv">2003</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>plt.imshow(X_test[:,i].reshape(<span class="dv">28</span>,<span class="dv">28</span>), cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>predictions[i]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>np.int64(5)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="1_deep_learning_files/figure-html/cell-29-output-2.png" width="424" height="420" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Try changing the index <code>i</code> to explore different test images. Compare the network‚Äôs predictions with what you see. When does it make mistakes? Are the errors understandable? This kind of error analysis is crucial in practical machine learning‚Äîunderstanding failure modes helps you improve your models.</p>
</section>
<section id="reflection-and-next-steps" class="level2">
<h2 class="anchored" data-anchor-id="reflection-and-next-steps">Reflection and Next Steps üéØ</h2>
<p>Congratulations! You‚Äôve built a complete neural network from scratch and trained it to recognize handwritten digits with reasonable accuracy. More importantly, you understand every component: forward propagation (matrix multiplications and activations), loss functions (quantifying error), backpropagation (computing gradients via chain rule), and gradient descent (iterative optimization).</p>
<p><strong>Key Insights for Physicists:</strong> The mathematical tools we used‚Äîlinear algebra, calculus, and optimization‚Äîare the same ones you use throughout physics. Neural networks are fundamentally just differentiable programs that we optimize. The loss landscape we navigate with gradient descent is analogous to potential energy surfaces in molecular dynamics or action functionals in field theory.</p>
<p><strong>What We Built:</strong> Our final network has ~50,000 parameters organized into two layers. It processes 784-dimensional input vectors (flattened images) through learned transformations to produce 10-dimensional output probability distributions. Training involved computing gradients for all 50,000 parameters on 60,000 examples, iterating 200 times‚Äîabout 600 million gradient computations in total, all completed in seconds thanks to vectorized NumPy operations.</p>
<p><strong>Limitations and Improvements:</strong> Our network treats pixels as independent features, ignoring spatial structure. Convolutional neural networks (CNNs) exploit the 2D structure of images, achieving much better performance. We also used a fixed learning rate and simple gradient descent; modern optimizers like Adam adapt the learning rate automatically. And we‚Äôve barely scratched the surface of architecture design‚Äîdepth, width, activation functions, regularization, and many other choices affect performance.</p>
<p><strong>In the Next Seminar:</strong> We‚Äôll explore high-level frameworks like TensorFlow and PyTorch that make building and training complex networks much easier. We‚Äôll also see how to apply these techniques to physics problems‚Äîfrom analyzing experimental data to solving differential equations with neural networks. The principles remain the same; the tools just become more powerful.</p>
<p><strong>Final Thought:</strong> Machine learning is rapidly transforming physics research. From discovering new particles in collider data to controlling quantum systems to simulating complex materials, neural networks are becoming as essential as Fourier transforms or numerical integration. By understanding these tools at a fundamental level, you‚Äôre well-prepared to apply them creatively in your own research. ```</p>


<script type="ojs-module-contents">
eyJjb250ZW50cyI6W119
</script>
<div id="exercise-loading-indicator" class="exercise-loading-indicator d-none d-flex align-items-center gap-2">
<div id="exercise-loading-status" class="d-flex gap-2">

</div>
<div class="spinner-grow spinner-grow-sm">

</div>
</div>
<script type="vfs-file">
W10=
</script>
</section>

</main> <!-- /main -->
<script type="ojs-module-contents">
eyJjb250ZW50cyI6W119
</script>
<script type="module">
if (window.location.protocol === "file:") { alert("The OJS runtime does not work with file:// URLs. Please use a web server to view this document."); }
window._ojs.paths.runtimeToDoc = "../../seminars/seminar01";
window._ojs.paths.runtimeToRoot = "../..";
window._ojs.paths.docToRoot = "../..";
window._ojs.selfContained = false;
window._ojs.runtime.interpretFromScriptTags();
</script>
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/fcichos\.github\.io\/EMPP24\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../seminars/seminar01/2_reinforcement_learning.html" class="pagination-link" aria-label="Advanced: Reinforcement Learning">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Advanced: Reinforcement Learning</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->




</body></html>