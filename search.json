[
  {
    "objectID": "CLAUDE.html",
    "href": "CLAUDE.html",
    "title": "CLAUDE.md",
    "section": "",
    "text": "This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n\nThis is EMPP (Einf√ºhrung in die Modellierung Physikalischer Prozesse / Introduction to Modeling Physical Processes), a Python-based computational physics course taught in German at a university level. The course website is built with Quarto and contains lecture materials (.qmd files) and seminar exercises covering topics from basic Python to advanced physics simulations.\nRepository: https://github.com/fcichos/EMPP Website: https://fcichos.github.io/EMPP/ Primary Language: Course is taught in German, but code/documentation may be in English.\n\n\n\n\n\n# Activate UV environment\ndefault\n\n# Render the website\nquarto render\nThis renders all .qmd files to HTML and outputs to the docs/ directory.\nNote: The _quarto.yml does NOT have an explicit render: section. Quarto auto-discovers all .qmd files in the project. Do not add a render: list unless you need to exclude specific files.\n\n\n\nquarto preview\nLaunches a local server with live reload.\n\n\n\nquarto render lectures/lecture01/jupyter-notebooks.qmd\nquarto render seminars/seminar06/publication-ready-figures.qmd\n\n\n\nquarto check\n\n\n\n\n\n\nEMPP25/\n‚îú‚îÄ‚îÄ lectures/          # Main lecture content (Week 1-14)\n‚îÇ   ‚îú‚îÄ‚îÄ lecture01/     # Week 1: Jupyter notebooks, plotting basics\n‚îÇ   ‚îú‚îÄ‚îÄ lecture02/     # Week 2-3: Variables, datatypes, functions\n‚îÇ   ‚îú‚îÄ‚îÄ lecture03/     # Week 4: NumPy arrays\n‚îÇ   ‚îú‚îÄ‚îÄ lecture05/     # Week 7: Classes and OOP\n‚îÇ   ‚îú‚îÄ‚îÄ lecture06/     # Week 8: Input/Output, data files\n‚îÇ   ‚îú‚îÄ‚îÄ lecture07/     # Week 5 & 10: Differentiation, curve fitting\n‚îÇ   ‚îú‚îÄ‚îÄ lecture08/     # Week 5: Integration, solving ODEs\n‚îÇ   ‚îú‚îÄ‚îÄ lecture09/     # Week 8-9: Coupled pendula, Fourier analysis\n‚îÇ   ‚îú‚îÄ‚îÄ lecture10/     # Week 6: Planetary motion, spring pendulum\n‚îÇ   ‚îú‚îÄ‚îÄ lecture11/     # Week 11: Repetition/review\n‚îÇ   ‚îú‚îÄ‚îÄ lecture12/     # Week 13: Diffusion equation (PDEs)\n‚îÇ   ‚îú‚îÄ‚îÄ lecture13/     # Week 9 & 12: Waves (plane, spherical)\n‚îÇ   ‚îú‚îÄ‚îÄ lecture14/     # Week 9: Huygens principle\n‚îÇ   ‚îî‚îÄ‚îÄ lecture15/     # Week 11 & 14: ODE review, quantum mechanics\n‚îú‚îÄ‚îÄ seminars/          # Seminar exercises and assignments\n‚îú‚îÄ‚îÄ course-info/       # Syllabus, instructors, resources, exam info\n‚îú‚îÄ‚îÄ _quarto.yml        # Main Quarto configuration\n‚îî‚îÄ‚îÄ docs/              # Generated website output (for GitHub Pages)\n\n\n\n\n_quarto.yml: Main configuration defining website structure, navigation, and rendering options\nstyles.css: Custom CSS styling for the website\nPedagogical Documentation:\n\nlectures/PEDAGOGICAL_NOTES.md: Explains Brownian motion two-stage teaching approach\nLECTURE_ORDER_RATIONALE.md: Justifies Week 6 lecture ordering (planetary motion ‚Üí spring pendulum)\nCHANGES_SUMMARY.md: Details Week 6 restructuring decisions\n\n\n\n\n\n\n\n\n\nSpiral Learning: Topics like Brownian motion are taught twice:\n\nWeek 2-3 (lecture02/brownian-motion-simple.qmd): Simple functional approach, focus on physics\nWeek 6-7 (lecture05/brownian-motion-advanced.qmd): Object-oriented refactor, focus on code organization\n\nProgressive Difficulty: Week 6 is carefully ordered:\n\nFirst: Planetary motion (stable, intuitive, single force)\nSecond: Spring pendulum (chaotic, complex, coupled forces)\nThis creates confidence before complexity\n\nPhysics First, Code Later: Students learn physics with simple code, then refactor with better practices when they understand why organization matters.\n\n\n\n\nWeek 1  ‚Üí Setup: Jupyter notebooks, first plots\nWeek 2-3 ‚Üí Python basics: variables, datatypes, functions\nWeek 4  ‚Üí NumPy arrays for physics\nWeek 5  ‚Üí Numerical methods: differentiation, integration, ODEs\nWeek 6  ‚Üí APPLY Week 5 tools: Planetary motion + Spring pendulum\nWeek 7  ‚Üí Code organization: Classes (motivated by Week 6 complexity)\nWeek 8  ‚Üí OOP applications: Advanced Brownian motion, I/O\nWeek 9  ‚Üí Oscillations + Waves: Fourier analysis, wave equations\nWeek 10 ‚Üí Data analysis: Curve fitting\nWeek 11-12 ‚Üí Review and repetition\nWeek 13-14 ‚Üí PDEs: Diffusion, quantum mechanics\n\n\n\n\n\n\n\nLectures are .qmd (Quarto markdown) files combining markdown, LaTeX, and Python code\nCode blocks use {python} syntax\nSupport for both HTML and PDF rendering\n\n\n\n\nPython code blocks:\n#| echo: true\n#| eval: true\nimport numpy as np\nimport matplotlib.pyplot as plt\nSlides format (for some lectures):\n---\nformat: revealjs\n---\n\n\n\n\nFiles in _unused/ or lectures/lectureXX/_unused/ directories are archived previous versions\nThese contain old approaches that were pedagogically replaced\nDo NOT delete without understanding the restructuring rationale documented in PEDAGOGICAL_NOTES.md\n\n\n\n\n\n\n\nThe lecture directories (lecture01-lecture15) don‚Äôt directly map to weeks because: - Content was reorganized for pedagogical flow - Some lectures span multiple weeks - Review weeks reuse earlier lecture materials - Always refer to _quarto.yml for the actual week-to-content mapping\n\n\n\nSee lectures/PEDAGOGICAL_NOTES.md for full rationale: - Teaching OOP in Week 2-3 creates cognitive overload - Students learn physics with functions first - Later revisit with classes when they understand WHY organization helps - Direct comparison shows value of OOP\n\n\n\nSee LECTURE_ORDER_RATIONALE.md for full rationale: - Planetary motion: stable, intuitive, single force (confidence builder) - Spring pendulum: chaotic, complex, coupled forces (interesting challenge) - Natural progression from simple ‚Üí complex\n\n\n\n\n\n\n\nCreate .qmd file in appropriate lectures/lectureXX/ directory\nAdd entry to _quarto.yml in correct week section\nTest render: quarto render lectures/lectureXX/new-file.qmd\nFull site render: quarto render\n\n\n\n\n\nAlways read pedagogical docs first: PEDAGOGICAL_NOTES.md, LECTURE_ORDER_RATIONALE.md\nUpdate _quarto.yml to reflect changes\nConsider downstream effects on dependent lectures\nUpdate any cross-references in other files\n\n\n\n\n# Render and preview locally\nquarto preview\n\n# Check for broken links/references\nquarto check\n\n# Render full site\nquarto render\n\n\n\n\nPhysics simulations use: numpy, scipy, matplotlib\nODE solving: scipy.integrate.odeint\nStandard imports are typically at top of each .qmd file\nCode is pedagogical (prioritizes clarity over performance)\n\n\n\n\n\nStandard scientific stack: - numpy - Arrays and numerical operations - scipy - ODE solving (scipy.integrate.odeint), optimization, signal processing - matplotlib - Plotting and visualization\nThese are assumed to be available in the student/teaching environment.\n\n\n\n\n\n\n.quarto/ - Quarto cache and intermediate files\nKeynote/EMPP24_25.key - Large Keynote presentation file\nStandard Python/Jupyter ignores (.ipynb_checkpoints, __pycache__)\n\n\n\n\n\nMain branch: main\nActive development appears to happen directly on main\n\n\n\n\n\n\n\n\nHTML output goes to docs/ directory\nThis directory is served via GitHub Pages\ndocs/ directory is committed to git (contains final website)\n\n\n\n\nStandard Quarto Workflow:\n# Activate UV environment\ndefault\n\n# Publish to GitHub Pages\nquarto publish gh-pages\nThis command: 1. Renders the site 2. Commits to gh-pages branch 3. Pushes to GitHub 4. GitHub Pages automatically deploys\nAlternative: Manual Publishing\nIf quarto publish gh-pages has issues, use manual git workflow:\n# 1. Build\ndefault && quarto render\n\n# 2. Commit changes\ngit add docs/\ngit commit -m \"Update website\"\n\n# 3. Push to GitHub\ngit push origin main\nGitHub Pages is configured to serve from the docs/ directory on the main branch.\n\n\n\nThe site includes privacy-friendly analytics via Plausible (configured in _quarto.yml).\n\n\n\n\n\n\n\nPrimary instruction language: German\nCode comments may be English or German\nWhen editing text content, maintain German unless explicitly educational code examples\n\n\n\n\nBefore modifying lecture structure or content: 1. Check if there‚Äôs a *_RATIONALE.md or *_NOTES.md file explaining the decision 2. Understand the cognitive load considerations 3. Consider the spiral learning approach 4. Don‚Äôt assume ‚Äúsimpler‚Äù or ‚Äúmore efficient‚Äù code is better - clarity for learning is priority\n\n\n\n\nIntroductory computational physics course\nStudents learning Python AND physics simultaneously\nCode should be educational, not production-quality\nPrioritize understanding over optimization\n\n\n\n\n\n\nQuarto documentation: https://quarto.org\nCourse materials are self-contained in this repository\nExternal references are in references.bib (bibliography file)"
  },
  {
    "objectID": "CLAUDE.html#project-overview",
    "href": "CLAUDE.html#project-overview",
    "title": "CLAUDE.md",
    "section": "",
    "text": "This is EMPP (Einf√ºhrung in die Modellierung Physikalischer Prozesse / Introduction to Modeling Physical Processes), a Python-based computational physics course taught in German at a university level. The course website is built with Quarto and contains lecture materials (.qmd files) and seminar exercises covering topics from basic Python to advanced physics simulations.\nRepository: https://github.com/fcichos/EMPP Website: https://fcichos.github.io/EMPP/ Primary Language: Course is taught in German, but code/documentation may be in English."
  },
  {
    "objectID": "CLAUDE.html#building-and-development",
    "href": "CLAUDE.html#building-and-development",
    "title": "CLAUDE.md",
    "section": "",
    "text": "# Activate UV environment\ndefault\n\n# Render the website\nquarto render\nThis renders all .qmd files to HTML and outputs to the docs/ directory.\nNote: The _quarto.yml does NOT have an explicit render: section. Quarto auto-discovers all .qmd files in the project. Do not add a render: list unless you need to exclude specific files.\n\n\n\nquarto preview\nLaunches a local server with live reload.\n\n\n\nquarto render lectures/lecture01/jupyter-notebooks.qmd\nquarto render seminars/seminar06/publication-ready-figures.qmd\n\n\n\nquarto check"
  },
  {
    "objectID": "CLAUDE.html#repository-architecture",
    "href": "CLAUDE.html#repository-architecture",
    "title": "CLAUDE.md",
    "section": "",
    "text": "EMPP25/\n‚îú‚îÄ‚îÄ lectures/          # Main lecture content (Week 1-14)\n‚îÇ   ‚îú‚îÄ‚îÄ lecture01/     # Week 1: Jupyter notebooks, plotting basics\n‚îÇ   ‚îú‚îÄ‚îÄ lecture02/     # Week 2-3: Variables, datatypes, functions\n‚îÇ   ‚îú‚îÄ‚îÄ lecture03/     # Week 4: NumPy arrays\n‚îÇ   ‚îú‚îÄ‚îÄ lecture05/     # Week 7: Classes and OOP\n‚îÇ   ‚îú‚îÄ‚îÄ lecture06/     # Week 8: Input/Output, data files\n‚îÇ   ‚îú‚îÄ‚îÄ lecture07/     # Week 5 & 10: Differentiation, curve fitting\n‚îÇ   ‚îú‚îÄ‚îÄ lecture08/     # Week 5: Integration, solving ODEs\n‚îÇ   ‚îú‚îÄ‚îÄ lecture09/     # Week 8-9: Coupled pendula, Fourier analysis\n‚îÇ   ‚îú‚îÄ‚îÄ lecture10/     # Week 6: Planetary motion, spring pendulum\n‚îÇ   ‚îú‚îÄ‚îÄ lecture11/     # Week 11: Repetition/review\n‚îÇ   ‚îú‚îÄ‚îÄ lecture12/     # Week 13: Diffusion equation (PDEs)\n‚îÇ   ‚îú‚îÄ‚îÄ lecture13/     # Week 9 & 12: Waves (plane, spherical)\n‚îÇ   ‚îú‚îÄ‚îÄ lecture14/     # Week 9: Huygens principle\n‚îÇ   ‚îî‚îÄ‚îÄ lecture15/     # Week 11 & 14: ODE review, quantum mechanics\n‚îú‚îÄ‚îÄ seminars/          # Seminar exercises and assignments\n‚îú‚îÄ‚îÄ course-info/       # Syllabus, instructors, resources, exam info\n‚îú‚îÄ‚îÄ _quarto.yml        # Main Quarto configuration\n‚îî‚îÄ‚îÄ docs/              # Generated website output (for GitHub Pages)\n\n\n\n\n_quarto.yml: Main configuration defining website structure, navigation, and rendering options\nstyles.css: Custom CSS styling for the website\nPedagogical Documentation:\n\nlectures/PEDAGOGICAL_NOTES.md: Explains Brownian motion two-stage teaching approach\nLECTURE_ORDER_RATIONALE.md: Justifies Week 6 lecture ordering (planetary motion ‚Üí spring pendulum)\nCHANGES_SUMMARY.md: Details Week 6 restructuring decisions"
  },
  {
    "objectID": "CLAUDE.html#course-structure-and-pedagogy",
    "href": "CLAUDE.html#course-structure-and-pedagogy",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Spiral Learning: Topics like Brownian motion are taught twice:\n\nWeek 2-3 (lecture02/brownian-motion-simple.qmd): Simple functional approach, focus on physics\nWeek 6-7 (lecture05/brownian-motion-advanced.qmd): Object-oriented refactor, focus on code organization\n\nProgressive Difficulty: Week 6 is carefully ordered:\n\nFirst: Planetary motion (stable, intuitive, single force)\nSecond: Spring pendulum (chaotic, complex, coupled forces)\nThis creates confidence before complexity\n\nPhysics First, Code Later: Students learn physics with simple code, then refactor with better practices when they understand why organization matters.\n\n\n\n\nWeek 1  ‚Üí Setup: Jupyter notebooks, first plots\nWeek 2-3 ‚Üí Python basics: variables, datatypes, functions\nWeek 4  ‚Üí NumPy arrays for physics\nWeek 5  ‚Üí Numerical methods: differentiation, integration, ODEs\nWeek 6  ‚Üí APPLY Week 5 tools: Planetary motion + Spring pendulum\nWeek 7  ‚Üí Code organization: Classes (motivated by Week 6 complexity)\nWeek 8  ‚Üí OOP applications: Advanced Brownian motion, I/O\nWeek 9  ‚Üí Oscillations + Waves: Fourier analysis, wave equations\nWeek 10 ‚Üí Data analysis: Curve fitting\nWeek 11-12 ‚Üí Review and repetition\nWeek 13-14 ‚Üí PDEs: Diffusion, quantum mechanics"
  },
  {
    "objectID": "CLAUDE.html#working-with-quarto-documents",
    "href": "CLAUDE.html#working-with-quarto-documents",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Lectures are .qmd (Quarto markdown) files combining markdown, LaTeX, and Python code\nCode blocks use {python} syntax\nSupport for both HTML and PDF rendering\n\n\n\n\nPython code blocks:\n#| echo: true\n#| eval: true\nimport numpy as np\nimport matplotlib.pyplot as plt\nSlides format (for some lectures):\n---\nformat: revealjs\n---\n\n\n\n\nFiles in _unused/ or lectures/lectureXX/_unused/ directories are archived previous versions\nThese contain old approaches that were pedagogically replaced\nDo NOT delete without understanding the restructuring rationale documented in PEDAGOGICAL_NOTES.md"
  },
  {
    "objectID": "CLAUDE.html#key-technical-decisions",
    "href": "CLAUDE.html#key-technical-decisions",
    "title": "CLAUDE.md",
    "section": "",
    "text": "The lecture directories (lecture01-lecture15) don‚Äôt directly map to weeks because: - Content was reorganized for pedagogical flow - Some lectures span multiple weeks - Review weeks reuse earlier lecture materials - Always refer to _quarto.yml for the actual week-to-content mapping\n\n\n\nSee lectures/PEDAGOGICAL_NOTES.md for full rationale: - Teaching OOP in Week 2-3 creates cognitive overload - Students learn physics with functions first - Later revisit with classes when they understand WHY organization helps - Direct comparison shows value of OOP\n\n\n\nSee LECTURE_ORDER_RATIONALE.md for full rationale: - Planetary motion: stable, intuitive, single force (confidence builder) - Spring pendulum: chaotic, complex, coupled forces (interesting challenge) - Natural progression from simple ‚Üí complex"
  },
  {
    "objectID": "CLAUDE.html#common-tasks",
    "href": "CLAUDE.html#common-tasks",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Create .qmd file in appropriate lectures/lectureXX/ directory\nAdd entry to _quarto.yml in correct week section\nTest render: quarto render lectures/lectureXX/new-file.qmd\nFull site render: quarto render\n\n\n\n\n\nAlways read pedagogical docs first: PEDAGOGICAL_NOTES.md, LECTURE_ORDER_RATIONALE.md\nUpdate _quarto.yml to reflect changes\nConsider downstream effects on dependent lectures\nUpdate any cross-references in other files\n\n\n\n\n# Render and preview locally\nquarto preview\n\n# Check for broken links/references\nquarto check\n\n# Render full site\nquarto render\n\n\n\n\nPhysics simulations use: numpy, scipy, matplotlib\nODE solving: scipy.integrate.odeint\nStandard imports are typically at top of each .qmd file\nCode is pedagogical (prioritizes clarity over performance)"
  },
  {
    "objectID": "CLAUDE.html#python-dependencies",
    "href": "CLAUDE.html#python-dependencies",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Standard scientific stack: - numpy - Arrays and numerical operations - scipy - ODE solving (scipy.integrate.odeint), optimization, signal processing - matplotlib - Plotting and visualization\nThese are assumed to be available in the student/teaching environment."
  },
  {
    "objectID": "CLAUDE.html#git-workflow",
    "href": "CLAUDE.html#git-workflow",
    "title": "CLAUDE.md",
    "section": "",
    "text": ".quarto/ - Quarto cache and intermediate files\nKeynote/EMPP24_25.key - Large Keynote presentation file\nStandard Python/Jupyter ignores (.ipynb_checkpoints, __pycache__)\n\n\n\n\n\nMain branch: main\nActive development appears to happen directly on main"
  },
  {
    "objectID": "CLAUDE.html#output-and-deployment",
    "href": "CLAUDE.html#output-and-deployment",
    "title": "CLAUDE.md",
    "section": "",
    "text": "HTML output goes to docs/ directory\nThis directory is served via GitHub Pages\ndocs/ directory is committed to git (contains final website)\n\n\n\n\nStandard Quarto Workflow:\n# Activate UV environment\ndefault\n\n# Publish to GitHub Pages\nquarto publish gh-pages\nThis command: 1. Renders the site 2. Commits to gh-pages branch 3. Pushes to GitHub 4. GitHub Pages automatically deploys\nAlternative: Manual Publishing\nIf quarto publish gh-pages has issues, use manual git workflow:\n# 1. Build\ndefault && quarto render\n\n# 2. Commit changes\ngit add docs/\ngit commit -m \"Update website\"\n\n# 3. Push to GitHub\ngit push origin main\nGitHub Pages is configured to serve from the docs/ directory on the main branch.\n\n\n\nThe site includes privacy-friendly analytics via Plausible (configured in _quarto.yml)."
  },
  {
    "objectID": "CLAUDE.html#special-considerations",
    "href": "CLAUDE.html#special-considerations",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Primary instruction language: German\nCode comments may be English or German\nWhen editing text content, maintain German unless explicitly educational code examples\n\n\n\n\nBefore modifying lecture structure or content: 1. Check if there‚Äôs a *_RATIONALE.md or *_NOTES.md file explaining the decision 2. Understand the cognitive load considerations 3. Consider the spiral learning approach 4. Don‚Äôt assume ‚Äúsimpler‚Äù or ‚Äúmore efficient‚Äù code is better - clarity for learning is priority\n\n\n\n\nIntroductory computational physics course\nStudents learning Python AND physics simultaneously\nCode should be educational, not production-quality\nPrioritize understanding over optimization"
  },
  {
    "objectID": "CLAUDE.html#references-and-resources",
    "href": "CLAUDE.html#references-and-resources",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Quarto documentation: https://quarto.org\nCourse materials are self-contained in this repository\nExternal references are in references.bib (bibliography file)"
  },
  {
    "objectID": "Assignment 4.html",
    "href": "Assignment 4.html",
    "title": "Assignment 3",
    "section": "",
    "text": "Problem 1\nThere are two files with a list of integers a.txt and b.txt. Calculate the greatest common divisors of each pair of values from these files and store them in the file result.txt. Check out the numpy function np.gcd() for this purpose.¬†\nAufgabenstellung klarer!\n\nimport numpy as np\n\nwith open('a.txt', 'r') as file_1, open('b.txt', 'r') as file_2, open('result.txt', 'w') as result:\n    for a, b in zip(file_1, file_2):\n        result.write(f\"{np.gcd(int(a),int(b))}\\n\")\n\n\nfile=open('a.txt','r')\nfile1=open('b.txt','r')\n\n\na=file.readlines()\nb=file1.readlines()\n\n\nlist(zip(a,b))\n\n[('12\\n', '5\\n'),\n ('2\\n', '8\\n'),\n ('4\\n', '4\\n'),\n ('3\\n', '6\\n'),\n ('4', '87\\n')]\n\n\nProblem 2\nAssign the 100 numbers between -10 and 10 including -10 and 10 to the variable x using the linspace function of numpy. Assign the function values ( x¬≤-20 ) to the variable y. Store the paired values in the file named ‚Äúdata.txt‚Äù in the form ‚Äúx, y‚Äù (e.g.¬†-10, 80) for each line of the file.\nLeerzeichen war die Falle!\n\nimport numpy as np\n\nx=np.linspace(-10,10,100)\ny=x**2-20\n\nwith open('data.txt', 'w') as file:\n    for x, y in zip(x, y):\n        file.write(f\"{x}, {y}\\n\")\n\nProblem 3\nWrite a Python function projectile_motion(v0, theta, dt)¬†that simulates the projectile motion of an object launched at an initial velocity v0 (in m/s) and angle theta (in degrees). The function should return a tuple containing:\nThe maximum height reached by the object (in meters). The total time of flight (in seconds). The horizontal range (in meters).\nUse a time step dt (in seconds) for the simulation. Advance the time inside the function with a loop. Note that calling the function is not required for the task! Can be done only with the math module.¬†\nWhat if one of the arguments is 0?\nAufgabenstellung klarer!\n\nimport math\n\ndef projectile_motion(v0, theta, dt):\n    if(v0 == 0 or theta == 0 or dt == 0): return (0,0,0)\n    \n    theta_rad = math.radians(theta)\n    \n    v0x = v0 * math.cos(theta_rad)\n    v0y = v0 * math.sin(theta_rad)\n    \n    x_points = []\n    y_points = []\n    \n    x = 0\n    y = 0\n    t = 0\n    \n    while y &gt;= 0:\n        x = v0x * t\n        y = v0y * t - 0.5 * 9.81 * t**2\n        x_points.append(x)\n        y_points.append(y)\n        t += dt\n    \n    max_height = max(y_points)\n    total_time = t - dt\n    horizontal_range = x_points[-1]\n    \n    return (max_height, total_time, horizontal_range)\n\n\nx=np.linspace(0,10,100)-5\ny=np.linspace(0,10,100)-5\n\n\nX,Y=np.meshgrid(x,y)\n\n\nr=np.sqrt(X**2+Y**2)\n\n\n\nimport matplotlib.pyplot as plt\n\n\nplt.contour(r)"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13.html",
    "href": "lectures/lecture13/repetition-week13.html",
    "title": "Repetition Meshgrid and Vector Fields",
    "section": "",
    "text": "This time, the exercises are a bit more complicated and perhaps only for the more advanced. You may, however, do some training with the easier repetitions before and come back and test yourself. We will focus on creating visualizations of vector fields using meshgrids and quiver plots. Vector fields are used to represent the spatial distribution of physical quantities like electric fields, fluid velocities, or magnetic fields. By visualizing vector fields, we can gain insights into the behavior of these quantities in different regions of space.\nNote that matplotlib and numpy are already imported and may be used with plt and np respectively.\n\n\n\n\n\n\nSelf-Exercise 1: Electric Field of a Point Charge\n\n\n\nCreate a 2D vector field plot of the electric field from a point charge located at the origin. The electric field vectors should point radially outward from the charge, with magnitude proportional to 1/r¬≤. This visualization helps understand the spatial distribution of electric fields.\nThe electric field is given by: \\(\\vec{E}(\\vec{r}) = \\frac{q}{4\\pi\\epsilon_0} \\frac{\\vec{r}}{r^3}\\)\nwhere:\n\n\\(\\vec{E}\\) is the electric field vector\n\\(q\\) is the charge\n\\(\\epsilon_0\\) is the permittivity of free space\n\\(\\vec{r}\\) is the position vector\n\\(r\\) is the distance from the charge\n\nLook up the plt.quiver() function in the Matplotlib documentation for plotting vector fields or check out the hint.\n\n\n\n\n\n\n\n\nUse np.meshgrid(x, y) to create X, Y grids\nCalculate R = sqrt(X¬≤ + Y¬≤) for distance\nCalculate Ex = X/R¬≥ and Ey = Y/R¬≥\nUse plt.quiver(X, Y, Ex, Ey) for the vector field\nRemember to avoid division by zero at the origin\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Exercise 2: Standing Wave Pattern\n\n\n\nVisualize a 2D standing wave pattern that might occur in a square membrane (like a drum head). This type of visualization is useful in understanding wave modes in musical instruments or quantum mechanical systems.\nThe wave function is given by: \\(\\psi(x,y,t) = \\sin(mx)\\cos(ny)\\)\nwhere: - \\(m, n\\) are mode numbers (integers) - \\(x, y\\) are positions on the membrane - \\(\\psi\\) is the displacement amplitude\n\n\n\n\n\n\n\n\nUse np.meshgrid(x, y) to create X, Y grids\nTry different mode numbers (m, n) for different patterns\nUse plt.contourf() for filled contours\nAdd a colorbar to show amplitude scale\nLook up the matplotlib plotting of surfaces with ax = plt.subplot(122, projection='3d') surf = ax.plot_surface()\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Exercise 3: Quantum Wave Packet\n\n\n\nCreate a 3D visualization of a Gaussian wave packet, which represents a localized quantum particle. This type of visualization is fundamental in understanding quantum mechanical states and probability distributions.\nThe wave function is given by: \\(\\psi(x,y) = A\\exp\\left(-\\frac{(x-x_0)^2 + (y-y_0)^2}{2\\sigma^2}\\right)\\)\nwhere: - \\(A\\) is the amplitude - \\((x_0, y_0)\\) is the center position - \\(\\sigma\\) is the width of the packet\n\n\n\n\n\n\n\n\nUse np.meshgrid(x, y) to create X, Y grids\nCalculate œà using the Gaussian formula\nCreate both a 3D surface plot and a 2D probability density plot\nUse as the probability density is |œà|¬≤\nLook up the matplotlib plotting of surfaces with ax = plt.subplot(133, projection='3d') surf = ax.plot_surface(X, Y, prob_density, cmap='viridis')\n\n\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#todays-focus-visualization",
    "href": "lectures/lecture13/repetition-week13-slides.html#todays-focus-visualization",
    "title": "Meshgrid and Vector Fields",
    "section": "Today‚Äôs Focus: Visualization",
    "text": "Today‚Äôs Focus: Visualization\nGoal: Master techniques for visualizing multidimensional physics\n\nMeshgrids ‚Äî Creating 2D coordinate systems\nVector fields ‚Äî Electric fields, velocities, forces\nScalar fields ‚Äî Wave patterns, probability distributions\n3D surfaces ‚Äî Quantum wave packets\n\n\nüì± Open now: https://fcichos.github.io/EMPP/lectures/lecture13/4_repetition.qmd"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#why-vector-fields-matter",
    "href": "lectures/lecture13/repetition-week13-slides.html#why-vector-fields-matter",
    "title": "Meshgrid and Vector Fields",
    "section": "Why Vector Fields Matter",
    "text": "Why Vector Fields Matter\nPhysics is full of vector quantities that vary in space:\n\n\n\nField\nSymbol\nUnits\n\n\n\n\nElectric field\n\\(\\vec{E}(\\vec{r})\\)\nN/C\n\n\nMagnetic field\n\\(\\vec{B}(\\vec{r})\\)\nT\n\n\nVelocity field\n\\(\\vec{v}(\\vec{r})\\)\nm/s\n\n\nForce field\n\\(\\vec{F}(\\vec{r})\\)\nN\n\n\n\n\nChallenge: How do we visualize something that has both magnitude and direction at every point?"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#the-fundamental-tool-meshgrid",
    "href": "lectures/lecture13/repetition-week13-slides.html#the-fundamental-tool-meshgrid",
    "title": "Meshgrid and Vector Fields",
    "section": "The Fundamental Tool: Meshgrid",
    "text": "The Fundamental Tool: Meshgrid\nProblem: We want to evaluate a function at many 2D points\n\nSolution: Create coordinate grids with np.meshgrid()\nx = np.linspace(-2, 2, 5)\ny = np.linspace(-1, 1, 3)\n\nX, Y = np.meshgrid(x, y)\n\n\nResult: Two 2D arrays containing all combinations of x and y coordinates"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#understanding-meshgrid-visual",
    "href": "lectures/lecture13/repetition-week13-slides.html#understanding-meshgrid-visual",
    "title": "Meshgrid and Vector Fields",
    "section": "Understanding Meshgrid: Visual",
    "text": "Understanding Meshgrid: Visual\nx = np.array([0, 1, 2])\ny = np.array([0, 1])\n\nX, Y = np.meshgrid(x, y)\n\nX array:\n[[0, 1, 2],\n [0, 1, 2]]\nY array:\n[[0, 0, 0],\n [1, 1, 1]]\n\n\nEach position (i,j) in X and Y gives you a coordinate pair!"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#meshgrid-the-concept",
    "href": "lectures/lecture13/repetition-week13-slides.html#meshgrid-the-concept",
    "title": "Meshgrid and Vector Fields",
    "section": "Meshgrid: The Concept",
    "text": "Meshgrid: The Concept\nThink of it as a grid of points:\ny\n^\n1 | (0,1)  (1,1)  (2,1)\n  |\n0 | (0,0)  (1,0)  (2,0)\n  +-------------------&gt; x\n    0      1      2\n\nKey insight: X[i,j] and Y[i,j] together specify one point on this grid"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#why-meshgrid",
    "href": "lectures/lecture13/repetition-week13-slides.html#why-meshgrid",
    "title": "Meshgrid and Vector Fields",
    "section": "Why Meshgrid?",
    "text": "Why Meshgrid?\nWithout meshgrid (painful):\nfor i, xi in enumerate(x):\n    for j, yj in enumerate(y):\n        result[i,j] = function(xi, yj)\n\nWith meshgrid (vectorized):\nX, Y = np.meshgrid(x, y)\nresult = function(X, Y)  # Operates on entire grid at once!\n\n\nBenefits: Faster, cleaner, more readable"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#exercise-1-electric-field",
    "href": "lectures/lecture13/repetition-week13-slides.html#exercise-1-electric-field",
    "title": "Meshgrid and Vector Fields",
    "section": "Exercise 1: Electric Field",
    "text": "Exercise 1: Electric Field\nPhysics: Point charge at origin creates radial electric field\n\\[\\vec{E}(\\vec{r}) = \\frac{q}{4\\pi\\epsilon_0} \\frac{\\vec{r}}{r^3}\\]\n\nIn 2D Cartesian coordinates:\n\\[E_x = \\frac{kq \\cdot x}{r^3}, \\quad E_y = \\frac{kq \\cdot y}{r^3}\\]\nwhere \\(r = \\sqrt{x^2 + y^2}\\)\n\n\nChallenge: Visualize this field!"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#the-plan-electric-field",
    "href": "lectures/lecture13/repetition-week13-slides.html#the-plan-electric-field",
    "title": "Meshgrid and Vector Fields",
    "section": "The Plan: Electric Field",
    "text": "The Plan: Electric Field\n\nCreate a grid of points in the x-y plane\nAt each point, calculate the distance \\(r\\) from origin\nCalculate the x and y components of \\(\\vec{E}\\)\nUse plt.quiver() to draw arrows\n\n\nKey function: plt.quiver(X, Y, Ex, Ey) - X, Y: positions of arrow tails - Ex, Ey: arrow directions (field components)"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#quiver-plots-concept",
    "href": "lectures/lecture13/repetition-week13-slides.html#quiver-plots-concept",
    "title": "Meshgrid and Vector Fields",
    "section": "Quiver Plots: Concept",
    "text": "Quiver Plots: Concept\nplt.quiver(X, Y, Ex, Ey)\n\nWhat it does: - At position (X[i,j], Y[i,j]) - Draw an arrow - Pointing in direction (Ex[i,j], Ey[i,j]) - Length proportional to magnitude\n\n\nPerfect for vector fields!"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#step-1-create-the-grid",
    "href": "lectures/lecture13/repetition-week13-slides.html#step-1-create-the-grid",
    "title": "Meshgrid and Vector Fields",
    "section": "Step 1: Create the Grid",
    "text": "Step 1: Create the Grid\n# Define coordinate ranges\nx = np.linspace(-2, 2, 20)\ny = np.linspace(-2, 2, 20)\n\n# Create meshgrid\nX, Y = np.meshgrid(x, y)\n\nResult: 20√ó20 grid of points covering the region -2 ‚â§ x ‚â§ 2, -2 ‚â§ y ‚â§ 2"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#step-2-calculate-distance",
    "href": "lectures/lecture13/repetition-week13-slides.html#step-2-calculate-distance",
    "title": "Meshgrid and Vector Fields",
    "section": "Step 2: Calculate Distance",
    "text": "Step 2: Calculate Distance\n# Distance from origin\nR = np.sqrt(X**2 + Y**2)\n\nProblem: What happens at the origin? Division by zero!\n\n\nSolution: Add a tiny number\nR = np.sqrt(X**2 + Y**2 + 1e-16)\n\n\nNow R is never exactly zero, avoiding singularities"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#step-3-calculate-field-components",
    "href": "lectures/lecture13/repetition-week13-slides.html#step-3-calculate-field-components",
    "title": "Meshgrid and Vector Fields",
    "section": "Step 3: Calculate Field Components",
    "text": "Step 3: Calculate Field Components\nFor a unit charge at origin:\n# Electric field components (simplified, k*q = 1)\nEx = X / R**3\nEy = Y / R**3\n\nWhy this works: - \\(\\vec{r} = (X, Y)\\) is the position vector - \\(r^3 = R^3\\) is the distance cubed - Field points radially outward: \\(\\vec{E} \\propto \\vec{r}/r^3\\)"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#step-4-plot-the-vector-field",
    "href": "lectures/lecture13/repetition-week13-slides.html#step-4-plot-the-vector-field",
    "title": "Meshgrid and Vector Fields",
    "section": "Step 4: Plot the Vector Field",
    "text": "Step 4: Plot the Vector Field\nplt.figure(figsize=(8, 8))\nplt.quiver(X, Y, Ex, Ey)\nplt.xlabel('x position (m)')\nplt.ylabel('y position (m)')\nplt.axis('equal')\nplt.show()\n\nResult: Arrows pointing radially outward from the origin!"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#complete-electric-field-code",
    "href": "lectures/lecture13/repetition-week13-slides.html#complete-electric-field-code",
    "title": "Meshgrid and Vector Fields",
    "section": "Complete Electric Field Code",
    "text": "Complete Electric Field Code\nx = np.linspace(-2, 2, 20)\ny = np.linspace(-2, 2, 20)\n\nX, Y = np.meshgrid(x, y)\n\nR = np.sqrt(X**2 + Y**2 + 1e-16)\n\nEx = X/R**3\nEy = Y/R**3\n\nplt.figure(figsize=(8, 8))\nplt.quiver(X, Y, Ex, Ey)\nplt.xlabel('x position (m)')\nplt.ylabel('y position (m)')\nplt.axis('equal')\nplt.show()"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#enhancing-the-visualization",
    "href": "lectures/lecture13/repetition-week13-slides.html#enhancing-the-visualization",
    "title": "Meshgrid and Vector Fields",
    "section": "Enhancing the Visualization",
    "text": "Enhancing the Visualization\nOptional: Add field magnitude as background\nE_mag = np.sqrt(Ex**2 + Ey**2)\n\nplt.figure(figsize=(8, 8))\nplt.contourf(X, Y, E_mag, levels=20, cmap='viridis')\nplt.colorbar(label='Field Magnitude (N/C)')\nplt.quiver(X, Y, Ex, Ey, color='white')\nplt.xlabel('x position (m)')\nplt.ylabel('y position (m)')\nplt.axis('equal')\n\nColor shows magnitude, arrows show direction!"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#your-turn-exercise-1",
    "href": "lectures/lecture13/repetition-week13-slides.html#your-turn-exercise-1",
    "title": "Meshgrid and Vector Fields",
    "section": "üéØ Your Turn: Exercise 1",
    "text": "üéØ Your Turn: Exercise 1\nOn the webpage, create the electric field visualization:\n\nCreate coordinate grid with np.linspace()\nMake meshgrid with np.meshgrid()\nCalculate distance R (avoid division by zero!)\nCalculate field components Ex, Ey\nPlot with plt.quiver()\n\n‚è±Ô∏è Time: 10 minutes\nüí° Hint: Remember to add 1e-16 to R to avoid singularity"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#exercise-2-standing-waves",
    "href": "lectures/lecture13/repetition-week13-slides.html#exercise-2-standing-waves",
    "title": "Meshgrid and Vector Fields",
    "section": "Exercise 2: Standing Waves",
    "text": "Exercise 2: Standing Waves\nPhysics: Vibrating membrane (drum head) has standing wave modes\n\\[\\psi(x,y) = \\sin(mx)\\cos(ny)\\]\n\nParameters: - \\((m, n)\\) ‚Äî mode numbers (integers) - Determine the pattern of nodes and antinodes\n\n\nExamples: - \\((1, 1)\\) ‚Äî Fundamental mode - \\((2, 3)\\) ‚Äî Higher harmonic with complex pattern"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#what-are-standing-waves",
    "href": "lectures/lecture13/repetition-week13-slides.html#what-are-standing-waves",
    "title": "Meshgrid and Vector Fields",
    "section": "What Are Standing Waves?",
    "text": "What Are Standing Waves?\nStanding wave: Combination of waves that doesn‚Äôt propagate\n\nNodes: Points that never move (œà = 0)\nAntinodes: Points with maximum amplitude\n\n\nMusical connection: - Different \\((m,n)\\) ‚Üí different notes - Higher modes ‚Üí higher pitches - Node patterns ‚Üí timbre (sound quality)"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#visualizing-2d-functions",
    "href": "lectures/lecture13/repetition-week13-slides.html#visualizing-2d-functions",
    "title": "Meshgrid and Vector Fields",
    "section": "Visualizing 2D Functions",
    "text": "Visualizing 2D Functions\nTwo main approaches:\n\nContour plot: View from above, color indicates value\n\nplt.contour() ‚Äî contour lines\nplt.contourf() ‚Äî filled contours\n\nSurface plot: 3D perspective\n\nax.plot_surface() ‚Äî 3D surface\n\n\n\nBest practice: Show both views!"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#step-1-create-fine-grid",
    "href": "lectures/lecture13/repetition-week13-slides.html#step-1-create-fine-grid",
    "title": "Meshgrid and Vector Fields",
    "section": "Step 1: Create Fine Grid",
    "text": "Step 1: Create Fine Grid\n# Need many points for smooth visualization\nx = np.linspace(-np.pi, np.pi, 100)\ny = np.linspace(-np.pi, np.pi, 100)\n\nX, Y = np.meshgrid(x, y)\n\nNote: More points (100) than vector field (20) for smooth contours"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#step-2-calculate-wave-pattern",
    "href": "lectures/lecture13/repetition-week13-slides.html#step-2-calculate-wave-pattern",
    "title": "Meshgrid and Vector Fields",
    "section": "Step 2: Calculate Wave Pattern",
    "text": "Step 2: Calculate Wave Pattern\n# Mode numbers\nm, n = 2, 3\n\n# Wave function\npsi = np.sin(m*X) * np.cos(n*Y)\n\nTry different modes: - (1, 1) ‚Äî Simple pattern - (2, 3) ‚Äî Complex pattern - (3, 3) ‚Äî Symmetric pattern"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#step-3-create-contour-plot",
    "href": "lectures/lecture13/repetition-week13-slides.html#step-3-create-contour-plot",
    "title": "Meshgrid and Vector Fields",
    "section": "Step 3: Create Contour Plot",
    "text": "Step 3: Create Contour Plot\nplt.figure(figsize=(8, 8))\nplt.contourf(X, Y, psi, levels=20, cmap='RdBu')\nplt.colorbar(label='Amplitude')\nplt.xlabel('x position')\nplt.ylabel('y position')\nplt.axis('equal')\nplt.show()\n\nColor scheme: - Red: Positive displacement - Blue: Negative displacement - White: Nodes (zero displacement)"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#step-4-add-3d-surface-plot",
    "href": "lectures/lecture13/repetition-week13-slides.html#step-4-add-3d-surface-plot",
    "title": "Meshgrid and Vector Fields",
    "section": "Step 4: Add 3D Surface Plot",
    "text": "Step 4: Add 3D Surface Plot\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(projection='3d')\n\nsurf = ax.plot_surface(X, Y, psi, cmap='RdBu')\nax.set_xlabel('x position')\nax.set_ylabel('y position')\nax.set_zlabel('Amplitude')\nplt.colorbar(surf)\n\n3D view shows: - Height = wave amplitude - Peaks and valleys clearly visible"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#complete-standing-wave-code",
    "href": "lectures/lecture13/repetition-week13-slides.html#complete-standing-wave-code",
    "title": "Meshgrid and Vector Fields",
    "section": "Complete Standing Wave Code",
    "text": "Complete Standing Wave Code\nx = np.linspace(-np.pi, np.pi, 100)\ny = np.linspace(-np.pi, np.pi, 100)\nX, Y = np.meshgrid(x, y)\n\nm, n = 2, 3\npsi = np.sin(m*X) * np.cos(n*Y)\n\nplt.figure(figsize=(16, 8))\n\n# 2D contour\nplt.subplot(121)\nplt.contourf(X, Y, psi, levels=20, cmap='RdBu')\nplt.colorbar(label='Amplitude')\nplt.xlabel('x position')\nplt.ylabel('y position')\nplt.axis('equal')\n\n# 3D surface\nax = plt.subplot(122, projection='3d')\nax.plot_surface(X, Y, psi, cmap='RdBu')\nax.set_xlabel('x position')\nax.set_ylabel('y position')\nax.set_zlabel('Amplitude')\n\nplt.tight_layout()"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#your-turn-exercise-2",
    "href": "lectures/lecture13/repetition-week13-slides.html#your-turn-exercise-2",
    "title": "Meshgrid and Vector Fields",
    "section": "üéØ Your Turn: Exercise 2",
    "text": "üéØ Your Turn: Exercise 2\nOn the webpage, visualize standing waves:\n\nCreate fine grid: 100 points in each direction\nChoose mode numbers (m, n)\nCalculate œà = sin(mx)¬∑cos(ny)\nCreate both 2D contour and 3D surface plots\n\n‚è±Ô∏è Time: 10 minutes\nüí° Bonus: Try different (m,n) values to see different patterns!"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#exercise-3-quantum-wave-packet",
    "href": "lectures/lecture13/repetition-week13-slides.html#exercise-3-quantum-wave-packet",
    "title": "Meshgrid and Vector Fields",
    "section": "Exercise 3: Quantum Wave Packet",
    "text": "Exercise 3: Quantum Wave Packet\nPhysics: Localized quantum particle described by Gaussian wave function\n\\[\\psi(x,y) = A\\exp\\left(-\\frac{(x-x_0)^2 + (y-y_0)^2}{2\\sigma^2}\\right)\\]\n\nParameters: - \\((x_0, y_0)\\) ‚Äî center position (where particle is likely) - \\(\\sigma\\) ‚Äî width (uncertainty in position) - \\(A\\) ‚Äî amplitude (normalization)"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#quantum-interpretation",
    "href": "lectures/lecture13/repetition-week13-slides.html#quantum-interpretation",
    "title": "Meshgrid and Vector Fields",
    "section": "Quantum Interpretation",
    "text": "Quantum Interpretation\nWave function œà: Describes quantum state\nProbability density: \\(P(x,y) = |\\psi(x,y)|^2\\)\n\nPhysical meaning: - \\(P(x,y)\\) gives probability of finding particle at (x,y) - Larger \\(\\sigma\\) ‚Üí more uncertain position - Peak at \\((x_0, y_0)\\) ‚Üí most likely location\n\n\nHeisenberg uncertainty principle: Can‚Äôt know position exactly!"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#the-gaussian-function",
    "href": "lectures/lecture13/repetition-week13-slides.html#the-gaussian-function",
    "title": "Meshgrid and Vector Fields",
    "section": "The Gaussian Function",
    "text": "The Gaussian Function\nWhy Gaussian?\n\nSmooth ‚Äî No sharp edges\nLocalized ‚Äî Falls off exponentially\nWell-behaved ‚Äî Easy to normalize\nNatural ‚Äî Appears everywhere in physics\nMinimum uncertainty ‚Äî Saturates Heisenberg limit"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#step-1-create-the-grid-1",
    "href": "lectures/lecture13/repetition-week13-slides.html#step-1-create-the-grid-1",
    "title": "Meshgrid and Vector Fields",
    "section": "Step 1: Create the Grid",
    "text": "Step 1: Create the Grid\nx = np.linspace(-5, 5, 100)\ny = np.linspace(-5, 5, 100)\n\nX, Y = np.meshgrid(x, y)\n\nRange from -5 to 5 to capture most of the wave packet"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#step-2-set-parameters",
    "href": "lectures/lecture13/repetition-week13-slides.html#step-2-set-parameters",
    "title": "Meshgrid and Vector Fields",
    "section": "Step 2: Set Parameters",
    "text": "Step 2: Set Parameters\nx0, y0 = 1.0, 0.0  # Center position\nsigma = 1.0        # Width (standard deviation)\nA = 1.0            # Amplitude\n\nTry different values: - Move center: change (x0, y0) - Widen packet: increase œÉ - Narrow packet: decrease œÉ"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#step-3-calculate-wave-function",
    "href": "lectures/lecture13/repetition-week13-slides.html#step-3-calculate-wave-function",
    "title": "Meshgrid and Vector Fields",
    "section": "Step 3: Calculate Wave Function",
    "text": "Step 3: Calculate Wave Function\n# Wave function\npsi = A * np.exp(-((X-x0)**2 + (Y-y0)**2)/(2*sigma**2))\n\n# Probability density\nprob_density = np.abs(psi)**2\n\nFor real œà: \\(|\\psi|^2 = \\psi^2\\)\nFor complex œà: \\(|\\psi|^2 = \\psi^* \\psi\\) (use np.abs())"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#step-4-visualize-three-ways",
    "href": "lectures/lecture13/repetition-week13-slides.html#step-4-visualize-three-ways",
    "title": "Meshgrid and Vector Fields",
    "section": "Step 4: Visualize Three Ways",
    "text": "Step 4: Visualize Three Ways\n\nWave function amplitude ‚Äî œà itself\nProbability density ‚Äî |œà|¬≤\n3D surface ‚Äî See the Gaussian peak\n\n\nplt.figure(figsize=(15, 5))\n\nplt.subplot(131)\nplt.contourf(X, Y, psi)\nplt.title('Wave Function')\n\nplt.subplot(132)\nplt.contourf(X, Y, prob_density)\nplt.title('Probability Density')\n\nax = plt.subplot(133, projection='3d')\nax.plot_surface(X, Y, prob_density)"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#complete-wave-packet-code",
    "href": "lectures/lecture13/repetition-week13-slides.html#complete-wave-packet-code",
    "title": "Meshgrid and Vector Fields",
    "section": "Complete Wave Packet Code",
    "text": "Complete Wave Packet Code\nx = np.linspace(-5, 5, 100)\ny = np.linspace(-5, 5, 100)\nX, Y = np.meshgrid(x, y)\n\nx0, y0 = 1.0, 0.0\nsigma = 1.0\nA = 1.0\n\npsi = A * np.exp(-((X-x0)**2 + (Y-y0)**2)/(2*sigma**2))\nprob_density = np.abs(psi)**2\n\nplt.figure(figsize=(15, 5))\n\nplt.subplot(131)\nplt.contourf(X, Y, psi, levels=20, cmap='RdBu')\nplt.xlabel('x'); plt.ylabel('y')\nplt.title('Wave Function')\nplt.axis('equal')\n\nplt.subplot(132)\nplt.contourf(X, Y, prob_density, levels=20, cmap='viridis')\nplt.xlabel('x'); plt.ylabel('y')\nplt.title('Probability Density')\nplt.axis('equal')\n\nax = plt.subplot(133, projection='3d')\nax.plot_surface(X, Y, prob_density, cmap='viridis')\nax.set_xlabel('x'); ax.set_ylabel('y')\nax.set_zlabel('Probability')"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#your-turn-exercise-3",
    "href": "lectures/lecture13/repetition-week13-slides.html#your-turn-exercise-3",
    "title": "Meshgrid and Vector Fields",
    "section": "üéØ Your Turn: Exercise 3",
    "text": "üéØ Your Turn: Exercise 3\nOn the webpage, create the wave packet visualization:\n\nCreate coordinate grid\nSet parameters: x0=1, y0=0, œÉ=1, A=1\nCalculate œà using Gaussian formula\nCalculate probability density |œà|¬≤\nCreate three plots: œà, |œà|¬≤, and 3D surface\n\n‚è±Ô∏è Time: 12 minutes\nüí° Experiment with different œÉ values!"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#comparison-the-three-exercises",
    "href": "lectures/lecture13/repetition-week13-slides.html#comparison-the-three-exercises",
    "title": "Meshgrid and Vector Fields",
    "section": "Comparison: The Three Exercises",
    "text": "Comparison: The Three Exercises\n\n\n\nExercise\nType\nVisualization\nPhysics\n\n\n\n\nElectric field\nVector field\nquiver()\nElectromagnetism\n\n\nStanding wave\nScalar field\ncontourf()\nOscillations\n\n\nWave packet\nProbability\nplot_surface()\nQuantum mechanics\n\n\n\n\nCommon tool: Meshgrid for all three!"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#key-matplotlib-functions",
    "href": "lectures/lecture13/repetition-week13-slides.html#key-matplotlib-functions",
    "title": "Meshgrid and Vector Fields",
    "section": "Key Matplotlib Functions",
    "text": "Key Matplotlib Functions\n\n\n\nFunction\nUse\nExample\n\n\n\n\nnp.meshgrid()\nCreate 2D grids\nFundamental tool\n\n\nplt.quiver()\nVector fields\nElectric/magnetic fields\n\n\nplt.contour()\nContour lines\nTopographic maps\n\n\nplt.contourf()\nFilled contours\nScalar fields\n\n\nax.plot_surface()\n3D surfaces\nWave functions\n\n\nplt.colorbar()\nAdd color scale\nInterpret values"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#colormap-choices-matter",
    "href": "lectures/lecture13/repetition-week13-slides.html#colormap-choices-matter",
    "title": "Meshgrid and Vector Fields",
    "section": "Colormap Choices Matter",
    "text": "Colormap Choices Matter\nCommon colormaps:\n\n'viridis' ‚Äî Perceptually uniform, good default\n'RdBu' ‚Äî Red-blue diverging, good for ¬±values\n'hot' ‚Äî Heat map\n'coolwarm' ‚Äî Another diverging option\n\n\nRules of thumb: - Diverging (RdBu): Data with positive and negative values - Sequential (viridis): Data with one polarity - Always add colorbar!"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#d-plotting-projection",
    "href": "lectures/lecture13/repetition-week13-slides.html#d-plotting-projection",
    "title": "Meshgrid and Vector Fields",
    "section": "3D Plotting: Projection",
    "text": "3D Plotting: Projection\nTo create 3D plots:\nfig = plt.figure()\nax = fig.add_subplot(projection='3d')\n\n# or\n\nax = plt.subplot(111, projection='3d')\n\nThen use: - ax.plot_surface() ‚Äî Surface plots - ax.plot_wireframe() ‚Äî Wireframe - ax.contour3D() ‚Äî 3D contours"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#common-pitfalls",
    "href": "lectures/lecture13/repetition-week13-slides.html#common-pitfalls",
    "title": "Meshgrid and Vector Fields",
    "section": "Common Pitfalls",
    "text": "Common Pitfalls\n\n\n\n\n\n\n\n\nProblem\nCause\nSolution\n\n\n\n\nArrows too small/large\nScale issue\nUse scale parameter in quiver\n\n\nDivision by zero\nSingularities\nAdd 1e-16 to denominator\n\n\nBlocky contours\nToo few levels\nIncrease levels= parameter\n\n\nWrong grid size\nX.shape ‚â† Y.shape\nCheck meshgrid output\n\n\nDistorted plots\nUnequal axes\nUse plt.axis('equal')"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#debugging-meshgrid-issues",
    "href": "lectures/lecture13/repetition-week13-slides.html#debugging-meshgrid-issues",
    "title": "Meshgrid and Vector Fields",
    "section": "Debugging Meshgrid Issues",
    "text": "Debugging Meshgrid Issues\nCheck array shapes:\nprint(X.shape, Y.shape)  # Should be the same!\nprint(psi.shape)         # Should match X and Y\n\nVisualize the grid:\nplt.plot(X, Y, 'k.')  # Plot all grid points\n\n\nCheck a single point:\nprint(f\"Point: ({X[0,0]}, {Y[0,0]})\")"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#performance-tips",
    "href": "lectures/lecture13/repetition-week13-slides.html#performance-tips",
    "title": "Meshgrid and Vector Fields",
    "section": "Performance Tips",
    "text": "Performance Tips\nFor large grids:\n\nUse appropriate resolution (balance quality vs.¬†speed)\nAvoid explicit loops ‚Äî use vectorized operations\nFor animations, reuse figure objects\nUse sparse sampling for quiver plots (every Nth point)\n\n\n# Sample every 2nd point for faster quiver\nplt.quiver(X[::2, ::2], Y[::2, ::2], \n           Ex[::2, ::2], Ey[::2, ::2])"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#physical-insights-from-visualization",
    "href": "lectures/lecture13/repetition-week13-slides.html#physical-insights-from-visualization",
    "title": "Meshgrid and Vector Fields",
    "section": "Physical Insights from Visualization",
    "text": "Physical Insights from Visualization\nVector fields reveal: - Direction of forces/flows - Strength variations - Source/sink locations - Symmetries\n\nScalar fields reveal: - Gradients (steep vs.¬†gentle changes) - Extrema (maxima and minima) - Patterns and symmetries - Spatial frequencies"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#extensions-multiple-charges",
    "href": "lectures/lecture13/repetition-week13-slides.html#extensions-multiple-charges",
    "title": "Meshgrid and Vector Fields",
    "section": "Extensions: Multiple Charges",
    "text": "Extensions: Multiple Charges\nExercise 1 extension: Add a second charge\n# Charge 1 at (-1, 0)\nR1 = np.sqrt((X+1)**2 + Y**2 + 1e-16)\nEx1 = (X+1)/R1**3\nEy1 = Y/R1**3\n\n# Charge 2 at (1, 0)\nR2 = np.sqrt((X-1)**2 + Y**2 + 1e-16)\nEx2 = (X-1)/R2**3\nEy2 = Y/R2**3\n\n# Total field (superposition)\nEx = Ex1 + Ex2\nEy = Ey1 + Ey2"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#extensions-time-evolution",
    "href": "lectures/lecture13/repetition-week13-slides.html#extensions-time-evolution",
    "title": "Meshgrid and Vector Fields",
    "section": "Extensions: Time Evolution",
    "text": "Extensions: Time Evolution\nMake waves move:\nfor t in np.linspace(0, 2*np.pi, 50):\n    psi = np.sin(m*X - omega*t) * np.cos(n*Y)\n    plt.contourf(X, Y, psi)\n    plt.pause(0.1)\n    plt.clf()\n\nCreates animation of traveling wave!"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#extensions-complex-wave-functions",
    "href": "lectures/lecture13/repetition-week13-slides.html#extensions-complex-wave-functions",
    "title": "Meshgrid and Vector Fields",
    "section": "Extensions: Complex Wave Functions",
    "text": "Extensions: Complex Wave Functions\nAdd phase:\nk = 2  # Wave number\n\n# Complex wave function\npsi = A * np.exp(-((X-x0)**2 + (Y-y0)**2)/(2*sigma**2)) \\\n        * np.exp(1j * k * X)\n\n# Real part\npsi_real = np.real(psi)\n\n# Probability still real!\nprob = np.abs(psi)**2"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#real-world-applications",
    "href": "lectures/lecture13/repetition-week13-slides.html#real-world-applications",
    "title": "Meshgrid and Vector Fields",
    "section": "Real-World Applications",
    "text": "Real-World Applications\n\nEngineering: Stress fields in materials, heat distribution\nMeteorology: Wind velocity fields, pressure maps\nOceanography: Current patterns, temperature distributions\nElectromagnetism: Field patterns around antennas\nQuantum chemistry: Molecular orbitals\nComputer graphics: Procedural textures, terrain generation"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#summary-the-workflow",
    "href": "lectures/lecture13/repetition-week13-slides.html#summary-the-workflow",
    "title": "Meshgrid and Vector Fields",
    "section": "Summary: The Workflow",
    "text": "Summary: The Workflow\n\nDefine coordinates ‚Äî x = np.linspace(...)\nCreate meshgrid ‚Äî X, Y = np.meshgrid(x, y)\nCalculate values ‚Äî Evaluate function at all points\nChoose visualization:\n\nVector field ‚Üí quiver()\nScalar field ‚Üí contourf()\n3D view ‚Üí plot_surface()\n\nAdd labels and colorbar\nInterpret the physics!"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#key-concepts-to-remember",
    "href": "lectures/lecture13/repetition-week13-slides.html#key-concepts-to-remember",
    "title": "Meshgrid and Vector Fields",
    "section": "Key Concepts to Remember",
    "text": "Key Concepts to Remember\n\n\n\nConcept\nKey Point\n\n\n\n\nMeshgrid\nCreates all combinations of x,y coordinates\n\n\nVector fields\nUse quiver plots for direction+magnitude\n\n\nScalar fields\nUse contour plots for values\n\n\n3D surfaces\nUse plot_surface for terrain-like data\n\n\nColormaps\nChoose appropriate for your data type\n\n\nResolution\nBalance between quality and speed"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#from-1d-to-2d-to-3d",
    "href": "lectures/lecture13/repetition-week13-slides.html#from-1d-to-2d-to-3d",
    "title": "Meshgrid and Vector Fields",
    "section": "From 1D to 2D to 3D",
    "text": "From 1D to 2D to 3D\nProgression in complexity:\n\n1D plots: plt.plot(x, y) ‚Äî Lines\n2D plots: plt.contourf(X, Y, Z) ‚Äî Fields\n3D plots: ax.plot_surface(X, Y, Z) ‚Äî Surfaces\n4D plots: Color as 4th dimension on 3D surface\n\n\nHigher dimensions ‚Üí Need creative visualization!"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#practice-recommendations",
    "href": "lectures/lecture13/repetition-week13-slides.html#practice-recommendations",
    "title": "Meshgrid and Vector Fields",
    "section": "Practice Recommendations",
    "text": "Practice Recommendations\nAfter completing the exercises:\n\nModify parameters and observe changes\nCombine techniques (quiver + contourf)\nAdd multiple sources/sinks\nCreate animations\nApply to your own physics problems"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#common-patterns-youll-use-often",
    "href": "lectures/lecture13/repetition-week13-slides.html#common-patterns-youll-use-often",
    "title": "Meshgrid and Vector Fields",
    "section": "Common Patterns You‚Äôll Use Often",
    "text": "Common Patterns You‚Äôll Use Often\nPattern 1: Scalar field from function\nX, Y = np.meshgrid(x, y)\nZ = function(X, Y)\nplt.contourf(X, Y, Z)\n\nPattern 2: Vector field from gradients\nX, Y = np.meshgrid(x, y)\nU, V = gradient_x(X, Y), gradient_y(X, Y)\nplt.quiver(X, Y, U, V)"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#connecting-to-previous-topics",
    "href": "lectures/lecture13/repetition-week13-slides.html#connecting-to-previous-topics",
    "title": "Meshgrid and Vector Fields",
    "section": "Connecting to Previous Topics",
    "text": "Connecting to Previous Topics\nWeek 4-5: Arrays and operations - Meshgrid creates 2D arrays - Broadcasting enables vectorized calculations\nWeek 8-9: ODEs and oscillations - Standing waves are solutions to wave equation - Time evolution connects to PDEs\nWeek 10: Diffusion equation - Similar visualization techniques - Scalar field evolution in time"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#looking-ahead",
    "href": "lectures/lecture13/repetition-week13-slides.html#looking-ahead",
    "title": "Meshgrid and Vector Fields",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nWith these tools, you can visualize:\n\nElectromagnetic phenomena (Maxwell‚Äôs equations)\nFluid dynamics (velocity fields)\nQuantum mechanics (wave functions, orbitals)\nRelativity (curved spacetime)\nStatistical mechanics (phase space)\n\n\nVisualization is understanding!"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#tips-for-your-own-projects",
    "href": "lectures/lecture13/repetition-week13-slides.html#tips-for-your-own-projects",
    "title": "Meshgrid and Vector Fields",
    "section": "Tips for Your Own Projects",
    "text": "Tips for Your Own Projects\n\nStart simple ‚Äî Get basic plot working first\nCheck dimensions ‚Äî Print shapes to debug\nUse appropriate resolution ‚Äî Not too coarse, not too fine\nLabel everything ‚Äî Axes, colorbar, title\nChoose colors wisely ‚Äî Accessible colormaps\nSave figures ‚Äî plt.savefig('filename.png', dpi=300)"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#resources-for-further-learning",
    "href": "lectures/lecture13/repetition-week13-slides.html#resources-for-further-learning",
    "title": "Meshgrid and Vector Fields",
    "section": "Resources for Further Learning",
    "text": "Resources for Further Learning\nDocumentation: - Matplotlib Gallery ‚Äî Examples - NumPy Broadcasting ‚Äî Array operations\nBooks: - ‚ÄúPython for Data Analysis‚Äù ‚Äî McKinney - ‚ÄúEffective Computation in Physics‚Äù ‚Äî Scopatz & Huff\nPractice: - Implement physics from your other courses - Explore different visualization styles"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#quick-reference-card",
    "href": "lectures/lecture13/repetition-week13-slides.html#quick-reference-card",
    "title": "Meshgrid and Vector Fields",
    "section": "Quick Reference Card",
    "text": "Quick Reference Card\n# Setup\nX, Y = np.meshgrid(x, y)\n\n# Vector field\nplt.quiver(X, Y, U, V)\n\n# Scalar field (2D)\nplt.contourf(X, Y, Z, levels=20)\nplt.colorbar()\n\n# Scalar field (3D)\nax = plt.subplot(projection='3d')\nax.plot_surface(X, Y, Z, cmap='viridis')\n\n# Essential\nplt.xlabel('x'); plt.ylabel('y')\nplt.axis('equal')\nplt.tight_layout()"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#what-we-learned-today",
    "href": "lectures/lecture13/repetition-week13-slides.html#what-we-learned-today",
    "title": "Meshgrid and Vector Fields",
    "section": "What We Learned Today",
    "text": "What We Learned Today\n\nMeshgrid ‚Äî Creating coordinate grids for 2D functions\nQuiver plots ‚Äî Visualizing vector fields\nContour plots ‚Äî Visualizing scalar fields\n3D surfaces ‚Äî Alternative perspective on data\nPhysics applications ‚Äî EM fields, waves, quantum mechanics\n\n\nKey insight: Same tools apply across all physics domains!"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#exercise-summary",
    "href": "lectures/lecture13/repetition-week13-slides.html#exercise-summary",
    "title": "Meshgrid and Vector Fields",
    "section": "Exercise Summary",
    "text": "Exercise Summary\n\n\n\nExercise\nTechnique\nDifficulty\nPhysics\n\n\n\n\nElectric field\nQuiver plot\n‚≠ê‚≠ê\nCoulomb‚Äôs law\n\n\nStanding waves\nContour + 3D\n‚≠ê‚≠ê\nWave equation\n\n\nWave packet\nMultiple views\n‚≠ê‚≠ê‚≠ê\nQuantum mechanics\n\n\n\n\nAll three use meshgrid as foundation!"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#questions",
    "href": "lectures/lecture13/repetition-week13-slides.html#questions",
    "title": "Meshgrid and Vector Fields",
    "section": "Questions?",
    "text": "Questions?\nThe complete exercises with solutions are available online.\n\nRemember: - X, Y = np.meshgrid(x, y) ‚Äî Creates the grid - plt.quiver(X, Y, U, V) ‚Äî Vectors - plt.contourf(X, Y, Z) ‚Äî Scalar fields - ax.plot_surface(X, Y, Z) ‚Äî 3D view\n\n\nNow practice on the webpage! üöÄ"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#bonus-advanced-techniques",
    "href": "lectures/lecture13/repetition-week13-slides.html#bonus-advanced-techniques",
    "title": "Meshgrid and Vector Fields",
    "section": "Bonus: Advanced Techniques",
    "text": "Bonus: Advanced Techniques\nStreamlines (follow field lines):\nplt.streamplot(X, Y, U, V)\nVector field magnitude + direction:\nmagnitude = np.sqrt(U**2 + V**2)\nplt.quiver(X, Y, U, V, magnitude)\n3D quiver:\nax.quiver(X, Y, Z, U, V, W)"
  },
  {
    "objectID": "lectures/lecture13/repetition-week13-slides.html#final-challenge",
    "href": "lectures/lecture13/repetition-week13-slides.html#final-challenge",
    "title": "Meshgrid and Vector Fields",
    "section": "Final Challenge",
    "text": "Final Challenge\nCan you combine all three techniques?\n\nElectric field from two charges (quiver)\nField magnitude as contour background\n3D surface of potential energy\n\n\nThis creates a comprehensive field visualization!\nGood luck with the exercises! üéì"
  },
  {
    "objectID": "lectures/lecture14/huygens-principle.html",
    "href": "lectures/lecture14/huygens-principle.html",
    "title": "Huygens principle",
    "section": "",
    "text": "Huygens‚Äô principle is a theorem from wave optics that states that each point in space experiencing an electromagnetic wave acts as a source of spherical waves. This means that any wave can be expanded into a superposition of spherical waves, which is fundamental to phenomena like Mie scattering. While the classical understanding is that accelerated charges are the source of electromagnetic waves, Huygens‚Äô principle provides a powerful mathematical framework for describing wave propagation and diffraction effects."
  },
  {
    "objectID": "lectures/lecture14/huygens-principle.html#diffraction-pattern-of-a-single-slit",
    "href": "lectures/lecture14/huygens-principle.html#diffraction-pattern-of-a-single-slit",
    "title": "Huygens principle",
    "section": "Diffraction pattern of a single slit",
    "text": "Diffraction pattern of a single slit\nLet‚Äôs use Huygens principle to understand how waves behave when they pass through a small opening like a slit. We can do this by treating many points along the slit as sources of waves, and seeing how these waves combine. This will help us understand diffraction - what happens when waves bend around edges. Remember the function we wrote down the last lecture, i.e.\n\\[\\begin{equation}\nE=\\frac{E_{0}}{|\\vec{r}-\\vec{r}_{0}|}e^{i k|\\vec{r}-\\vec{r}_{0}|} e^{-i\\omega t}\n\\end{equation}\\]\nthat describes the spherical wave. We can use this as a python function\n\n\n\n\n\n\nto calculate the electric field at a point in space. Let‚Äôs show first of all that we can reconstruct a plane wave from many spherical waves. We can do this by summing up many spherical waves with the same wavevector \\(\\vec{k}\\) but different positions \\(\\vec{r}_{0}\\). The code below does this for a plane wave propagating in the z-direction.\n\n\n\n\n\n\nThe result nicely shows the emergence of a plane wave from a sum of spherical waves. The intensity pattern is almost constant in the x-z plane, which is what we expect from a plane wave. The samll deviations are the result of the limited number of sources we used to sum up the spherical waves.\nWe now want to do the same but only over a limited range of x. This is what we would do for a single slit. The code below does this for a single slit of width \\(d=2\\) ¬µm. The next cell defines the space for our calculation again. The value of \\(d\\) denotes the slit width, which we want to vary to see the effect of changing slit width vs.¬†wavelength, which we chose to be \\(\\lambda=532\\) nm.\n\n\n\n\n\n\nThe next cell sums up the electric field of 200 spherical waves in the x-z plane, similar to the plane_wave_sum() function above but limited to sources within the slit width, such that we can plot the intensity or the field in space. Like the plane wave example, this demonstrates how multiple spherical waves combine, but now with sources constrained to a finite region.\n\n\n\n\n\n\nLet us plot the wavefronts and the intensity pattern in space. As the intensity decays strongly with distance from the slit, we do that by taking the log of the intensity. The left plot shows the real part of the field, revealing the wave fronts as they emerge from the slit and propagate outward. The right plot shows the logarithm of the intensity pattern, which helps visualize how the light spreads out as it diffracts through the slit opening. The wave fronts start planar at the slit but gradually curve outward, while the intensity pattern shows characteristic diffraction fringes."
  },
  {
    "objectID": "lectures/lecture14/huygens-principle.html#farfield-vs.-nearfield",
    "href": "lectures/lecture14/huygens-principle.html#farfield-vs.-nearfield",
    "title": "Huygens principle",
    "section": "Farfield vs.¬†nearfield",
    "text": "Farfield vs.¬†nearfield\nWhen light passes through a slit, the pattern of light we observe depends on how far away we measure it from the slit. We can divide this into two regions: the ‚Äúnear field‚Äù (at distance \\(r \\sim \\lambda\\)) and the ‚Äúfar field‚Äù (at distance \\(r \\gg \\lambda\\)). In the near field, which is typically within a few wavelengths \\(\\lambda\\) from the slit, the light pattern closely resembles the shape of the slit itself. However, in the far field, which is at distances \\(r \\gg \\lambda\\), the light spreads out significantly and creates distinctive patterns of bright and dark bands. This spreading out of light is called diffraction. To demonstrate this difference, let‚Äôs look at two distances: a near field measurement at \\(r = 1\\) ¬µm from the slit, and a far field measurement at \\(r = 100\\) ¬µm away. These measurements will show how dramatically different the light patterns can be in these two regions.\n\n\n\n\n\n\nThe two plots below show the drastic difference between the diffraction pattern in the near field and the far field. The near field resembles indeed the shadow picture, while the far field intensity pattern is considerable wider than the slit. This even becomes worse, if we make the slit narrower."
  },
  {
    "objectID": "lectures/lecture14/huygens-principle.html#comparison-to-the-analytical-solution",
    "href": "lectures/lecture14/huygens-principle.html#comparison-to-the-analytical-solution",
    "title": "Huygens principle",
    "section": "Comparison to the analytical solution",
    "text": "Comparison to the analytical solution\nNow that we understand how to calculate the diffraction pattern by adding up many Huygens sources (those spherical waves we created above), we can compare this to what physicists have worked out mathematically. When physicists solve this problem on paper, they do essentially the same thing we did in our code - they add up the contributions from many points along the slit, each acting as a source of waves. After doing all the math (which involves some complex calculus we won‚Äôt worry about now), they get a relatively simple formula for the intensity pattern far from the slit:\n\\[\\begin{equation}\nI=I_{0}\\left (\\frac{\\sin(\\delta)}{\\delta}\\right )^2\n\\end{equation}\\]\nwhere \\[\\begin{equation}\n\\delta=\\frac{\\pi d}{\\lambda}\\sin(\\theta)\n\\end{equation}\\]\nHere, \\(I_0\\) is just the maximum intensity, \\(d\\) is the width of our slit, \\(\\lambda\\) is the wavelength of light we‚Äôre using, and \\(\\theta\\) is the angle away from the center of the pattern (imagine drawing a line from the slit to any point on our screen - \\(\\theta\\) is the angle between this line and the straight-ahead direction). Let‚Äôs see how well this mathematical formula matches up with our numerical calculation where we added up all those Huygens sources one by one.\n\n\n\n\n\n\n\n\n\n\n\n\nThe plot below compares two ways of calculating the same thing: the black dotted line shows our numerical simulation using 200 point sources of waves, while the solid black line shows the mathematical formula that physicists derived. As you can see, they match quite well! This tells us that our computer simulation using Huygens‚Äô principle (adding up lots of wave sources) gives nearly the same result as the mathematical solution.\nAn interesting question to consider is: how many wave sources do we need to get an accurate result? Right now we‚Äôre using 200 sources spread across the width of the slit, but would 100 be enough? What about 1000? You can modify the code above to try different numbers of sources and see how it affects the accuracy of the simulation compared to the mathematical solution. This helps us understand how many ‚Äúpoints‚Äù of light we need to consider to get a good approximation of reality.\n\n\n\n\n\n\nLet‚Äôs explore how changing different parameters affects our diffraction pattern! Two key things we can adjust are:\n\nThe wavelength of light (Œª) - try using red light (650nm) versus blue light (450nm)\nThe width of the slit (d) - what happens when you make it wider or narrower?\n\nYou can modify these values in the code above and observe how the pattern changes. What patterns do you notice? Does the diffraction spread out more or less when you use longer wavelengths? What about with wider slits?\nAn even more interesting case is when we have multiple slits side by side - this is called a diffraction grating. Diffraction gratings are used in many real-world applications, from spectrometers that analyze starlight to the rainbow patterns you see on DVDs and CDs.\nLet‚Äôs look at what happens with 10 slits in a row, each separated by a distance D. The mathematical formula for this gets a bit more complicated, but it‚Äôs just combining two effects:\n\nThe single-slit diffraction pattern we saw before (the sin(Œ¥)/Œ¥ term)\nA new term that accounts for how the waves from multiple slits interfere (the sin(NŒ≥)/sin(Œ≥) term)\n\nHere‚Äôs the full formula for the intensity pattern:\n\\[\\begin{equation}\nI=I_{0}\\left (\\frac{\\sin(\\delta)}{\\delta}\\right )^2\\left (\\frac{\\sin(N\\gamma)}{\\sin(\\gamma)}\\right )^2\n\\end{equation}\\]\nwhere\n\\[\\begin{equation}\n  \\gamma=\\frac{\\pi D}{\\lambda}\\sin(\\theta)\n\\end{equation}\\]\nN is the number of slits (10 in our case), D is the distance between slits, and the other terms are the same as before.\nTry modifying the code to simulate this multiple-slit case! Some questions to consider:\n\nWhat happens when you change the spacing D between slits?\nHow does the pattern change if you use more or fewer slits?\nCan you explain why a CD or DVD creates rainbow patterns in sunlight based on what you‚Äôve learned about diffraction gratings?"
  },
  {
    "objectID": "lectures/lecture14/huygens-principle.html#focus-pattern-of-a-spherical-mirror",
    "href": "lectures/lecture14/huygens-principle.html#focus-pattern-of-a-spherical-mirror",
    "title": "Huygens principle",
    "section": "Focus pattern of a spherical mirror",
    "text": "Focus pattern of a spherical mirror\nHuygens‚Äô principle can also be used to understand how light reflects off a spherical mirror. When light hits a mirror, it reflects off at an angle equal to the angle of incidence. This means that the light rays all converge at a single point called the focal point \\(f\\). We can use Huygens‚Äô principle to understand how this happens by treating each point on the mirror as a source of waves. The code below calculates the electric field at a point in space due to a spherical mirror with radius of curvature \\(R = 10\\) ¬µm.\n\n\n\n\n\n\nThe two images show the field and the intensity pattern of the spherical mirror. The field plot shows the wavefronts converging at the focal point, while the intensity plot shows the bright spot at the focal point where the light rays all converge. The spot in the center is the focal point. The intensity pattern is called the point-spread function of the mirror, and it tells us how light from a point source will spread out after reflecting off the mirror."
  },
  {
    "objectID": "lectures/lecture15/ode-review.html",
    "href": "lectures/lecture15/ode-review.html",
    "title": "Repetition Differential Equations",
    "section": "",
    "text": "In today‚Äôs lecture, we will review and expand upon the numerical methods for solving differential equations that you‚Äôve encountered in your mathematics courses. This is particularly important for physics students, as differential equations are the language in which many physical laws are written. Whether we‚Äôre dealing with \\(F = ma\\) in classical mechanics, Maxwell‚Äôs equations in electromagnetism, or Schr√∂dinger‚Äôs equation in quantum mechanics, the ability to solve differential equations is essential.\nWhile analytical solutions are elegant and preferred when available, many real-world physical problems require numerical approaches. We will focus on numerical methods that you can implement yourself, starting with the simplest approaches and building up to more sophisticated techniques.\nTo begin this review, we will first have a look back at the Taylor expansion of a function \\(f(t)\\) around a point \\(t_0\\):\n\\[\nf(t) = f(t_{0}) + (t - t_0) f^{\\prime}(t_0) + \\frac{(t - t_0)^2}{2!} f^{\\prime\\prime}(t_0) + \\frac{(t - t_0)^3}{3!} f^{(3)}(t_0) + \\ldots\n\\]\nwhich tells us that the function \\(f(t)\\) can be approximated by a polynomial of increasing order around the point \\(t_0\\). The first term is the value of the function at \\(t_0\\), the second term is the slope of the function at \\(t_0\\), the third term is the curvature of the function at \\(t_0\\), and so on.\nThe Taylor expansion is the basis for many numerical methods for solving differential equations. The simplest of these methods is the Euler method, which approximates the function by a straight line. We will start with the Euler method and then move on to more advanced techniques like the improved Euler method."
  },
  {
    "objectID": "lectures/lecture15/ode-review.html#euler-method",
    "href": "lectures/lecture15/ode-review.html#euler-method",
    "title": "Repetition Differential Equations",
    "section": "Euler Method",
    "text": "Euler Method\nIf we truncate the series at the first term, we get the linear approximation of the function:\n\\[\nf(t) \\approx f(t_{0}) + (t - t_0) f^{\\prime}(t)= f(t_{0}) + \\Delta t f^{\\prime}(t)\n\\]\nThis is the equation of a straight line with slope \\(f^{\\prime}(t)\\) and intercept \\(f(t_0)\\) and we abbreviated \\(\\Delta t = t - t_0\\).\n\nEuler Method for Radioactive Decay\nWe would like to try the method with some simple first order problem, which is the radioactive decay. The decay of a radioactive nucleus is described by the first-order differential equation:\n\\[\n\\frac{dN(t)}{dt} = -k N(t)\n\\]\nwhere \\(k\\) is a decay constant. If \\(N(t)\\) is the number of radioactive nuclei at time \\(t\\), the equation tells us that the rate of decay is proportional to the number of nuclei present and some rate constant \\(k\\). The solution to this differential equation is:\n\\[\nN(t) = N(0) e^{-kt}\n\\]\nwhere \\(N(0)\\) is the number of nuclei at time \\(t=0\\). We therefore need to know some initial condition, which we define as \\(N(0) = 100\\). Comparing to our Taylor expansion, we can see that \\(N=f(t)\\) and the slope of the function is \\(f^{\\prime}(t) = -kN(t)\\). Further we have the value of the function at \\(t_0 = 0\\). We can now use the linear approximation to solve the ODE numerically. Inserting the values into the linear approximation, we get:\n\\[\nN(t + \\Delta t) = N(t) + \\Delta t f^{\\prime}(t)\n\\]\nWe can now implement the Euler method to solve the ODE numerically.\nExercise 1\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution Radioactive Decay\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImproved Euler Method\nWhile the above code is what we did fo the simples numerical integration, you can now modify the code inside the euler function to implement any improved Euler method. An improved Euler method (also called Heun‚Äôs method) is based on averaging the slopes at two points. To understand why this works better than the simple Euler method, let‚Äôs look at the Taylor expansion again around \\(t_0\\):\n\\[\nf(t_0 + \\Delta t) \\approx f(t_0) + \\Delta t f'(t_0)\n\\]\nWe can improve this simple formula by taking the avarage of the derivative at \\(t_0\\) and the derivative at the predicted point \\(t_0 + \\Delta t\\):\n\\[\nf(t_0 + \\Delta t) \\approx f(t_0) + \\frac{\\Delta t}{2} (f'(t_0) + f'(t_0 + \\Delta t))\n\\]\nThis effectively now introduces the curvature of the function at \\(t_0\\) into the approximation. We can see this by expanding the improved Euler method in a Taylor series. The first derivative at \\(t_0\\) is \\(f'(t_0)\\), and the derivative at the predicted point \\(f'(t_0 + \\Delta t)\\) can be expanded as \\(f'(t_0) + \\Delta t f''(t_0)\\). When we average these two derivatives and multiply by \\(\\Delta t\\), we get \\(\\Delta t f'(t_0) + \\frac{\\Delta t^2}{2} f''(t_0)\\), which matches the first two terms in the Taylor expansion. This is why the improved Euler method has an error of order \\(O(\\Delta t^2)\\) compared to \\(O(\\Delta t)\\) for the simple Euler method.\nExercise 2\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution Radioactive Decay Improved Euler\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis already provides a very good approximation to the analytical solution.\n\n\nHigher order differential equations\nThe above methods can be easily extended to higher order differential equations. For example, the second order differential equation\n\\[\n\\frac{d^2 y}{dt^2} = -k^2 y\n\\]\ncan be converted into two first order differential equations by introducing a new variable \\(v = \\frac{dy}{dt}\\):\n\\[\n\\begin{align}\n\\frac{dy}{dt} &= v \\\\\n\\frac{dv}{dt} &= -k^2 y\n\\end{align}\n\\]\nWe can now use the improved Euler method to solve these two equations simultaneously. The code below shows how to solve the above second order differential equation.\nExercise 3\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution Harmonic Oscillator"
  },
  {
    "objectID": "lectures/lecture15/ode-review.html#solving-coupled-differential-equations",
    "href": "lectures/lecture15/ode-review.html#solving-coupled-differential-equations",
    "title": "Repetition Differential Equations",
    "section": "Solving Coupled Differential Equations",
    "text": "Solving Coupled Differential Equations\nWe may also solve systems of coupled differential equations such as the Lotka-Volterra equations which describe predator-prey dynamics. In this system, we have two populations \\(x(t)\\) (prey) and \\(y(t)\\) (predators) that interact according to:\n\\[\n\\begin{align}\n\\frac{dx}{dt} &= \\alpha x - \\beta xy \\\\\n\\frac{dy}{dt} &= \\delta xy - \\gamma y\n\\end{align}\n\\]\nwhere \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\), and \\(\\delta\\) are positive constants representing the interaction between the species. Here \\(\\alpha\\) is the growth rate of prey in absence of predators, \\(\\beta\\) is the rate at which predators eat prey, \\(\\delta\\) is the efficiency of turning eaten prey into new predators, and \\(\\gamma\\) is the death rate of predators in absence of prey.\nExercise 4\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution Lotka-Volterra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Lotka-Volterra equations show the cyclic behavior of predator-prey populations. The prey population grows until it reaches a maximum, at which point the predator population increases due to the abundance of prey. The predators then reduce the prey population, leading to a decrease in the predator population. This cycle continues indefinitely, demonstrating the complex dynamics of ecological systems.\n\n\n\n\n\n\nThe population values below 1 mean that we have less than one individual in our population, which is not physically meaningful in a biological context. Our model treats populations as continuous variables, but real populations consist of discrete individuals. When the numerical solution shows population values less than 1, this indicates that the model has entered a regime where its continuous approximation breaks down and we should instead use a discrete model that can properly handle individual organisms. In practice, once a population drops below 1, we should consider it extinct.\nThis limitation of continuous models is particularly important in conservation biology and population management, where understanding the threshold for population viability is crucial. When modeling endangered species or small populations, we often need to switch to discrete population models or include additional factors like demographic stochasticity that become important at low population numbers."
  },
  {
    "objectID": "lectures/lecture15/particle-in-box-slides.html#today-applying-the-recipe",
    "href": "lectures/lecture15/particle-in-box-slides.html#today-applying-the-recipe",
    "title": "Particle in a Box",
    "section": "Today: Applying the Recipe",
    "text": "Today: Applying the Recipe\nWe‚Äôll solve a complete quantum mechanics problem step by step:\n\nDefine the physical problem and parameters\nBuild the potential energy matrix\nBuild the kinetic energy matrix\nAssemble the Hamiltonian\nSolve and visualize the results\n\n\nüì± Open now: 2_particle_in_a_box.qmd"
  },
  {
    "objectID": "lectures/lecture15/particle-in-box-slides.html#the-problem-particle-in-a-box",
    "href": "lectures/lecture15/particle-in-box-slides.html#the-problem-particle-in-a-box",
    "title": "Particle in a Box",
    "section": "The Problem: Particle in a Box",
    "text": "The Problem: Particle in a Box\nA particle confined to a rectangular potential well:\n\\[V(x) = \\begin{cases} 0 & \\text{if } |x| &lt; d/2 \\\\ V_0 & \\text{if } |x| \\geq d/2 \\end{cases}\\]"
  },
  {
    "objectID": "lectures/lecture15/particle-in-box-slides.html#physical-intuition",
    "href": "lectures/lecture15/particle-in-box-slides.html#physical-intuition",
    "title": "Particle in a Box",
    "section": "Physical Intuition",
    "text": "Physical Intuition\nThink of a guitar string:\n\nFixed at both ends (boundary conditions)\nOnly certain wavelengths fit ‚Üí discrete frequencies\nFundamental, first harmonic, second harmonic, ‚Ä¶\n\n\nSame idea here:\n\nWavefunction confined to the box\nOnly certain wavelengths fit ‚Üí discrete energies\nGround state, first excited state, second excited state, ‚Ä¶"
  },
  {
    "objectID": "lectures/lecture15/particle-in-box-slides.html#what-do-we-expect",
    "href": "lectures/lecture15/particle-in-box-slides.html#what-do-we-expect",
    "title": "Particle in a Box",
    "section": "What Do We Expect?",
    "text": "What Do We Expect?\nFor an infinite square well (analytical solution):\n\\[E_n = \\frac{n^2 \\pi^2 \\hbar^2}{2mL^2}, \\quad n = 1, 2, 3, \\ldots\\]\n\nKey predictions:\n\n\n\nProperty\nExpectation\n\n\n\n\nEnergy spacing\nDiscrete levels\n\n\nEnergy scaling\n\\(E_n \\propto n^2\\)\n\n\nWavefunctions\nStanding waves with \\(n-1\\) nodes\n\n\n\n\n\nLet‚Äôs verify this numerically!"
  },
  {
    "objectID": "lectures/lecture15/particle-in-box-slides.html#step-1-define-parameters",
    "href": "lectures/lecture15/particle-in-box-slides.html#step-1-define-parameters",
    "title": "Particle in a Box",
    "section": "Step 1: Define Parameters",
    "text": "Step 1: Define Parameters\n# Physical constants\nhbar = 1.055e-34  # Planck's constant / 2œÄ in J¬∑s\nm_e = 9.109e-31   # electron mass in kg\nV_0 = 20 * 1.602e-19  # barrier height: 20 eV in Joules\n\n# Spatial grid\nN = 1001          # number of grid points\nd = 1e-9          # box width: 1 nm\nL = 10e-9         # total domain: 10 nm\n\nx = np.linspace(-L/2, L/2, N)\ndx = x[1] - x[0]\n\nWhy these choices?\n\n\\(N = 1001\\) points for good resolution\nDomain larger than box to see decay outside\n\\(V_0 = 20\\) eV gives ~7 bound states"
  },
  {
    "objectID": "lectures/lecture15/particle-in-box-slides.html#why-v‚ÇÄ-20-ev",
    "href": "lectures/lecture15/particle-in-box-slides.html#why-v‚ÇÄ-20-ev",
    "title": "Particle in a Box",
    "section": "Why V‚ÇÄ = 20 eV?",
    "text": "Why V‚ÇÄ = 20 eV?\nThe ground state energy for a 1 nm box:\n\\[E_1 \\approx \\frac{\\pi^2 \\hbar^2}{2m_e d^2} \\approx 0.38 \\text{ eV}\\]\n\nSince \\(E_n \\propto n^2\\):\n\n\n\nn\n\\(E_n\\) (approx)\nBound if \\(V_0 = 20\\) eV?\n\n\n\n\n1\n0.38 eV\n‚úì\n\n\n2\n1.5 eV\n‚úì\n\n\n3\n3.4 eV\n‚úì\n\n\n4\n6.0 eV\n‚úì\n\n\n5\n9.4 eV\n‚úì\n\n\n6\n13.5 eV\n‚úì\n\n\n7\n18.4 eV\n‚úì\n\n\n8\n24 eV\n‚úó (above barrier)"
  },
  {
    "objectID": "lectures/lecture15/particle-in-box-slides.html#step-2-build-the-potential-matrix",
    "href": "lectures/lecture15/particle-in-box-slides.html#step-2-build-the-potential-matrix",
    "title": "Particle in a Box",
    "section": "Step 2: Build the Potential Matrix",
    "text": "Step 2: Build the Potential Matrix\n# Create the potential energy vector\nU_vec = np.zeros(N)\nU_vec[np.abs(x) &gt; d/2] = V_0  # V_0 outside, 0 inside\n\n# Create diagonal sparse matrix\nU = diags([U_vec], [0])\n\nWhat this does:\n\nnp.abs(x) &gt; d/2 selects points outside the box\nSets potential to \\(V_0\\) there, zero inside\ndiags creates a sparse diagonal matrix"
  },
  {
    "objectID": "lectures/lecture15/particle-in-box-slides.html#visualizing-the-potential",
    "href": "lectures/lecture15/particle-in-box-slides.html#visualizing-the-potential",
    "title": "Particle in a Box",
    "section": "Visualizing the Potential",
    "text": "Visualizing the Potential\nplt.figure(figsize=(8, 3))\nplt.plot(x * 1e9, U_vec * 6.242e18, 'b-', linewidth=2)\nplt.xlabel('Position x [nm]')\nplt.ylabel('V(x) [eV]')\nplt.title('The Potential Energy Function')\nplt.xlim([-3, 3])\nplt.ylim([-1, 25])\nplt.show()\n\nThe rectangular well: zero inside \\(|x| &lt; 0.5\\) nm, 20 eV outside."
  },
  {
    "objectID": "lectures/lecture15/particle-in-box-slides.html#step-3-build-the-kinetic-energy-matrix",
    "href": "lectures/lecture15/particle-in-box-slides.html#step-3-build-the-kinetic-energy-matrix",
    "title": "Particle in a Box",
    "section": "Step 3: Build the Kinetic Energy Matrix",
    "text": "Step 3: Build the Kinetic Energy Matrix\nThe second derivative matrix times the prefactor:\n\\[\\mathbf{T} = -\\frac{\\hbar^2}{2m \\cdot dx^2} \\begin{pmatrix} -2 & 1 & 0 & \\cdots \\\\ 1 & -2 & 1 & \\cdots \\\\ \\vdots & \\ddots & \\ddots & \\ddots \\end{pmatrix}\\]\n\nprefactor = -hbar**2 / (2 * m_e * dx**2)\nT = prefactor * diags([-2., 1., 1.], [0, -1, 1], shape=(N, N))\n\n\nNote: The prefactor includes the physical constants!"
  },
  {
    "objectID": "lectures/lecture15/particle-in-box-slides.html#step-4-assemble-the-hamiltonian",
    "href": "lectures/lecture15/particle-in-box-slides.html#step-4-assemble-the-hamiltonian",
    "title": "Particle in a Box",
    "section": "Step 4: Assemble the Hamiltonian",
    "text": "Step 4: Assemble the Hamiltonian\nSimply add the two matrices:\n\\[\\mathbf{H} = \\mathbf{T} + \\mathbf{V}\\]\nH = T + U\n\nprint(f\"Matrix shape: {H.shape}\")  # (1001, 1001)\nprint(f\"Non-zero elements: {H.nnz}\")  # ~3000 (tridiagonal + diagonal)\n\nSparse storage: Only ~3000 non-zero elements stored, not 1,002,001!"
  },
  {
    "objectID": "lectures/lecture15/particle-in-box-slides.html#step-5-solve-the-eigenvalue-problem",
    "href": "lectures/lecture15/particle-in-box-slides.html#step-5-solve-the-eigenvalue-problem",
    "title": "Particle in a Box",
    "section": "Step 5: Solve the Eigenvalue Problem",
    "text": "Step 5: Solve the Eigenvalue Problem\nfrom scipy.sparse.linalg import eigsh\n\n# Find the 20 smallest eigenvalues\nn = 20\neigenvalues, eigenvectors = eigsh(H, k=n, which='SM')\n\nParameters explained:\n\n\n\nParameter\nMeaning\n\n\n\n\nH\nThe Hamiltonian matrix\n\n\nk=n\nNumber of eigenvalues to find\n\n\nwhich='SM'\nSmallest Magnitude\n\n\n\n\n\nReturns: eigenvalues (array) and eigenvectors (matrix, columns are eigenvectors)"
  },
  {
    "objectID": "lectures/lecture15/particle-in-box-slides.html#the-results-eigenvalues",
    "href": "lectures/lecture15/particle-in-box-slides.html#the-results-eigenvalues",
    "title": "Particle in a Box",
    "section": "The Results: Eigenvalues",
    "text": "The Results: Eigenvalues\neV = 6.242e18  # conversion factor J -&gt; eV\n\nprint(\"First 10 eigenvalues (energies) in eV:\")\nfor i in range(10):\n    print(f\"  E_{i+1} = {eigenvalues[i] * eV:.4f} eV\")\n\nOutput:\n  E_1 = 0.3758 eV\n  E_2 = 1.5012 eV\n  E_3 = 3.3692 eV\n  E_4 = 5.9695 eV\n  ...\n\n\nThese match our analytical predictions!"
  },
  {
    "objectID": "lectures/lecture15/particle-in-box-slides.html#visualizing-the-wavefunctions",
    "href": "lectures/lecture15/particle-in-box-slides.html#visualizing-the-wavefunctions",
    "title": "Particle in a Box",
    "section": "Visualizing the Wavefunctions",
    "text": "Visualizing the Wavefunctions\nfor k in range(11):\n    vec = eigenvectors[:, k]\n    # Normalize\n    mag = np.sqrt(np.dot(vec, vec))\n    vec = vec / mag\n    \n    # Plot energy level\n    plt.axhline(y=eigenvalues[k] * escale, ls='--', color='gray')\n    \n    # Plot |Œ®|¬≤ shifted to energy level\n    plt.plot(x * scale, psiscale * vec**2 + eigenvalues[k] * escale)"
  },
  {
    "objectID": "lectures/lecture15/particle-in-box-slides.html#the-complete-picture",
    "href": "lectures/lecture15/particle-in-box-slides.html#the-complete-picture",
    "title": "Particle in a Box",
    "section": "The Complete Picture",
    "text": "The Complete Picture\n\nEach curve: \\(|\\Psi_n(x)|^2\\) (probability density) plotted at its energy \\(E_n\\)"
  },
  {
    "objectID": "lectures/lecture15/particle-in-box-slides.html#interpreting-the-results",
    "href": "lectures/lecture15/particle-in-box-slides.html#interpreting-the-results",
    "title": "Particle in a Box",
    "section": "Interpreting the Results",
    "text": "Interpreting the Results\nObservation 1: Discrete eigenvalues\nOnly certain energies are allowed ‚Äî the horizontal dashed lines.\n\nObservation 2: Node counting\n\n\n\nState\nNodes (zeros inside box)\n\n\n\n\n\\(\\Psi_1\\) (ground)\n0\n\n\n\\(\\Psi_2\\)\n1\n\n\n\\(\\Psi_3\\)\n2\n\n\n\\(\\Psi_n\\)\n\\(n-1\\)\n\n\n\n\n\nGeneral rule: Higher energy = more oscillations = more nodes"
  },
  {
    "objectID": "lectures/lecture15/particle-in-box-slides.html#interpreting-the-results-continued",
    "href": "lectures/lecture15/particle-in-box-slides.html#interpreting-the-results-continued",
    "title": "Particle in a Box",
    "section": "Interpreting the Results (continued)",
    "text": "Interpreting the Results (continued)\nObservation 3: Exponential decay outside\nThe wavefunctions don‚Äôt stop at the box edge ‚Äî they decay exponentially!\n\nMathematical reason: In the region where \\(E &lt; V(x)\\):\n\\[\\frac{d^2\\Psi}{dx^2} = \\frac{2m}{\\hbar^2}(V - E)\\Psi &gt; 0\\]\nPositive curvature ‚Üí exponential behavior (not oscillatory)\n\n\nThis is called tunneling ‚Äî the particle has some probability of being found outside the classically allowed region!"
  },
  {
    "objectID": "lectures/lecture15/particle-in-box-slides.html#eigenvalue-scaling",
    "href": "lectures/lecture15/particle-in-box-slides.html#eigenvalue-scaling",
    "title": "Particle in a Box",
    "section": "Eigenvalue Scaling",
    "text": "Eigenvalue Scaling\nindices = np.arange(1, 11)\nenergies = eigenvalues[:10] * escale\n\nplt.plot(indices, energies, 'o', markersize=10, label='Numerical')\n\n# Quadratic fit\ncoeffs = np.polyfit(indices, energies, 2)\nplt.plot(x_fit, np.polyval(coeffs, x_fit), 'r--', label='Quadratic fit')\n\nResult: \\(E_n \\propto n^2\\) ‚Äî matches the analytical prediction!"
  },
  {
    "objectID": "lectures/lecture15/particle-in-box-slides.html#live-coding-run-the-example",
    "href": "lectures/lecture15/particle-in-box-slides.html#live-coding-run-the-example",
    "title": "Particle in a Box",
    "section": "üéØ Live Coding: Run the Example",
    "text": "üéØ Live Coding: Run the Example\nLet‚Äôs run through the complete example together:\n\nOpen 2_particle_in_a_box.qmd\nExecute each code cell\nObserve the eigenvalues and eigenfunctions\n\n‚è±Ô∏è Time: 5 minutes"
  },
  {
    "objectID": "lectures/lecture15/particle-in-box-slides.html#modifying-the-problem",
    "href": "lectures/lecture15/particle-in-box-slides.html#modifying-the-problem",
    "title": "Particle in a Box",
    "section": "Modifying the Problem",
    "text": "Modifying the Problem\nWhat if we change the parameters?\n\n\n\nChange\nEffect on eigenvalues\n\n\n\n\nWider box (\\(d\\) larger)\nEnergies decrease (\\(E \\propto 1/d^2\\))\n\n\nDeeper well (\\(V_0\\) larger)\nMore bound states\n\n\nHeavier particle (\\(m\\) larger)\nEnergies decrease (\\(E \\propto 1/m\\))\n\n\n\n\nTry it! Modify d, V_0, or m_e and re-run."
  },
  {
    "objectID": "lectures/lecture15/particle-in-box-slides.html#exercise-double-well-potential",
    "href": "lectures/lecture15/particle-in-box-slides.html#exercise-double-well-potential",
    "title": "Particle in a Box",
    "section": "üéØ Exercise: Double Well Potential",
    "text": "üéØ Exercise: Double Well Potential\nCreate two wells separated by a barrier:"
  },
  {
    "objectID": "lectures/lecture15/particle-in-box-slides.html#double-well-the-setup",
    "href": "lectures/lecture15/particle-in-box-slides.html#double-well-the-setup",
    "title": "Particle in a Box",
    "section": "Double Well: The Setup",
    "text": "Double Well: The Setup\nParameters to try:\n\nTwo wells of width 1 nm each\nBarrier between them: try 0.2 nm and 2 nm\nSame depth \\(V_0 = 20\\) eV\n\n\n# Hint: Modify U_vec like this\nU_vec = np.ones(N) * V_0  # Start with barrier everywhere\n\n# First well: centered at -0.6 nm\nU_vec[(x &gt; -1.1e-9) & (x &lt; -0.1e-9)] = 0\n\n# Second well: centered at +0.6 nm  \nU_vec[(x &gt; 0.1e-9) & (x &lt; 1.1e-9)] = 0"
  },
  {
    "objectID": "lectures/lecture15/particle-in-box-slides.html#double-well-what-to-expect",
    "href": "lectures/lecture15/particle-in-box-slides.html#double-well-what-to-expect",
    "title": "Particle in a Box",
    "section": "Double Well: What to Expect",
    "text": "Double Well: What to Expect\nWhen wells are far apart:\n\nEach well acts independently\nEigenvalues come in nearly-degenerate pairs\n\\(E_1 \\approx E_2\\), \\(E_3 \\approx E_4\\), ‚Ä¶\n\n\nWhen wells are close:\n\nWavefunctions can ‚Äútunnel‚Äù through the barrier\nEigenvalues split apart\nEnergy splitting increases with coupling strength\n\n\n\nThis is the physics behind chemical bonds!"
  },
  {
    "objectID": "lectures/lecture15/particle-in-box-slides.html#your-turn-implement-double-well",
    "href": "lectures/lecture15/particle-in-box-slides.html#your-turn-implement-double-well",
    "title": "Particle in a Box",
    "section": "üéØ Your Turn: Implement Double Well",
    "text": "üéØ Your Turn: Implement Double Well\n\nModify U_vec to create two wells\nRun the eigenvalue solver\nPlot the first few eigenfunctions\nCompare eigenvalues for different barrier widths\n\n‚è±Ô∏è Time: 10 minutes"
  },
  {
    "objectID": "lectures/lecture15/particle-in-box-slides.html#double-well-solution-structure",
    "href": "lectures/lecture15/particle-in-box-slides.html#double-well-solution-structure",
    "title": "Particle in a Box",
    "section": "Double Well: Solution Structure",
    "text": "Double Well: Solution Structure\n# Complete double well setup\nU_vec = np.ones(N) * V_0\nwell_width = 1e-9\nbarrier_width = 0.2e-9  # Try also 2e-9\n\n# Left well\nleft_center = -(well_width + barrier_width) / 2\nU_vec[(x &gt; left_center - well_width/2) & \n      (x &lt; left_center + well_width/2)] = 0\n\n# Right well\nright_center = (well_width + barrier_width) / 2\nU_vec[(x &gt; right_center - well_width/2) & \n      (x &lt; right_center + well_width/2)] = 0"
  },
  {
    "objectID": "lectures/lecture15/particle-in-box-slides.html#comparing-single-vs-double-well",
    "href": "lectures/lecture15/particle-in-box-slides.html#comparing-single-vs-double-well",
    "title": "Particle in a Box",
    "section": "Comparing Single vs Double Well",
    "text": "Comparing Single vs Double Well\n\n\n\nSingle Well\nDouble Well\n\n\n\n\nNon-degenerate levels\nPaired levels\n\n\nLocalized wavefunctions\nSymmetric/antisymmetric pairs\n\n\n\\(E_1, E_2, E_3, ...\\)\n\\((E_1, E_2), (E_3, E_4), ...\\)\n\n\n\n\nThe splitting \\(\\Delta E = E_2 - E_1\\) depends on:\n\nBarrier height\nBarrier width\nParticle mass"
  },
  {
    "objectID": "lectures/lecture15/particle-in-box-slides.html#summary-what-we-did",
    "href": "lectures/lecture15/particle-in-box-slides.html#summary-what-we-did",
    "title": "Particle in a Box",
    "section": "Summary: What We Did",
    "text": "Summary: What We Did\n\nDefined a rectangular potential well\nBuilt the sparse matrices \\(\\mathbf{T}\\) and \\(\\mathbf{V}\\)\nAssembled the Hamiltonian \\(\\mathbf{H} = \\mathbf{T} + \\mathbf{V}\\)\nSolved using scipy.sparse.linalg.eigsh\nVisualized eigenfunctions and eigenvalues\nVerified \\(E_n \\propto n^2\\) scaling"
  },
  {
    "objectID": "lectures/lecture15/particle-in-box-slides.html#summary-what-we-learned",
    "href": "lectures/lecture15/particle-in-box-slides.html#summary-what-we-learned",
    "title": "Particle in a Box",
    "section": "Summary: What We Learned",
    "text": "Summary: What We Learned\n\n\n\nConcept\nKey Insight\n\n\n\n\nDiscretization\nFunctions ‚Üí vectors, operators ‚Üí matrices\n\n\nSparse matrices\nEfficient for tridiagonal systems\n\n\nEigenvalue solver\neigsh(H, k=n, which='SM')\n\n\nBound states\nDiscrete energies, exponential decay outside\n\n\nNode counting\nState \\(n\\) has \\(n-1\\) nodes"
  },
  {
    "objectID": "lectures/lecture15/particle-in-box-slides.html#the-power-of-this-approach",
    "href": "lectures/lecture15/particle-in-box-slides.html#the-power-of-this-approach",
    "title": "Particle in a Box",
    "section": "The Power of This Approach",
    "text": "The Power of This Approach\nThe same method works for any potential \\(V(x)\\):\n\nHarmonic oscillator: \\(V(x) = \\frac{1}{2}kx^2\\)\nMorse potential: \\(V(x) = D_e(1 - e^{-a(x-x_e)})^2\\)\nPeriodic potential: Crystal lattices\nDouble well: Molecular bonds, ammonia inversion\nArbitrary shapes: Quantum dots, heterostructures\n\n\nJust change U_vec! The rest of the code stays the same."
  },
  {
    "objectID": "lectures/lecture15/particle-in-box-slides.html#beyond-1d",
    "href": "lectures/lecture15/particle-in-box-slides.html#beyond-1d",
    "title": "Particle in a Box",
    "section": "Beyond 1D",
    "text": "Beyond 1D\nThe same principles extend to higher dimensions:\n\\[-\\frac{\\hbar^2}{2m}\\nabla^2\\Psi + V(\\vec{r})\\Psi = E\\Psi\\]\n\nThe approach:\n\nDiscretize on a 2D or 3D grid\nBuild the Laplacian matrix (sparse!)\nAdd the potential\nSolve the eigenvalue problem\n\n\n\nThe matrices get bigger, but the method is identical."
  },
  {
    "objectID": "lectures/lecture15/particle-in-box-slides.html#questions",
    "href": "lectures/lecture15/particle-in-box-slides.html#questions",
    "title": "Particle in a Box",
    "section": "Questions?",
    "text": "Questions?\nKey equations:\nPotential matrix: \\(\\mathbf{V} = \\text{diag}(V(x_1), V(x_2), \\ldots, V(x_N))\\)\nKinetic matrix: \\(\\mathbf{T} = -\\frac{\\hbar^2}{2m \\cdot dx^2}\\begin{pmatrix} -2 & 1 \\\\ 1 & -2 & 1 \\\\ & \\ddots & \\ddots & \\ddots \\end{pmatrix}\\)\nEigenvalue problem: \\((\\mathbf{T} + \\mathbf{V})\\vec{\\Psi} = E\\vec{\\Psi}\\)\n\nNext: Try the exercises and explore different potentials! üöÄ"
  },
  {
    "objectID": "lectures/lecture15/quantum-mechanics-slides.html#why-the-schr√∂dinger-equation",
    "href": "lectures/lecture15/quantum-mechanics-slides.html#why-the-schr√∂dinger-equation",
    "title": "Solving the Schr√∂dinger Equation",
    "section": "Why the Schr√∂dinger Equation?",
    "text": "Why the Schr√∂dinger Equation?\nThe Schr√∂dinger equation is one of the most important equations in physics.\n\nIt describes the behavior of matter at the atomic scale:\n\nWhy atoms have discrete energy levels\nHow electrons behave in materials\nThe structure of molecules and chemical bonds\nHow semiconductors and lasers work\n\n\n\nWithout it, we couldn‚Äôt explain most of modern technology!"
  },
  {
    "objectID": "lectures/lecture15/quantum-mechanics-slides.html#the-impact-what-depends-on-quantum-mechanics",
    "href": "lectures/lecture15/quantum-mechanics-slides.html#the-impact-what-depends-on-quantum-mechanics",
    "title": "Solving the Schr√∂dinger Equation",
    "section": "The Impact: What Depends on Quantum Mechanics?",
    "text": "The Impact: What Depends on Quantum Mechanics?\n\n\n\nTechnology\nQuantum Principle\n\n\n\n\nSemiconductors & computers\nBand structure of electrons\n\n\nLasers\nDiscrete energy transitions\n\n\nMRI scanners\nNuclear spin states\n\n\nLEDs & solar cells\nElectron-hole recombination\n\n\nQuantum computers\nSuperposition & entanglement\n\n\n\n\nAll of these require solving the Schr√∂dinger equation!"
  },
  {
    "objectID": "lectures/lecture15/quantum-mechanics-slides.html#dont-worry-about-the-physics",
    "href": "lectures/lecture15/quantum-mechanics-slides.html#dont-worry-about-the-physics",
    "title": "Solving the Schr√∂dinger Equation",
    "section": "Don‚Äôt Worry About the Physics!",
    "text": "Don‚Äôt Worry About the Physics!\n\nThe Schr√∂dinger equation is ‚Äújust‚Äù a differential equation\nWe‚Äôll treat it as a mathematical problem\nThe same numerical techniques work for many equations\n\n\nOur goal: Apply the matrix methods we‚Äôve learned to solve this famous equation.\n\n\nThink of it as practicing your numerical skills on an important real-world problem!"
  },
  {
    "objectID": "lectures/lecture15/quantum-mechanics-slides.html#todays-goal",
    "href": "lectures/lecture15/quantum-mechanics-slides.html#todays-goal",
    "title": "Solving the Schr√∂dinger Equation",
    "section": "Today‚Äôs Goal",
    "text": "Today‚Äôs Goal\n\nUnderstand the Schr√∂dinger equation as a differential equation\nConvert it to a matrix eigenvalue problem\nBuild the matrices using finite differences\nSolve using scipy‚Äôs eigenvalue solver\n\n\nNo quantum mechanics knowledge required!\nThis is about applying numerical methods to a famous equation."
  },
  {
    "objectID": "lectures/lecture15/quantum-mechanics-slides.html#what-well-connect-today",
    "href": "lectures/lecture15/quantum-mechanics-slides.html#what-well-connect-today",
    "title": "Solving the Schr√∂dinger Equation",
    "section": "What We‚Äôll Connect Today",
    "text": "What We‚Äôll Connect Today\n\n\n\n\n\n\n\nPrevious Knowledge\nToday‚Äôs Application\n\n\n\n\nFinite differences\nDiscretize derivatives\n\n\nMatrix operations\nBuild the Hamiltonian\n\n\nEigenvalue problems (\\(A\\vec{x} = \\lambda\\vec{x}\\))\nSolve for energies and wavefunctions\n\n\n\n\nKey insight: The Schr√∂dinger equation is ‚Äújust‚Äù a differential equation that becomes a matrix eigenvalue problem!"
  },
  {
    "objectID": "lectures/lecture15/quantum-mechanics-slides.html#the-schr√∂dinger-equation",
    "href": "lectures/lecture15/quantum-mechanics-slides.html#the-schr√∂dinger-equation",
    "title": "Solving the Schr√∂dinger Equation",
    "section": "The Schr√∂dinger Equation",
    "text": "The Schr√∂dinger Equation\nThe stationary Schr√∂dinger equation in one dimension:\n\\[-\\frac{\\hbar^2}{2m}\\frac{d^2 \\Psi(x)}{dx^2} + V(x)\\Psi(x) = E\\Psi(x)\\]\n\nDon‚Äôt panic! Let‚Äôs break this down mathematically:\n\n\n\nSymbol\nMathematical Role\nPhysical Meaning (optional)\n\n\n\n\n\\(\\Psi(x)\\)\nUnknown function\nWave function\n\n\n\\(\\frac{d^2 \\Psi}{dx^2}\\)\nSecond derivative\nKinetic energy term\n\n\n\\(V(x)\\)\nGiven function\nPotential energy\n\n\n\\(E\\)\nUnknown constant\nEigenvalue (energy)"
  },
  {
    "objectID": "lectures/lecture15/quantum-mechanics-slides.html#what-kind-of-problem-is-this",
    "href": "lectures/lecture15/quantum-mechanics-slides.html#what-kind-of-problem-is-this",
    "title": "Solving the Schr√∂dinger Equation",
    "section": "What Kind of Problem Is This?",
    "text": "What Kind of Problem Is This?\nThis is an eigenvalue problem!\n\nCompare to linear algebra:\n\\[A\\vec{x} = \\lambda\\vec{x}\\]\nMeaning: Find special vectors \\(\\vec{x}\\) that, when multiplied by matrix \\(A\\), just get scaled by a constant \\(\\lambda\\) (not rotated or changed in direction).\n\n\nThe Schr√∂dinger equation:\n\\[\\underbrace{\\left(-\\frac{\\hbar^2}{2m}\\frac{d^2}{dx^2} + V(x)\\right)}_{\\text{Operator } \\hat{H}} \\Psi(x) = E\\Psi(x)\\]\n\n\nWe‚Äôre looking for:\n\nEigenfunctions \\(\\Psi(x)\\): special functions unchanged (up to a constant) by the operator\nEigenvalues \\(E\\): the constants (allowed energies)"
  },
  {
    "objectID": "lectures/lecture15/quantum-mechanics-slides.html#the-strategy-convert-to-matrix-problem",
    "href": "lectures/lecture15/quantum-mechanics-slides.html#the-strategy-convert-to-matrix-problem",
    "title": "Solving the Schr√∂dinger Equation",
    "section": "The Strategy: Convert to Matrix Problem",
    "text": "The Strategy: Convert to Matrix Problem\n\nDiscretize the spatial domain \\(x \\to x_1, x_2, \\ldots, x_N\\)\nApproximate the second derivative as a matrix\nBuild the potential as a diagonal matrix\nCombine into the Hamiltonian matrix \\(\\mathbf{H}\\)\nSolve \\(\\mathbf{H}\\vec{\\Psi} = E\\vec{\\Psi}\\) with scipy\n\n\nThis is exactly like what we did for ODEs, but now we solve an eigenvalue problem instead of time-stepping!"
  },
  {
    "objectID": "lectures/lecture15/quantum-mechanics-slides.html#step-1-discretize-the-domain",
    "href": "lectures/lecture15/quantum-mechanics-slides.html#step-1-discretize-the-domain",
    "title": "Solving the Schr√∂dinger Equation",
    "section": "Step 1: Discretize the Domain",
    "text": "Step 1: Discretize the Domain\nInstead of \\(\\Psi(x)\\) for all \\(x\\), evaluate at discrete points:\n\\[x_1, x_2, x_3, \\ldots, x_N \\quad \\text{with spacing } \\delta x\\]\n\nThe function becomes a vector:\n\\[\\Psi(x) \\to \\vec{\\Psi} = \\begin{pmatrix} \\Psi(x_1) \\\\ \\Psi(x_2) \\\\ \\vdots \\\\ \\Psi(x_N) \\end{pmatrix}\\]\n\n\nThis is familiar! Same discretization we used for ODEs and PDEs."
  },
  {
    "objectID": "lectures/lecture15/quantum-mechanics-slides.html#step-2-the-second-derivative-as-a-matrix",
    "href": "lectures/lecture15/quantum-mechanics-slides.html#step-2-the-second-derivative-as-a-matrix",
    "title": "Solving the Schr√∂dinger Equation",
    "section": "Step 2: The Second Derivative as a Matrix",
    "text": "Step 2: The Second Derivative as a Matrix\nRemember the finite difference formula:\n\\[\\frac{d^2\\Psi}{dx^2}\\bigg|_{x_i} \\approx \\frac{\\Psi(x_{i+1}) - 2\\Psi(x_i) + \\Psi(x_{i-1})}{\\delta x^2}\\]\n\nKey insight: This combines values at neighboring points with weights!\n\nWeight \\(+1\\) for \\(\\Psi(x_{i+1})\\)\nWeight \\(-2\\) for \\(\\Psi(x_i)\\)\nWeight \\(+1\\) for \\(\\Psi(x_{i-1})\\)"
  },
  {
    "objectID": "lectures/lecture15/quantum-mechanics-slides.html#the-second-derivative-matrix",
    "href": "lectures/lecture15/quantum-mechanics-slides.html#the-second-derivative-matrix",
    "title": "Solving the Schr√∂dinger Equation",
    "section": "The Second Derivative Matrix",
    "text": "The Second Derivative Matrix\n\\[\\frac{d^2}{dx^2}\\vec{\\Psi} \\approx \\frac{1}{\\delta x^2}\n\\begin{pmatrix}\n-2 & 1  & 0 & 0 & \\cdots & 0\\\\\n1 & -2 & 1 & 0 & \\cdots & 0\\\\\n0 & 1  & -2 & 1 & \\cdots & 0\\\\\n\\vdots & & \\ddots & \\ddots & \\ddots & \\vdots\\\\\n0 & \\cdots & 0  & 1 & -2 & 1\\\\\n0 & \\cdots & 0  &  0 &  1 & -2\\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n\\Psi_1\\\\\n\\Psi_2\\\\\n\\Psi_3\\\\\n\\vdots\\\\\n\\Psi_{N-1}\\\\\n\\Psi_N\n\\end{pmatrix}\\]\n\nThis is a tridiagonal matrix:\n\nMain diagonal: \\(-2\\)\nOff-diagonals: \\(+1\\)"
  },
  {
    "objectID": "lectures/lecture15/quantum-mechanics-slides.html#building-the-matrix-in-python",
    "href": "lectures/lecture15/quantum-mechanics-slides.html#building-the-matrix-in-python",
    "title": "Solving the Schr√∂dinger Equation",
    "section": "Building the Matrix in Python",
    "text": "Building the Matrix in Python\nfrom scipy.sparse import diags\nimport numpy as np\n\nN = 1001  # number of grid points\ndx = 0.01  # grid spacing\n\n# Create the tridiagonal matrix\n# diags([values], [positions]) where 0=main, ¬±1=off-diagonals\nD2 = diags([-2, 1, 1], [0, -1, 1], shape=(N, N)) / dx**2\n\nWhy sparse? The matrix is mostly zeros ‚Äî sparse storage is efficient!"
  },
  {
    "objectID": "lectures/lecture15/quantum-mechanics-slides.html#boundary-conditions",
    "href": "lectures/lecture15/quantum-mechanics-slides.html#boundary-conditions",
    "title": "Solving the Schr√∂dinger Equation",
    "section": "Boundary Conditions",
    "text": "Boundary Conditions\nThe matrix implicitly assumes:\n\\[\\Psi(x_0) = 0 \\quad \\text{and} \\quad \\Psi(x_{N+1}) = 0\\]\n\nThese are Dirichlet boundary conditions ‚Äî the wavefunction vanishes at the boundaries.\n\n\nPhysical interpretation: The particle is confined to a region and cannot escape to infinity."
  },
  {
    "objectID": "lectures/lecture15/quantum-mechanics-slides.html#step-3-the-potential-energy-matrix",
    "href": "lectures/lecture15/quantum-mechanics-slides.html#step-3-the-potential-energy-matrix",
    "title": "Solving the Schr√∂dinger Equation",
    "section": "Step 3: The Potential Energy Matrix",
    "text": "Step 3: The Potential Energy Matrix\nThe term \\(V(x)\\Psi(x)\\) is simpler ‚Äî just multiplication at each point!\n\nIn matrix form, this is a diagonal matrix:\n\\[\\mathbf{V} =\n\\begin{pmatrix}\nV(x_1) & 0  & \\cdots & 0\\\\\n0 & V(x_2) & \\cdots & 0\\\\\n\\vdots & & \\ddots & \\vdots\\\\\n0 & \\cdots &  0 & V(x_N)\\\\\n\\end{pmatrix}\\]"
  },
  {
    "objectID": "lectures/lecture15/quantum-mechanics-slides.html#building-the-potential-matrix",
    "href": "lectures/lecture15/quantum-mechanics-slides.html#building-the-potential-matrix",
    "title": "Solving the Schr√∂dinger Equation",
    "section": "Building the Potential Matrix",
    "text": "Building the Potential Matrix\n# Example: Particle in a box (rectangular well)\nV_0 = 20  # barrier height in eV\nd = 1e-9  # box width\n\n# Potential is 0 inside the box, V_0 outside\nV_vec = np.zeros(N)\nV_vec[np.abs(x) &gt; d/2] = V_0\n\n# Create diagonal matrix\nV = diags([V_vec], [0])\n\nThe potential defines the problem! Different \\(V(x)\\) = different physics."
  },
  {
    "objectID": "lectures/lecture15/quantum-mechanics-slides.html#step-4-assemble-the-hamiltonian",
    "href": "lectures/lecture15/quantum-mechanics-slides.html#step-4-assemble-the-hamiltonian",
    "title": "Solving the Schr√∂dinger Equation",
    "section": "Step 4: Assemble the Hamiltonian",
    "text": "Step 4: Assemble the Hamiltonian\nCombining kinetic and potential energy:\n\\[\\mathbf{H} = -\\frac{\\hbar^2}{2m}\\mathbf{D}_2 + \\mathbf{V}\\]\n\nhbar = 1.055e-34  # J¬∑s\nm_e = 9.109e-31   # kg (electron mass)\n\n# Kinetic energy matrix\nT = -hbar**2 * D2 / (2 * m_e)\n\n# Total Hamiltonian\nH = T + V\n\n\nThat‚Äôs it! We‚Äôve converted the differential equation to a matrix."
  },
  {
    "objectID": "lectures/lecture15/quantum-mechanics-slides.html#step-5-solve-the-eigenvalue-problem",
    "href": "lectures/lecture15/quantum-mechanics-slides.html#step-5-solve-the-eigenvalue-problem",
    "title": "Solving the Schr√∂dinger Equation",
    "section": "Step 5: Solve the Eigenvalue Problem",
    "text": "Step 5: Solve the Eigenvalue Problem\nNow we solve \\(\\mathbf{H}\\vec{\\Psi} = E\\vec{\\Psi}\\):\nfrom scipy.sparse.linalg import eigsh\n\n# Find the n smallest eigenvalues and eigenvectors\nn = 10  # number of states to find\neigenvalues, eigenvectors = eigsh(H, k=n, which='SM')\n\nWhat we get:\n\neigenvalues: Array of energies \\(E_1, E_2, \\ldots, E_n\\)\neigenvectors: Matrix where column \\(k\\) is \\(\\vec{\\Psi}_k\\)"
  },
  {
    "objectID": "lectures/lecture15/quantum-mechanics-slides.html#understanding-the-output",
    "href": "lectures/lecture15/quantum-mechanics-slides.html#understanding-the-output",
    "title": "Solving the Schr√∂dinger Equation",
    "section": "Understanding the Output",
    "text": "Understanding the Output\n# eigenvalues are the allowed energies\nprint(\"First 5 energy levels (eV):\")\nfor i in range(5):\n    print(f\"  E_{i+1} = {eigenvalues[i] * 6.242e18:.3f} eV\")\n\n# eigenvectors are the wavefunctions\npsi_1 = eigenvectors[:, 0]  # ground state\npsi_2 = eigenvectors[:, 1]  # first excited state\n\nKey result: Only certain discrete energies are allowed ‚Äî these are the eigenvalues!"
  },
  {
    "objectID": "lectures/lecture15/quantum-mechanics-slides.html#visualizing-the-solutions",
    "href": "lectures/lecture15/quantum-mechanics-slides.html#visualizing-the-solutions",
    "title": "Solving the Schr√∂dinger Equation",
    "section": "Visualizing the Solutions",
    "text": "Visualizing the Solutions\n\nEach curve shows \\(|\\Psi(x)|^2\\) (probability density) at its energy level."
  },
  {
    "objectID": "lectures/lecture15/quantum-mechanics-slides.html#the-recipe-summary",
    "href": "lectures/lecture15/quantum-mechanics-slides.html#the-recipe-summary",
    "title": "Solving the Schr√∂dinger Equation",
    "section": "The Recipe: Summary",
    "text": "The Recipe: Summary\n\nDefine the grid: x = np.linspace(-L/2, L/2, N)\nBuild kinetic matrix: T = -hbar¬≤/(2m) √ó diags([-2,1,1], [0,-1,1]) / dx¬≤\nBuild potential matrix: V = diags([V_vec], [0])\nCombine: H = T + V\nSolve: eigenvalues, eigenvectors = eigsh(H, k=n, which='SM')\nPlot and interpret!"
  },
  {
    "objectID": "lectures/lecture15/quantum-mechanics-slides.html#why-does-this-work",
    "href": "lectures/lecture15/quantum-mechanics-slides.html#why-does-this-work",
    "title": "Solving the Schr√∂dinger Equation",
    "section": "Why Does This Work?",
    "text": "Why Does This Work?\n\n\n\nContinuous Problem\nDiscrete Problem\n\n\n\n\nDifferential operator \\(\\hat{H}\\)\nMatrix \\(\\mathbf{H}\\)\n\n\nEigenfunction \\(\\Psi(x)\\)\nEigenvector \\(\\vec{\\Psi}\\)\n\n\nEigenvalue \\(E\\)\nEigenvalue \\(E\\)\n\n\nInfinite dimensions\n\\(N\\) dimensions\n\n\n\n\nAs \\(N \\to \\infty\\) and \\(\\delta x \\to 0\\), the discrete solution approaches the exact solution!"
  },
  {
    "objectID": "lectures/lecture15/quantum-mechanics-slides.html#exercise-build-the-second-derivative-matrix",
    "href": "lectures/lecture15/quantum-mechanics-slides.html#exercise-build-the-second-derivative-matrix",
    "title": "Solving the Schr√∂dinger Equation",
    "section": "üéØ Exercise: Build the Second Derivative Matrix",
    "text": "üéØ Exercise: Build the Second Derivative Matrix\nTask: Create the tridiagonal matrix and test it on \\(f(x) = x^2\\)\n\nWe know: \\(\\frac{d^2}{dx^2}(x^2) = 2\\)\n\n\nN = 101\nx = np.linspace(0, 1, N)\ndx = x[1] - x[0]\nf = x**2\n\n# Build D2 matrix and apply to f\nD2 = ____  # Your code here\nf_second = D2 @ f\n\n# Should give approximately 2 everywhere!\n‚è±Ô∏è Time: 5 minutes"
  },
  {
    "objectID": "lectures/lecture15/quantum-mechanics-slides.html#solution-building-d2",
    "href": "lectures/lecture15/quantum-mechanics-slides.html#solution-building-d2",
    "title": "Solving the Schr√∂dinger Equation",
    "section": "Solution: Building D2",
    "text": "Solution: Building D2\nimport numpy as np\n\nN = 101\nx = np.linspace(0, 1, N)\ndx = x[1] - x[0]\nf = x**2\n\n# Method 1: Using np.diag\nmain = np.diag(np.full(N, -2))\nupper = np.diag(np.ones(N-1), 1)\nlower = np.diag(np.ones(N-1), -1)\nD2 = (main + upper + lower) / dx**2\n\n# Apply to f\nf_second = D2 @ f\nprint(f\"f''(0.5) = {f_second[N//2]:.4f}\")  # Should be ‚âà 2"
  },
  {
    "objectID": "lectures/lecture15/quantum-mechanics-slides.html#different-potentials-different-physics",
    "href": "lectures/lecture15/quantum-mechanics-slides.html#different-potentials-different-physics",
    "title": "Solving the Schr√∂dinger Equation",
    "section": "Different Potentials = Different Physics",
    "text": "Different Potentials = Different Physics\n\n\n\nPotential \\(V(x)\\)\nPhysical System\n\n\n\n\nRectangular well\nParticle in a box\n\n\n\\(\\frac{1}{2}kx^2\\)\nHarmonic oscillator\n\n\n\\(-\\frac{e^2}{4\\pi\\varepsilon_0 r}\\)\nHydrogen atom\n\n\nPeriodic wells\nCrystal lattice\n\n\n\n\nThe method is the same! Only the potential matrix changes."
  },
  {
    "objectID": "lectures/lecture15/quantum-mechanics-slides.html#comparing-with-analytical-solutions",
    "href": "lectures/lecture15/quantum-mechanics-slides.html#comparing-with-analytical-solutions",
    "title": "Solving the Schr√∂dinger Equation",
    "section": "Comparing with Analytical Solutions",
    "text": "Comparing with Analytical Solutions\nFor a particle in an infinite box of width \\(L\\):\n\\[E_n = \\frac{n^2 \\pi^2 \\hbar^2}{2mL^2}, \\quad n = 1, 2, 3, \\ldots\\]\n\nKey observation: \\(E_n \\propto n^2\\)\n\n\nOur numerical solution should match this scaling!"
  },
  {
    "objectID": "lectures/lecture15/quantum-mechanics-slides.html#energy-level-scaling",
    "href": "lectures/lecture15/quantum-mechanics-slides.html#energy-level-scaling",
    "title": "Solving the Schr√∂dinger Equation",
    "section": "Energy Level Scaling",
    "text": "Energy Level Scaling\n# Plot eigenvalues vs quantum number\nn = np.arange(1, 11)\nplt.plot(n, eigenvalues[:10], 'o', label='Numerical')\n\n# Compare with n¬≤ scaling\nE1 = eigenvalues[0]\nplt.plot(n, E1 * n**2, '--', label='$E_1 \\\\times n^2$')\n\nplt.xlabel('Quantum number n')\nplt.ylabel('Energy')\nplt.legend()\n\nThe numerical results confirm \\(E_n \\propto n^2\\)!"
  },
  {
    "objectID": "lectures/lecture15/quantum-mechanics-slides.html#what-we-learned-today",
    "href": "lectures/lecture15/quantum-mechanics-slides.html#what-we-learned-today",
    "title": "Solving the Schr√∂dinger Equation",
    "section": "What We Learned Today",
    "text": "What We Learned Today\n\n\n\nConcept\nKey Point\n\n\n\n\nSchr√∂dinger equation\nA differential eigenvalue problem\n\n\nDiscretization\nConvert functions to vectors\n\n\nSecond derivative\nTridiagonal matrix with \\((-2, 1, 1)\\)\n\n\nPotential energy\nDiagonal matrix\n\n\nSolution\nscipy.sparse.linalg.eigsh\n\n\n\n\nKey insight: Differential equations become matrix equations when discretized!"
  },
  {
    "objectID": "lectures/lecture15/quantum-mechanics-slides.html#connection-to-previous-topics",
    "href": "lectures/lecture15/quantum-mechanics-slides.html#connection-to-previous-topics",
    "title": "Solving the Schr√∂dinger Equation",
    "section": "Connection to Previous Topics",
    "text": "Connection to Previous Topics\n\nFinite differences ‚Äî Same technique as for ODEs\nSparse matrices ‚Äî Efficient for tridiagonal systems\nEigenvalues ‚Äî From linear algebra\nBoundary conditions ‚Äî Same concept as heat equation\n\n\nYou already had all the tools ‚Äî we just combined them!"
  },
  {
    "objectID": "lectures/lecture15/quantum-mechanics-slides.html#common-mistakes-to-avoid",
    "href": "lectures/lecture15/quantum-mechanics-slides.html#common-mistakes-to-avoid",
    "title": "Solving the Schr√∂dinger Equation",
    "section": "Common Mistakes to Avoid",
    "text": "Common Mistakes to Avoid\n\n\n\n\n\n\n\n\nMistake\nSymptom\nFix\n\n\n\n\nWrong \\(\\delta x\\)\nEigenvalues off by factor\nCheck dx = x[1] - x[0]\n\n\nForgetting \\(\\hbar^2/2m\\)\nWrong energy scale\nInclude physical constants\n\n\nToo few grid points\nPoor accuracy\nIncrease \\(N\\)\n\n\nWrong which in eigsh\nGetting largest eigenvalues\nUse which='SM'"
  },
  {
    "objectID": "lectures/lecture15/quantum-mechanics-slides.html#next-particle-in-a-box",
    "href": "lectures/lecture15/quantum-mechanics-slides.html#next-particle-in-a-box",
    "title": "Solving the Schr√∂dinger Equation",
    "section": "Next: Particle in a Box",
    "text": "Next: Particle in a Box\nIn the next section, we‚Äôll apply this recipe to solve a complete problem:\n\nDefine a rectangular potential well\nBuild the Hamiltonian matrix\nFind the bound states\nVisualize the wavefunctions\nAnalyze the energy spectrum\n\n\nOpen the notebook: 2_particle_in_a_box.qmd"
  },
  {
    "objectID": "lectures/lecture15/quantum-mechanics-slides.html#questions",
    "href": "lectures/lecture15/quantum-mechanics-slides.html#questions",
    "title": "Solving the Schr√∂dinger Equation",
    "section": "Questions?",
    "text": "Questions?\nThe key equations:\nSecond derivative matrix: \\[\\mathbf{D}_2 = \\frac{1}{\\delta x^2}\\begin{pmatrix} -2 & 1 & 0 \\\\ 1 & -2 & 1 \\\\ 0 & 1 & -2 \\end{pmatrix}\\]\nHamiltonian: \\[\\mathbf{H} = -\\frac{\\hbar^2}{2m}\\mathbf{D}_2 + \\mathbf{V}\\]\nEigenvalue problem: \\[\\mathbf{H}\\vec{\\Psi} = E\\vec{\\Psi}\\]\n\nLet‚Äôs solve some quantum mechanics! üöÄ"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation.html",
    "href": "lectures/lecture12/diffusion-equation.html",
    "title": "Diffusion equation",
    "section": "",
    "text": "So far, we have always looked at ordinary differential equations, i.e.¬†differential equations where the physical quantity we considered was depending only on one variable. In a lot of physical problems, the observable quantities depend on multiple variables like time and space. The differential equations, which govern those problems are partial differential equations. The diffusion equation is one of them. It pops up in various forms in physics, describing also heat conduction and in a slighly modified way this is corresponding to the time dependent Schr√∂dinger equation."
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation.html#physical-model",
    "href": "lectures/lecture12/diffusion-equation.html#physical-model",
    "title": "Diffusion equation",
    "section": "Physical Model",
    "text": "Physical Model\nYou‚Äôve probably seen how ink spreads in water - this spreading is called diffusion and is the result of the Brownian motion of ink particles in water. We have already simulated the random motion in Lecture 5 and the real-time animation below shows this process, when all particles are starting at the center of the box. The diffusion equation describes how the concentration of particles (the number of particles per unit volume) changes over time and space.\nWhile physicists describe this with complex equations, our goal in this course is to learn how to simulate this process using Python. So the main task will be to split the diffusion equation into small pieces that we can calculate with a computer.\nThe basic diffusion equation looks like this:\n\\[\\begin{equation}\n\\frac{\\partial c({\\bf r},t)}{\\partial t}=D\\Delta c ({\\bf r},t)\n\\end{equation}\\]\nHere, \\(c\\) represents the concentration (like how much ink is at each point), \\(t\\) is time, and \\({\\bf r}\\) is position. \\(D\\) is just a number that tells us how fast the diffusion happens. Its unit is length squared per time.\nTo make this easier to program, we‚Äôll look at diffusion in just one direction (like along a line). This gives us:\n\\[\\begin{equation}\n\\frac{\\partial c(x,t)}{\\partial t}=D\\frac{\\partial^2 c(x,t)}{\\partial x^{2}}\n\\end{equation}\\]\nTo turn this equation into code, we need to break up space and time into small pieces. We‚Äôll use \\(c^{n}_{i}\\) in our program, where \\({\\bf n}\\) is which time step we‚Äôre on, and \\({\\bf i}\\) tells us which point in space we‚Äôre looking at.\n\n\n\n\n\nwidth = 600\nheight = 600\nmargin = ({top: 20, right: 30, bottom: 20, left: 40})\nplotHeight = 100\n\nviewof simulation = {\n  // Create main container\n  const container = d3.create(\"div\")\n    .style(\"display\", \"flex\")\n    .style(\"flex-direction\", \"column\");\n\n  // Create SVG for particle simulation\n  const svg = container.append(\"svg\")\n    .attr(\"width\", width)\n    .attr(\"height\", height - plotHeight)\n    .attr(\"viewBox\", [0, 0, width, height - plotHeight]);\n\n  // Create SVG for histogram\n  const histogramSvg = container.append(\"svg\")\n    .attr(\"width\", width)\n    .attr(\"height\", plotHeight)\n    .attr(\"viewBox\", [0, 0, width, plotHeight]);\n\n  const numParticles = 1000;\n  const D = 0.5; // Diffusion coefficient\n  const numBins = 80;\n\n  // Create particles at the center\n  const particles = Array.from({length: numParticles}, () =&gt; ({\n    x: width / 2,\n    y: (height - plotHeight) / 2,\n    vx: 0,\n    vy: 0\n  }));\n\n  // Setup scales for histogram\n  const xScale = d3.scaleLinear()\n    .domain([0, width])\n    .range([margin.left, width - margin.right]);\n\n  const yScale = d3.scaleLinear()\n    .domain([0, numParticles/5])\n    .range([plotHeight - margin.bottom, margin.top]);\n\n  // Create histogram generator\n  const histogram = d3.bin()\n    .domain(xScale.domain())\n    .thresholds(xScale.ticks(numBins))\n    .value(d =&gt; d.x);\n\n  // Create histogram group\n  const histogramGroup = histogramSvg.append(\"g\");\n\n  // Add axes\n  histogramSvg.append(\"g\")\n    .attr(\"transform\", `translate(0,${plotHeight - margin.bottom})`)\n    .call(d3.axisBottom(xScale));\n/*\n  histogramSvg.append(\"g\")\n    .attr(\"transform\", `translate(${margin.left},0)`)\n    .call(d3.axisLeft(yScale));\n*/\n  // Animation function\n  function animate() {\n    particles.forEach(particle =&gt; {\n      // Random walk implementation based on diffusion equation\n      const randomAngle = Math.random() * 2 * Math.PI;\n      const displacement = Math.sqrt(2 * D);\n\n      particle.x += displacement * Math.cos(randomAngle);\n      particle.y += displacement * Math.sin(randomAngle);\n\n      // Bounce off walls\n      if (particle.x &lt; 0) particle.x = 0;\n      if (particle.x &gt; width) particle.x = width;\n      if (particle.y &lt; 0) particle.y = 0;\n      if (particle.y &gt; (height - plotHeight)) particle.y = height - plotHeight;\n    });\n\n    // Update particle positions\n    circles\n      .attr(\"cx\", d =&gt; d.x)\n      .attr(\"cy\", d =&gt; d.y);\n\n    // Update histogram\n    const bins = histogram(particles);\n\n    const bars = histogramGroup.selectAll(\"rect\")\n      .data(bins);\n\n    bars.enter()\n      .append(\"rect\")\n      .merge(bars)\n      .attr(\"x\", d =&gt; xScale(d.x0))\n      .attr(\"y\", d =&gt; yScale(d.length))\n      .attr(\"width\", d =&gt; Math.max(0, xScale(d.x1) - xScale(d.x0) - 1))\n      .attr(\"height\", d =&gt; yScale(0) - yScale(d.length))\n      .attr(\"fill\", \"steelblue\");\n\n    bars.exit().remove();\n  }\n\n  // Create circles for particles\n  const circles = svg.selectAll(\"circle\")\n    .data(particles)\n    .join(\"circle\")\n    .attr(\"r\", 2)\n    .attr(\"fill\", \"steelblue\")\n    .attr(\"opacity\", 0.6);\n\n  // Start animation\n  d3.timer(animate);\n\n  return container.node();\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†1: Simulation showing the diffusion of particles in a 2D box. Particles move randomly based on the diffusion equation. The histogram shows the distribution of particles along the x-axis.\n\n\n\n\n\nSpatial derivative\n\n\nSpatial derivative\nLet‚Äôs break down how we handle changes in space. Just like before, we can estimate how quickly the concentration changes in space using three points:\n\\[\\begin{equation}\n\\frac{\\partial^{2} c(x,t)}{\\partial x^2}\\approx\\frac{c_{i+1}^{n}-2c_{i}^{n}+c_{i-1}^{n}}{\\Delta x^2}\n\\end{equation}\\]\nThink of this as looking at how the concentration changes between neighboring points. We‚Äôll collect all these concentrations at a particular time \\(n\\) into a list: \\({\\bf C}=\\lbrace c_{0}^{n},c_{1}^{n},c_{2}^{n}, \\ldots, c_{5}^{n}\\rbrace\\).\nTo make calculations easier for the computer, we can write this as a matrix equation:\n\\(M=\\frac{\\partial^2}{\\partial x^2}=\\frac{1}{\\delta x^2}\n\\begin{bmatrix}\n-2 & 1  & 0 & 0 & 0 & 0\\\\\n1 & -2 & 1 & 0 & 0 & 0\\\\\n0 & 1  & -2 & 1 & 0 & 0\\\\\n0 & 0  & 1  & -2 & 1 & 0\\\\\n0 & 0  & 0  &  1 & -2 & 1\\\\\n0 & 0  & 0  &  0 &  1 & -2\\\\\n\\end{bmatrix}\\)\nEach row in this matrix represents how we calculate the change at one point using its neighbors. We haven‚Äôt yet considered what happens at the edges of our system (the boundary conditions).\nThis lets us write our diffusion equation in a simpler form:\n\\[\\begin{equation}\n\\frac{\\partial c(x,t)}{\\partial t}\\approx DM{\\bf C}^{n}\n\\end{equation}\\]\nHere, \\(M\\) is our matrix from above, and \\({\\bf C}\\) is our list of concentrations. The \\(n\\) tells us we‚Äôre looking at a specific moment in time.\n\n\nTemporal derivative\nJust like we split up space into points, we also need to split up time into small steps. The change in concentration over time can be estimated by looking at how much it changes between two time steps:\n\\[\\begin{equation}\n\\frac{\\partial c(x,t)}{\\partial t}=\\frac{c_{i}^{n+1}-c_{i}^{n}}{\\delta t}\n\\end{equation}\\]\nHere, \\(n\\) tells us which time step we‚Äôre on (just like \\(i\\) told us which space point we were looking at). This equation works for any point in space \\(i\\).\nTo make our calculation more accurate, we use something called the Crank Nicolson scheme. First, we write our time derivative as:\n\\[\\begin{equation}\n\\frac{\\partial c}{\\partial t} = f(x)\n\\end{equation}\\]\nwhere \\(f(x)\\) is the spatial part we found earlier:\n\\[\\begin{equation}\nf(x)=D\\frac{\\partial^2 c(x,t)}{\\partial x^{2}}\n\\end{equation}\\]\nThe Crank Nicolson scheme tells us to take the average of this function at the current time step and the next time step:\n\\[\\begin{equation}\n\\frac{\\partial c}{\\partial t} \\approx \\frac{1}{2}\\left ( f^{n+1}(x)+f^{n}(x)\\right)\n\\end{equation}\\]\nwhere \\(n\\) keeps track of which time step we‚Äôre on.\n\n\nBringing all together\nWe can now bring all sides together to develop our implicit scheme.\n\\[\\begin{equation}\n\\frac{{\\bf C^{n+1}}-{\\bf C}^{n}}{\\delta t}=\\frac{1}{2} \\left (D M {\\bf C}^{n+1}+D M {\\bf C}^{n} \\right)\n\\end{equation}\\]\nWe can transform the last equation to yield the value of of the concentration at the time index \\(n+1\\), i.e.\n\\[\\begin{equation}\n\\left({\\bf I}-\\frac{\\delta t}{2}D M \\right ){\\bf C}^{n+1}=\\left({\\bf I}+\\frac{\\delta t}{2}D M \\right ){\\bf C}^{n}\n\\end{equation}\\]\nwhere \\({\\bf I}\\) is the identity matrix. This will correspond in our code to\n\\[\\begin{equation}\n{\\bf A}{\\bf C}^{n+1}={\\bf B}{\\bf C}^{n}\n\\end{equation}\\]\nwhere \\({\\bf A}=\\left({\\bf I}-\\frac{\\delta t}{2}D M \\right )\\) and \\({\\bf B}=\\left({\\bf I}+\\frac{\\delta t}{2}D M \\right )\\)."
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation.html#numerical-solution",
    "href": "lectures/lecture12/diffusion-equation.html#numerical-solution",
    "title": "Diffusion equation",
    "section": "Numerical Solution",
    "text": "Numerical Solution\nWe are now ready to write some code. To simulate diffusion, we need two key pieces of information: what happens at the edges of our system (boundary conditions) and how the concentration looks at the start (initial condition).\nLet‚Äôs imagine we‚Äôre looking at diffusion along a line of length L=1. At both ends of this line (x=0 and x=L), we‚Äôll keep the concentration at zero throughout the simulation. This might represent, for example, a situation where any particles that reach the edges are immediately removed:\n\\[\\begin{equation}\nc(0,t)=c(L,t)=0\n\\end{equation}\\]\nFor our starting condition, we‚Äôll create a bell-shaped curve (Gaussian distribution) centered in the middle of our line (at x=L/2). This is like placing a concentrated drop of ink at the center:\n\\[\\begin{equation}\nc(x,0)=\\frac{1}{\\sigma\\sqrt{2\\pi }}e^{-\\frac{(x-L/2)^2}{2\\sigma^2}}\n\\end{equation}\\]\nHere, œÉ=0.05 controls how narrow or wide our initial distribution is - a smaller œÉ means a more concentrated initial drop.\n\nSetup Domain\n\n\n\n\n\n\n\n\nInitial Conditions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMatrix Setup\n\n\n\n\n\n\n\n\nSolution\nTo solve this system of equations, we‚Äôll use the spsolve function from the scipy.sparse.linalg module. This function is designed to solve sparse linear systems efficiently. Here is a detailed explanation of the code:\n1data = []\n2data.append(c)\n\n3for i in range(NT):\n4    A = (I -dt/2*D*M)\n5    B = (I + dt/2*D*M)*c\n6    c = sparse.linalg.spsolve(A, B)\n7    c = np.array(c)\n8    data.append(c)\n\n1\n\nInitialize empty list to store concentration profiles\n\n2\n\nStore initial condition as first element\n\n3\n\nLoop over all time steps\n\n4\n\nCreate matrix A for left side of equation: (I - dt/2D‚àÇ¬≤/‚àÇx¬≤)\n\n5\n\nCreate matrix B times current concentration: (I + dt/2D‚àÇ¬≤/‚àÇx¬≤)*c^n\n\n6\n\nSolve linear system Ac^(n+1) = Bc^n for next time step\n\n7\n\nConvert solution to numpy array for consistency\n\n8\n\nStore concentration profile of current time step"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation.html#where-to-go-from-here",
    "href": "lectures/lecture12/diffusion-equation.html#where-to-go-from-here",
    "title": "Diffusion equation",
    "section": "Where to go from here",
    "text": "Where to go from here\nThe diffusion equation we have solved here is a simple example of a partial differential equation. A similar type of equation is the heat equation, which describes how heat spreads in a material. Finally, also the Schr√∂dinger equation is a partial differential equation, which describes the time evolution of a quantum system. You could apply your knowledge of the diffusion equation to solve these more complex problems."
  },
  {
    "objectID": "lectures/lecture01/variables-and-numbers.html",
    "href": "lectures/lecture01/variables-and-numbers.html",
    "title": "Variables & Numbers",
    "section": "",
    "text": "In the previous lessons, you created plots and calculations using code like:\nm = 2.0    # mass\ng = 9.81   # gravity\nh = 10.0   # height\nE_pot = m * g * h\nBut what exactly are m, g, and h? These are variables - containers that store values you can use in calculations.\n\n\n\n\n\n\nWhy Variables Matter in Physics\n\n\n\nIn physics, we work with:\n\nPhysical quantities: mass, velocity, energy, time\nConstants: gravitational acceleration, speed of light, Planck‚Äôs constant\nCalculated results: kinetic energy, momentum, force\n\nVariables let us store these values and use them in equations, just like in mathematical physics notation!"
  },
  {
    "objectID": "lectures/lecture01/variables-and-numbers.html#what-you-just-used",
    "href": "lectures/lecture01/variables-and-numbers.html#what-you-just-used",
    "title": "Variables & Numbers",
    "section": "",
    "text": "In the previous lessons, you created plots and calculations using code like:\nm = 2.0    # mass\ng = 9.81   # gravity\nh = 10.0   # height\nE_pot = m * g * h\nBut what exactly are m, g, and h? These are variables - containers that store values you can use in calculations.\n\n\n\n\n\n\nWhy Variables Matter in Physics\n\n\n\nIn physics, we work with:\n\nPhysical quantities: mass, velocity, energy, time\nConstants: gravitational acceleration, speed of light, Planck‚Äôs constant\nCalculated results: kinetic energy, momentum, force\n\nVariables let us store these values and use them in equations, just like in mathematical physics notation!"
  },
  {
    "objectID": "lectures/lecture01/variables-and-numbers.html#variables-in-python",
    "href": "lectures/lecture01/variables-and-numbers.html#variables-in-python",
    "title": "Variables & Numbers",
    "section": "Variables in Python",
    "text": "Variables in Python\n\nSymbol Names\nIn physics, we use symbols like \\(m\\) for mass, \\(v\\) for velocity, and \\(F\\) for force. Python variable names work similarly, but with some rules.\nVariable names in Python can include alphanumerical characters a-z, A-Z, 0-9, and the special character _. Normal variable names must start with a letter or an underscore. By convention, variable names typically start with a lower-case letter, while Class names start with a capital letter and internal variables start with an underscore.\n\n\n\n\n\n\nPhysics Variable Naming Tips\n\n\n\nGood physics variable names:\n\nm, mass for mass\nv, velocity for velocity\nE_kin, E_pot for kinetic and potential energy\ntheta, phi for angles (Greek letters spelled out)\ng for gravitational acceleration\nc for speed of light\n\nDescriptive names for clarity:\n\nelectron_mass instead of just m when you have multiple masses\ninitial_velocity, final_velocity instead of v1, v2\n\n\n\n\n\n\n\n\n\nReserved Keywords\n\n\n\nThere are a number of Python keywords that cannot be used as variable names because Python uses them for other things. These keywords are:\nand, as, assert, break, class, continue, def, del, elif, else, except, exec, finally, for, from, global, if, import, in, is, lambda, not, or, pass, print, raise, return, try, while, with, yield\nImportant for physics: The keyword lambda cannot be used as a variable name (common for wavelength). Use lambda_ or wavelength instead!\n\n\n\n\nVariable Assignment\nThe assignment operator in Python is =. Python is a dynamically typed language, so we do not need to specify the type of a variable when we create one.\nLet‚Äôs assign some physics values:\n\n\n\n\n\n\nImportant: In Python, = means assignment, not equality!\n\nMath: \\(F = ma\\) (force equals mass times acceleration)\nPython: F = m * a (calculate m*a and store in F)\n\nAlthough not explicitly specified, a variable does have a type associated with it (e.g., integer, float, string). The type is derived from the value that was assigned to it. To determine the type of a variable, we can use the type function.\n\n\n\n\n\n\nMost physics calculations use floats (floating-point numbers) because they have decimal points.\nIf we assign a new value to a variable, its type can change.\n\n\n\n\n\n\nIf we try to use a variable that has not yet been defined, we get a NameError error.\n\n\n\n\n\n\nCommon Mistake\n\n\n\nTrying to use a variable before defining it:\nE_kin = 0.5 * m * v**2  # ERROR if m and v aren't defined yet!\nAlways define variables before using them in calculations."
  },
  {
    "objectID": "lectures/lecture01/variables-and-numbers.html#number-types",
    "href": "lectures/lecture01/variables-and-numbers.html#number-types",
    "title": "Variables & Numbers",
    "section": "Number Types",
    "text": "Number Types\nPython supports various number types, including integers, floating-point numbers, and complex numbers. In physics, you‚Äôll primarily use floats for continuous quantities and integers for counting (like number of particles, timesteps, etc.).\n\n\n\n\n\n\nWhich Type for Physics?\n\n\n\n\nFloats: Mass, energy, position, velocity, time ‚Üí anything with units and decimals\nIntegers: Particle counts, loop iterations, array indices\nComplex: Quantum mechanics, wave functions, AC circuits\n\n\n\n\nComparison of Number Types\n\n\n\n\n\n\n\n\n\nType\nExample\nDescription\nPhysics Use Cases\n\n\n\n\nint\n42\nWhole numbers\nParticle count, quantum number \\(n\\), array index\n\n\nfloat\n3.14159\nDecimal numbers (15-17 digit precision)\nMass, energy, position, velocity, time\n\n\ncomplex\n2 + 3j\nNumbers with real and imaginary parts\nWave functions \\(\\psi\\), AC impedance\n\n\nbool\nTrue / False\nLogical values\nCollision detection, boundary conditions\n\n\n\n\n\n\n\n\n\nExamples for Number Types\n\n\n\n\n\n\nIntegers\nInteger Representation: Integers are whole numbers without a decimal point.\n\n\n\n\n\n\nPhysics Example:\n\n\n\n\n\n\nBinary, Octal, and Hexadecimal: Integers can be represented in different bases:\n\n\n\n\n\n\n\n\nFloating Point Numbers\nFloating Point Representation: Numbers with a decimal point are treated as floating-point values. This is what you‚Äôll use most in physics!\n\n\n\n\n\n\nScientific Notation:\n\n\n\n\n\n\nMaximum Float Value: Python handles large floats, converting them to infinity if they exceed the maximum representable value (~10¬≥‚Å∞‚Å∏).\n\n\n\n\n\n\n\n\n\n\n\n\nFloat Precision in Physics\n\n\n\nFloats have ~15-17 significant digits. This is usually more than enough for physics calculations, but be aware:\ng = 9.81                    # Good enough for most problems\ng_precise = 9.80665         # Standard gravity (more digits)\nFor most undergraduate physics, 3-4 significant figures are sufficient!\n\n\n\n\nComplex Numbers\nComplex Number Representation: Complex numbers have a real and an imaginary part. Essential in quantum mechanics and AC circuit analysis!\n\n\n\n\n\n\n\n\n\n\n\n\nPhysics Applications\n\n\n\nQuantum Mechanics: Wave functions are complex: \\(\\psi = a + bi\\)\nAC Circuits: Impedance \\(Z = R + iX\\) where \\(R\\) is resistance, \\(X\\) is reactance\nWave Propagation: \\(E = E_0 e^{i(kx - \\omega t)}\\)\n\n\n\nAccessors for Complex Numbers:\n\nc.real: Real part of the complex number.\nc.imag: Imaginary part of the complex number.\n\n\n\n\n\n\n\n\nComplex Conjugate: Use the .conjugate() method to get the complex conjugate (important for calculating probabilities in QM: \\(|\\psi|^2 = \\psi \\cdot \\psi^*\\)).\n\n\n\n\n\n\nAbsolute Value (Magnitude):"
  },
  {
    "objectID": "lectures/lecture01/01-lecture01.html",
    "href": "lectures/lecture01/01-lecture01.html",
    "title": "Jupyter Notebooks",
    "section": "",
    "text": "A Jupyter Notebook is a web browser based interactive computing environment that enables users to create documents that include code to be executed, results from the executed code such as plots and images,and finally also an additional documentation in form of markdown text including equations in LaTeX.\nThese documents provide a complete and self-contained record of a computation that can be converted to various formats and shared with others using email, version control systems (like git/GitHub) or nbviewer.jupyter.org.\n\n\nThe Jupyter Notebook ecosystem consists of three main components:\n\nNotebook Editor\nKernels\nNotebook Documents\n\nLet‚Äôs explore each of these components in detail:\n\n\nThe Notebook editor is an interactive web-based application for creating and editing notebook documents. It enables users to write and run code, add rich text, and multimedia content. When running Jupyter on a server, users typically use either the classic Jupyter Notebook interface or JupyterLab, an advanced version with more features.\nKey features of the Notebook editor include:\n\nCode Editing: Write and edit code in individual cells.\nCode Execution: Run code cells in any order and display computation results in various formats (HTML, LaTeX, PNG, SVG, PDF).\nInteractive Widgets: Create and use JavaScript widgets that connect user interface controls to kernel-side computations.\nRich Text: Add documentation using Markdown markup language, including LaTeX equations.\n\n\n\n\n\n\n\nAdvance Notebook Editor Info\n\n\n\n\n\nThe Notebook editor in Jupyter offers several advanced features:\n\nCell Metadata: Each cell has associated metadata that can be used to control its behavior. This includes tags for slideshows, hiding code cells, and controlling cell execution.\nMagic Commands: Special commands prefixed with % (line magics) or %% (cell magics) that provide additional functionality, such as timing code execution or displaying plots inline.\nAuto-completion: The editor provides context-aware auto-completion for Python code, helping users write code more efficiently.\nCode Folding: Users can collapse long code blocks for better readability.\nMultiple Cursors: Advanced editing with multiple cursors for simultaneous editing at different locations.\nSplit View: The ability to split the notebook view, allowing users to work on different parts of the notebook simultaneously.\nVariable Inspector: A tool to inspect and manage variables in the kernel‚Äôs memory.\nIntegrated Debugger: Some Jupyter environments offer an integrated debugger for step-by-step code execution and inspection.\n\n\n\n\n\n\n\n\nKernels are the computational engines that execute the code contained in a notebook. They are separate processes that run independently of the notebook editor.\nKey responsibilities of kernels include: * Executing user code * Returning computation results to the notebook editor * Handling computations for interactive widgets * Providing features like tab completion and introspection\n\n\n\n\n\n\nAdvanced Kernel Info\n\n\n\n\n\nJupyter notebooks are language-agnostic. Different kernels can be installed to support various programming languages such as Python, R, Julia, and many others. The default kernel runs Python code, but users can select different kernels for each notebook via the Kernel menu.\nKernels communicate with the notebook editor using a JSON-based protocol over ZeroMQ/WebSockets. For more technical details, see the messaging specification.\nEach kernel runs in its own environment, which can be customized to include specific libraries and dependencies. This allows users to create isolated environments for different projects, ensuring that dependencies do not conflict.\nKernels also support interactive features such as:\n\nTab Completion: Provides suggestions for variable names, functions, and methods as you type, improving coding efficiency.\nIntrospection: Allows users to inspect objects, view documentation, and understand the structure of code elements.\nRich Output: Supports various output formats, including text, images, videos, and interactive widgets, enhancing the interactivity of notebooks.\n\nAdvanced users can create custom kernels to support additional languages or specialized computing environments. This involves writing a kernel specification and implementing the necessary communication protocols.\nFor managing kernels, Jupyter provides several commands and options:\n\nStarting a Kernel: Automatically starts when a notebook is opened.\nInterrupting a Kernel: Stops the execution of the current code cell, useful for halting long-running computations.\nRestarting a Kernel: Clears the kernel‚Äôs memory and restarts it, useful for resetting the environment or recovering from errors.\nShutting Down a Kernel: Stops the kernel and frees up system resources.\n\nUsers can also monitor kernel activity and resource usage through the Jupyter interface, ensuring efficient and effective use of computational resources.\n\n\n\n\n\n\nNotebook documents are self-contained files that encapsulate all content created in the notebook editor. They include code inputs/outputs, Markdown text, equations, images, and other media. Each document is associated with a specific kernel and serves as both a human-readable record of analysis and an executable script to reproduce the work.\nCharacteristics of notebook documents:\n\nFile Extension: Notebooks are stored as files with a .ipynb extension.\nStructure: Notebooks consist of a linear sequence of cells, which can be one of three types:\n\nCode cells: Contain executable code and its output.\nMarkdown cells: Contain formatted text, including LaTeX equations.\nRaw cells: Contain unformatted text, preserved when converting notebooks to other formats.\n\n\n\n\n\n\n\n\nAdvanced Notebook Documents Info\n\n\n\n\n\n\nVersion Control: Notebook documents can be version controlled using systems like Git. This allows users to track changes, collaborate with others, and revert to previous versions if needed. Tools like nbdime provide diff and merge capabilities specifically designed for Jupyter Notebooks.\nCell Tags: Cells in a notebook can be tagged with metadata to control their behavior during execution, export, or presentation. For example, tags can be used to hide input or output, skip execution, or designate cells as slides in a presentation.\nInteractive Widgets: Notebook documents can include interactive widgets that allow users to manipulate parameters and visualize changes in real-time. This is particularly useful for data exploration and interactive simulations.\nExtensions: The Jupyter ecosystem supports a wide range of extensions that enhance the functionality of notebook documents. These extensions can add features like spell checking, code formatting, and integration with external tools and services.\nSecurity: Notebook documents can include code that executes on the user‚Äôs machine, which poses security risks. Jupyter provides mechanisms to sanitize notebooks and prevent the execution of untrusted code. Users should be cautious when opening notebooks from unknown sources.\nCollaboration: Jupyter Notebooks can be shared and collaboratively edited in real-time using platforms like Google Colab, Microsoft Azure Notebooks, and JupyterHub. These platforms provide cloud-based environments where multiple users can work on the same notebook simultaneously.\nCustomization: Users can customize the appearance and behavior of notebook documents using CSS and JavaScript. This allows for the creation of tailored interfaces and enhanced user experiences.\nExport Options: In addition to static formats, notebooks can be exported to interactive formats like dashboards and web applications. Tools like Voila convert notebooks into standalone web applications that can be shared and deployed.\nProvenance: Notebooks can include provenance information that tracks the origin and history of data and computations. This is important for reproducibility and transparency in scientific research.\nDocumentation: Notebook documents can serve as comprehensive documentation for projects, combining code, results, and narrative text. This makes them valuable for teaching, tutorials, and sharing research findings.\nPerformance: Large notebooks with many cells and outputs can become slow and unwieldy. Techniques like cell output clearing, using lightweight data formats, and splitting notebooks into smaller parts can help maintain performance.\nIntegration: Jupyter Notebooks can integrate with a wide range of data sources, libraries, and tools. This includes databases, cloud storage, machine learning frameworks, and visualization libraries, making them a versatile tool for data science and research.\nInternal Format: Notebook files are JSON text files with binary data encoded in base64, making them easy to manipulate programmatically.\nExportability: Notebooks can be exported to various static formats (HTML, reStructuredText, LaTeX, PDF, slide shows) using Jupyter‚Äôs nbconvert utility.\nSharing: Notebooks can be shared via nbviewer, which renders notebooks from public URLs or GitHub as static web pages, allowing others to view the content without installing Jupyter.\n\n\n\n\nThis integrated system of editor, kernels, and documents makes Jupyter Notebooks a powerful tool for interactive computing, data analysis, and sharing of computational narratives."
  },
  {
    "objectID": "lectures/lecture01/01-lecture01.html#what-is-jupyter-notebook",
    "href": "lectures/lecture01/01-lecture01.html#what-is-jupyter-notebook",
    "title": "Jupyter Notebooks",
    "section": "",
    "text": "A Jupyter Notebook is a web browser based interactive computing environment that enables users to create documents that include code to be executed, results from the executed code such as plots and images,and finally also an additional documentation in form of markdown text including equations in LaTeX.\nThese documents provide a complete and self-contained record of a computation that can be converted to various formats and shared with others using email, version control systems (like git/GitHub) or nbviewer.jupyter.org.\n\n\nThe Jupyter Notebook ecosystem consists of three main components:\n\nNotebook Editor\nKernels\nNotebook Documents\n\nLet‚Äôs explore each of these components in detail:\n\n\nThe Notebook editor is an interactive web-based application for creating and editing notebook documents. It enables users to write and run code, add rich text, and multimedia content. When running Jupyter on a server, users typically use either the classic Jupyter Notebook interface or JupyterLab, an advanced version with more features.\nKey features of the Notebook editor include:\n\nCode Editing: Write and edit code in individual cells.\nCode Execution: Run code cells in any order and display computation results in various formats (HTML, LaTeX, PNG, SVG, PDF).\nInteractive Widgets: Create and use JavaScript widgets that connect user interface controls to kernel-side computations.\nRich Text: Add documentation using Markdown markup language, including LaTeX equations.\n\n\n\n\n\n\n\nAdvance Notebook Editor Info\n\n\n\n\n\nThe Notebook editor in Jupyter offers several advanced features:\n\nCell Metadata: Each cell has associated metadata that can be used to control its behavior. This includes tags for slideshows, hiding code cells, and controlling cell execution.\nMagic Commands: Special commands prefixed with % (line magics) or %% (cell magics) that provide additional functionality, such as timing code execution or displaying plots inline.\nAuto-completion: The editor provides context-aware auto-completion for Python code, helping users write code more efficiently.\nCode Folding: Users can collapse long code blocks for better readability.\nMultiple Cursors: Advanced editing with multiple cursors for simultaneous editing at different locations.\nSplit View: The ability to split the notebook view, allowing users to work on different parts of the notebook simultaneously.\nVariable Inspector: A tool to inspect and manage variables in the kernel‚Äôs memory.\nIntegrated Debugger: Some Jupyter environments offer an integrated debugger for step-by-step code execution and inspection.\n\n\n\n\n\n\n\n\nKernels are the computational engines that execute the code contained in a notebook. They are separate processes that run independently of the notebook editor.\nKey responsibilities of kernels include: * Executing user code * Returning computation results to the notebook editor * Handling computations for interactive widgets * Providing features like tab completion and introspection\n\n\n\n\n\n\nAdvanced Kernel Info\n\n\n\n\n\nJupyter notebooks are language-agnostic. Different kernels can be installed to support various programming languages such as Python, R, Julia, and many others. The default kernel runs Python code, but users can select different kernels for each notebook via the Kernel menu.\nKernels communicate with the notebook editor using a JSON-based protocol over ZeroMQ/WebSockets. For more technical details, see the messaging specification.\nEach kernel runs in its own environment, which can be customized to include specific libraries and dependencies. This allows users to create isolated environments for different projects, ensuring that dependencies do not conflict.\nKernels also support interactive features such as:\n\nTab Completion: Provides suggestions for variable names, functions, and methods as you type, improving coding efficiency.\nIntrospection: Allows users to inspect objects, view documentation, and understand the structure of code elements.\nRich Output: Supports various output formats, including text, images, videos, and interactive widgets, enhancing the interactivity of notebooks.\n\nAdvanced users can create custom kernels to support additional languages or specialized computing environments. This involves writing a kernel specification and implementing the necessary communication protocols.\nFor managing kernels, Jupyter provides several commands and options:\n\nStarting a Kernel: Automatically starts when a notebook is opened.\nInterrupting a Kernel: Stops the execution of the current code cell, useful for halting long-running computations.\nRestarting a Kernel: Clears the kernel‚Äôs memory and restarts it, useful for resetting the environment or recovering from errors.\nShutting Down a Kernel: Stops the kernel and frees up system resources.\n\nUsers can also monitor kernel activity and resource usage through the Jupyter interface, ensuring efficient and effective use of computational resources.\n\n\n\n\n\n\nNotebook documents are self-contained files that encapsulate all content created in the notebook editor. They include code inputs/outputs, Markdown text, equations, images, and other media. Each document is associated with a specific kernel and serves as both a human-readable record of analysis and an executable script to reproduce the work.\nCharacteristics of notebook documents:\n\nFile Extension: Notebooks are stored as files with a .ipynb extension.\nStructure: Notebooks consist of a linear sequence of cells, which can be one of three types:\n\nCode cells: Contain executable code and its output.\nMarkdown cells: Contain formatted text, including LaTeX equations.\nRaw cells: Contain unformatted text, preserved when converting notebooks to other formats.\n\n\n\n\n\n\n\n\nAdvanced Notebook Documents Info\n\n\n\n\n\n\nVersion Control: Notebook documents can be version controlled using systems like Git. This allows users to track changes, collaborate with others, and revert to previous versions if needed. Tools like nbdime provide diff and merge capabilities specifically designed for Jupyter Notebooks.\nCell Tags: Cells in a notebook can be tagged with metadata to control their behavior during execution, export, or presentation. For example, tags can be used to hide input or output, skip execution, or designate cells as slides in a presentation.\nInteractive Widgets: Notebook documents can include interactive widgets that allow users to manipulate parameters and visualize changes in real-time. This is particularly useful for data exploration and interactive simulations.\nExtensions: The Jupyter ecosystem supports a wide range of extensions that enhance the functionality of notebook documents. These extensions can add features like spell checking, code formatting, and integration with external tools and services.\nSecurity: Notebook documents can include code that executes on the user‚Äôs machine, which poses security risks. Jupyter provides mechanisms to sanitize notebooks and prevent the execution of untrusted code. Users should be cautious when opening notebooks from unknown sources.\nCollaboration: Jupyter Notebooks can be shared and collaboratively edited in real-time using platforms like Google Colab, Microsoft Azure Notebooks, and JupyterHub. These platforms provide cloud-based environments where multiple users can work on the same notebook simultaneously.\nCustomization: Users can customize the appearance and behavior of notebook documents using CSS and JavaScript. This allows for the creation of tailored interfaces and enhanced user experiences.\nExport Options: In addition to static formats, notebooks can be exported to interactive formats like dashboards and web applications. Tools like Voila convert notebooks into standalone web applications that can be shared and deployed.\nProvenance: Notebooks can include provenance information that tracks the origin and history of data and computations. This is important for reproducibility and transparency in scientific research.\nDocumentation: Notebook documents can serve as comprehensive documentation for projects, combining code, results, and narrative text. This makes them valuable for teaching, tutorials, and sharing research findings.\nPerformance: Large notebooks with many cells and outputs can become slow and unwieldy. Techniques like cell output clearing, using lightweight data formats, and splitting notebooks into smaller parts can help maintain performance.\nIntegration: Jupyter Notebooks can integrate with a wide range of data sources, libraries, and tools. This includes databases, cloud storage, machine learning frameworks, and visualization libraries, making them a versatile tool for data science and research.\nInternal Format: Notebook files are JSON text files with binary data encoded in base64, making them easy to manipulate programmatically.\nExportability: Notebooks can be exported to various static formats (HTML, reStructuredText, LaTeX, PDF, slide shows) using Jupyter‚Äôs nbconvert utility.\nSharing: Notebooks can be shared via nbviewer, which renders notebooks from public URLs or GitHub as static web pages, allowing others to view the content without installing Jupyter.\n\n\n\n\nThis integrated system of editor, kernels, and documents makes Jupyter Notebooks a powerful tool for interactive computing, data analysis, and sharing of computational narratives."
  },
  {
    "objectID": "lectures/lecture01/01-lecture01.html#using-the-notebook-editor",
    "href": "lectures/lecture01/01-lecture01.html#using-the-notebook-editor",
    "title": "Jupyter Notebooks",
    "section": "Using the Notebook Editor",
    "text": "Using the Notebook Editor\n\n\n\nJupyter Notebook Editor\n\n\nThe Jupyter Notebook editor provides an interactive environment for writing code, creating visualizations, and documenting computational workflows. It consists of a web-based interface that allows users to create and edit notebook documents containing code, text, equations, images, and interactive elements. A Jupyter Notebook provides an interface with essentially two modes of operation:\n\nedit mode the mode where you edit a cells content.\ncommand mode the mode where you execute the cells content.\n\nIn the more advanced version of JupyterLab you can also have a presentation mode where you can present your notebook as a slideshow.\n\nEdit mode\nEdit mode is indicated by a blue cell border and a prompt showing in the editor area when a cell is selected. You can enter edit mode by pressing Enter or using the mouse to click on a cell‚Äôs editor area.\n\n\n\nEdit Mode\n\n\nWhen a cell is in edit mode, you can type into the cell, like a normal text editor\n\n\nCommand mode\nCommand mode is indicated by a grey cell border with a blue left margin. When you are in command mode, you are able to edit the notebook as a whole, but not type into individual cells. Most importantly, in command mode, the keyboard is mapped to a set of shortcuts that let you perform notebook and cell actions efficiently.\n\n\n\nCommand Mode\n\n\nIf you have a hardware keyboard connected to your iOS device, you can use Jupyter keyboard shortcuts. The modal user interface of the Jupyter Notebook has been optimized for efficient keyboard usage. This is made possible by having two different sets of keyboard shortcuts: one set that is active in edit mode and another in command mode.\n\n\nKeyboard navigation\nIn edit mode, most of the keyboard is dedicated to typing into the cell‚Äôs editor area. Thus, in edit mode there are relatively few shortcuts available. In command mode, the entire keyboard is available for shortcuts, so there are many more. Most important ones are:\n\nSwitch command and edit mods: Enter for edit mode, and Esc or Control for command mode.\nBasic navigation: ‚Üë/k, ‚Üì/j\nRun or render currently selected cell: Shift+Enter or Control+Enter\nSaving the notebook: s\nChange Cell types: y to make it a code cell, m for markdown and r for raw\nInserting new cells: a to insert above, b to insert below\nManipulating cells using pasteboard: x for cut, c for copy, v for paste, d for delete and z for undo delete\nKernel operations: i to interrupt and 0 to restart\n\n\n\nRunning code\nCode cells allow you to enter and run code. Run a code cell by pressing the ‚ñ∂Ô∏é button in the bottom-right panel, or Control+Enter on your hardware keyboard.\n\nv = 23752636\nprint(v)\n\n23752636\n\n\nThere are a couple of keyboard shortcuts for running code:\n\nControl+Enter run the current cell and enters command mode.\nShift+Enter runs the current cell and moves selection to the one below.\nOption+Enter runs the current cell and inserts a new one below."
  },
  {
    "objectID": "lectures/lecture01/01-lecture01.html#managing-the-kernel",
    "href": "lectures/lecture01/01-lecture01.html#managing-the-kernel",
    "title": "Jupyter Notebooks",
    "section": "Managing the kernel",
    "text": "Managing the kernel\nCode is run in a separate process called the kernel, which can be interrupted or restarted. You can see kernel indicator in the top-right corner reporting current kernel state: ‚ö™Ô∏é means kernel is ready to execute code, and ‚ö´Ô∏é means kernel is currently busy. Tapping kernel indicator will open kernel menu, where you can reconnect, interrupt or restart kernel.\nTry running the following cell ‚Äî kernel indicator will switch from ‚ö™Ô∏é to ‚ö´Ô∏é, i.e.¬†reporting kernel as ‚Äúbusy‚Äù. This means that you won‚Äôt be able to run any new cells until current execution finishes, or until kernel is interrupted. You can then go to kernel menu by tapping the kernel indicator and select ‚ÄúInterrupt‚Äù."
  },
  {
    "objectID": "lectures/lecture01/01-lecture01.html#markdown-in-notebooks",
    "href": "lectures/lecture01/01-lecture01.html#markdown-in-notebooks",
    "title": "Jupyter Notebooks",
    "section": "Markdown in Notebooks",
    "text": "Markdown in Notebooks\nText can be added to Jupyter Notebooks using Markdown cells. This is extremely useful providing a complete documentation of your calculations or simulations. In this way, everything really becomes an notebook. You can change the cell type to Markdown by using the ‚ÄúCell Actions‚Äù menu, or with a hardware keyboard shortcut m. Markdown is a popular markup language that is a superset of HTML. Its specification can be found here:\nhttps://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet\nMarkdown cells can either be rendered or unrendered.\nWhen they are rendered, you will see a nice formatted representation of the cell‚Äôs contents.\nWhen they are unrendered, you will see the raw text source of the cell. To render the selected cell, click the ‚ñ∂Ô∏é button or shift+ enter. To unrender, select the markdown cell, and press enter or just double click.\n\nMarkdown basics\nBelow are some basic markdown examples, in its rendered form. If you which to access how to create specific appearances, double click the individual cells to put the into an unrendered edit mode.\nYou can make text italic or bold. You can build nested itemized or enumerated lists:\n\nFirst item\n\nFirst subitem\n\nFirst sub-subitem\n\nSecond subitem\n\nFirst subitem of second subitem\nSecond subitem of second subitem\n\n\nSecond item\n\nFirst subitem\n\nThird item\n\nFirst subitem\n\n\nNow another list:\n\nHere we go\n\nSublist\n\nSublist\n\n\nThere we go\nNow this\n\nHere is a blockquote:\n\nBeautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren‚Äôt special enough to break the rules. Namespaces are one honking great idea ‚Äì let‚Äôs do more of those!\n\nAnd Web links:\nJupyter‚Äôs website\n\n\nHeadings\nYou can add headings by starting a line with one (or multiple) # followed by a space and the title of your section. The number of # you use will determine the size of the heading\n# Heading 1\n# Heading 2\n## Heading 2.1\n## Heading 2.2\n### Heading 2.2.1\n\n\nEmbedded code\nYou can embed code meant for illustration instead of execution in Python:\ndef f(x):\n    \"\"\"a docstring\"\"\"\n    return x**2\n\n\nLaTeX equations\nCourtesy of MathJax, you can include mathematical expressions both inline: \\(e^{i\\pi} + 1 = 0\\) and displayed:\n\\[e^x=\\sum_{i=0}^\\infty \\frac{1}{i!}x^i\\]\nInline expressions can be added by surrounding the latex code with $:\n$e^{i\\pi} + 1 = 0$\nExpressions on their own line are surrounded by $$:\n$$e^x=\\sum_{i=0}^\\infty \\frac{1}{i!}x^i$$\n\n\nImages\nImages may be also directly integrated into a Markdown block.\nTo include images use\n![alternative text](url)\nfor example\n\n\n\nalternative text\n\n\n\n\nVideos\nTo include videos, we use HTML code like\n&lt;video src=\"mov/movie.mp4\" width=\"320\" height=\"200\" controls preload&gt;&lt;/video&gt;\nin the Markdown cell. This works with videos stored locally.\n\n\nYou can embed YouTube Videos as well by using the IPython module.\n\nfrom IPython.display import YouTubeVideo\nYouTubeVideo('QlLx32juGzI',width=600)"
  },
  {
    "objectID": "lectures/lecture06/input-output.html",
    "href": "lectures/lecture06/input-output.html",
    "title": "Input and output",
    "section": "",
    "text": "To try out all of the functions of todays notebook, we will need to use the embedded JupyterLite notebook. To use the notebook, click ob File -&gt; Open from URL and paste the following link into the input field:\nhttps://raw.githubusercontent.com/fcichos/EMPP24/refs/heads/main/seminars/1_input_output.ipynb\nTo download the data files, click ob File -&gt; Open from URL and paste the following links into the input field:\nhttps://raw.githubusercontent.com/fcichos/EMPP24/refs/heads/main/seminars/MyData.txt\nhttps://raw.githubusercontent.com/fcichos/EMPP24/refs/heads/main/seminars/2018-04-11_sds011_sensor_12253.csv\nhttps://raw.githubusercontent.com/fcichos/EMPP24/refs/heads/main/seminars/2018-04-12_sds011_sensor_12253.csv\nYou should then have 3 data files and one notebook. You can then go into fullscreen mode.\nFull Screen"
  },
  {
    "objectID": "lectures/lecture09/fourier-analysis.html",
    "href": "lectures/lecture09/fourier-analysis.html",
    "title": "Fourier Analysis",
    "section": "",
    "text": "Fourier analysis, or the description of functions as a series of sine and cosine functions, serves as a powerful tool in both numerical data analysis and the solution of differential equations. In experimental physics, Fourier transforms find widespread applications. For instance, optical tweezers utilize frequency spectra to characterize positional fluctuations, while Lock-In detection employs Fourier analysis for specific frequency signals. Additionally, many optical phenomena can be understood through the lens of Fourier transforms.\nFourier analysis extends far beyond these examples, finding applications across numerous fields of physics and engineering. In this lecture, we will examine Fourier Series and Fourier transforms from a mathematical perspective. We will apply these concepts to analyze the frequency spectrum of oscillations in coupled pendula, and later revisit them when simulating the motion of a Gaussian wavepacket in quantum mechanics."
  },
  {
    "objectID": "lectures/lecture09/fourier-analysis.html#fourier-series",
    "href": "lectures/lecture09/fourier-analysis.html#fourier-series",
    "title": "Fourier Analysis",
    "section": "Fourier series",
    "text": "Fourier series\n\nBasic Idea\nThe fundamental idea behind Fourier analysis is that any periodic function can be decomposed into a sum of simple sine and cosine waves. This is similar to how white light can be decomposed into its color spectrum using a prism. Just as white light is a superposition of many colors (frequencies), any periodic signal is a superposition of simple oscillations at different frequencies.\n\n\nMathematical Definition\nA Fourier series represents a periodic function \\(f(t)\\) with period \\(2\\pi\\) or, more generally, any arbitrary interval \\(T\\) as a sum of sine and cosine functions:\n\\[\nf(t)=\\frac{A_{0}}{2}+\\sum_{k=1}^{\\infty}\\left ( A_{k}\\cos\\left (\\omega_k t\\right) + B_{k}\\sin\\left (\\omega_k t\\right)\\right )\n\\tag{1}\\]\nwhere \\(\\omega_k=\\frac{2\\pi k}{T}\\). Here, \\(T\\) represents the period of the cosine and sine functions, with their amplitudes defined by the coefficients \\(A_k\\) and \\(B_k\\). The term \\(A_0\\) represents a constant offset added to the oscillating functions. Equation¬†1 expresses an arbitrary periodic function \\(f(t)\\) on an interval T as a sum of oscillating sine and cosine functions with discrete frequencies (\\(\\omega_k\\)):\n\\[\\begin{equation*}\n\\omega_k= 0, \\frac{2\\pi}{T}, \\frac{4\\pi}{T}, \\frac{6\\pi}{T}, ... , \\frac{n\\pi}{T}\n\\end{equation*}\\]\nand varying amplitudes.\nPhysical interpretation: The coefficient \\(A_0/2\\) represents the average value (DC component) of the function. Each pair of terms with frequency \\(\\omega_k\\) represents an oscillation at that frequency, where \\(A_k\\) controls the amplitude of the cosine part and \\(B_k\\) controls the amplitude of the sine part.\n\n\nOrthogonality of Basis Functions\nThe key mathematical property that makes Fourier series work is orthogonality: different sine and cosine functions are orthogonal to each other when integrated over one period. This is analogous to perpendicular vectors in geometry.\nWe can demonstrate that the cosine and sine functions in the sum (Equation¬†1) are orthogonal using the trigonometric identity:\n\\[\n\\sin(\\omega_{i} t)\\sin(\\omega_{k}t )=\\frac{1}{2}\\left\\{\\cos((\\omega_{i}-\\omega_{k})t)- \\cos((\\omega_{i}+\\omega_{k})t)\\right\\}\n\\]\nThis leads to the integral:\n\\[\\begin{equation}\n\\int\\limits_{-\\frac{T}{2}}^{+\\frac{T}{2}}  \\sin(\\omega_{i}t)\\sin (\\omega_k t) dt\n\\end{equation}\\]\nwhich splits into two integrals over cosine functions with sum \\((\\omega_{1}+\\omega_{2})\\) and difference frequency \\((\\omega_{1}-\\omega_{2})\\). With \\(\\omega_k=k 2\\pi/T\\), \\((k \\in \\mathbb{Z}^+ )\\), this evaluates to:\n\\[\\begin{equation}\n\\int\\limits_{-\\frac{T}{2}}^{+\\frac{T}{2}}  \\sin(\\omega_{i}t)\\sin (\\omega_k t) dt  =\\begin{cases}\n0 &\\text{for }  i\\neq k, \\\\\nT/2 &\\text{for }  i=k\n\\end{cases}\n\\end{equation}\\]\nA similar result holds for cosine functions:\n\\[\\begin{equation}\n\\int\\limits_{-\\frac{T}{2}}^{+\\frac{T}{2}}  \\cos(\\omega_{i}t)\\cos (\\omega_k t) dt  =\\begin{cases}\n0 &\\text{for }  i\\neq k, \\\\\nT/2 &\\text{for }  i=k\n\\end{cases}\n\\end{equation}\\]\nWe can verify this orthogonality numerically:\n\n\n\n\n\n\nThe coefficients \\(A_k\\) and \\(B_k\\) are determined by projecting the function \\(f(t)\\) onto these basis functions:\n\\[\\begin{align}\n\\int\\limits_{-\\frac{T}{2}}^{+\\frac{T}{2}} & \\cos (\\omega_k t) dt  =\\begin{cases}\n0 &\\text{for }  k\\neq0, \\\\\nT &\\text{for }  k=0\n\\end{cases} \\\\\n\\int\\limits_{-\\frac{T}{2}}^{+\\frac{T}{2}} & \\sin(\\omega_k t) dt=0  \\text{ for all }k\n\\end{align}\\]\nFor the cosine coefficients:\n\\[\\begin{equation}\\label{A_k}\nA_k=\\frac{2}{T}\\int\\limits_{-\\frac{T}{2}}^{+\\frac{T}{2}} f(t)\\cos(\\omega_k t) dt  \\text{ for } k \\neq 0\n\\end{equation}\\]\nand the constant term:\n\\[\\begin{equation}\nA_0= \\frac{1}{T}\\int\\limits_{-\\frac{T}{2}}^{+\\frac{T}{2}} f(t) dt\n\\end{equation}\\]\nFinally, for the sine coefficients:\n\\[\\begin{equation}\\label{B_k}\nB_k=\\frac{2}{T}\\int\\limits_{-\\frac{T}{2}}^{+\\frac{T}{2}} f(t) \\sin(\\omega_k t) dt,\\,  \\forall k\n\\end{equation}\\]\n\n\nSimple Example: Square Wave\nLet‚Äôs visualize how a Fourier series works by approximating a square wave:\n\n\n\n\n\n\nObservation: As we add more terms, the Fourier series approximates the square wave better. This demonstrates that complex periodic functions can be built from simple sine and cosine components.\nAnalyzing the Fourier coefficients:\nFor a square wave, the Fourier series contains only odd sine terms with coefficients \\(B_k = \\frac{4}{k\\pi}\\) for odd \\(k\\), and \\(A_k = 0\\) for all \\(k\\). Let‚Äôs visualize how the amplitude of these components decreases with frequency:\n\n\n\n\n\n\nObservation: The amplitudes decay as \\(1/k\\), meaning higher frequency components contribute less to the overall signal. This is why even with a finite number of terms, we can get a reasonable approximation of the square wave. The rapid transitions (edges) of the square wave require high-frequency components, which is why we see the Gibbs phenomenon (overshoot near discontinuities) in the finite approximation."
  },
  {
    "objectID": "lectures/lecture09/fourier-analysis.html#fourier-transform",
    "href": "lectures/lecture09/fourier-analysis.html#fourier-transform",
    "title": "Fourier Analysis",
    "section": "Fourier transform",
    "text": "Fourier transform\n\nFrom Series to Transform\nThe Fourier transform extends the concept of Fourier series to non-periodic functions. While Fourier series use discrete frequencies (\\(\\omega_k = 2\\pi k/T\\)) for periodic functions, the Fourier transform uses a continuous spectrum of frequencies for arbitrary functions.\nKey differences:\n\nFourier Series: Periodic functions ‚Üí discrete frequencies (sum)\nFourier Transform: Non-periodic functions ‚Üí continuous frequencies (integral)\n\nThe Fourier transform represents arbitrary non-periodic functions \\(f(t)\\) through a continuous spectrum of complex functions \\(\\exp(i\\omega t)\\). This approach replaces the discrete frequency sum of sine and cosine functions found in Fourier series with an integral over complex exponential functions \\(\\exp(i\\omega t)\\) spanning continuous frequency values \\(\\omega\\).\n\n\nMathematical Definition\nThe Fourier transform of a function \\(f(t)\\) is defined as:\n\\[\nF(\\omega)=\\int\\limits_{-\\infty}^{+\\infty}f(t)e^{-i\\omega t}dt\n\\tag{2}\\]\nwhere \\(F(\\omega)\\) represents the spectrum of frequencies present in \\(f(t)\\). The inverse Fourier transform recovers the original function \\(f(t)\\) from its frequency spectrum (Equation¬†2):\n\\[\\begin{equation}\\label{eq:inverse_FT}\nf(t)=\\frac{1}{2\\pi}\\int\\limits_{-\\infty}^{+\\infty}F(\\omega)e^{+i\\omega t}dt\n\\end{equation}\\]\nThe Fourier transform \\(F(\\omega)\\) yields a complex number, encoding both phase and amplitude information of the oscillations. The phase of oscillation at frequency \\(\\omega\\) is given by:\n\\[\\begin{equation}\n\\phi=\\tan^{-1}\\left(\\frac{Im(F(\\omega))}{Re(F(\\omega))}\\right)\n\\end{equation}\\]\nwhile the amplitude at frequency \\(\\omega\\) is:\n\\[\\begin{equation}\n|F(\\omega)|=\\sqrt{\\text{Re}(F(\\omega))^2 + \\text{Im}(F(\\omega))^2}\n\\end{equation}\\]\n\n\nDiscrete Fourier Transform (DFT) and FFT\nIn practice, we work with sampled data (discrete time points), not continuous functions. The Discrete Fourier Transform (DFT) is the computational version of the Fourier transform for sampled data:\n\\[\nF_k = \\sum_{n=0}^{N-1} f_n e^{-2\\pi i kn/N}\n\\]\nwhere \\(f_n\\) are the \\(N\\) sampled data points and \\(F_k\\) are the frequency components.\nFast Fourier Transform (FFT): Computing the DFT naively requires \\(O(N^2)\\) operations. The FFT is an algorithm that computes the same result in only \\(O(N \\log N)\\) operations, making it practical for large datasets.\nModern computing libraries like NumPy implement the FFT. Here‚Äôs how to use it:\n\n\n\n\n\n\nKey NumPy FFT functions:\n\nnp.fft.fft(signal): Computes the FFT of the signal\nnp.fft.fftfreq(N, dt): Generates the corresponding frequency axis\n\nN: number of data points\ndt: time step between samples"
  },
  {
    "objectID": "lectures/lecture09/fourier-analysis.html#frequency-analysis-of-our-coupled-pendula",
    "href": "lectures/lecture09/fourier-analysis.html#frequency-analysis-of-our-coupled-pendula",
    "title": "Fourier Analysis",
    "section": "Frequency analysis of our coupled pendula",
    "text": "Frequency analysis of our coupled pendula\nLet us now apply Fourier analysis to examine the data from our previous simulation of coupled pendula. We need to first generate the data by solving the coupled pendula equations for the three different cases: normal mode 1 (in-phase), normal mode 2 (out-of-phase), and beats.\n\nGenerate the coupled pendula data\n\n\n\n\n\n\n\n\nVisualize the time-domain signals first\nBefore performing Fourier analysis, let‚Äôs look at the oscillations in the time domain:\n\n\n\n\n\n\n\n\nPerform Fourier transform and visualize frequency spectra\nNow we perform the Fourier transform of our signals to see which frequencies are present:\n\n\n\n\n\n\n\n\nCompare with theoretical predictions\nLet‚Äôs calculate the theoretical normal mode frequencies and compare with our FFT results:\n\n\n\n\n\n\n\n\nPhysical Interpretation\nKey observations:\n\nNormal mode 1 (in-phase): The FFT shows a single peak at frequency \\(f_1 \\approx\\) 0.288 Hz. This corresponds to the natural frequency of an uncoupled pendulum \\(\\omega_1 = \\sqrt{g/L}\\).\nNormal mode 2 (out-of-phase): The FFT shows a single peak at a higher frequency \\(f_2 \\approx\\) 0.295 Hz. The spring coupling increases the restoring force, resulting in faster oscillations.\nBeat mode: The FFT shows two peaks at both \\(f_1\\) and \\(f_2\\)! This confirms that the beat motion is a superposition of the two normal modes. The energy oscillation between pendula (beats) occurs because these two frequency components interfere.\n\nFundamental principle: Any motion of a coupled oscillator system can be decomposed into a superposition of its normal modes with specific amplitudes and phases. The Fourier transform reveals this decomposition explicitly!\nThis is analogous to how white light can be decomposed into its color spectrum, or how a musical chord can be decomposed into individual notes."
  },
  {
    "objectID": "lectures/lecture07/differentiation.html",
    "href": "lectures/lecture07/differentiation.html",
    "title": "Numerical Differentiation",
    "section": "",
    "text": "Differentiation is one of the most fundamental operations in physics‚Äîit allows us to describe rates of change, from the motion of particles to the evolution of fields. While we briefly encountered derivatives when exploring array slicing, we‚Äôll now develop a deeper understanding of numerical differentiation and the mathematical machinery that makes it work on a computer."
  },
  {
    "objectID": "lectures/lecture07/differentiation.html#introduction-why-derivatives-matter-in-physics",
    "href": "lectures/lecture07/differentiation.html#introduction-why-derivatives-matter-in-physics",
    "title": "Numerical Differentiation",
    "section": "Introduction: Why Derivatives Matter in Physics",
    "text": "Introduction: Why Derivatives Matter in Physics\nDerivatives describe how quantities change, making them indispensable throughout physics. Consider velocity, which is the derivative of position with respect to time: \\[v = \\frac{dx}{dt}\\]. This tells us how rapidly an object‚Äôs position changes. Taking another derivative gives acceleration: \\[a = \\frac{dv}{dt}\\], describing how velocity itself changes. These concepts combine in Newton‚Äôs second law, \\[F = ma = m\\frac{d^2x}{dt^2}\\], connecting force to the second derivative of position.\nIn calculus courses, you learn to differentiate functions analytically‚Äîapplying rules to obtain exact symbolic expressions. But in computational physics, we often work with discrete data points from experiments or numerical simulations, not continuous analytical functions. We might measure a particle‚Äôs position at specific time intervals, or simulate a system on a finite grid. In these cases, we need numerical differentiation‚Äîmethods to approximate derivatives from discrete data.\nThe challenge is that the mathematical definition of a derivative involves a limit as the interval approaches zero, but on a computer, we work with finite intervals. How accurately can we approximate derivatives from discrete points? What methods work best? These are the questions we‚Äôll explore in this lecture."
  },
  {
    "objectID": "lectures/lecture07/differentiation.html#first-order-derivative",
    "href": "lectures/lecture07/differentiation.html#first-order-derivative",
    "title": "Numerical Differentiation",
    "section": "First Order Derivative",
    "text": "First Order Derivative\n\nThe Basic Idea: From Calculus to Computation\nRecall from calculus that the derivative of a function \\(f(x)\\) at a point \\(x\\) is defined as the limit of the difference quotient as the interval \\(\\Delta x\\) shrinks to zero:\n\\[\nf^{\\prime}(x) = \\lim_{\\Delta x \\rightarrow 0} \\frac{f(x + \\Delta x) - f(x)}{\\Delta x}\n\\]\nThis limit captures the instantaneous rate of change‚Äîthe slope of the tangent line at a single point. But computers can‚Äôt handle infinitesimals or continuous limits. Instead, we work with finite differences. The simplest approximation drops the limit and uses a small but finite \\(\\Delta x\\):\n\\[\nf^{\\prime}_{i} \\approx \\frac{f_{i+1} - f_{i}}{\\Delta x}\n\\]\nIn this forward difference method, we approximate the derivative at position \\(i\\) by comparing the function value at \\(i\\) with the value one step ahead at \\(i+1\\). The name ‚Äúforward‚Äù comes from the fact that we look ahead (to the right) from our current position. Geometrically, this computes the slope of a secant line connecting two nearby points rather than the true tangent.\nThe forward difference is intuitive and easy to implement, but it‚Äôs not the most accurate approach. Let‚Äôs see how we can do better using Taylor series‚Äîa powerful tool that connects the discrete and continuous worlds.\n\n\nImproving Accuracy: The Central Difference Formula\nThe forward difference works, but we can achieve significantly better accuracy using a clever trick from calculus: the Taylor series expansion. Taylor series express a function‚Äôs values near a point in terms of its derivatives at that point‚Äîexactly what we need!\nConsider the Taylor expansion of our function around position \\(x_0\\):\n\\[\nf(x) = f(x_{0}) + (x - x_0) f^{\\prime}(x) + \\frac{(x - x_0)^2}{2!} f^{\\prime\\prime}(x) + \\frac{(x - x_0)^3}{3!} f^{(3)}(x) + \\ldots\n\\]\nFor discrete points separated by spacing \\(\\Delta x\\), we can write the function value at position \\(i+1\\) as:\n\\[\nf_{i+1} = f_{i} + \\Delta x f_{i}^{\\prime} + \\frac{\\Delta x^2}{2!} f_{i}^{\\prime\\prime} + \\frac{\\Delta x^3}{3!} f_{i}^{(3)} + \\ldots\n\\]\nSimilarly, expanding backward to position \\(i-1\\) gives:\n\\[\nf_{i-1} = f_{i} - \\Delta x f_{i}^{\\prime} + \\frac{\\Delta x^2}{2!} f_{i}^{\\prime\\prime} - \\frac{\\Delta x^3}{3!} f_{i}^{(3)} + \\ldots\n\\]\nNow comes the elegant part: if we subtract the backward expansion from the forward expansion, something remarkable happens:\n\\[\nf_{i+1} - f_{i-1} = 2 \\Delta x f_{i}^{\\prime} + O(\\Delta x^3)\n\\]\nNotice what happened‚Äîthe constant term \\(f_i\\) cancels, and more importantly, the second-order term \\(\\frac{\\Delta x^2}{2!} f_{i}^{\\prime\\prime}\\) also cancels (it appeared with opposite signs). The first-order derivative term doubles to \\(2\\Delta x f_{i}^{\\prime}\\), while the third-order and higher terms combine with odd powers of \\(\\Delta x\\). Solving for the derivative and neglecting terms of order \\(\\Delta x^3\\) and higher, we obtain the central difference formula:\n\\[\nf^{\\prime}_{i} \\approx \\frac{f_{i+1} - f_{i-1}}{2 \\Delta x}\n\\]\nThis is significantly more accurate than the forward difference because the leading error term is proportional to \\(\\Delta x^3\\) instead of \\(\\Delta x^2\\). In physics terms, if you halve your grid spacing, the forward difference error decreases by a factor of 4, but the central difference error decreases by a factor of 8. This makes central differences the preferred choice for most applications.\nThe key insight is that by using information from both sides of point \\(i\\) (symmetrically sampling at \\(i-1\\) and \\(i+1\\)), we achieve cancellation of the second-order error. This is analogous to how symmetric configurations often lead to simplifications in physics‚Äîthink of symmetric charge distributions simplifying electric field calculations, or how symmetric potentials lead to parity conservation in quantum mechanics.\n\n\nExample: Derivative of sin(x) using Arrays\nLet‚Äôs implement the central difference formula using arrays. We‚Äôll calculate the derivative of \\(\\sin(x)\\), which we know should give us \\(\\cos(x)\\).\n\n\n\n\n\n\nLet‚Äôs plot the results:\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Array Slicing for Derivatives\n\n\n\nWhen we write y[2:] we get all elements from index 2 to the end: \\([f_2, f_3, f_4, ..., f_n]\\)\nWhen we write y[:-2] we get all elements from the start up to (but not including) the last 2: \\([f_0, f_1, f_2, ..., f_{n-2}]\\)\nSo y[2:] - y[:-2] gives us: \\([f_2 - f_0, f_3 - f_1, f_4 - f_2, ..., f_n - f_{n-2}]\\)\nThis is exactly \\(f_{i+1} - f_{i-1}\\) for each point \\(i\\)!\n\n\n\n\nHandling Boundaries: Forward and Backward Differences\nThe central difference formula requires points on both sides of our evaluation point, which creates a problem at the boundaries. At the leftmost point (index 0), there‚Äôs no point to the left; at the rightmost point, there‚Äôs no point to the right. We need special treatment for these edge cases.\nAt the left boundary, we use the forward difference: \\[\nf^{\\prime}_{0} \\approx \\frac{f_{1} - f_{0}}{\\Delta x}\n\\]\nAt the right boundary, we use the backward difference: \\[\nf^{\\prime}_{n} \\approx \\frac{f_{n} - f_{n-1}}{\\Delta x}\n\\]\nThese are less accurate than central differences (with \\(O(\\Delta x^2)\\) error instead of \\(O(\\Delta x^3)\\)), but they‚Äôre the best we can do with the available points at the boundaries. In some applications, more sophisticated boundary treatments using multiple points or extrapolation techniques can improve accuracy, but for most purposes, these simple formulas suffice.\nLet‚Äôs implement a complete derivative function that combines central differences in the interior with forward/backward differences at the boundaries:\n\n\n\n\n\n\nNow let‚Äôs test it:\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced: Matrix Representation of Derivatives\n\n\n\n\n\nHere‚Äôs a cool alternative perspective! We can represent the derivative operation as a matrix multiplication. This might seem fancy, but it‚Äôs actually quite elegant and shows that differentiation is just a linear operation.\n\nThe Linear Operator Perspective\nHere‚Äôs a profound insight: differentiation is a linear operator. This means that the derivative of a sum equals the sum of derivatives, and constants can be pulled out: \\(\\frac{d}{dx}[af(x) + bg(x)] = a\\frac{df}{dx} + b\\frac{dg}{dx}\\). All linear operators on finite-dimensional spaces can be represented as matrices!\nFor discrete points \\(x_i\\), we can express the forward difference formula as a matrix-vector multiplication:\n\\[\nf^{\\prime} = \\frac{1}{\\Delta x}\n\\begin{bmatrix}\n-1 & 1  & 0 & 0 & 0 & 0\\\\\n0 & -1 & 1 & 0 & 0 & 0\\\\\n0 & 0  & -1 & 1 & 0 & 0\\\\\n0 & 0  & 0  & -1 & 1 & 0\\\\\n0 & 0  & 0  &  0 & -1 & 1\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nf_{1}\\\\\nf_{2}\\\\\nf_{3}\\\\\nf_{4}\\\\\nf_{5}\\\\\nf_{6}\\\\\n\\end{bmatrix}\n\\]\nEach row of this matrix encodes the finite difference formula for one point. The first row computes \\(-f_1 + f_2\\) (which is \\(f_2 - f_1\\)), the second row computes \\(-f_2 + f_3\\), and so on. When we multiply this matrix by the vector of function values and divide by \\(\\Delta x\\), we get all the derivatives at once!\nThis matrix perspective might seem like mathematical overkill for a simple derivative calculation, but it reveals deep structure. The differentiation matrix is sparse (mostly zeros) and has a specific banded structure‚Äînonzero elements only appear near the diagonal. This sparsity makes operations computationally efficient even for large grids.\n\n\nImplementation with SciPy\n\n\n\n\n\n\n\n\nFor Central Differences\nFor central differences \\(\\frac{f_{i+1} - f_{i-1}}{2\\Delta x}\\), we use:\n\n\n\n\n\n\n\n\nWhy This Matrix Approach Matters\nThe matrix representation offers several advantages beyond conceptual elegance. First, it provides clarity‚Äîseeing differentiation as matrix multiplication makes explicit that we‚Äôre applying a linear operator. This connects to the broader mathematical framework of linear algebra that pervades physics, from quantum mechanics (where operators act on state vectors) to continuum mechanics (where differential operators describe field evolution).\nSecond, it‚Äôs computationally efficient. Modern numerical libraries like NumPy are highly optimized for matrix operations, often using hardware-accelerated BLAS (Basic Linear Algebra Subprograms) implementations. By formulating differentiation as a matrix operation, we leverage decades of optimization work.\nThird, this perspective becomes essential when solving partial differential equations (PDEs), which we‚Äôll encounter in more advanced computational physics. Many PDE solution methods involve constructing large differentiation matrices and solving linear systems. Understanding the matrix representation now prepares you for these more complex problems.\nFinally, it demonstrates that seemingly different mathematical operations can be equivalent‚Äîfinite differences and matrix multiplication are just two views of the same underlying computation. This kind of mathematical flexibility is valuable throughout physics.\nNote that the @ operator in Python denotes matrix multiplication (following proper linear algebra rules), distinct from element-wise multiplication using *."
  },
  {
    "objectID": "lectures/lecture07/differentiation.html#second-order-derivative-measuring-curvature",
    "href": "lectures/lecture07/differentiation.html#second-order-derivative-measuring-curvature",
    "title": "Numerical Differentiation",
    "section": "Second Order Derivative: Measuring Curvature",
    "text": "Second Order Derivative: Measuring Curvature\nThe second derivative measures curvature‚Äîhow rapidly the rate of change itself is changing. In physics, second derivatives are ubiquitous. They appear in Newton‚Äôs second law \\[F = m\\ddot{x}\\] in the wave equation \\[\\frac{\\partial^2 u}{\\partial t^2} = c^2\\frac{\\partial^2 u}{\\partial x^2}\\] and in the Schr√∂dinger equation \\[-\\frac{\\hbar^2}{2m}\\frac{d^2\\psi}{dx^2} + V\\psi = E\\psi\\]. Understanding how to compute second derivatives numerically is therefore essential.\nWe can derive the second derivative formula using the same Taylor series approach. This time, instead of subtracting the forward and backward expansions to eliminate the second derivative term, we add them to isolate it:\n\\[\nf_{i+1} + f_{i-1} = 2f_{i} + \\Delta x^2 f_{i}^{\\prime\\prime} + O(\\Delta x^4)\n\\]\nNotice that the first derivative terms \\(\\pm\\Delta x f_{i}^{\\prime}\\) cancel when we add, while the second derivative terms double. The third-order terms also cancel due to their opposite signs. Solving for the second derivative and neglecting fourth-order and higher terms:\n\\[\nf_{i}^{\\prime\\prime}\\approx \\frac{f_{i-1}-2f_{i}+f_{i+1}}{\\Delta x^2}\n\\]\nThis formula has the same order of accuracy as our central difference formula for the first derivative‚Äîthe error is \\(O(\\Delta x^2)\\). The symmetric combination of three points (\\(i-1\\), \\(i\\), and \\(i+1\\)) captures the local curvature at point \\(i\\).\nPhysically, you can interpret this formula as measuring how much the function deviates from linear behavior. If \\(f\\) is perfectly linear, the three points lie on a straight line, and \\(f_{i-1} - 2f_i + f_{i+1} = 0\\), giving zero second derivative. Any curvature‚Äîeither concave up (positive second derivative) or concave down (negative)‚Äîproduces a nonzero result.\n\nExample: Second Derivative of sin(x)\nThe second derivative of \\(\\sin(x)\\) is \\(-\\sin(x)\\)‚Äîa beautiful result showing that sine is an eigenfunction of the second derivative operator with eigenvalue \\(-1\\). This property underlies simple harmonic motion and wave phenomena throughout physics. Let‚Äôs verify that our numerical formula reproduces this result:"
  },
  {
    "objectID": "lectures/lecture07/differentiation.html#using-scipy-standing-on-the-shoulders-of-giants",
    "href": "lectures/lecture07/differentiation.html#using-scipy-standing-on-the-shoulders-of-giants",
    "title": "Numerical Differentiation",
    "section": "Using SciPy: Standing on the Shoulders of Giants",
    "text": "Using SciPy: Standing on the Shoulders of Giants\nWhile implementing differentiation algorithms yourself builds understanding, production code should use well-tested libraries. The SciPy module provides robust, optimized differentiation functions that handle edge cases and numerical precision issues more carefully than our simple implementations.\nThe scipy.misc.derivative function calculates numerical derivatives of any function you provide. It uses adaptive algorithms that automatically choose appropriate step sizes and combines multiple Richardson extrapolation steps to improve accuracy:\n\n\n\n\n\n\nYou can also calculate higher derivatives by specifying the n parameter:\n\n\n\n\n\n\nThe order parameter controls the number of points used in the finite difference formula. Higher-order formulas use more neighboring points to achieve better accuracy by canceling more error terms in the Taylor expansion. This is analogous to using higher-order approximations in perturbation theory or more terms in a multipole expansion‚Äîmore computational effort yields more accurate results:"
  },
  {
    "objectID": "lectures/lecture07/differentiation.html#summary-from-continuous-to-discrete-calculus",
    "href": "lectures/lecture07/differentiation.html#summary-from-continuous-to-discrete-calculus",
    "title": "Numerical Differentiation",
    "section": "Summary: From Continuous to Discrete Calculus",
    "text": "Summary: From Continuous to Discrete Calculus\nThis lecture bridged the gap between the continuous calculus you learn in mathematics courses and the discrete computations required for numerical physics. We developed methods to approximate derivatives from finite data points, understanding both how these methods work and why they achieve different levels of accuracy.\nNumerical differentiation approximates derivatives using finite differences‚Äîcomputing slopes from discrete function values rather than taking continuous limits. The simplest approach is the forward difference, but we showed that central differences achieve higher accuracy by symmetrically sampling points on both sides of our evaluation point. The cancellation of even-order error terms in the Taylor expansion makes central differences significantly more accurate than one-sided methods.\nThe key formulas for uniformly spaced grids are:\n\nFirst derivative: \\(f^{\\prime}_{i} \\approx \\frac{f_{i+1} - f_{i-1}}{2 \\Delta x}\\) with error \\(O(\\Delta x^2)\\)\nSecond derivative: \\(f^{\\prime\\prime}_{i} \\approx \\frac{f_{i-1}-2f_{i}+f_{i+1}}{\\Delta x^2}\\) with error \\(O(\\Delta x^2)\\)\n\nWe implemented these formulas using NumPy array slicing, leveraging vectorization for efficiency. We also explored the matrix representation of differentiation, revealing that finite difference operations are linear transformations that can be expressed as sparse matrix multiplication. This perspective becomes crucial when solving partial differential equations.\nFinally, we examined SciPy‚Äôs production-ready differentiation functions, which employ sophisticated adaptive algorithms and Richardson extrapolation to achieve high accuracy.\nThese numerical differentiation techniques are fundamental tools for computational physics. They allow us to work with experimental data, approximate solutions to differential equations, and simulate physical systems on computers. In the next lectures, we‚Äôll apply these tools to solve ordinary and partial differential equations, bringing us closer to simulating real physical phenomena.\n\n\n\n\n\n\nThe Bigger Picture: Why Numerical Differentiation Matters\n\n\n\nPhysics is fundamentally a science of change, and derivatives are the mathematical language we use to describe that change. Most of the laws governing the physical universe are expressed as differential equations‚Äîrelationships involving derivatives of physical quantities. Newton‚Äôs second law relates force to the second time derivative of position: \\(F = m\\frac{d^2x}{dt^2}\\). Maxwell‚Äôs equations describe how electric and magnetic fields evolve through space and time using partial derivatives. The Schr√∂dinger equation governs quantum mechanics through a second spatial derivative. The heat equation, wave equation, diffusion equation‚Äîall involve derivatives.\nWhen we solve these equations on a computer, we must discretize them, replacing continuous derivatives with finite difference approximations. Every numerical simulation of physical phenomena‚Äîfrom weather forecasting to quantum chemistry to computational fluid dynamics‚Äîrelies on the principles of numerical differentiation we‚Äôve explored here.\nIn upcoming lectures, we‚Äôll apply these tools to solve ordinary differential equations (ODEs), allowing us to predict the time evolution of dynamical systems. Later, we‚Äôll extend to partial differential equations (PDEs), simulating fields and waves. The numerical differentiation techniques you‚Äôve learned today form the foundation for all of these more advanced topics in computational physics."
  },
  {
    "objectID": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html",
    "href": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html",
    "title": "Brownian Motion Course Restructuring",
    "section": "",
    "text": "Date: January 2025 Status: ‚úÖ Complete\n\n\nBrownian Motion content has been split into two modules to improve learning outcomes:\n\nWeek 2-3: Simple, function-based approach (NEW)\nWeek 6-7: Advanced OOP approach (EXISTING, moved from Week 2-3)\n\nThis change reduces cognitive load, allows students to focus on physics first, and demonstrates the value of OOP through direct comparison.\n\n\n\n\n\n\n\nlectures/lecture02/4_brownian_motion_simple.qmd\n\nSimple Brownian motion using only functions and numpy arrays\nNo classes, no OOP\nFocus on physics and basic programming\nSuitable for Week 2-3 when students are still learning fundamentals\n\nlectures/lecture05/brownian_motion_comparison.qmd\n\nSide-by-side comparison of function-based vs.¬†OOP approaches\nShows when to use each approach\nDemonstrates scalability issues with functions\nOptional supplementary material\n\nlectures/PEDAGOGICAL_NOTES.md\n\nDetailed rationale for the restructuring\nTeaching tips for each stage\nAssessment guidance\nSuccess metrics\n\n\n\n\n\n\n_quarto.yml\n\nWeek 2-3 now links to 4_brownian_motion_simple.qmd\nWeek 6-7 section uncommented with Classes + OOP Brownian Motion\nProper progression: simple ‚Üí advanced\n\nlectures/lecture05/2_brownian_motion.qmd\n\nUpdated title to ‚ÄúBrownian Motion with Object-Oriented Programming‚Äù\nAdded note comparing to the simple approach\nClarified this is the OOP version\n\n\n\n\n\n\n\nWeek 2-3: Modeling Motion\n‚îú‚îÄ‚îÄ Datatypes for Physics\n‚îú‚îÄ‚îÄ Functions: Reusable Physics Equations\n‚îú‚îÄ‚îÄ Application: Brownian Motion (Simple)    ‚Üê NEW\n‚îî‚îÄ‚îÄ Arrays with Numpy\n\nWeek 6-7: Code Organization\n‚îú‚îÄ‚îÄ Modules: Organizing Your Code\n‚îú‚îÄ‚îÄ Classes & Objects\n‚îú‚îÄ‚îÄ Advanced: Brownian Motion with OOP       ‚Üê MOVED HERE\n‚îî‚îÄ‚îÄ Working with Data Files\n\n\n\n\n\n\nOriginal structure taught classes in Week 2-3 via Brownian Motion, causing:\n\n‚ùå Cognitive overload (too many new concepts at once)\n‚ùå Students confused by self, __init__, OOP paradigm\n‚ùå Physics understanding suffered\n‚ùå Students asked ‚Äúwhy do we need this?‚Äù (no motivation for OOP yet)\n\n\n\n\nTwo-stage approach:\nStage 1 (Week 2-3): Physics First - Students understand Brownian motion conceptually - Build confidence with functions, loops, arrays - See immediate results ‚Üí motivation boost - Natural programming progression\nStage 2 (Week 6-7): Professional Code - Revisit familiar physics with better tools - Students SEE why classes help (direct comparison) - Natural motivation: ‚Äúmy Week 2 code was messy, this is better!‚Äù - Learning OOP makes sense in context\n\n\n\n\n\n\n\nFile: lectures/lecture02/4_brownian_motion_simple.qmd\nLearning Goals: - Understand Brownian motion physics - Generate random steps with np.random.normal() - Store trajectories in arrays - Compute and plot mean squared displacement - Build confidence with functions\nTeaching Tips: - Emphasize: ‚ÄúWe‚Äôre focusing on physics first!‚Äù - Celebrate when students get their first random walk working - Allow experimentation with parameters (D, dt, N) - Don‚Äôt mention ‚Äúwe‚Äôll do this better later‚Äù - let them enjoy the success\nCommon Questions: - Q: ‚ÄúWhy lists instead of something fancier?‚Äù - A: ‚ÄúKeeping it simple so we focus on understanding the physics!‚Äù\n\n\n\nFile: lectures/lecture05/2_brownian_motion.qmd\nLearning Goals: - Understand when to use classes vs.¬†functions - Design and implement classes for physics simulations - Appreciate code organization benefits - Compare approaches critically\nTeaching Tips: - Start with: ‚ÄúRemember our Brownian motion? Let‚Äôs improve it!‚Äù - Show the comparison document (optional) - Emphasize scalability: ‚ÄúImagine 1000 particles with different properties‚Ä¶‚Äù - Let students refactor their Week 2 code into classes\nCommon Questions: - Q: ‚ÄúIs OOP really necessary?‚Äù - A: ‚ÄúFor small problems, no. But you‚Äôre becoming a computational physicist - you‚Äôll write complex simulations. This is how professionals do it.‚Äù\n\n\n\n\n\n\n\nTest: - Physics: What causes Brownian motion? - Coding: Generate random steps, plot trajectories - Analysis: Interpret MSD plots - Problem-solving: Modify simulation parameters\nAvoid: - Questions about classes or OOP - Complex data structures\n\n\n\nTest: - Design: When to use classes? - Implementation: Write a simple class - Comparison: Function vs.¬†OOP tradeoffs - Application: Extend the Colloid class\nCan include: - ‚ÄúCompare your Week 2 solution to this OOP version‚Äù - ‚ÄúRefactor this function-based code into a class‚Äù\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFile\nWeek\nPurpose\nStatus\n\n\n\n\nlecture02/4_brownian_motion_simple.qmd\n2-3\nSimple function-based simulation\n‚úÖ NEW\n\n\nlecture05/1_classes.qmd\n6-7\nClasses introduction\n‚úÖ Existing\n\n\nlecture05/2_brownian_motion.qmd\n6-7\nOOP Brownian motion\n‚úÖ Updated\n\n\n\n\n\n\n\n\n\n\n\n\n\nFile\nPurpose\n\n\n\n\nlecture05/brownian_motion_comparison.qmd\nSide-by-side comparison (optional)\n\n\nlectures/PEDAGOGICAL_NOTES.md\nDetailed teaching rationale\n\n\nlectures/BROWNIAN_MOTION_RESTRUCTURE_README.md\nThis document\n\n\n\n\n\n\n\n\nYou‚Äôll know this restructuring is working when:\n‚úÖ Week 2-3: Students excited about physics simulations (not frustrated by syntax)\n‚úÖ Week 6-7: Students appreciate OOP value (not asking ‚Äúwhy bother?‚Äù)\n‚úÖ Homework: More physics questions, fewer ‚Äúhow does self work?‚Äù questions\n‚úÖ Course feedback: Students mention clear progression and building confidence\n‚úÖ Code quality: Students use classes appropriately in later projects\n\n\n\n\nConsider for future iterations:\n\nVideo comparison: Show both approaches being coded live\nDebugging exercise: Give students messy function-based code ‚Üí refactor to classes\nProject: Students implement both versions and write reflection\nAdvanced extension: Particle interactions (naturally benefits from OOP)\nApply pattern to other topics: Planetary motion, wave simulations, etc.\n\n\n\n\n\nBefore teaching:\n\nTest all code cells in both Brownian motion notebooks\nVerify Quarto builds correctly\nCheck that links in _quarto.yml work\nReview pedagogical notes\nPrepare example solutions for homework\nCreate answer key showing both approaches\n\n\n\n\n\nFor pedagogical questions: See PEDAGOGICAL_NOTES.md\nFor technical issues: Check notebook code cells execute properly\nFor course structure: Consult _quarto.yml and verify week numbering\n\n\n\n\nThis restructuring addresses student feedback indicating confusion with early OOP introduction. The two-stage approach follows best practices in computational science education:\n\nConcrete before abstract (Papert)\nSpiral learning (Bruner)\nJust-in-time teaching (Novak)\nCognitive load management (Sweller)\n\n\nLast Updated: January 2025 Maintained By: Course Instructor Version: 1.0"
  },
  {
    "objectID": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#executive-summary",
    "href": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#executive-summary",
    "title": "Brownian Motion Course Restructuring",
    "section": "",
    "text": "Brownian Motion content has been split into two modules to improve learning outcomes:\n\nWeek 2-3: Simple, function-based approach (NEW)\nWeek 6-7: Advanced OOP approach (EXISTING, moved from Week 2-3)\n\nThis change reduces cognitive load, allows students to focus on physics first, and demonstrates the value of OOP through direct comparison."
  },
  {
    "objectID": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#what-changed",
    "href": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#what-changed",
    "title": "Brownian Motion Course Restructuring",
    "section": "",
    "text": "lectures/lecture02/4_brownian_motion_simple.qmd\n\nSimple Brownian motion using only functions and numpy arrays\nNo classes, no OOP\nFocus on physics and basic programming\nSuitable for Week 2-3 when students are still learning fundamentals\n\nlectures/lecture05/brownian_motion_comparison.qmd\n\nSide-by-side comparison of function-based vs.¬†OOP approaches\nShows when to use each approach\nDemonstrates scalability issues with functions\nOptional supplementary material\n\nlectures/PEDAGOGICAL_NOTES.md\n\nDetailed rationale for the restructuring\nTeaching tips for each stage\nAssessment guidance\nSuccess metrics\n\n\n\n\n\n\n_quarto.yml\n\nWeek 2-3 now links to 4_brownian_motion_simple.qmd\nWeek 6-7 section uncommented with Classes + OOP Brownian Motion\nProper progression: simple ‚Üí advanced\n\nlectures/lecture05/2_brownian_motion.qmd\n\nUpdated title to ‚ÄúBrownian Motion with Object-Oriented Programming‚Äù\nAdded note comparing to the simple approach\nClarified this is the OOP version"
  },
  {
    "objectID": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#course-structure-updated",
    "href": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#course-structure-updated",
    "title": "Brownian Motion Course Restructuring",
    "section": "",
    "text": "Week 2-3: Modeling Motion\n‚îú‚îÄ‚îÄ Datatypes for Physics\n‚îú‚îÄ‚îÄ Functions: Reusable Physics Equations\n‚îú‚îÄ‚îÄ Application: Brownian Motion (Simple)    ‚Üê NEW\n‚îî‚îÄ‚îÄ Arrays with Numpy\n\nWeek 6-7: Code Organization\n‚îú‚îÄ‚îÄ Modules: Organizing Your Code\n‚îú‚îÄ‚îÄ Classes & Objects\n‚îú‚îÄ‚îÄ Advanced: Brownian Motion with OOP       ‚Üê MOVED HERE\n‚îî‚îÄ‚îÄ Working with Data Files"
  },
  {
    "objectID": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#why-this-change",
    "href": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#why-this-change",
    "title": "Brownian Motion Course Restructuring",
    "section": "",
    "text": "Original structure taught classes in Week 2-3 via Brownian Motion, causing:\n\n‚ùå Cognitive overload (too many new concepts at once)\n‚ùå Students confused by self, __init__, OOP paradigm\n‚ùå Physics understanding suffered\n‚ùå Students asked ‚Äúwhy do we need this?‚Äù (no motivation for OOP yet)\n\n\n\n\nTwo-stage approach:\nStage 1 (Week 2-3): Physics First - Students understand Brownian motion conceptually - Build confidence with functions, loops, arrays - See immediate results ‚Üí motivation boost - Natural programming progression\nStage 2 (Week 6-7): Professional Code - Revisit familiar physics with better tools - Students SEE why classes help (direct comparison) - Natural motivation: ‚Äúmy Week 2 code was messy, this is better!‚Äù - Learning OOP makes sense in context"
  },
  {
    "objectID": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#teaching-guide",
    "href": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#teaching-guide",
    "title": "Brownian Motion Course Restructuring",
    "section": "",
    "text": "File: lectures/lecture02/4_brownian_motion_simple.qmd\nLearning Goals: - Understand Brownian motion physics - Generate random steps with np.random.normal() - Store trajectories in arrays - Compute and plot mean squared displacement - Build confidence with functions\nTeaching Tips: - Emphasize: ‚ÄúWe‚Äôre focusing on physics first!‚Äù - Celebrate when students get their first random walk working - Allow experimentation with parameters (D, dt, N) - Don‚Äôt mention ‚Äúwe‚Äôll do this better later‚Äù - let them enjoy the success\nCommon Questions: - Q: ‚ÄúWhy lists instead of something fancier?‚Äù - A: ‚ÄúKeeping it simple so we focus on understanding the physics!‚Äù\n\n\n\nFile: lectures/lecture05/2_brownian_motion.qmd\nLearning Goals: - Understand when to use classes vs.¬†functions - Design and implement classes for physics simulations - Appreciate code organization benefits - Compare approaches critically\nTeaching Tips: - Start with: ‚ÄúRemember our Brownian motion? Let‚Äôs improve it!‚Äù - Show the comparison document (optional) - Emphasize scalability: ‚ÄúImagine 1000 particles with different properties‚Ä¶‚Äù - Let students refactor their Week 2 code into classes\nCommon Questions: - Q: ‚ÄúIs OOP really necessary?‚Äù - A: ‚ÄúFor small problems, no. But you‚Äôre becoming a computational physicist - you‚Äôll write complex simulations. This is how professionals do it.‚Äù"
  },
  {
    "objectID": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#assessment-recommendations",
    "href": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#assessment-recommendations",
    "title": "Brownian Motion Course Restructuring",
    "section": "",
    "text": "Test: - Physics: What causes Brownian motion? - Coding: Generate random steps, plot trajectories - Analysis: Interpret MSD plots - Problem-solving: Modify simulation parameters\nAvoid: - Questions about classes or OOP - Complex data structures\n\n\n\nTest: - Design: When to use classes? - Implementation: Write a simple class - Comparison: Function vs.¬†OOP tradeoffs - Application: Extend the Colloid class\nCan include: - ‚ÄúCompare your Week 2 solution to this OOP version‚Äù - ‚ÄúRefactor this function-based code into a class‚Äù"
  },
  {
    "objectID": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#files-reference",
    "href": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#files-reference",
    "title": "Brownian Motion Course Restructuring",
    "section": "",
    "text": "File\nWeek\nPurpose\nStatus\n\n\n\n\nlecture02/4_brownian_motion_simple.qmd\n2-3\nSimple function-based simulation\n‚úÖ NEW\n\n\nlecture05/1_classes.qmd\n6-7\nClasses introduction\n‚úÖ Existing\n\n\nlecture05/2_brownian_motion.qmd\n6-7\nOOP Brownian motion\n‚úÖ Updated\n\n\n\n\n\n\n\n\n\n\n\n\n\nFile\nPurpose\n\n\n\n\nlecture05/brownian_motion_comparison.qmd\nSide-by-side comparison (optional)\n\n\nlectures/PEDAGOGICAL_NOTES.md\nDetailed teaching rationale\n\n\nlectures/BROWNIAN_MOTION_RESTRUCTURE_README.md\nThis document"
  },
  {
    "objectID": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#success-metrics",
    "href": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#success-metrics",
    "title": "Brownian Motion Course Restructuring",
    "section": "",
    "text": "You‚Äôll know this restructuring is working when:\n‚úÖ Week 2-3: Students excited about physics simulations (not frustrated by syntax)\n‚úÖ Week 6-7: Students appreciate OOP value (not asking ‚Äúwhy bother?‚Äù)\n‚úÖ Homework: More physics questions, fewer ‚Äúhow does self work?‚Äù questions\n‚úÖ Course feedback: Students mention clear progression and building confidence\n‚úÖ Code quality: Students use classes appropriately in later projects"
  },
  {
    "objectID": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#optional-enhancements",
    "href": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#optional-enhancements",
    "title": "Brownian Motion Course Restructuring",
    "section": "",
    "text": "Consider for future iterations:\n\nVideo comparison: Show both approaches being coded live\nDebugging exercise: Give students messy function-based code ‚Üí refactor to classes\nProject: Students implement both versions and write reflection\nAdvanced extension: Particle interactions (naturally benefits from OOP)\nApply pattern to other topics: Planetary motion, wave simulations, etc."
  },
  {
    "objectID": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#testing-checklist",
    "href": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#testing-checklist",
    "title": "Brownian Motion Course Restructuring",
    "section": "",
    "text": "Before teaching:\n\nTest all code cells in both Brownian motion notebooks\nVerify Quarto builds correctly\nCheck that links in _quarto.yml work\nReview pedagogical notes\nPrepare example solutions for homework\nCreate answer key showing both approaches"
  },
  {
    "objectID": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#questions-or-issues",
    "href": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#questions-or-issues",
    "title": "Brownian Motion Course Restructuring",
    "section": "",
    "text": "For pedagogical questions: See PEDAGOGICAL_NOTES.md\nFor technical issues: Check notebook code cells execute properly\nFor course structure: Consult _quarto.yml and verify week numbering"
  },
  {
    "objectID": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#acknowledgments",
    "href": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#acknowledgments",
    "title": "Brownian Motion Course Restructuring",
    "section": "",
    "text": "This restructuring addresses student feedback indicating confusion with early OOP introduction. The two-stage approach follows best practices in computational science education:\n\nConcrete before abstract (Papert)\nSpiral learning (Bruner)\nJust-in-time teaching (Novak)\nCognitive load management (Sweller)\n\n\nLast Updated: January 2025 Maintained By: Course Instructor Version: 1.0"
  },
  {
    "objectID": "lectures/lecture10/spring-pendulum.html",
    "href": "lectures/lecture10/spring-pendulum.html",
    "title": "Spring Pendulum: When Two Motions Combine",
    "section": "",
    "text": "In the previous lecture, we applied odeint to planetary motion - a system with radial and angular motion coupled by Newton‚Äôs gravity. The mathematics was elegant, and the orbits were stable and predictable.\nNow let‚Äôs explore a seemingly simpler system that turns out to be much more complex: the spring pendulum.\nImagine hanging a weight from a spring. If you pull it down and release it, the spring bounces up and down. If you push it sideways, it swings like a pendulum. But what happens if you do both at the same time?\nThis seemingly simple question leads to one of the most fascinating systems in classical mechanics. Like planetary motion, it involves radial and angular coordinates. But unlike the stable ellipses we saw with planets, the spring pendulum can:"
  },
  {
    "objectID": "lectures/lecture10/spring-pendulum.html#the-physical-setup",
    "href": "lectures/lecture10/spring-pendulum.html#the-physical-setup",
    "title": "Spring Pendulum: When Two Motions Combine",
    "section": "The Physical Setup",
    "text": "The Physical Setup\n\n\n\nSpringPendulum.png\n\n\nThe spring pendulum consists of: - A spring attached to a fixed point (spring constant \\(k\\)) - A mass \\(m\\) hanging from the spring - The spring‚Äôs natural (rest) length is \\(L_0\\)\n\nTwo Ways to Describe Motion\nWe need two coordinates to describe where the mass is at any time:\n\n\\(L\\): How much the spring is stretched (or compressed) beyond its natural length\n\n\\(L = 0\\) means the spring is at its natural length\n\\(L &gt; 0\\) means stretched\n\\(L &lt; 0\\) means compressed\n\n\\(\\theta\\): The angle from vertical\n\n\\(\\theta = 0\\) means hanging straight down\n\\(\\theta &gt; 0\\) means swinging to the right\n\\(\\theta &lt; 0\\) means swinging to the left\n\n\nThe total length of the pendulum at any moment is \\(L_0 + L\\).\n\n\n\n\n\n\nWhy These Coordinates?\n\n\n\nWe call \\(L\\) and \\(\\theta\\) generalized coordinates. They‚Äôre natural choices because: - They directly measure the spring stretch and pendulum swing - They simplify the equations of motion - They‚Äôre called ‚Äúpolar coordinates‚Äù (radius and angle)"
  },
  {
    "objectID": "lectures/lecture10/spring-pendulum.html#the-equations-of-motion",
    "href": "lectures/lecture10/spring-pendulum.html#the-equations-of-motion",
    "title": "Spring Pendulum: When Two Motions Combine",
    "section": "The Equations of Motion",
    "text": "The Equations of Motion\nJust like we learned in Week 5, we need to convert Newton‚Äôs second law (\\(F = ma\\)) into equations we can solve with odeint. For the spring pendulum, we get two coupled differential equations:\n\nRadial Motion (Spring Direction)\n\\[\\begin{equation}\n\\ddot{L} = (L_0+L)\\dot{\\theta}^2-\\frac{k}{m}L+g\\cos(\\theta)\n\\end{equation}\\]\nLet‚Äôs understand each term:\n\n\\((L_0+L)\\dot{\\theta}^2\\): The centrifugal effect - when the pendulum swings, the mass ‚Äúwants‚Äù to fly outward, stretching the spring\n\\(-\\frac{k}{m}L\\): The spring restoring force (Hooke‚Äôs law) - pulls back toward the natural length\n\\(g\\cos(\\theta)\\): The radial component of gravity - when tilted, gravity has a component along the spring\n\n\n\nAngular Motion (Swinging Direction)\n\\[\\begin{equation}\n\\ddot{\\theta} = -\\frac{1}{L_0+L}\\left[g\\sin(\\theta)+2\\dot{L}\\dot{\\theta}\\right]\n\\end{equation}\\]\nUnderstanding these terms:\n\n\\(g\\sin(\\theta)\\): The gravitational torque - this is what makes a normal pendulum swing\n\\(2\\dot{L}\\dot{\\theta}\\): The coupling term - when the spring length changes, it affects the swing speed (like an ice skater spinning faster when pulling arms in!)\n\\(\\frac{1}{L_0+L}\\): The effect of changing pendulum length on rotation\n\n\n\n\n\n\n\nThe Key Insight: Coupling\n\n\n\nThese equations are coupled - they depend on each other! The spring stretch affects the swing, and the swing affects the spring stretch. This coupling creates the rich, complex behavior we‚Äôll explore.\n\n\n\n\n\n\n\n\nComparing to Planetary Motion\n\n\n\nNotice the similarities to the previous lecture: - Planetary Motion: Radial force is \\(-GM/r^2\\) (Newton‚Äôs inverse square law) - Spring Pendulum: Radial force is \\(-kL\\) (Hooke‚Äôs linear law)\nSame mathematical structure (centrifugal term + radial force + coupling), different force laws! The spring‚Äôs linear force creates very different behavior than gravity‚Äôs inverse square law."
  },
  {
    "objectID": "lectures/lecture10/spring-pendulum.html#converting-to-first-order-system",
    "href": "lectures/lecture10/spring-pendulum.html#converting-to-first-order-system",
    "title": "Spring Pendulum: When Two Motions Combine",
    "section": "Converting to First-Order System",
    "text": "Converting to First-Order System\nRemember from Week 5: odeint needs first-order differential equations. We have second-order equations (involving \\(\\ddot{L}\\) and \\(\\ddot{\\theta}\\)), so we need to convert them.\nWe introduce new variables for the velocities: - \\(v = \\dot{L}\\) (rate of spring stretch) - \\(\\omega = \\dot{\\theta}\\) (angular velocity)\nThis gives us four first-order equations:\n\\[\\begin{align}\n\\dot{L} &= v \\\\\n\\dot{v} &= (L_0+L)\\omega^2-\\frac{k}{m}L+g\\cos(\\theta) \\\\\n\\dot{\\theta} &= \\omega \\\\\n\\dot{\\omega} &= -\\frac{1}{L_0+L}\\left[g\\sin(\\theta)+2v\\omega\\right]\n\\end{align}\\]\nOur state vector is: \\([L, v, \\theta, \\omega]\\)"
  },
  {
    "objectID": "lectures/lecture10/spring-pendulum.html#implementing-the-equations",
    "href": "lectures/lecture10/spring-pendulum.html#implementing-the-equations",
    "title": "Spring Pendulum: When Two Motions Combine",
    "section": "Implementing the Equations",
    "text": "Implementing the Equations\nNow let‚Äôs write the function that odeint will use. This should look familiar from Week 5!\n\n\n\n\n\n\n\n\n\n\n\n\nCode Organization Tip\n\n\n\nNotice how we: 1. Documented what the function does 2. Unpacked the state for readability 3. Calculated each derivative clearly 4. Returned them in the same order as the state\nThis makes debugging much easier!"
  },
  {
    "objectID": "lectures/lecture10/spring-pendulum.html#setting-up-the-simulation",
    "href": "lectures/lecture10/spring-pendulum.html#setting-up-the-simulation",
    "title": "Spring Pendulum: When Two Motions Combine",
    "section": "Setting Up the Simulation",
    "text": "Setting Up the Simulation\n\nDefining Physical Parameters\nLet‚Äôs start with realistic values:\n\n\n\n\n\n\n\n\nInitial Conditions: Small Displacement\nLet‚Äôs start simple - pull the spring down slightly and see what happens:\n\n\n\n\n\n\n\n\nSolving the ODEs\nNow we use odeint - just like in Week 5!"
  },
  {
    "objectID": "lectures/lecture10/spring-pendulum.html#visualizing-the-motion",
    "href": "lectures/lecture10/spring-pendulum.html#visualizing-the-motion",
    "title": "Spring Pendulum: When Two Motions Combine",
    "section": "Visualizing the Motion",
    "text": "Visualizing the Motion\n\nConverting to Cartesian Coordinates\nTo see the actual path of the mass, we convert from \\((L, \\theta)\\) to \\((x, y)\\):\n\n\n\n\n\n\n\n\nTrajectory Plot\nLet‚Äôs see the path the mass follows:\n\n\n\n\n\n\n\n\n\n\n\n\nWhat Do You See?\n\n\n\nWith small initial displacements, you should see a relatively simple pattern - maybe some oscillation with a bit of wobble. This is the system in a regular regime.\nTry to identify: - Is the mass moving more up/down or side-to-side? - Does the pattern repeat? - Can you see both spring and pendulum motion?"
  },
  {
    "objectID": "lectures/lecture10/spring-pendulum.html#understanding-phase-space",
    "href": "lectures/lecture10/spring-pendulum.html#understanding-phase-space",
    "title": "Spring Pendulum: When Two Motions Combine",
    "section": "Understanding Phase Space",
    "text": "Understanding Phase Space\nPhase space plots are a powerful tool for understanding dynamical systems. Instead of plotting position vs.¬†time, we plot position vs.¬†velocity.\n\nWhy Phase Space?\n\nEach point represents the complete state of one degree of freedom\nThe trajectory shows how the system evolves\nRegular motion creates closed curves\nChaotic motion creates strange patterns\n\n\n\nPhase Space: Spring Motion\nLet‚Äôs look at the spring stretch vs.¬†its velocity:\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting Phase Space Plots\n\n\n\n\nClosed loops: Regular, periodic motion\nSpiral inward: Damped oscillation (not in our case - we have no damping!)\nSpiral outward: Growing amplitude\nTangled mess: Chaotic motion\n\n\n\n\n\nPhase Space: Pendulum Motion\nNow let‚Äôs look at the angle vs.¬†angular velocity:"
  },
  {
    "objectID": "lectures/lecture10/spring-pendulum.html#time-evolution-energy-exchange",
    "href": "lectures/lecture10/spring-pendulum.html#time-evolution-energy-exchange",
    "title": "Spring Pendulum: When Two Motions Combine",
    "section": "Time Evolution: Energy Exchange",
    "text": "Time Evolution: Energy Exchange\nOne of the most interesting features of the spring pendulum is the exchange of energy between the spring mode and the pendulum mode.\n\n\n\n\n\n\n\n\n\n\n\n\nLook for Patterns!\n\n\n\nCan you see: - Do both motions have the same frequency? - When the spring oscillation is large, is the pendulum swing small (or vice versa)? - Does energy seem to slosh back and forth between the two modes?"
  },
  {
    "objectID": "lectures/lecture10/spring-pendulum.html#exploration-different-regimes",
    "href": "lectures/lecture10/spring-pendulum.html#exploration-different-regimes",
    "title": "Spring Pendulum: When Two Motions Combine",
    "section": "Exploration: Different Regimes",
    "text": "Exploration: Different Regimes\nThe spring pendulum can show dramatically different behavior depending on the initial conditions and parameters. Let‚Äôs explore!\n\nExperiment 1: Larger Initial Angle\nTry starting with a bigger swing:\n\n\n\n\n\n\n\n\nExperiment 2: Initial Velocity\nWhat if we give the mass a push?\n\n\n\n\n\n\n\n\nExperiment 3: Changing Parameters\nTry different physical parameters:\n\n\n\n\n\n\n\n\n\n\n\n\nYour Turn to Experiment!\n\n\n\nTry modifying: 1. Spring constant k: What happens with a very soft spring (k=1) vs.¬†very stiff (k=100)? 2. Pendulum length L_o: Longer pendulum (L_o=10) vs.¬†shorter (L_o=2)? 3. Initial conditions: Large angles, large velocities, or both?\nCan you find conditions that create: - Nearly circular motion? - Figure-eight patterns? - Complex, irregular motion?"
  },
  {
    "objectID": "lectures/lecture10/spring-pendulum.html#the-road-to-chaos",
    "href": "lectures/lecture10/spring-pendulum.html#the-road-to-chaos",
    "title": "Spring Pendulum: When Two Motions Combine",
    "section": "The Road to Chaos",
    "text": "The Road to Chaos\nWith certain combinations of parameters and initial conditions, the spring pendulum can exhibit chaotic motion - extremely sensitive to initial conditions and unpredictable over long times.\n\nChaotic Parameters\n\n\n\n\n\n\nLook at the phase space for the chaotic case:\n\n\n\n\n\n\n\n\n\n\n\n\nSignatures of Chaos\n\n\n\nChaotic motion shows: - Dense, irregular trajectories that fill space - No repeating patterns even over long times - Extreme sensitivity to initial conditions (change the starting point by 0.001 and get completely different behavior!) - Phase space ‚Äúattractors‚Äù with fractal structure\nThis is deterministic chaos - the equations are completely deterministic (no randomness!), but the behavior appears random."
  },
  {
    "objectID": "lectures/lecture10/spring-pendulum.html#energy-conservation-check",
    "href": "lectures/lecture10/spring-pendulum.html#energy-conservation-check",
    "title": "Spring Pendulum: When Two Motions Combine",
    "section": "Energy Conservation Check",
    "text": "Energy Conservation Check\nEven though the motion can be complex, energy should be conserved (we have no damping).\nLet‚Äôs verify:\n\n\n\n\n\n\nCheck how well conserved it is:\n\n\n\n\n\n\n\n\n\n\n\n\nNumerical Accuracy\n\n\n\nThe energy should be nearly constant. Small variations (typically &lt; 0.1%) come from:\n\nNumerical errors in odeint\nFinite time step size\n\nIf you see large energy drift, try:\n\nIncreasing the number of time points N\nUsing tighter tolerances in odeint (see documentation)"
  },
  {
    "objectID": "lectures/lecture10/spring-pendulum.html#summary-what-weve-learned",
    "href": "lectures/lecture10/spring-pendulum.html#summary-what-weve-learned",
    "title": "Spring Pendulum: When Two Motions Combine",
    "section": "Summary: What We‚Äôve Learned",
    "text": "Summary: What We‚Äôve Learned\n\n\n\n\n\n\nüéì Key Takeaways\n\n\n\n\nCoupled Systems: The spring pendulum combines two degrees of freedom that influence each other\nODE Solving Workflow (from Week 5):\n\nWrite equations of motion\nConvert to first-order system\nDefine derivative function\nUse odeint to solve\n\nPhase Space: A powerful visualization tool for understanding dynamics\n\nClosed curves = periodic motion\nComplex patterns = chaotic motion\n\nEnergy Exchange: Energy can flow between different modes of motion\nChaos: Even simple deterministic systems can show complex, unpredictable behavior\nParameter Exploration: Small changes in parameters or initial conditions can dramatically change behavior"
  },
  {
    "objectID": "lectures/lecture10/spring-pendulum.html#challenge-exercises",
    "href": "lectures/lecture10/spring-pendulum.html#challenge-exercises",
    "title": "Spring Pendulum: When Two Motions Combine",
    "section": "Challenge Exercises",
    "text": "Challenge Exercises\n\n\n\n\n\n\nüéØ Try These!\n\n\n\n\nFind the Beat Pattern: Adjust parameters to create ‚Äúbeats‚Äù - where energy periodically sloshes between spring and pendulum modes\nCircular Orbit: Can you find initial conditions that create nearly circular motion? (Hint: balance centrifugal and spring forces)\nSensitivity to Initial Conditions: Take the chaotic case and run two simulations with initial angles differing by only 0.001 radians. How quickly do they diverge?\nFrequency Analysis: The spring has natural frequency \\(\\omega_s = \\sqrt{k/m}\\) and the pendulum has \\(\\omega_p = \\sqrt{g/L_0}\\). What happens when these are equal? When one is twice the other?\nPhase Space Animation: Can you create an animated plot showing how the system evolves in phase space over time?"
  },
  {
    "objectID": "lectures/lecture10/spring-pendulum.html#whats-next",
    "href": "lectures/lecture10/spring-pendulum.html#whats-next",
    "title": "Spring Pendulum: When Two Motions Combine",
    "section": "What‚Äôs Next?",
    "text": "What‚Äôs Next?\nYou‚Äôve now mastered applying odeint to two major physics problems in Week 6: 1. Planetary Motion (previous lecture) - stable, predictable orbits 2. Spring Pendulum (this lecture) - complex behavior and chaos\nBoth used the same mathematical framework but showed very different behaviors because of their different force laws. This demonstrates the power and generality of numerical methods!\nIn Week 7, we‚Äôll step back and learn better ways to organize our code using modules and classes. After working with these complex simulations, you‚Äôll understand why good code organization is essential as projects grow. The Brownian motion example in Week 7 will show how object-oriented programming can make simulations cleaner and more powerful! üéØ"
  },
  {
    "objectID": "lectures/lecture11/repetition.html",
    "href": "lectures/lecture11/repetition.html",
    "title": "Repetition",
    "section": "",
    "text": "After we have completed the first part of the course, we will have a repetition session to review the main concepts and topics covered so far. This will help reinforce your understanding and prepare you for the final exam. This part contains a number of exercises you can work on to test your knowledge and skills."
  },
  {
    "objectID": "lectures/lecture11/repetition.html#what-is-a-program",
    "href": "lectures/lecture11/repetition.html#what-is-a-program",
    "title": "Repetition",
    "section": "What is a Program?",
    "text": "What is a Program?\nA program is a sequence of instructions that tells a computer how to perform a specific task. These instructions must be:\n\nPrecise and unambiguous\nWritten in a language the computer understands\nLogically structured\nDesigned to achieve a specific goal"
  },
  {
    "objectID": "lectures/lecture11/repetition.html#basic-elements-of-python",
    "href": "lectures/lecture11/repetition.html#basic-elements-of-python",
    "title": "Repetition",
    "section": "Basic Elements of Python",
    "text": "Basic Elements of Python\n\nVariables and Data Types\nIn Python, variables are containers for storing data values. Python is dynamically typed, meaning you don‚Äôt need to declare variable types explicitly.\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Exercise 1: Unit Conversion\n\n\n\n\n\nWrite a program that converts a temperature from Celsius to Fahrenheit and Kelvin. Use the formulas:\n\n¬∞F = (¬∞C √ó 9/5) + 32\nK = ¬∞C + 273.15\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumerical Operations\nPython supports all basic mathematical operations:\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Exercise 2: Basic Kinematics\n\n\n\n\n\nCalculate the final velocity of an object given its initial velocity, acceleration, and time. Use the formula: v = v‚ÇÄ + at\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLists and Arrays\nFor physics calculations, we often need to work with collections of numbers:\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Exercise 3: Force Calculations\n\n\n\n\n\nCreate an array of masses (in kg) and calculate the force of gravity on each mass. Use F = mg where g = 9.81 m/s¬≤. Numpy is already imported and can be used with np\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "lectures/lecture11/repetition.html#control-structures",
    "href": "lectures/lecture11/repetition.html#control-structures",
    "title": "Repetition",
    "section": "Control Structures",
    "text": "Control Structures\n\nConditional Statements\nConditional statements allow programs to make decisions:\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Exercise 4: Phase of Matter\n\n\n\n\n\nWrite a program that determines the phase of water based on its temperature (assume standard pressure):\n\nBelow 0¬∞C: Solid (Ice)\n0-100¬∞C: Liquid\nAbove 100¬∞C: Gas (Steam)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Exercise 5: Projectile Range Calculator\n\n\n\n\n\nWrite a program that calculates if a projectile will hit a target given:\n\nInitial velocity\nLaunch angle\nTarget distance\n\nUse the range formula: \\(R = \\frac{v_0^2 \\sin(2\\theta)}{g}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLoops\nLoops allow repetition of code:\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Exercise 6: Radioactive Decay Calculator\n\n\n\n\n\nWrite a program that simulates radioactive decay over multiple half-lives:\n\nStart with an initial number of atoms (\\(N_0\\))\nCalculate remaining atoms after each half-life period\nContinue for 5 half-lives\n\nUse the formula \\(N(t) = N_0 \\cdot \\left(\\frac{1}{2}\\right)^{t/t_{1/2}}\\) where \\(t_{1/2}\\) is the half-life.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Exercise 7: Time to Ground Calculator\n\n\n\n\n\nWrite a program that calculates how long it takes for an object to reach the ground when dropped from different heights:\n\nStart with an initial height \\(h_0\\)\nCalculate position using \\(y = h_0 - \\frac{1}{2}gt^2\\)\nFind the time when \\(y = 0\\)\nUse small time steps (\\(dt = 0.01s\\))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "lectures/lecture11/repetition.html#basic-numerical-calculations",
    "href": "lectures/lecture11/repetition.html#basic-numerical-calculations",
    "title": "Repetition",
    "section": "Basic Numerical Calculations",
    "text": "Basic Numerical Calculations\nFor physics, we often use NumPy for numerical calculations:"
  },
  {
    "objectID": "lectures/lecture11/repetition.html#simple-physics-example",
    "href": "lectures/lecture11/repetition.html#simple-physics-example",
    "title": "Repetition",
    "section": "Simple Physics Example",
    "text": "Simple Physics Example\nLet‚Äôs calculate the position of a projectile under gravity:"
  },
  {
    "objectID": "lectures/lecture11/repetition.html#code-organization",
    "href": "lectures/lecture11/repetition.html#code-organization",
    "title": "Repetition",
    "section": "Code Organization",
    "text": "Code Organization\n\nUse meaningful variable names\nAdd comments to explain complex logic\nBreak down complex problems into smaller functions\nUse consistent indentation"
  },
  {
    "objectID": "lectures/lecture11/repetition.html#example-of-well-organized-code",
    "href": "lectures/lecture11/repetition.html#example-of-well-organized-code",
    "title": "Repetition",
    "section": "Example of Well-Organized Code",
    "text": "Example of Well-Organized Code\nHere‚Äôs an example calculating the period of a simple pendulum:"
  },
  {
    "objectID": "lectures/lecture11/repetition.html#classes-and-objects",
    "href": "lectures/lecture11/repetition.html#classes-and-objects",
    "title": "Repetition",
    "section": "Classes and Objects",
    "text": "Classes and Objects\nClasses are blueprints for creating objects that combine data (attributes) and functions (methods). This is particularly useful for modeling physical systems.\n\nBasic Class Structure\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced Self-Exercise 8: Electric Charge Class\n\n\n\n\n\nCreate a class representing an electric charge that can:\n\nStore position \\((x, y)\\), charge magnitude \\((q)\\), and mass \\((m)\\)\nCalculate electric potential at a point using \\(V = \\frac{kq}{r}\\)\nCalculate electric force on another charge using \\(F = \\frac{kq_1q_2}{r^2}\\)\nCalculate the direction of force (attraction/repulsion)\n\nUse \\(k = 8.99 \\times 10^9\\) N\\(\\cdot\\)m¬≤/C¬≤ (Coulomb‚Äôs constant)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "lectures/lecture11/repetition.html#a-physics-example-harmonic-oscillator",
    "href": "lectures/lecture11/repetition.html#a-physics-example-harmonic-oscillator",
    "title": "Repetition",
    "section": "A Physics Example: Harmonic Oscillator",
    "text": "A Physics Example: Harmonic Oscillator\nHere‚Äôs a more complete example modeling a harmonic oscillator:"
  },
  {
    "objectID": "lectures/lecture11/repetition.html#inheritance",
    "href": "lectures/lecture11/repetition.html#inheritance",
    "title": "Repetition",
    "section": "Inheritance",
    "text": "Inheritance\nClasses can inherit properties and methods from other classes:"
  },
  {
    "objectID": "lectures/lecture11/repetition.html#key-points-about-classes",
    "href": "lectures/lecture11/repetition.html#key-points-about-classes",
    "title": "Repetition",
    "section": "Key Points About Classes",
    "text": "Key Points About Classes\n\nClasses combine data (attributes) and functions (methods)\nThe __init__ method initializes new objects\nself refers to the instance of the class\nMethods can modify the object‚Äôs state\nInheritance allows creating specialized versions of classes\nClasses help organize code and model real-world systems\n\nClasses are particularly useful in physics for: - Modeling physical systems - Organizing simulation code - Creating reusable components - Building hierarchies of related objects"
  },
  {
    "objectID": "lectures/lecture05/classes-and-objects.html#object-oriented-programming",
    "href": "lectures/lecture05/classes-and-objects.html#object-oriented-programming",
    "title": "Classes and Objects",
    "section": "Object oriented programming",
    "text": "Object oriented programming\nA very useful programming concept is object oriented programming. In all the programs we wrote till now, we have designed our program around functions i.e.¬†blocks of statements which manipulate data. This is called the procedure-oriented way of programming.\nThere is another way of organizing your program which is to combine data and functionality and wrap it inside something called an object. This is called the object oriented programming paradigm, which will be useful especially for larger programs.\n\nClasses and Objects\nObject-oriented programming is built upon two fundamental concepts: classes and objects.\n\nA class is a blueprint or template that defines a new type of object. Think of it as a mold that creates objects with specific characteristics and behaviors.\nObjects are specific instances of a class. They have two main components:\n\nProperties (also called attributes or fields): Variables that store data within the object\nMethods: Functions that define what the object can do\n\nProperties come in two varieties:\n\nInstance variables: Unique to each object instance (each object has its own copy)\nClass variables: Shared among all instances of the class (one copy for the entire class)\n\n\nFor example, if you had a Car class:\n\nInstance variables might include color and mileage (unique to each car)\nClass variables might include number_of_wheels (same for all cars)\nMethods might include start_engine() or brake()\n\n\n\nCreating Classes\nTo define a class in Python, we use this basic syntax:\nclass ClassName:\n    # Class content goes here\nThe definition starts with the class keyword, followed by the class name, and a colon. The class content is indented and contains all properties and methods of the class.\nHere‚Äôs a minimal example:\n\n\n\n\n\n\nTo create an object (an instance) of this class:"
  },
  {
    "objectID": "lectures/lecture05/classes-and-objects.html#class-methods",
    "href": "lectures/lecture05/classes-and-objects.html#class-methods",
    "title": "Classes and Objects",
    "section": "Class Methods",
    "text": "Class Methods\nMethods are functions that belong to a class. They define the behavior of the class and can operate on the class‚Äôs properties.\n\n\n\n\n\n\nUnderstanding self in Python Classes\n\n\n\nEvery method in a Python class automatically receives a special first parameter, conventionally named self. This parameter refers to the specific instance of the class that calls the method.\nKey points about self: - It‚Äôs automatically passed by Python when you call a method - It gives the method access to the instance‚Äôs properties - By convention, we name it self (though technically you could use any valid name) - You don‚Äôt include it when calling the method\nExample:\nclass Colloid:\n    def type(self):  # self is automatically provided\n        print('I am a plastic colloid')\n\n# Usage:\nparticle = Colloid()\nparticle.type()  # Notice: no argument needed for self\nIn this example, even though type() appears to take no arguments when called, it actually receives the particle object as self.\n\n\n\n\n\n\n\n\n\nThe Constructor Method: __init__\nThe __init__ method (called the constructor) is a special method that initializes a new object when it‚Äôs created. It allows you to: - Set up initial values for the object‚Äôs properties - Perform any setup the object needs when it‚Äôs created\nThe name has double underscores (dunders) at both ends: __init__\nHere‚Äôs an example:\n\n\n\n\n\n\nUsing the class:\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPython also provides a __del__ method (destructor) that‚Äôs called when an object is deleted. We‚Äôll see this in action later.\n\n\n\n\nThe String Representation: __str__ Method\nThe __str__ method defines how an object should be represented as a string. Python automatically calls this method when:\n\nYou use print(object)\nYou convert the object to a string using str(object)\n\nHere‚Äôs an example:\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe .1f format specification means the radius will be displayed with one decimal place. You can customize this string representation to show whatever information about your object is most relevant.\n\n\nLet‚Äôs see it in action:"
  },
  {
    "objectID": "lectures/lecture05/classes-and-objects.html#class-and-instance-variables",
    "href": "lectures/lecture05/classes-and-objects.html#class-and-instance-variables",
    "title": "Classes and Objects",
    "section": "Class and Instance Variables",
    "text": "Class and Instance Variables\nFor most of your work, you‚Äôll primarily use instance variables - data unique to each object that you define in __init__. However, Python also supports class variables that are shared among all instances.\n\nInstance Variables (What You‚Äôll Use Most)\n\nUnique to each instance/object\nDefined in __init__ using self\nEach object has its own copy\nChanges only affect that specific instance\n\nExample:\nclass Colloid:\n    def __init__(self, R):\n        self.R = R  # Instance variable - each particle has its own radius\n\n\nClass Variables (Advanced)\n\nShared among all instances of a class\nDefined inside the class but outside any method\nAll objects share the same copy\nUseful for tracking data common to all instances (like counters)\n\nPractical example:\n\n\n\n\n\n\nLet‚Äôs see how it works:\n\n\n\n\n\n\nAccessing class variables:\nprint(Colloid.total_particles)  # Via the class (recommended)\nprint(p1.total_particles)        # Via an instance (works but less clear)\nWhen to use class variables:\n\nCounters (like tracking total instances)\nConstants shared by all instances\nConfiguration values for all objects\n\nFor your first simulations, focus on instance variables - they‚Äôre more intuitive and cover most use cases!"
  },
  {
    "objectID": "lectures/lecture05/classes-and-objects.html#advanced-inheritance---creating-related-classes",
    "href": "lectures/lecture05/classes-and-objects.html#advanced-inheritance---creating-related-classes",
    "title": "Classes and Objects",
    "section": "Advanced: Inheritance - Creating Related Classes",
    "text": "Advanced: Inheritance - Creating Related Classes\nInheritance allows you to create new classes based on existing ones. The new class (called a child class or subclass) inherits all properties and methods from the existing class (called a parent class or superclass), and can add or modify functionality.\n\nWhy Use Inheritance?\n\nCode reuse: Don‚Äôt repeat common functionality\nLogical hierarchy: Model natural relationships (e.g., ‚Äúa silica colloid is a type of colloid‚Äù)\nExtensibility: Add specialized behavior without modifying the original class\n\n\n\nBasic Inheritance Syntax\nclass ParentClass:\n    # Parent class definition\n    pass\n\nclass ChildClass(ParentClass):\n    # Child class inherits from ParentClass\n    pass\n\n\nPractical Example: Specialized Colloids\nLet‚Äôs create specialized colloid types that inherit from our base Colloid class:\n\n\n\n\n\n\nLet‚Äôs see inheritance in action:\n\n\n\n\n\n\n\n\n\n\n\n\nThe super() Function\n\n\n\nsuper() gives you access to the parent class. It‚Äôs commonly used in __init__ to call the parent‚Äôs constructor before adding child-specific initialization.\ndef __init__(self, R, new_parameter):\n    super().__init__(R)  # Initialize parent class\n    self.new_parameter = new_parameter  # Add child-specific property\n\n\n\n\n\n\n\n\nAdvanced: A Comprehensive Example\n\n\n\n\n\nThis example brings together all concepts including inheritance. Feel free to skip this on first reading!\n\nPutting It All Together: A Complete Example\nLet‚Äôs create a comprehensive example that demonstrates all the concepts we‚Äôve learned. We‚Äôll build a system to simulate a colloidal suspension:\n\n\n\n\n\n\nThis example demonstrates:\n\nClass variables (total_particles)\nInstance variables (R, x, y, charge)\nMethods (move(), volume(), distance_to(), electrostatic_energy())\nSpecial methods (__init__, __str__, __del__)\nInheritance (ChargedColloid extends Colloid)\nMethod overriding (__str__ is customized in the child class)\nUsing super() to call parent methods"
  },
  {
    "objectID": "lectures/lecture05/classes-and-objects.html#putting-it-all-together-a-complete-example",
    "href": "lectures/lecture05/classes-and-objects.html#putting-it-all-together-a-complete-example",
    "title": "Classes and Objects",
    "section": "Putting It All Together: A Complete Example",
    "text": "Putting It All Together: A Complete Example\nLet‚Äôs create a comprehensive example that demonstrates all the concepts we‚Äôve learned. We‚Äôll build a system to simulate a colloidal suspension:\n\n\n\n\n\n\nThis example demonstrates:\n\nClass variables (total_particles)\nInstance variables (R, x, y, charge)\nMethods (move(), volume(), distance_to(), electrostatic_energy())\nSpecial methods (__init__, __str__, __del__)\nInheritance (ChargedColloid extends Colloid)\nMethod overriding (__str__ is customized in the child class)\nUsing super() to call parent methods"
  },
  {
    "objectID": "lectures/lecture05/classes-and-objects.html#summary-what-weve-learned",
    "href": "lectures/lecture05/classes-and-objects.html#summary-what-weve-learned",
    "title": "Classes and Objects",
    "section": "Summary: What We‚Äôve Learned",
    "text": "Summary: What We‚Äôve Learned\nIn this lecture, we‚Äôve covered the fundamentals of object-oriented programming in Python:\nCore Concepts: - Classes are blueprints that define new types of objects with properties and methods - Objects are instances of classes with their own data - Methods are functions that belong to a class and operate on its data - self refers to the specific instance calling a method\nEssential Features You‚Äôll Use: - Instance variables: Data unique to each object (like self.R, self.x) - __init__ method: Initialize objects when they‚Äôre created - __str__ method: Define how objects are displayed - Regular methods: Define what objects can do (like get_size(), move())\nAdvanced Features (Optional): - Class variables: Data shared among all instances - Inheritance: Creating specialized classes based on existing ones - super(): Accessing parent class methods\nWhy OOP Matters: - Organizes complex code into logical units - Each object ‚Äúknows‚Äù its own properties and behaviors - Makes programs easier to maintain and extend - Models real-world concepts naturally (particles, sensors, etc.)\nObject-oriented programming is essential for building larger scientific applications. You‚Äôll find these concepts throughout Python‚Äôs scientific ecosystem (NumPy arrays, Matplotlib figures, etc.) and in the next lecture on Brownian motion!"
  },
  {
    "objectID": "lectures/lecture05/classes-and-objects.html#practice-exercises",
    "href": "lectures/lecture05/classes-and-objects.html#practice-exercises",
    "title": "Classes and Objects",
    "section": "üéØ Practice Exercises",
    "text": "üéØ Practice Exercises\nTry these exercises to reinforce your understanding of classes and objects!\n\n\n\n\n\n\nSelf-Exercise 1: Temperature Sensor Class\n\n\n\n\n\nCreate a TemperatureSensor class that: - Stores a sensor ID and current temperature in __init__ - Has a method read_temperature() that returns the temperature - Has a method set_temperature(T) that updates the temperature - Implements __str__ to display sensor information\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Exercise 2: Rectangle Class with Methods\n\n\n\n\n\nCreate a Rectangle class with: - Properties: width and height - Methods: area(), perimeter(), and is_square() - A class variable total_rectangles that tracks total number of rectangles created\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced Exercises\n\n\n\n\n\n\nExercise 3: Particle Inheritance\nExtend the Colloid class to create a MagneticColloid class that: - Inherits from Colloid - Adds a magnetic_moment property - Adds a method magnetic_force(field_strength) that calculates force = moment √ó field - Overrides __str__ to include magnetic properties\n\n\n\n\n\n\n\n\nExercise 4: Particle System Simulation\nCreate a ParticleSystem class that: - Stores a list of particles - Has methods to add and remove particles - Has a method center_of_mass() that calculates the system‚Äôs center of mass - Has a method total_volume() that sums all particle volumes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips for the Exercises\n\n\n\n\nStart by defining the class with __init__\nAdd methods one at a time and test as you go\nUse self to access instance variables within methods\nDon‚Äôt forget to return values from methods that calculate something!\nTest your class after each new method you add\n\nFor the advanced exercises: - For inheritance, use super().__init__() to call the parent constructor - Remember that child classes inherit all parent methods automatically\n\nWhat‚Äôs Next?\nCongratulations! You now understand the fundamentals of object-oriented programming in Python. These concepts form the foundation for writing clean, maintainable scientific code.\nWhat You Should Focus On: - Creating classes with __init__ to initialize objects - Defining methods that use self to access instance variables - Using __str__ for readable object display - Understanding that each object has its own data\nAdvanced topics like inheritance and class variables are powerful tools, but don‚Äôt worry if they seem complicated at first. You‚Äôll see them in action in the next lecture and can return to them later!\nIn the next lecture, we‚Äôll immediately apply what you‚Äôve learned by simulating Brownian motion using a Colloid class. You‚Äôll see how OOP makes managing multiple particles with different properties much cleaner than working with separate lists and functions. This practical application will solidify your understanding!\n\n\nFurther Reading\nIf you want to dive deeper into object-oriented programming in Python:\n\nPython Documentation: Classes Tutorial\nReal Python: Object-Oriented Programming in Python 3\nComposition over Inheritance: When your programs grow, sometimes composition (objects containing other objects) is better than inheritance\nData Classes: Python 3.7+ introduced @dataclass decorator for classes that primarily store data"
  },
  {
    "objectID": "lectures/lecture05/classes-and-objects.html#whats-next",
    "href": "lectures/lecture05/classes-and-objects.html#whats-next",
    "title": "Classes and Objects",
    "section": "What‚Äôs Next?",
    "text": "What‚Äôs Next?\nCongratulations! You now understand the fundamentals of object-oriented programming in Python. These concepts form the foundation for writing clean, maintainable scientific code.\nWhat You Should Focus On: - Creating classes with __init__ to initialize objects - Defining methods that use self to access instance variables - Using __str__ for readable object display - Understanding that each object has its own data\nAdvanced topics like inheritance and class variables are powerful tools, but don‚Äôt worry if they seem complicated at first. You‚Äôll see them in action in the next lecture and can return to them later!\nIn the next lecture, we‚Äôll immediately apply what you‚Äôve learned by simulating Brownian motion using a Colloid class. You‚Äôll see how OOP makes managing multiple particles with different properties much cleaner than working with separate lists and functions. This practical application will solidify your understanding!"
  },
  {
    "objectID": "lectures/lecture05/classes-and-objects.html#further-reading",
    "href": "lectures/lecture05/classes-and-objects.html#further-reading",
    "title": "Classes and Objects",
    "section": "Further Reading",
    "text": "Further Reading\nIf you want to dive deeper into object-oriented programming in Python:\n\nPython Documentation: Classes Tutorial\nReal Python: Object-Oriented Programming in Python 3\nComposition over Inheritance: When your programs grow, sometimes composition (objects containing other objects) is better than inheritance\nData Classes: Python 3.7+ introduced @dataclass decorator for classes that primarily store data"
  },
  {
    "objectID": "lectures/lecture02/brownian-motion-simple.html#introduction-what-is-brownian-motion",
    "href": "lectures/lecture02/brownian-motion-simple.html#introduction-what-is-brownian-motion",
    "title": "Brownian Motion: Random Walks in Physics",
    "section": "Introduction: What is Brownian Motion?",
    "text": "Introduction: What is Brownian Motion?\nImagine a tiny dust particle floating in water. If you look at it under a microscope, you‚Äôll see something surprising: the particle doesn‚Äôt sit still! Instead, it moves in a random, zigzag pattern. This phenomenon is called Brownian motion, named after the botanist Robert Brown who first described it in 1827.\n\n\n\n\n\n\nAnimation of Brownian motion (c) Wikipedia.\n\n\nWhy Does This Happen?\nWater isn‚Äôt just a smooth, continuous fluid‚Äîit‚Äôs made up of countless tiny molecules in constant motion. These water molecules continuously collide with our particle from all directions:\n\nEach individual collision is tiny and barely noticeable\nMillions of collisions happen every second\nThe collisions come from random directions\nThe combined effect creates the zigzag motion we observe\n\nThis is a beautiful example of how microscopic randomness creates observable macroscopic behavior!\n\n\n\n\n\n\nWhy This Discovery Was Revolutionary\n\n\n\nWhen Robert Brown first observed this motion in 1827, nobody could explain it. For decades, scientists debated: Are atoms and molecules real, or just a useful fiction?\nIn 1905, Albert Einstein provided the crucial breakthrough. He showed mathematically that if matter were made of atoms, Brownian motion would occur exactly as observed. A few years later, Jean Perrin performed careful experiments measuring Brownian motion and used Einstein‚Äôs theory to:\n\nCalculate Avogadro‚Äôs number (the number of molecules in a sample)\nProve that atoms exist! This convinced the remaining skeptics that matter truly is made of discrete particles\n\nThis wasn‚Äôt just about tiny particles jiggling around‚Äîit was definitive proof of the atomic nature of matter.\n\n\n\n\n\n\n\n\nDeep Connections in Physics\n\n\n\nBrownian motion reveals several fundamental principles that appear throughout physics:\nTemperature has a microscopic meaning: The energy of molecular collisions is directly related to temperature. Warmer water means faster-moving molecules, which create more vigorous Brownian motion. This connects the abstract concept of temperature to the kinetic energy of individual atoms!\nFluctuation-dissipation relation: The same molecular collisions that cause random fluctuations (the zigzag motion) also create friction that slows the particle down (dissipation). These aren‚Äôt separate phenomena‚Äîthey‚Äôre two sides of the same coin! This deep connection appears everywhere in physics, from electrical circuits to quantum mechanics.\nStatistical mechanics: Individual molecular collisions are impossibly complex to track, yet their collective behavior follows simple, predictable laws. This is the power of statistical thinking in physics.\nYou‚Äôll encounter these ideas again and again as you progress in physics‚Äîin thermal physics, quantum mechanics, and even in understanding phenomena like noise in electronic circuits or the behavior of polymers. Brownian motion is where many of these big ideas first became concrete and measurable!"
  },
  {
    "objectID": "lectures/lecture02/brownian-motion-simple.html#the-physics-behind-random-walks",
    "href": "lectures/lecture02/brownian-motion-simple.html#the-physics-behind-random-walks",
    "title": "Brownian Motion: Random Walks in Physics",
    "section": "The Physics Behind Random Walks",
    "text": "The Physics Behind Random Walks\n\nKey Ideas\nWhen a particle undergoes Brownian motion:\n\nEach step is random in direction\nThe step size depends on:\n\nTemperature (warmer ‚Üí more energetic collisions ‚Üí bigger steps)\nTime between observations (longer time ‚Üí more collisions ‚Üí bigger displacement)\nParticle size (smaller particles move more easily)\nAll captured in the diffusion coefficient \\(D\\)\n\n\n\n\nThe Simple Math\nThe typical distance a particle moves grows with time, but not linearly! Instead:\n\\[\\text{typical distance} \\propto \\sqrt{t}\\]\nMore precisely, the mean squared displacement is:\n\\[\\langle r^2 \\rangle = 4Dt\\]\nwhere:\n\n\\(D\\) is the diffusion coefficient (units: m¬≤/s)\n\\(t\\) is the elapsed time\nThe factor of 4 comes from motion in 2D (it would be 6 in 3D)\n\n\n\n\n\n\n\nRandom Steps from a Normal Distribution\n\n\n\nTo simulate random motion, we use np.random.normal() which generates random numbers from a normal (Gaussian) distribution.\nFor Brownian motion, each step has:\n\nMean = 0 (no preferred direction)\nStandard deviation = \\(\\sqrt{2D\\Delta t}\\) (typical step size)\n\nTry it:\nstep = np.random.normal(0, 1.0)  # mean=0, std=1"
  },
  {
    "objectID": "lectures/lecture02/brownian-motion-simple.html#lets-simulate-one-particle",
    "href": "lectures/lecture02/brownian-motion-simple.html#lets-simulate-one-particle",
    "title": "Brownian Motion: Random Walks in Physics",
    "section": "Let‚Äôs Simulate One Particle!",
    "text": "Let‚Äôs Simulate One Particle!\nWe‚Äôll build up our simulation step by step, starting with the simplest case: one particle moving in 2D.\n\nStep 1: Setting Up Parameters\n\n\n\n\n\n\n\n\nStep 2: Creating a Function for One Random Step\nLet‚Äôs write a function that takes a current position and returns the new position after one random step:\n\n\n\n\n\n\n\n\nStep 3: Simulating a Complete Trajectory\nNow let‚Äôs create a function that simulates many steps:\n\n\n\n\n\n\n\n\nStep 4: Plotting the Trajectory\nLet‚Äôs visualize the random walk:\n\n\n\n\n\n\n\n\n\n\n\n\nExperiment!\n\n\n\nRun the simulation multiple times. Notice how:\n\nEach trajectory is different\nThe particle can end up far from the origin or close to it\nThe path is jagged and random\nThere‚Äôs no preferred direction\n\nTry changing N (number of steps) or dt (time step) to see how it affects the motion!"
  },
  {
    "objectID": "lectures/lecture02/brownian-motion-simple.html#multiple-particles",
    "href": "lectures/lecture02/brownian-motion-simple.html#multiple-particles",
    "title": "Brownian Motion: Random Walks in Physics",
    "section": "Multiple Particles",
    "text": "Multiple Particles\nNow let‚Äôs simulate many particles at once to see the statistical behavior:\n\n\n\n\n\n\n\nVisualizing Multiple Particles\n\n\n\n\n\n\n\n\n\n\n\n\nWhat Do You See?\n\n\n\nEven though each trajectory is random and unpredictable, the ensemble (all particles together) shows clear statistical patterns:\n\nMost particles stay within a certain radius\nThe density decreases with distance from origin\nThe distribution is roughly circular (no preferred direction)"
  },
  {
    "objectID": "lectures/lecture02/brownian-motion-simple.html#analyzing-the-motion-mean-squared-displacement",
    "href": "lectures/lecture02/brownian-motion-simple.html#analyzing-the-motion-mean-squared-displacement",
    "title": "Brownian Motion: Random Walks in Physics",
    "section": "Analyzing the Motion: Mean Squared Displacement",
    "text": "Analyzing the Motion: Mean Squared Displacement\nInstead of looking at individual trajectories, let‚Äôs measure the mean squared displacement (MSD), which tells us how far particles have moved on average.\n\nComputing MSD for One Particle\n\n\n\n\n\n\n\n\nMSD for Many Particles (Better Statistics!)\n\n\n\n\n\n\n\n\n\n\n\n\nKey Observations\n\n\n\n\nIndividual trajectories fluctuate a lot around the theoretical prediction\nOn average, the MSD follows \\(\\langle r^2 \\rangle = 4Dt\\)\nFluctuations are larger at long times (fewer data points to average)\nThis is why experimentalists use many particles or long trajectories to measure \\(D\\) accurately!"
  },
  {
    "objectID": "lectures/lecture02/brownian-motion-simple.html#challenge-measuring-the-diffusion-coefficient",
    "href": "lectures/lecture02/brownian-motion-simple.html#challenge-measuring-the-diffusion-coefficient",
    "title": "Brownian Motion: Random Walks in Physics",
    "section": "Challenge: Measuring the Diffusion Coefficient",
    "text": "Challenge: Measuring the Diffusion Coefficient\nCan you extract the diffusion coefficient from your simulation data?\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe MSD is linear in time: \\(\\text{MSD} = 4Dt\\)\nSo the slope of MSD vs.¬†time gives you \\(4D\\).\nYou can use linear regression or just compute: \\(D = \\text{slope} / 4\\)\nTry fitting a line to your MSD data!"
  },
  {
    "objectID": "lectures/lecture02/brownian-motion-simple.html#summary",
    "href": "lectures/lecture02/brownian-motion-simple.html#summary",
    "title": "Brownian Motion: Random Walks in Physics",
    "section": "Summary",
    "text": "Summary\nIn this notebook, you learned:\n‚úÖ What Brownian motion is: Random motion caused by molecular collisions\n‚úÖ How to simulate it: Using random steps from a normal distribution\n‚úÖ Key equation: Mean squared displacement \\(\\langle r^2 \\rangle = 4Dt\\)\n‚úÖ Programming skills:\n\nWriting functions with multiple return values\nUsing numpy arrays for trajectories\nCreating loops to simulate multiple particles\nPlotting and analyzing data\n\n‚úÖ Physics insights:\n\nIndividual trajectories are random and unpredictable\nStatistical averages follow predictable patterns\nThe diffusion coefficient can be measured from MSD"
  },
  {
    "objectID": "lectures/lecture02/brownian-motion-simple.html#next-steps",
    "href": "lectures/lecture02/brownian-motion-simple.html#next-steps",
    "title": "Brownian Motion: Random Walks in Physics",
    "section": "Next Steps",
    "text": "Next Steps\nLater in the course, we‚Äôll revisit Brownian motion using object-oriented programming (classes), which will make it easier to:\n\nManage many particles with different properties\nOrganize complex simulations\nAdd features like particle interactions\n\nBut for now, you have all the tools to simulate and analyze Brownian motion! üéâ"
  },
  {
    "objectID": "lectures/lecture02/brownian-motion-simple.html#optional-further-explorations",
    "href": "lectures/lecture02/brownian-motion-simple.html#optional-further-explorations",
    "title": "Brownian Motion: Random Walks in Physics",
    "section": "Optional: Further Explorations",
    "text": "Optional: Further Explorations\nTry modifying the code to explore:\n\nEffect of particle size: Change \\(D\\) (smaller \\(D\\) = larger particle)\n3D Brownian motion: Add a \\(z\\) coordinate (hint: MSD becomes \\(6Dt\\))\nDifferent time steps: How does dt affect the simulation?\nTemperature effects: \\(D\\) is proportional to temperature!\nConfined motion: Add boundaries (particles bounce off walls)"
  },
  {
    "objectID": "lectures/lecture03/numpy-arrays.html",
    "href": "lectures/lecture03/numpy-arrays.html",
    "title": "NumPy Module",
    "section": "",
    "text": "Numpy is the core library for scientific computing in Python. It provides a high-performance multidimensional array object and tools for working with these arrays. The NumPy array, formally called ndarray in NumPy documentation, is the real workhorse of data structures for scientific and engineering applications. The NumPy array is similar to a list but where all the elements of the list are of the same type. The elements of a NumPy array are usually numbers, but can also be booleans, strings, or other objects. When the elements are numbers, they must all be of the same type."
  },
  {
    "objectID": "lectures/lecture03/numpy-arrays.html#creating-numpy-arrays",
    "href": "lectures/lecture03/numpy-arrays.html#creating-numpy-arrays",
    "title": "NumPy Module",
    "section": "Creating Numpy Arrays",
    "text": "Creating Numpy Arrays\nThere are a number of ways to initialize new numpy arrays, for example from\n\na Python list or tuples\nusing functions that are dedicated to generating numpy arrays, such as arange, linspace, etc.\nreading data from files which will be covered in the files section\n\n\nFrom lists\nFor example, to create new vector and matrix arrays from Python lists we can use the numpy.array function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing array-generating functions\nFor larger arrays it is inpractical to initialize the data manually, using explicit python lists. Instead we can use one of the many functions in numpy that generate arrays of different forms. Some of the more common are:\n\n\n\n\n\n\n\n\n\n\n\n\n\nlinspace and logspace\nThe linspace function creates an array of N evenly spaced points between a starting point and an ending point. The form of the function is linspace(start, stop, N).If the third argument N is omitted,then N=50.\n\n\n\n\n\n\nlogspace is doing equivelent things with logaritmic spacing. Other types of array creation techniques are listed below. Try around with these commands to get a feeling what they do.\n\n\n\n\n\n\n\n\nmgrid\nmgrid generates a multi-dimensional matrix with increasing value entries, for example in columns and rows:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndiag\ndiag generates a diagonal matrix with the list supplied to it. The values can be also offset from the main diagonal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nzeros and ones\nzeros and ones creates a matrix with the dimensions given in the argument and filled with 0 or 1."
  },
  {
    "objectID": "lectures/lecture03/numpy-arrays.html#manipulating-numpy-arrays",
    "href": "lectures/lecture03/numpy-arrays.html#manipulating-numpy-arrays",
    "title": "NumPy Module",
    "section": "Manipulating NumPy arrays",
    "text": "Manipulating NumPy arrays\n\nSlicing\nSlicing is the name for extracting part of an array by the syntax M[lower:upper:step]\n\n\n\n\n\n\n\n\n\n\n\n\nAny of the three parameters in M[lower:upper:step] can be ommited.\n\n\n\n\n\n\n\n\n\n\n\n\nNegative indices counts from the end of the array (positive index from the begining):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndex slicing works exactly the same way for multidimensional arrays:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDifferences\n\n\n\nSlicing can be effectively used to calculate differences for example for the calculation of derivatives. Here the position \\(y_i\\) of an object has been measured at times \\(t_i\\) and stored in an array each. We wish to calculate the average velocity at the times \\(t_{i}\\) from the arrays by\n\\[\\begin{equation}\nv_{i}=\\frac{y_i-y_{i-1}}{t_{i}-t_{i-1}}\n\\end{equation}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReshaping\nArrays can be reshaped into any form, which contains the same number of elements.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdding a new dimension: newaxis\nWith newaxis, we can insert new dimensions in an array, for example converting a vector to a column or row matrix.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStacking and repeating arrays\n\n\n\n\n\nUsing function repeat, tile, vstack, hstack, and concatenate we can create larger vectors and matrices from smaller ones. Please try the individual functions yourself in your notebook. We wont discuss them in detail.\n\nTile and repeat\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConcatenate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHstack and vstack"
  },
  {
    "objectID": "lectures/lecture03/numpy-arrays.html#applying-mathematical-functions",
    "href": "lectures/lecture03/numpy-arrays.html#applying-mathematical-functions",
    "title": "NumPy Module",
    "section": "Applying mathematical functions",
    "text": "Applying mathematical functions\nAll kinds of mathematica operations can be carried out on arrays. Typically these operation act element wise as seen from the examples below.\n\nOperation involving one array\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperations involving multiple arrays\nVector operations enable efficient element-wise calculations where corresponding elements at matching positions are processed simultaneously. Instead of handling elements one by one, these operations work on entire arrays at once, making them particularly fast. When multiplying two vectors using these operations, the result is not a single number (as in a dot product) but rather a new array where each element is the product of the corresponding elements from the input vectors. This element-wise multiplication is just one example of vector operations, which can include addition, subtraction, and other mathematical functions."
  },
  {
    "objectID": "lectures/lecture03/numpy-arrays.html#practice-exercises",
    "href": "lectures/lecture03/numpy-arrays.html#practice-exercises",
    "title": "NumPy Module",
    "section": "Practice Exercises",
    "text": "Practice Exercises\nNow it‚Äôs time to practice what you‚Äôve learned! Try these exercises to strengthen your NumPy skills.\n\n\n\n\n\n\nSelf-Exercise 1: Temperature Array\n\n\n\n\n\nCreate an array of temperatures in Celsius from 0 to 100 in steps of 10 using np.arange(). Then convert all temperatures to Fahrenheit using the formula:\n\\[F = C \\times \\frac{9}{5} + 32\\]\nPrint both arrays to compare.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Exercise 2: Position and Velocity\n\n\n\n\n\nGiven the following position measurements of a particle at different times, use array slicing to calculate the velocity between each consecutive time step using:\n\\[v_i = \\frac{x_{i+1} - x_i}{t_{i+1} - t_i}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Exercise 3: Gravitational Potential Energy\n\n\n\n\n\nCreate an array of heights from 0 to 50 meters with 11 evenly spaced points using np.linspace(). Calculate the gravitational potential energy at each height for a mass of 2 kg using:\n\\[E_p = mgh\\]\nwhere \\(g = 9.81\\) m/s¬≤. Then find: - The maximum potential energy - The height where potential energy is closest to 500 J\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Exercise 4: 2D Array Operations\n\n\n\n\n\nCreate a 3√ó3 array containing the numbers 1 through 9 using np.arange() and reshape(). Then:\n\nExtract the middle row\nExtract the last column\nCalculate the sum of each row\nCalculate the sum of each column\n\nHint: Use np.sum() with the axis parameter.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "lectures/lecture03/numpy-arrays.html#summary",
    "href": "lectures/lecture03/numpy-arrays.html#summary",
    "title": "NumPy Module",
    "section": "Summary",
    "text": "Summary\nYou‚Äôve now learned the essential NumPy operations for scientific computing:\n\nCreating arrays with np.array(), np.arange(), np.linspace(), np.zeros(), np.ones()\nSlicing and indexing to extract and manipulate array elements\nReshaping arrays with .reshape() and np.newaxis\nElement-wise operations that work on entire arrays efficiently\nMathematical functions like np.sin(), np.exp(), np.sqrt()\n\nThese tools are fundamental for numerical physics simulations, data analysis, and solving differential equations. Practice these operations until they become second nature!"
  },
  {
    "objectID": "seminars/Assignment 2.html",
    "href": "seminars/Assignment 2.html",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "Define a function year_to_century that returns the century for a given year. The first century spans from the year 1 up to and including the year 100, the second - from the year 101 up to and including the year 200, etc.\n\ndef year_to_century(year):\n    return (year + 99) // 100\n\nyear_to_century(2024)\n\n21\n\n\nWrite python code to import the numpy module under the namespace np.\n\nimport numpy as np\n\nWrite a Python function inner_product that computes the inner product of two vectors of the same length.¬†Do not use any external modules.\nHint: You can use the raise keyword to throw an error.\n\ndef inner_product(vec1, vec2):\n    \n    if len(vec1) != len(vec2):\n        raise SystemError(\"Vectors must be of the same length\")\n    \n    a=0\n    for i in range(0,len(vec1)):\n        a += vec1[i] * vec2[i]\n    return a\n\n\ninner_product([1, 2, 3], [3, 4, 5])\n\n26\n\n\nDefine a function triangle that takes the 3 sides of a triangle and returns one of the following:¬†\n‚ÄúNo Triangle‚Äù:¬†If the sides don‚Äôt form a valid triangle (degenerate case).\n‚ÄúAcute Triangle‚Äù: If all angles are acute.\n‚ÄúRight Triangle‚Äù: If one angle is exactly 90¬∞.\n‚ÄúObtuse Triangle‚Äù: If one angle is obtuse.\n\ndef triangle(a, b, c):\n    sides = sorted([a, b, c]) # Sort sides so that a &lt;= b &lt;= c\n    a, b, c = sides\n    \n    if a + b &lt;= c:\n        return \"No Triangle\"\n    \n    if a**2 + b**2 == c**2:\n        return \"Right Triangle\"\n    \n    if a**2 + b**2 &gt; c**2:\n        return \"Acute Triangle\"\n    \n    return \"Obtuse Triangle\"\n\nAssign the 100 numbers between -10 and 10 including -10 and 10 to the variable x using the linspace function of numpy. Assign the function values \\(x¬≥ - 3x¬≤ - 10\\) to the variable y. Calculate the numerical derivative $ $ from the differences between neighbouring values in the¬†y and the x array and store it in the variable derivative.\n\nimport numpy as np\n\nx=np.linspace(-10,10,100)\ny=x**3 - 3*x**2 - 10\n\nderivative=(y[1:]-y[:-1])/(x[1:]-x[:-1])\n\n\nx=np.random.randint(10,size=(10,8))\nx\n\narray([[2, 0, 6, 0, 1, 5, 0, 7],\n       [4, 2, 5, 7, 9, 8, 2, 1],\n       [4, 1, 3, 9, 9, 3, 8, 8],\n       [9, 4, 0, 6, 9, 2, 0, 3],\n       [2, 5, 2, 0, 1, 2, 4, 8],\n       [6, 1, 1, 7, 5, 5, 8, 2],\n       [0, 1, 0, 0, 2, 0, 1, 4],\n       [7, 4, 4, 2, 3, 7, 0, 3],\n       [4, 3, 6, 8, 3, 1, 0, 4],\n       [1, 0, 5, 6, 7, 2, 3, 9]])\n\n\n\ndt=0.05\nD=2.2e-13\nN=200\nsigma=np.sqrt(2*D*dt)\nscale=1e6\nn_trajectories=1000\n\nplt.figure(figsize=(6,6))\n[plt.plot(np.random.normal(0,sigma,size=N).cumsum()*scale,np.random.normal(0,sigma,size=N).cumsum()*scale,'k-',alpha=0.2) for _ in range(n_trajectories)]\nplt.xlim(-5,5)\nplt.ylim(-5,5)\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.plot(dx)\n\n\n\n\n\n\n\n\n\n?np.random.randint\n\n\nSignature: np.random.randint(low, high=None, size=None, dtype=&lt;class 'int'&gt;)\n\nDocstring:\n\nrandint(low, high=None, size=None, dtype=int)\n\n\n\nReturn random integers from `low` (inclusive) to `high` (exclusive).\n\n\n\nReturn random integers from the \"discrete uniform\" distribution of\n\nthe specified dtype in the \"half-open\" interval [`low`, `high`). If\n\n`high` is None (the default), then results are from [0, `low`).\n\n\n\n.. note::\n\n    New code should use the `~numpy.random.Generator.integers`\n\n    method of a `~numpy.random.Generator` instance instead;\n\n    please see the :ref:`random-quick-start`.\n\n\n\nParameters\n\n----------\n\nlow : int or array-like of ints\n\n    Lowest (signed) integers to be drawn from the distribution (unless\n\n    ``high=None``, in which case this parameter is one above the\n\n    *highest* such integer).\n\nhigh : int or array-like of ints, optional\n\n    If provided, one above the largest (signed) integer to be drawn\n\n    from the distribution (see above for behavior if ``high=None``).\n\n    If array-like, must contain integer values\n\nsize : int or tuple of ints, optional\n\n    Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n\n    ``m * n * k`` samples are drawn.  Default is None, in which case a\n\n    single value is returned.\n\ndtype : dtype, optional\n\n    Desired dtype of the result. Byteorder must be native.\n\n    The default value is long.\n\n\n\n    .. warning::\n\n      This function defaults to the C-long dtype, which is 32bit on windows\n\n      and otherwise 64bit on 64bit platforms (and 32bit on 32bit ones).\n\n      Since NumPy 2.0, NumPy's default integer is 32bit on 32bit platforms\n\n      and 64bit on 64bit platforms.  Which corresponds to `np.intp`.\n\n      (`dtype=int` is not the same as in most NumPy functions.)\n\n\n\nReturns\n\n-------\n\nout : int or ndarray of ints\n\n    `size`-shaped array of random integers from the appropriate\n\n    distribution, or a single such random int if `size` not provided.\n\n\n\nSee Also\n\n--------\n\nrandom_integers : similar to `randint`, only for the closed\n\n    interval [`low`, `high`], and 1 is the lowest value if `high` is\n\n    omitted.\n\nrandom.Generator.integers: which should be used for new code.\n\n\n\nExamples\n\n--------\n\n&gt;&gt;&gt; np.random.randint(2, size=10)\n\narray([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]) # random\n\n&gt;&gt;&gt; np.random.randint(1, size=10)\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\n\nGenerate a 2 x 4 array of ints between 0 and 4, inclusive:\n\n\n\n&gt;&gt;&gt; np.random.randint(5, size=(2, 4))\n\narray([[4, 0, 2, 1], # random\n\n       [3, 2, 2, 0]])\n\n\n\nGenerate a 1 x 3 array with 3 different upper bounds\n\n\n\n&gt;&gt;&gt; np.random.randint(1, [3, 5, 10])\n\narray([2, 2, 9]) # random\n\n\n\nGenerate a 1 by 3 array with 3 different lower bounds\n\n\n\n&gt;&gt;&gt; np.random.randint([1, 5, 7], 10)\n\narray([9, 8, 7]) # random\n\n\n\nGenerate a 2 by 4 array using broadcasting with dtype of uint8\n\n\n\n&gt;&gt;&gt; np.random.randint([1, 3, 5, 7], [[10], [20]], dtype=np.uint8)\n\narray([[ 8,  6,  9,  7], # random\n\n       [ 1, 16,  9, 12]], dtype=uint8)\n\nType:      method\n\n\n\n\nlist(range(0,6))\n\n[0, 1, 2, 3, 4, 5]\n\n\n\n    \n\n\nimport matplotlib.pyplot as plt \n\ndifference=derivative-np.array(derivative1)\nplt.plot(x[:-1],difference)\n\n\n\n\n\n\n\n\n\n\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[45], line 1\n----&gt; 1 x.diff()\n\nAttributeError: 'numpy.ndarray' object has no attribute 'diff'\n\n\n\n\nx=np.linspace(-10,10,100)\n\n\ndir(x)\n\n['T',\n '__abs__',\n '__add__',\n '__and__',\n '__array__',\n '__array_finalize__',\n '__array_function__',\n '__array_interface__',\n '__array_namespace__',\n '__array_priority__',\n '__array_struct__',\n '__array_ufunc__',\n '__array_wrap__',\n '__bool__',\n '__buffer__',\n '__class__',\n '__class_getitem__',\n '__complex__',\n '__contains__',\n '__copy__',\n '__deepcopy__',\n '__delattr__',\n '__delitem__',\n '__dir__',\n '__divmod__',\n '__dlpack__',\n '__dlpack_device__',\n '__doc__',\n '__eq__',\n '__float__',\n '__floordiv__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__iadd__',\n '__iand__',\n '__ifloordiv__',\n '__ilshift__',\n '__imatmul__',\n '__imod__',\n '__imul__',\n '__index__',\n '__init__',\n '__init_subclass__',\n '__int__',\n '__invert__',\n '__ior__',\n '__ipow__',\n '__irshift__',\n '__isub__',\n '__iter__',\n '__itruediv__',\n '__ixor__',\n '__le__',\n '__len__',\n '__lshift__',\n '__lt__',\n '__matmul__',\n '__mod__',\n '__mul__',\n '__ne__',\n '__neg__',\n '__new__',\n '__or__',\n '__pos__',\n '__pow__',\n '__radd__',\n '__rand__',\n '__rdivmod__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__rfloordiv__',\n '__rlshift__',\n '__rmatmul__',\n '__rmod__',\n '__rmul__',\n '__ror__',\n '__rpow__',\n '__rrshift__',\n '__rshift__',\n '__rsub__',\n '__rtruediv__',\n '__rxor__',\n '__setattr__',\n '__setitem__',\n '__setstate__',\n '__sizeof__',\n '__str__',\n '__sub__',\n '__subclasshook__',\n '__truediv__',\n '__xor__',\n 'all',\n 'any',\n 'argmax',\n 'argmin',\n 'argpartition',\n 'argsort',\n 'astype',\n 'base',\n 'byteswap',\n 'choose',\n 'clip',\n 'compress',\n 'conj',\n 'conjugate',\n 'copy',\n 'ctypes',\n 'cumprod',\n 'cumsum',\n 'data',\n 'device',\n 'diagonal',\n 'dot',\n 'dtype',\n 'dump',\n 'dumps',\n 'fill',\n 'flags',\n 'flat',\n 'flatten',\n 'getfield',\n 'imag',\n 'item',\n 'itemset',\n 'itemsize',\n 'mT',\n 'max',\n 'mean',\n 'min',\n 'nbytes',\n 'ndim',\n 'newbyteorder',\n 'nonzero',\n 'partition',\n 'prod',\n 'ptp',\n 'put',\n 'ravel',\n 'real',\n 'repeat',\n 'reshape',\n 'resize',\n 'round',\n 'searchsorted',\n 'setfield',\n 'setflags',\n 'shape',\n 'size',\n 'sort',\n 'squeeze',\n 'std',\n 'strides',\n 'sum',\n 'swapaxes',\n 'take',\n 'to_device',\n 'tobytes',\n 'tofile',\n 'tolist',\n 'trace',\n 'transpose',\n 'var',\n 'view']"
  },
  {
    "objectID": "seminars/seminar07/curve-fitting.html#introduction",
    "href": "seminars/seminar07/curve-fitting.html#introduction",
    "title": "Seminar: Curve Fitting Exercises",
    "section": "Introduction",
    "text": "Introduction\nIn this seminar, you will practice the curve fitting concepts from the lecture. Each exercise builds on the previous one, progressively developing your skills in:\n\nImplementing fitting formulas\nUsing scipy.optimize.curve_fit\nEvaluating fit quality\nHandling real-world complications\n\nInstructions: Work through the exercises in order. Each exercise has hints and solutions available. Try to solve each problem yourself before looking at the solution.\n\n\n\n\n\n\n\n\nExercise 1: Linear Regression by Hand\n\n\n\nOhm‚Äôs Law: The voltage across a resistor is proportional to the current: \\(V = IR\\)\nWe measured voltage at different currents to determine the resistance of an unknown resistor. Your task is to:\n\nImplement the weighted linear regression formulas from the lecture\nCalculate the resistance (slope) and its uncertainty\nCompare your result with curve_fit\n\nRecall the formulas (for \\(y = bx\\), forcing intercept through zero):\n\\[b = \\frac{S_{xy}}{S_{xx}}, \\quad \\sigma_b = \\sqrt{\\frac{1}{S_{xx}}}\\]\nwhere \\(S_{xy} = \\sum \\frac{x_i y_i}{\\sigma_i^2}\\) and \\(S_{xx} = \\sum \\frac{x_i^2}{\\sigma_i^2}\\)\nTime estimate: 25-30 minutes\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nFor the weighted sums, loop over all data points and accumulate: - S_xx += (I[i]**2) / (sigma[i]**2) - S_xy += (I[i] * V[i]) / (sigma[i]**2)\nFor curve_fit, you need to pass: the function, x-data, y-data, and use sigma= for uncertainties and absolute_sigma=True.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2: The Importance of Initial Guesses\n\n\n\nRadioactive Decay: The number of radioactive nuclei decreases exponentially:\n\\[N(t) = N_0 \\cdot e^{-\\lambda t} = N_0 \\cdot 2^{-t/t_{1/2}}\\]\nYour task is to:\n\nFit the decay data with good initial guesses\nTry fitting with bad initial guesses and observe what happens\nCalculate the half-life and its uncertainty\n\nTime estimate: 20-25 minutes\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nFrom the plot, estimate N0 as the first data point value (~1000)\nFind where N drops to about half of N0 to estimate the half-life (~3)\nFor curve_fit, pass sigma=N_err and p0=p0_good\nCalculate \\(\\chi^2 = \\sum((N - N_{model})^2 / N_{err}^2)\\) and divide by degrees of freedom (N_points - N_params)\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3: Model Comparison and Residual Analysis\n\n\n\nPendulum Period: The period of a simple pendulum depends on its length:\n\\[T = 2\\pi\\sqrt{\\frac{L}{g}}\\]\nWe can rewrite this as \\(T = A \\cdot L^n\\) where theoretically \\(A = 2\\pi/\\sqrt{g}\\) and \\(n = 0.5\\).\nYour task is to:\n\nFit the data with a power law model\nExtract the gravitational acceleration \\(g\\)\nAnalyze residuals to verify the model\nCompare with a linear model to see why it fails\n\nTime estimate: 25-30 minutes\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nUse curve_fit(power_law, L, T, sigma=T_err, p0=p0_power, absolute_sigma=True)\nFrom \\(A = 2\\pi/\\sqrt{g}\\), we get \\(g = (2\\pi/A)^2\\)\nResiduals: T - T_model\nChi-squared: np.sum(((T - T_model) / T_err)**2)\nLook for systematic patterns in residuals ‚Äî random scatter means good model\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 4: Gaussian Peak Fitting and Covariance\n\n\n\nSpectroscopy: A spectral line can be modeled as a Gaussian peak:\n\\[I(\\lambda) = A \\cdot \\exp\\left(-\\frac{(\\lambda - \\lambda_0)^2}{2\\sigma^2}\\right)\\]\nwhere \\(A\\) is the amplitude, \\(\\lambda_0\\) is the center wavelength, and \\(\\sigma\\) is the width.\nYour task is to:\n\nFit the spectral data to extract peak parameters\nReport parameters with uncertainties\nExamine the correlation matrix\nCalculate the Full Width at Half Maximum (FWHM = \\(2\\sqrt{2\\ln 2}\\sigma \\approx 2.355\\sigma\\))\n\nTime estimate: 20-25 minutes\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nA_guess = np.max(intensity)\nx0_guess = wavelength[np.argmax(intensity)]\nEstimate sigma_guess as roughly 1/4 of the visible peak width (~15)\nFWHM = 2.355 * sigma\nThe correlation matrix shows how parameters depend on each other\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 5: Fluorescence Lifetime with MLE (Capstone)\n\n\n\nTime-Correlated Single Photon Counting (TCSPC): In fluorescence lifetime measurements, we record the arrival time of individual photons after an excitation pulse. The arrival times follow an exponential distribution.\nAs discussed in the lecture, the MLE for the lifetime is simply the mean arrival time:\n\\[\\hat{\\tau}_{MLE} = \\frac{1}{N}\\sum_{i=1}^{N} t_i = \\bar{t}\\]\nYour task is to:\n\nAnalyze the simulated photon arrival data\nCalculate the lifetime using MLE (mean)\nCompare with histogram fitting\nDiscuss which method is better and why\n\nTime estimate: 25-30 minutes\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\ntau_mle = np.mean(arrival_times)\ntau_mle_err = tau_mle / np.sqrt(len(arrival_times))\nFor histogram fit: curve_fit(exponential_decay, bin_centers[mask], hist_counts[mask], sigma=hist_err[mask], p0=p0_hist, absolute_sigma=True)\nThe MLE method typically gives smaller uncertainties because it uses every photon‚Äôs exact arrival time, not binned data\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 6: Challenge - Weighted vs Unweighted Fitting\n\n\n\nChallenge: Investigate how varying uncertainties affect the fit results.\nCreate a dataset where uncertainties increase with x (heteroscedasticity), then compare: 1. Unweighted fit (ignoring uncertainties) 2. Weighted fit (using correct uncertainties) 3. Weighted fit with wrong uncertainties (constant)\nWhich approach gives the best estimate of the true parameters?\nTime estimate: 20-25 minutes (optional challenge)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote"
  },
  {
    "objectID": "seminars/seminar07/curve-fitting.html#summary",
    "href": "seminars/seminar07/curve-fitting.html#summary",
    "title": "Seminar: Curve Fitting Exercises",
    "section": "Summary",
    "text": "Summary\nIn this seminar, you practiced:\n\nLinear Regression by Hand: Implementing the weighted least squares formulas and verifying against curve_fit\nInitial Guesses: Seeing how good vs bad initial parameters affect convergence\nModel Comparison: Using \\(\\chi^2\\) and residuals to distinguish correct from incorrect models\nCovariance Analysis: Extracting uncertainties and understanding parameter correlations\nMLE for Exponential Data: Comparing histogram fitting vs direct MLE for lifetime measurements\nHeteroscedasticity: Understanding why correct uncertainty weighting matters\n\n\nKey Takeaways\n\nAlways plot your data before fitting\nProvide reasonable initial guesses based on physical intuition\nUse weighted fits when uncertainties are known and vary\nCheck \\(\\chi^2_\\nu \\approx 1\\) and examine residuals\nExtract and report parameter uncertainties from the covariance matrix\nFor some distributions (like exponential), MLE gives simple closed-form solutions"
  },
  {
    "objectID": "seminars/seminar01/deep-learning.html",
    "href": "seminars/seminar01/deep-learning.html",
    "title": "Neural Networks",
    "section": "",
    "text": "Neural networks have become one of the most powerful tools in modern computational science, with applications ranging from particle physics to materials science. The term deep neural networks refers to architectures with many layers of interconnected neurons, where ‚Äúdeep‚Äù simply indicates the number of these layers. But what makes these systems particularly fascinating for physicists is their ability to learn complex patterns from data‚Äîmuch like how we extract physical laws from experimental observations.\nIn this lecture, we‚Äôll build neural networks from scratch, gaining deep insight into their inner workings rather than treating them as black boxes. We won‚Äôt rely on high-level libraries like TensorFlow or Keras just yet; instead, we‚Äôll construct everything using NumPy, giving you the same kind of fundamental understanding you‚Äôd gain from deriving equations of motion from first principles rather than just applying them. This hands-on approach will help you understand what‚Äôs really happening under the hood when you train a neural network.\nThink of neural networks as complex function approximators. Just as you might fit a polynomial to experimental data, neural networks can approximate incredibly complicated functions‚Äîfunctions that might describe the relationship between input images and their classifications, or between system parameters and physical outcomes. The difference is that neural networks can handle much higher-dimensional spaces and more complex patterns than traditional fitting methods.\nOur goal today is to build a neural network that can recognize handwritten digits‚Äîa classic problem that will teach us all the essential concepts. We‚Äôll start simple with a single neuron (essentially logistic regression), then add hidden layers to increase the network‚Äôs representational power. By the end, you‚Äôll have a working digit classifier and a solid foundation for understanding deep learning. This notebook has been largely developed by Martin Fr√§nzl and adapted for physics students.\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\nplt.rcParams.update({'font.size': 18,\n                     'axes.titlesize': 20,\n                     'axes.labelsize': 20,\n                     'axes.labelpad': 1,\n                     'lines.linewidth': 2,\n                     'lines.markersize': 10,\n                     'xtick.labelsize' : 18,\n                     'ytick.labelsize' : 18,\n                     'xtick.top' : True,\n                     'xtick.direction' : 'in',\n                     'ytick.right' : True,\n                     'ytick.direction' : 'in'\n                    })"
  },
  {
    "objectID": "seminars/seminar01/deep-learning.html#overview-building-intelligence-from-scratch",
    "href": "seminars/seminar01/deep-learning.html#overview-building-intelligence-from-scratch",
    "title": "Neural Networks",
    "section": "Overview: Building Intelligence from Scratch üß†",
    "text": "Overview: Building Intelligence from Scratch üß†\nWe‚Äôre going to build a neural network from scratch using Python and NumPy, without relying on high-level libraries like Keras or TensorFlow (those will come in Part 2). Our task is to recognize handwritten digits using the famous MNIST dataset‚Äîa benchmark problem in machine learning that has driven much of the field‚Äôs development.\n\n\n\nMNIS\n\n\nOur learning journey proceeds in three stages, each building on the previous one. We‚Äôll start with the simplest possible ‚Äúnetwork‚Äù: a single neuron that learns to recognize just the digit 0. This is actually just logistic regression in disguise, but it will teach us the fundamental concepts of forward propagation, loss functions, and gradient descent. Think of this as the ‚Äúharmonic oscillator‚Äù of neural networks‚Äîthe simplest non-trivial system that captures the essential physics.\nNext, we‚Äôll add a hidden layer to our network, still focusing on recognizing zeros. This is where things get interesting: the hidden layer allows the network to learn intermediate representations, much like how eigenstates provide a natural basis for describing quantum systems. The network can now discover its own ‚Äúfeatures‚Äù rather than working directly with raw pixel values.\nFinally, we‚Äôll extend our network to recognize all digits from 0 through 9, creating a true multiclass classifier. This requires changing our output layer and loss function, but the core principles remain the same. By the end, we‚Äôll have a network that achieves around 84-92% accuracy‚Äînot state-of-the-art by modern standards, but impressive for a network we built entirely from scratch!"
  },
  {
    "objectID": "seminars/seminar01/deep-learning.html#the-mnist-data-set",
    "href": "seminars/seminar01/deep-learning.html#the-mnist-data-set",
    "title": "Neural Networks",
    "section": "The MNIST Data Set üî¢",
    "text": "The MNIST Data Set üî¢\nThe MNIST (Modified National Institute of Standards and Technology) dataset is to machine learning what the hydrogen atom is to quantum mechanics‚Äîa fundamental system that everyone studies first. It contains 70,000 images of handwritten digits, each 28 √ó 28 pixels in grayscale with pixel values ranging from 0 (black) to 255 (white). This gives us 784 input features per image (28 √ó 28 = 784), each representing the intensity of one pixel.\nFrom a physicist‚Äôs perspective, each image is a point in a 784-dimensional space. Our task is to partition this space into 10 regions, one for each digit. This is a classification problem, and neural networks provide a flexible way to learn these complex decision boundaries. We could download and preprocess the data ourselves, but the sklearn module has already done the heavy lifting for us:\n\nLoad the data\n\nfrom sklearn.datasets import fetch_openml\nX, y = fetch_openml('mnist_784', version=1, return_X_y=True,as_frame=False)\n\nThe images are now contained in the array X, while the labels (so which number it is) are contained in y. Let‚Äôs have a look at a random image and label.\n\ni = 33419\nplt.imshow(np.array(X)[i].reshape(28, 28), cmap='gray')\nplt.colorbar()\nplt.show()\nprint('label: ',y[i])\n\n\n\n\n\n\n\n\nlabel:  8\n\n\n\n\nNormalize the data\nNormalization is a crucial preprocessing step in machine learning. By dividing by 255, we scale all pixel values to the interval [0, 1]. This normalization serves several purposes: it prevents numerical overflow issues during computation, helps the gradient descent algorithm converge faster (since all features are on the same scale), and makes the learning rate easier to tune. Think of it as choosing appropriate units in physics‚Äîworking in natural units (‚Ñè = c = 1) often simplifies calculations, and similarly, normalized data simplifies neural network training.\n\nX = X/255\n\n\n\nPreparing training and testing data\nFor our first network, we‚Äôre building a binary classifier‚Äîa system that answers a yes/no question: ‚ÄúIs this digit a zero?‚Äù The default MNIST labels indicate which digit each image represents (‚Äò0‚Äô, ‚Äò1‚Äô, ‚Äò2‚Äô, etc.), but we need to convert this to binary labels: 1 if the image shows a zero, and 0 otherwise. This is analogous to simplifying a complex quantum system by projecting it onto a two-level system‚Äîwe‚Äôre reducing a 10-class problem to a binary one.\n\ny_new = np.zeros(y.shape)\ny_new[np.where(y == '0')[0]] = 1\ny = y_new\n\nWe now split the data into training and testing sets. This separation is crucial: we train on one set and evaluate on a completely different set to test our network‚Äôs ability to generalize. This is similar to how you might measure a physical quantity multiple times‚Äîsome measurements are used to calibrate your instrument (training), while others test its accuracy (testing). The MNIST images are pre-arranged so that the first 60,000 form the training set and the last 10,000 serve as the test set. We‚Äôll also transpose the data so that each example becomes a column rather than a row, which makes our matrix operations more intuitive:\n\nm = 60000\nm_test = X.shape[0] - m\n\nX_train, X_test = X[:m].T, X[m:].T\ny_train, y_test = y[:m].reshape(1,m), y[m:].reshape(1, m_test)\n\nFinally, we shuffle the training set. Shuffling ensures that our network doesn‚Äôt learn any spurious patterns related to the order of examples. If all the zeros came first, followed by all the ones, the network might learn temporal patterns that don‚Äôt actually exist in the data:\n\nnp.random.seed(1)\nshuffle_index = np.random.permutation(m)\nX_train, y_train = X_train[:,shuffle_index], y_train[:,shuffle_index]\n\nLet‚Äôs verify our preprocessing by examining a random image and its label:\n\ni = 39\nplt.imshow(X_train[:,i].reshape(28, 28), cmap='gray')\nplt.colorbar()\nplt.show()\nprint(y_train[:,i])\n\n\n\n\n\n\n\n\n[0.]\n\n\nTry changing the index i to explore different images. When you find a zero, verify that the corresponding label is 1, and when you find any other digit, check that the label is 0. This sanity check ensures our binary labeling worked correctly."
  },
  {
    "objectID": "seminars/seminar01/deep-learning.html#a-single-neuron-the-fundamental-building-block",
    "href": "seminars/seminar01/deep-learning.html#a-single-neuron-the-fundamental-building-block",
    "title": "Neural Networks",
    "section": "A Single Neuron: The Fundamental Building Block ‚öõÔ∏è",
    "text": "A Single Neuron: The Fundamental Building Block ‚öõÔ∏è\nThe basic unit of a neural network is an artificial neuron, inspired by biological neurons but mathematically much simpler. A neuron takes multiple inputs, performs a weighted sum (plus a bias), and passes the result through an activation function to produce a single output. The diagram below shows a simple neuron with two inputs:\n\n\n\nimage\n\n\nWhile the biological inspiration is interesting, it‚Äôs more useful for us as physicists to think of a neuron as a parametrized nonlinear function. The weights and bias are parameters we‚Äôll adjust during training, and the activation function introduces nonlinearity‚Äîcrucial because linear combinations of linear functions are still linear, but we need nonlinearity to approximate complex decision boundaries.\n\nForward Propagation: From Input to Output\nThe term ‚Äúforward propagation‚Äù describes how information flows through the network from inputs to outputs. A neuron performs three sequential operations on its inputs.\nFirst, each input value \\(x_i\\) is multiplied by its corresponding weight \\(w_i\\): \\[\\begin{eqnarray}\nx_{1}\\rightarrow x_{1} w_{1}\\\\\nx_{2}\\rightarrow x_{2} w_{2}\n\\end{eqnarray}\\]\nThese weights determine how much influence each input has on the output. In physics terms, think of the weights as coupling constants that determine the strength of interaction between input features and the output.\nSecond, all weighted inputs are summed and a bias term \\(b\\) is added: \\[\\begin{equation}\nz = x_{1} w_{1}+ x_{2} w_{2}+b\n\\end{equation}\\]\nThe bias allows the neuron to shift its activation threshold, similar to how an external potential can shift energy levels in quantum mechanics. It‚Äôs crucial for fitting data that doesn‚Äôt pass through the origin.\nThird, this weighted sum \\(z\\) is passed through an activation function \\(\\sigma(\\cdot)\\) to produce the final output: \\[\\begin{equation}\ny=\\sigma(z) = \\sigma( x_{1} w_{1}+ x_{2} w_{2}+b)\n\\end{equation}\\]\nThe activation function serves a critical purpose: it introduces nonlinearity into the network. Without it, stacking multiple layers would be pointless‚Äîany composition of linear transformations is itself linear. The activation function allows neural networks to approximate arbitrary nonlinear functions, much like how Fourier series can approximate arbitrary periodic functions through combinations of sines and cosines.\nA commonly used activation function is the sigmoid function, which smoothly maps any real number to the interval (0, 1).\nFor a single training example \\(x\\) (which is itself a vector of 784 pixel values), we can write this more compactly using vector notation:\n\\[\\begin{equation*}\n\\hat{y} = \\sigma(w^{\\rm T} x + b)\\ .\n\\end{equation*}\\]\nHere \\(w^{\\rm T} x\\) is the dot product of the weight vector with the input vector, \\(b\\) is a scalar bias, and \\(\\sigma\\) is the sigmoid activation function: \\[\\begin{equation*}\n\\sigma(z) = \\frac{1}{1+{\\rm e}^{-z}}\\ .\n\\end{equation*}\\]\nThe sigmoid function has a beautiful S-shaped curve. For large positive inputs it approaches 1, for large negative inputs it approaches 0, and it transitions smoothly in between. This makes it perfect for binary classification‚Äîwe can interpret the output as a probability. Interestingly, the sigmoid function is related to the Fermi-Dirac distribution in statistical mechanics (without the temperature parameter), which describes the occupation probability of quantum states.\nLet‚Äôs define and visualize the sigmoid function:\n\ndef sigmoid(z):\n    return 1/(1 + np.exp(-z))\n\n\nx=np.linspace(-5,5,100)\nplt.figure(figsize=(5,3))\nplt.plot(x,sigmoid(x))\nplt.xlabel('input')\nplt.ylabel('output')\nplt.grid()\nplt.show()\n\n\n\n\nSigmoid function\n\n\n\n\nExample calculation: Consider a simple two-input neuron with weights \\(w=[0,1]\\) and bias \\(b=4\\). If we provide the input \\(x=[2,3]\\), the neuron computes:\n\\[\\begin{equation}\nz = w\\cdot x+b = 0 \\cdot 2 + 1 \\cdot 3 + 4 = 7\n\\end{equation}\\]\nPassing this through the sigmoid gives: \\[\\begin{equation}\ny=\\sigma(7) = \\frac{1}{1+e^{-7}} \\approx 0.999\n\\end{equation}\\]\nThe output is very close to 1, indicating high confidence. This entire procedure‚Äîpropagating input values forward through the network to obtain an output‚Äîis called feedforward or forward propagation.\nOur immediate goal is to scale this up: we‚Äôll create a network with a single neuron that has 784 inputs (one for each pixel in our 28 √ó 28 images) and produces a single sigmoid output indicating whether the image is a zero.\nVectorization for efficiency: The real power of NumPy comes from vectorization‚Äîprocessing multiple examples simultaneously. Instead of computing predictions one image at a time, we‚Äôll stack all training examples side-by-side in a matrix \\(X\\), where each column is one example. The forward pass then becomes:\n\\[\\begin{equation*}\n\\hat{Y} = \\sigma(w^{\\rm T} X + b)\\ .\n\\end{equation*}\\]\nNote that \\(\\hat{Y}\\) is now a vector (one prediction per training example), not a scalar. This vectorized computation is orders of magnitude faster than looping over examples individually‚ÄîNumPy uses optimized linear algebra libraries (BLAS/LAPACK) under the hood, similar to how you‚Äôd use optimized FFT routines rather than implementing Fourier transforms naively.\nIn our code, we‚Äôll compute this in two stages: first Z = np.matmul(W.T, X) + b (the weighted sum), then A = sigmoid(Z) (the activation). We use A for ‚ÄúActivation‚Äù to match standard deep learning notation. This two-stage breakdown might seem unnecessary now, but it will make our backward propagation code clearer‚Äîeach stage needs its own gradient computation.\n\n\nLoss Function: Quantifying Prediction Error üìâ\nNow that we can make predictions, we need to quantify how wrong those predictions are. This quantification is called the loss (or cost) function. It‚Äôs analogous to defining an energy functional in variational methods‚Äîwe need a scalar quantity to minimize.\nYou might think to use mean squared error (MSE) from your curve-fitting experience:\n\\[\\begin{equation}\nMSE(y,\\hat{y})=\\frac{1}{n}\\sum_{i=1}^{n}(y-\\hat{y})^2\n\\end{equation}\\]\nwhere \\(\\hat{y}\\) represents our predictions and \\(y\\) represents the ground truth (actual labels from the training set). MSE works, but for classification problems, there‚Äôs a more principled choice.\nWe‚Äôll use binary cross-entropy loss, which comes from information theory. For a single training example:\n\\[\\begin{equation*}\nL(y,\\hat{y}) = -y\\log(\\hat{y})-(1-y)\\log(1-\\hat{y})\\ .\n\\end{equation*}\\]\nThis formula might look strange at first, but it has a beautiful interpretation. When \\(y=1\\) (the image is a zero), only the first term matters, and we‚Äôre penalized more heavily the further \\(\\hat{y}\\) is from 1. When \\(y=0\\) (not a zero), only the second term matters, penalizing predictions far from 0. The logarithm ensures that the penalty grows rapidly as predictions become more confident but wrong‚Äîbeing confidently wrong is much worse than being tentatively wrong.\nThis loss function derives from maximum likelihood estimation and is related to the Kullback-Leibler divergence between probability distributions. It‚Äôs the natural choice for binary classification with sigmoid outputs. For physicists, you might recognize similar logarithmic terms in the partition function of statistical mechanics.\nAveraging over a training set of \\(m\\) examples:\n\\[\\begin{equation*}\nL(Y,\\hat{Y}) = -\\frac{1}{m}\\sum_{i = 0}^{m}y^{(i)}\\log(\\hat{y}^{(i)})-(1-y^{(i)})\\log(1-\\hat{y}^{(i)})\\ .\n\\end{equation*}\\]\nIn Python code, this looks like\n\ndef compute_loss(Y, Y_hat):\n    m = Y.shape[1]\n    L = -(1./m)*(np.sum(np.multiply(np.log(Y_hat), Y)) + np.sum(np.multiply(np.log(1 - Y_hat), (1 - Y))))\n    return L"
  },
  {
    "objectID": "seminars/seminar01/deep-learning.html#training-the-network-learning-from-data",
    "href": "seminars/seminar01/deep-learning.html#training-the-network-learning-from-data",
    "title": "Neural Networks",
    "section": "Training the Network: Learning from Data üéì",
    "text": "Training the Network: Learning from Data üéì\nTraining a neural network means adjusting its parameters (weights and biases) to minimize the loss function. This is fundamentally an optimization problem, not unlike minimizing the action in Lagrangian mechanics or finding ground state energies in quantum systems. The key difference is that our loss function lives in a very high-dimensional space‚Äîwith 784 inputs, we have 784 weights plus 1 bias, giving us a 785-dimensional parameter space to navigate.\nThe optimization technique we‚Äôll use is called gradient descent‚Äîwe follow the downhill direction of the loss function, taking small steps toward the minimum. But how do we find this direction? We need gradients, which means we need derivatives of the loss with respect to all parameters. This is where backpropagation comes in.\n\nBackward Propagation: The Chain Rule in Action üîó\nThe output of our network depends entirely on the inputs (which are fixed for a given training example) and on the parameters we choose: the weights \\(w\\) and biases \\(b\\). We can therefore view the loss as a function of these parameters:\n\\[\nL(w_{1},w_{2},w_{3},\\ldots ,b_{1},b_{2},b_{3},\\ldots)\n\\]\nTo train the network through gradient descent, we need to know how the loss changes when we adjust each weight. This is captured by the partial derivative:\n\\[\n\\frac{\\partial L}{\\partial w_j}\n\\]\nThis derivative tells us the direction and magnitude of steepest increase in the loss. By moving in the opposite direction (negative gradient), we can reduce the loss. The process of computing these gradients by working backward from the loss through the network is called backpropagation‚Äîit‚Äôs just a clever application of the chain rule from calculus, applied systematically to a computational graph.\nBackpropagation is often presented as mysterious or complicated, but it‚Äôs really just careful bookkeeping with derivatives. As physicists, you‚Äôve used the chain rule countless times‚Äîbackpropagation is the same principle, just applied to a more complex composite function.\nDeriving the gradients\nLet‚Äôs focus on a single training example to keep the math clear. We can think of computing the loss in three stages: \\(w_j\\rightarrow z \\rightarrow \\hat{y} \\rightarrow L\\). The formulas for these stages are: \\[\\begin{align*}\nz &= w^{\\rm T} x + b\\ , \\\\\n\\hat{y} &= \\sigma(z)\\ , \\\\\nL(y,\\hat{y}) &= -y\\log(\\hat{y})-(1-y)\\log(1-\\hat{y})\\ .\n\\end{align*}\\]\nBy the chain rule from calculus, the gradient of the loss with respect to weight \\(w_j\\) factors into three partial derivatives:\n\\[\\begin{align*}\n\\frac{\\partial L}{\\partial w_j} = \\frac{\\partial L}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial z}\\frac{\\partial z}{\\partial w_j}\n\\end{align*}\\]\nThis factorization is the essence of backpropagation. Each partial derivative corresponds to one ‚Äúlink‚Äù in our computational chain. Let‚Äôs compute each factor systematically. The derivatives are a bit tedious to write out, but none of them are particularly complicated‚Äîthey‚Äôre all applications of basic calculus rules you already know.\n\\(\\partial L/\\partial\\hat{y}\\): \\[\\begin{align*}\n\\frac{\\partial L}{\\partial\\hat{y}} &= \\frac{\\partial}{\\partial\\hat{y}}\\left(-y\\log(\\hat{y})-(1-y)\\log(1-\\hat{y})\\right) \\\\\n&= -y\\frac{\\partial}{\\partial\\hat{y}}\\log(\\hat{y})-(1-y)\\frac{\\partial}{\\partial\\hat{y}}\\log(1-\\hat{y}) \\\\\n&= -\\frac{y}{\\hat{y}} +\\frac{(1 - y)}{1-\\hat{y}} \\\\\n&= \\frac{\\hat{y} - y}{\\hat{y}(1-\\hat{y})}\n\\end{align*}\\]\n\\(\\partial \\hat{y}/\\partial z\\): \\[\\begin{align*}\n\\frac{\\partial }{\\partial z}\\sigma(z)\n&= \\frac{\\partial }{\\partial z}\\left(\\frac{1}{1 + {\\rm e}^{-z}}\\right) \\\\\n&= \\frac{1}{(1 + {\\rm e}^{-z})^2}\\frac{\\partial }{\\partial z}(1 + {\\rm e}^{-z}) \\\\\n&= \\frac{{\\rm e}^{-z}}{(1 + {\\rm e}^{-z})^2} \\\\\n&= \\frac{1}{1 + {\\rm e}^{-z}}\\frac{{\\rm e}^{-z}}{1 + {\\rm e}^{-z}} \\\\\n&= \\frac{1}{1 + {\\rm e}^{-z}}\\left(1 - \\frac{1}{1 + {\\rm e}^{-z}}\\right) \\\\\n&= \\sigma(z)(1-\\sigma(z)) \\\\\n&= \\hat{y}(1-\\hat{y})\n\\end{align*}\\]\n\\(\\partial z/\\partial w_j\\): \\[\\begin{align*}\n\\frac{\\partial }{\\partial w_j}(w^{\\rm T} x + b) &= \\frac{\\partial }{\\partial w_j}(w_0x_0 + \\dots + w_nx_n + b) \\\\\n&= x_j\n\\end{align*}\\]\nSubstituting back into the chain rule yields: \\[\\begin{align*}\n\\frac{\\partial L}{\\partial w_j}\n&= \\frac{\\partial L}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial z}\\frac{\\partial z}{\\partial w_j} \\\\\n&= \\frac{\\hat{y} - y}{\\hat{y}(1-\\hat{y})}\\hat{y}(1-\\hat{y}) x_j \\\\\n&= (\\hat{y} - y)x_j\\ .\n\\end{align*}\\]\nwhich does not look that unfriendly anymore.\nVectorizing the gradients: When we have \\(m\\) training examples processed simultaneously, the weight gradient becomes: \\[\\begin{align*}\n\\frac{\\partial L}{\\partial w} = \\frac{1}{m} X(\\hat{Y} - Y)^{\\rm T}\\ .\n\\end{align*}\\]\nThe factor of \\(1/m\\) comes from averaging the loss over all examples. Notice how clean this is: the gradient is simply the input matrix multiplied by the error vector \\((\\hat{Y} - Y)\\).\nA similar derivation for the bias gradient yields, for a single example: \\[\\begin{align*}\n\\frac{\\partial L}{\\partial b} = (\\hat{y} - y)\\ .\n\\end{align*}\\]\nVectorized over \\(m\\) examples: \\[\\begin{align*}\n\\frac{\\partial L}{\\partial b} = \\frac{1}{m}\\sum_{i=1}^{m}{(\\hat{y}^{(i)} - y^{(i)})}\\ .\n\\end{align*}\\]\nIn our code, we‚Äôll follow the convention of labeling gradients with a d prefix: dW for \\(\\partial L/\\partial W\\) and db for \\(\\partial L/\\partial b\\). This notation reminds us that these are derivatives. The backpropagation step then consists of just two lines: dW = (1/m) * np.matmul(X, (A-Y).T) and db = (1/m)*np.sum(A-Y, axis=1, keepdims=True).\n\n\nGradient Descent: Following the Downhill Path ‚õ∞Ô∏è\nWe now have all the ingredients needed to train our neural network! The optimization algorithm we‚Äôll use is called gradient descent‚Äîconceptually simple but remarkably effective. The update rule for weights is:\n\\[\nw\\leftarrow w-\\eta\\frac{\\partial L}{\\partial w}\n\\]\nHere \\(\\eta\\) (Greek letter eta) is the learning rate, a hyperparameter that controls the step size. Think of gradient descent as navigating down a mountain in fog: the gradient tells you which direction is downhill, and the learning rate determines how big a step you take.\nThe logic is straightforward. If \\(\\partial L/\\partial w\\) is positive, the loss increases as \\(w\\) increases, so we should decrease \\(w\\) to reduce the loss. If \\(\\partial L/\\partial w\\) is negative, the loss decreases as \\(w\\) increases, so we should increase \\(w\\). The gradient descent update automatically does the right thing in both cases.\nThe identical update rule applies to the bias: \\(b\\leftarrow b-\\eta\\frac{\\partial L}{\\partial b}\\).\nWe repeat this update process many times‚Äîeach complete pass through all training data is called an epoch. This is analogous to iterative methods you might have encountered in computational physics, like the relaxation method for solving Laplace‚Äôs equation or the power method for finding eigenvectors. We start with a random guess and iteratively improve it until convergence.\nThe term ‚Äústochastic‚Äù gradient descent (SGD) actually refers to a variant where we don‚Äôt use all training examples at once but rather small random batches. For our purposes, we‚Äôll compute gradients using the full training set each iteration, which is sometimes called batch gradient descent. This is more stable but slower for large datasets.\n\n\nBuild and Train: Putting It All Together üî®\nNow comes the exciting moment‚Äîassembling all our components into a working neural network! Let‚Äôs review what we have: a forward propagation function (computing predictions), a loss function (measuring error), and backpropagation (computing gradients). The training loop combines these pieces iteratively to learn optimal weights.\nHere‚Äôs our complete training procedure for a single-neuron network recognizing zeros:\nThis architecture is actually equivalent to logistic regression‚Äîthe sigmoid function is called a ‚Äúlogistic‚Äù function, hence the name. Despite its simplicity, logistic regression is quite powerful and forms the foundation for more complex networks.\n\nlearning_rate = 1\n\nX = np.array(X_train)\nY = np.array(y_train)\n\nn_x = X.shape[0]\nm = X.shape[1]\n\nW = np.random.randn(n_x, 1) * 0.01\nb = np.zeros((1, 1))\n\nfor i in range(200):\n    Z = np.matmul(W.T, X) + b\n    A = sigmoid(Z)\n\n    loss = compute_loss(Y, A)\n\n    dW = (1/m)*np.matmul(X, (A-Y).T)\n    db = (1/m)*np.sum(A-Y, axis=1, keepdims=True)\n\n    W = W - learning_rate * dW\n    b = b - learning_rate * db\n\n    if i % 10 == 0:\n        print(\"Epoch\", i, \" loss: \", loss)\n\nprint(\"Final loss:\", loss)\n\nEpoch 0  loss:  0.7471125121616977\nEpoch 10  loss:  0.07308269582929021\n\n\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/1534748172.py:13: RuntimeWarning: divide by zero encountered in matmul\n  Z = np.matmul(W.T, X) + b\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/1534748172.py:13: RuntimeWarning: overflow encountered in matmul\n  Z = np.matmul(W.T, X) + b\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/1534748172.py:13: RuntimeWarning: invalid value encountered in matmul\n  Z = np.matmul(W.T, X) + b\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/1534748172.py:18: RuntimeWarning: divide by zero encountered in matmul\n  dW = (1/m)*np.matmul(X, (A-Y).T)\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/1534748172.py:18: RuntimeWarning: overflow encountered in matmul\n  dW = (1/m)*np.matmul(X, (A-Y).T)\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/1534748172.py:18: RuntimeWarning: invalid value encountered in matmul\n  dW = (1/m)*np.matmul(X, (A-Y).T)\n\n\nEpoch 20  loss:  0.061318323546277226\nEpoch 30  loss:  0.055230119812005714\nEpoch 40  loss:  0.051324320236142515\nEpoch 50  loss:  0.048540041963711845\nEpoch 60  loss:  0.04642485272904433\nEpoch 70  loss:  0.04474722082574825\nEpoch 80  loss:  0.043374333931969114\nEpoch 90  loss:  0.04222371551840797\nEpoch 100  loss:  0.041241045998320916\nEpoch 110  loss:  0.040388886511106684\nEpoch 120  loss:  0.03964048057966332\nEpoch 130  loss:  0.0389761317586134\nEpoch 140  loss:  0.03838097920656846\nEpoch 150  loss:  0.03784357458020544\nEpoch 160  loss:  0.037354939644815115\nEpoch 170  loss:  0.03690792357698068\nEpoch 180  loss:  0.03649675336766015\nEpoch 190  loss:  0.03611671225524699\nFinal loss: 0.03579805734492837\n\n\nWatching the loss decrease is encouraging‚Äîit means our network is learning! But how do we judge its actual performance? The loss function tells us about training progress, but what we really care about is classification accuracy on unseen data.\nTo evaluate our network properly, we‚Äôll use the confusion matrix‚Äîa fundamental tool in classification that shows not just whether predictions are right or wrong, but specifically how they‚Äôre wrong. The confusion matrix displays actual labels in the rows and predicted labels in the columns, giving us a complete picture of our classifier‚Äôs behavior.\n\n\n\nconfusion_matrix\n\n\nUnderstanding the confusion matrix is crucial. True positives (TP) are zeros correctly identified as zeros. False positives (FP) are other digits incorrectly classified as zeros. False negatives (FN) are zeros that we missed (classified as ‚Äúnot zero‚Äù). True negatives (TN) are other digits correctly identified as not-zeros.\nFrom these four quantities, we can derive useful metrics. Precision = TP/(TP+FP) tells us what fraction of predicted zeros are actually zeros. Recall = TP/(TP+FN) tells us what fraction of actual zeros we successfully identified. Accuracy = (TP+TN)/(TP+TN+FP+FN) tells us the overall fraction of correct predictions.\nFortunately, sklearn provides functions to compute all of this automatically. We just need to supply predictions and actual labels. Crucially, we evaluate on the test set X_test‚Äîdata the network has never seen during training. This tests generalization, not memorization.\n\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nZ = np.matmul(W.T,X_test) + b\nA = sigmoid(Z)\n\npredictions = (A&gt;.5)[0,:]\nlabels = (y_test == 1)[0,:]\n\nprint(confusion_matrix(predictions, labels))\n\n[[8973   33]\n [  47  947]]\n\n\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/2374582204.py:3: RuntimeWarning: divide by zero encountered in matmul\n  Z = np.matmul(W.T,X_test) + b\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/2374582204.py:3: RuntimeWarning: overflow encountered in matmul\n  Z = np.matmul(W.T,X_test) + b\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/2374582204.py:3: RuntimeWarning: invalid value encountered in matmul\n  Z = np.matmul(W.T,X_test) + b\n\n\n\nprint(classification_report(predictions, labels))\n\n              precision    recall  f1-score   support\n\n       False       0.99      1.00      1.00      9006\n        True       0.97      0.95      0.96       994\n\n    accuracy                           0.99     10000\n   macro avg       0.98      0.97      0.98     10000\nweighted avg       0.99      0.99      0.99     10000\n\n\n\n\n\nTesting Our Model: Individual Predictions üî¨\nBeyond aggregate statistics, it‚Äôs instructive to examine individual predictions. We can test our network on a single image and see what it predicts. Since sigmoid outputs range from 0 to 1, we use 0.5 as our decision threshold‚Äîoutputs above 0.5 are classified as zeros, below 0.5 as not-zeros. This threshold could be adjusted to trade off precision versus recall, but 0.5 is the natural choice for balanced classes.\n\ni=200\nbool(sigmoid(np.matmul(W.T, np.array(X_test)[:,i])+b)&gt;0.5)\n\nFalse\n\n\n\nplt.imshow(np.array(X_test)[:,i].reshape(28,28),cmap='gray')\n\n\n\n\nExample Image"
  },
  {
    "objectID": "seminars/seminar01/deep-learning.html#network-with-hidden-layers-increasing-representational-power",
    "href": "seminars/seminar01/deep-learning.html#network-with-hidden-layers-increasing-representational-power",
    "title": "Neural Networks",
    "section": "Network with Hidden Layers: Increasing Representational Power üß©",
    "text": "Network with Hidden Layers: Increasing Representational Power üß©\nSo far, our network consists of just an input layer (784 pixels) connected directly to a single output neuron. This is the simplest possible neural network architecture. To handle more complex patterns, we need to add hidden layers‚Äîintermediate layers between input and output.\nHidden layers are called ‚Äúhidden‚Äù because they‚Äôre internal to the network; we don‚Äôt directly observe their outputs during normal operation. But these hidden units are where the magic happens! They allow the network to learn intermediate representations‚Äîabstract features derived from the raw input pixels.\nThink of it hierarchically: the first layer might learn to detect edges and curves, while deeper layers combine these into higher-level features like loops and intersections. This compositional structure is what makes deep learning so powerful‚Äîcomplex concepts are built from simpler ones, much like how physical theories build complexity from fundamental principles.\nHere‚Äôs the architecture we‚Äôll build: 784 inputs ‚Üí 64 hidden units ‚Üí 1 output neuron.\n\n\n\nhidden\n\n\nOur network now has two sets of parameters: weights \\(W_1\\) and biases \\(b_1\\) connecting the input to the hidden layer (shape 64 √ó 784), and weights \\(W_2\\) and biases \\(b_2\\) connecting the hidden layer to the output (shape 1 √ó 64). This gives us roughly 50,000 trainable parameters‚Äîa significant increase from 785 in the single-neuron network!\nThe forward pass now has two stages: input ‚Üí hidden layer (apply \\(W_1\\), \\(b_1\\), then sigmoid), then hidden layer ‚Üí output (apply \\(W_2\\), \\(b_2\\), then sigmoid). Backpropagation also has two stages, working backward from the output error through each layer using the chain rule.\nWe won‚Äôt derive all the equations again‚Äîthey follow the same principles as before, just applied to each layer. The code extends naturally from our single-neuron implementation:\n\nX = X_train\nY = y_train\n\nn_x = X.shape[0]\nn_h = 64\nlearning_rate = 1\n\nW1 = np.random.randn(n_h, n_x)\nb1 = np.zeros((n_h, 1))\nW2 = np.random.randn(1, n_h)\nb2 = np.zeros((1, 1))\n\nfor i in range(100):\n\n    Z1 = np.matmul(W1, X) + b1\n    A1 = sigmoid(Z1)\n    Z2 = np.matmul(W2, A1) + b2\n    A2 = sigmoid(Z2)\n\n    loss = compute_loss(Y, A2)\n\n    dZ2 = A2-Y\n    dW2 = (1./m) * np.matmul(dZ2, A1.T)\n    db2 = (1./m) * np.sum(dZ2, axis=1, keepdims=True)\n\n    dA1 = np.matmul(W2.T, dZ2)\n    dZ1 = dA1 * sigmoid(Z1) * (1 - sigmoid(Z1))\n    dW1 = (1./m) * np.matmul(dZ1, X.T)\n    db1 = (1./m) * np.sum(dZ1, axis=1, keepdims=True)\n\n    W2 = W2 - learning_rate * dW2\n    b2 = b2 - learning_rate * db2\n    W1 = W1 - learning_rate * dW1\n    b1 = b1 - learning_rate * db1\n\n    if i % 10 == 0:\n        print(\"Epoch\", i, \"loss: \", loss)\n\nprint(\"Final loss:\", loss)\n\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/1376930165.py:15: RuntimeWarning: divide by zero encountered in matmul\n  Z1 = np.matmul(W1, X) + b1\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/1376930165.py:15: RuntimeWarning: overflow encountered in matmul\n  Z1 = np.matmul(W1, X) + b1\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/1376930165.py:15: RuntimeWarning: invalid value encountered in matmul\n  Z1 = np.matmul(W1, X) + b1\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/1376930165.py:17: RuntimeWarning: divide by zero encountered in matmul\n  Z2 = np.matmul(W2, A1) + b2\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/1376930165.py:17: RuntimeWarning: overflow encountered in matmul\n  Z2 = np.matmul(W2, A1) + b2\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/1376930165.py:17: RuntimeWarning: invalid value encountered in matmul\n  Z2 = np.matmul(W2, A1) + b2\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/1376930165.py:23: RuntimeWarning: divide by zero encountered in matmul\n  dW2 = (1./m) * np.matmul(dZ2, A1.T)\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/1376930165.py:23: RuntimeWarning: overflow encountered in matmul\n  dW2 = (1./m) * np.matmul(dZ2, A1.T)\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/1376930165.py:23: RuntimeWarning: invalid value encountered in matmul\n  dW2 = (1./m) * np.matmul(dZ2, A1.T)\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/1376930165.py:28: RuntimeWarning: divide by zero encountered in matmul\n  dW1 = (1./m) * np.matmul(dZ1, X.T)\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/1376930165.py:28: RuntimeWarning: overflow encountered in matmul\n  dW1 = (1./m) * np.matmul(dZ1, X.T)\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/1376930165.py:28: RuntimeWarning: invalid value encountered in matmul\n  dW1 = (1./m) * np.matmul(dZ1, X.T)\n\n\nEpoch 0 loss:  2.395166635058746\nEpoch 10 loss:  0.22074168759268956\nEpoch 20 loss:  0.1660154822272753\nEpoch 30 loss:  0.13990677867922954\nEpoch 40 loss:  0.12390102523919129\nEpoch 50 loss:  0.11269161497108851\nEpoch 60 loss:  0.10421329497723458\nEpoch 70 loss:  0.09747959072905935\nEpoch 80 loss:  0.09194898313097832\nEpoch 90 loss:  0.0872943606401609\nFinal loss: 0.08367740628296327\n\n\nTo evaluate our improved network, we again use the confusion matrix on the test set. You should notice better performance compared to the single-neuron network‚Äîthe hidden layer provides additional representational power.\n\nZ1 = np.matmul(W1, X_test) + b1\nA1 = sigmoid(Z1)\nZ2 = np.matmul(W2, A1) + b2\nA2 = sigmoid(Z2)\n\npredictions = (A2&gt;.5)[0,:]\nlabels = (y_test == 1)[0,:]\n\nprint(confusion_matrix(predictions, labels))\nprint(classification_report(predictions, labels))\n\n[[8905  178]\n [ 115  802]]\n              precision    recall  f1-score   support\n\n       False       0.99      0.98      0.98      9083\n        True       0.82      0.87      0.85       917\n\n    accuracy                           0.97     10000\n   macro avg       0.90      0.93      0.91     10000\nweighted avg       0.97      0.97      0.97     10000\n\n\n\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/3572825488.py:1: RuntimeWarning: divide by zero encountered in matmul\n  Z1 = np.matmul(W1, X_test) + b1\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/3572825488.py:1: RuntimeWarning: overflow encountered in matmul\n  Z1 = np.matmul(W1, X_test) + b1\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/3572825488.py:1: RuntimeWarning: invalid value encountered in matmul\n  Z1 = np.matmul(W1, X_test) + b1\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/3572825488.py:3: RuntimeWarning: divide by zero encountered in matmul\n  Z2 = np.matmul(W2, A1) + b2\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/3572825488.py:3: RuntimeWarning: overflow encountered in matmul\n  Z2 = np.matmul(W2, A1) + b2\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/3572825488.py:3: RuntimeWarning: invalid value encountered in matmul\n  Z2 = np.matmul(W2, A1) + b2"
  },
  {
    "objectID": "seminars/seminar01/deep-learning.html#multiclass-network-recognizing-all-digits",
    "href": "seminars/seminar01/deep-learning.html#multiclass-network-recognizing-all-digits",
    "title": "Neural Networks",
    "section": "Multiclass Network: Recognizing All Digits üî¢",
    "text": "Multiclass Network: Recognizing All Digits üî¢\nOur binary classifier successfully distinguishes zeros from non-zeros, but the real challenge is recognizing all ten digits (0-9). This requires extending to multiclass classification‚Äîinstead of a yes/no question, we‚Äôre asking ‚Äúwhich of these 10 categories?‚Äù\nThe key architectural change is replacing our single output neuron with 10 output neurons, one per digit class. Each neuron produces a score for its corresponding digit. We then interpret these scores as probabilities‚Äîthe neuron with the highest score determines our prediction.\nThis is analogous to expanding from a two-state quantum system (like spin-¬Ω) to a ten-state system‚Äîwe need ten basis states to represent all possibilities. The output layer produces a probability distribution over these ten states.\nFor example, the output array [0,1,0,0,0,0,0,0,0,0] would indicate the network predicts digit 1 (the second position, using zero-indexing). In practice, the outputs won‚Äôt be exactly 0 and 1, but rather probabilities that sum to 1. The predicted class is simply the position of the maximum value.\nTo train a multiclass network, we need to reload the original MNIST labels (not our binary 0/1 encoding):\n\nfrom sklearn.datasets import fetch_openml\nX, y = fetch_openml('mnist_784', version=1, return_X_y=True,as_frame=False)\n\nX = X / 255\n\nWe‚Äôll convert these labels to one-hot encoding‚Äîa representation where each label becomes a vector of length 10 with a single 1 and nine 0s. The digit ‚Äú3‚Äù becomes [0,0,0,1,0,0,0,0,0,0], for instance. This encoding matches our network‚Äôs output format and makes the loss calculation straightforward. The result is a 10 √ó 70,000 array where each column is one training example‚Äôs label:\n\ndigits = 10\nexamples = y.shape[0]\n\ny = y.reshape(1, examples)\n\nY_new = np.eye(digits)[y.astype('int32')]\nY_new = Y_new.T.reshape(digits, examples)\n\n\nY_new.shape\n\n(10, 70000)\n\n\nWe also separate into training and testing sets, maintaining the same 60,000/10,000 split:\n\nm = 60000\nm_test = X.shape[0] - m\n\nX_train, X_test = X[:m].T, X[m:].T\nY_train, Y_test = Y_new[:,:m], Y_new[:,m:]\n\nshuffle_index = np.random.permutation(m)\nX_train, Y_train = X_train[:, shuffle_index], Y_train[:, shuffle_index]\n\n\ni = 58\nplt.imshow(X_train[:,i].reshape(28,28), cmap='gray')\nplt.colorbar()\nplt.show()\nY_train[:,i]\n\n\n\n\n\n\n\n\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n\n\n\nChanges to the Model Architecture üîß\nLet‚Äôs systematically examine what changes are required to extend our binary classifier to multiclass. The input and hidden layers remain unchanged‚Äîthey still extract useful features. Only the output layer and loss function need modification.\n\nForward Pass: The Softmax Function\nThe critical change is in the output layer. Instead of a single sigmoid unit, we now have 10 units with a softmax activation function. Softmax generalizes sigmoid to multiple classes‚Äîit converts a vector of arbitrary real numbers (the logits \\(z\\)) into a probability distribution that sums to 1.\nThe softmax function computes the activation for each output unit \\(i\\) as: \\[\\begin{align*}\n\\sigma(z)_i = \\frac{{\\rm e}^{z_i}}{\\sum_{j=0}^9{\\rm e}^{z_i}}\\ .\n\\end{align*}\\]\nThe exponential ensures all outputs are positive, and the normalization ensures they sum to 1, giving us a valid probability distribution. Units with larger logits \\(z_i\\) get exponentially larger probabilities‚Äîthe softmax is ‚Äúsoft‚Äù because it doesn‚Äôt simply pick the maximum but gives a smooth distribution.\nIn vectorized NumPy code: A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0). The axis=0 parameter ensures we normalize each example independently (summing down columns, not across the entire matrix).\n\n\nLoss Function: Categorical Cross-Entropy\nWith multiple classes, we need to generalize our loss function. The categorical cross-entropy extends binary cross-entropy to \\(n\\) classes. For a single training example: \\[\\begin{align*}\nL(y,\\hat{y}) = -\\sum_{i=0}^n y_i\\log(\\hat{y}_i)\\ .\n\\end{align*}\\] This is the negative log-probability assigned to the correct class. Since the labels are one-hot encoded, only one term in the sum is nonzero (the term corresponding to the true class). This means we‚Äôre minimizing the negative log-likelihood‚Äîmaximizing the probability assigned to the correct class.\nFrom a physics perspective, this is deeply connected to statistical mechanics. The cross-entropy loss is equivalent to minimizing the Kullback-Leibler divergence between the true distribution (the one-hot labels) and the predicted distribution (softmax outputs). It‚Äôs the information-theoretic measure of how well our predictions match reality.\nAveraging over \\(m\\) training examples: \\[\\begin{align*}\nL(y,\\hat{y}) = -\\frac{1}{m}\\sum_{j=0}^m\\sum_{i=0}^n y_i^{(i)}\\log(\\hat{y}_i^{(i)})\\ .\n\\end{align*}\\]\nSo let‚Äôs define:\n\ndef compute_multiclass_loss(Y, Y_hat):\n    L_sum = np.sum(np.multiply(Y, np.log(Y_hat)))\n    m = Y.shape[1]\n    L = -(1/m) * L_sum\n    return L\n\n\n\nBackpropagation: A Fortunate Simplification\nHere‚Äôs a beautiful mathematical result: despite the complexity of the softmax function, the gradient simplifies dramatically. When we combine softmax activation with cross-entropy loss, the derivative of the loss with respect to the logits (the \\(z\\) values) is simply: \\[\\begin{align*}\n\\frac{\\partial L}{\\partial z_i} = \\hat{y}_i - y_i\\ .\n\\end{align*}\\]\nThis is exactly the same form we had with sigmoid and binary cross-entropy! The predicted probabilities minus the true labels. This isn‚Äôt a coincidence‚Äîit reflects the principled connection between exponential family distributions and their conjugate loss functions. The mathematical elegance here is reminiscent of how the Hamiltonian and Lagrangian formulations of mechanics lead to equivalent equations of motion.\nWe won‚Äôt walk through the full derivation (it requires careful application of the chain rule with matrix derivatives), but trust that this simple gradient makes implementation straightforward.\n\n\n\nBuild and Train: The Complete Network üöÄ\nNow we assemble our final, full-featured digit classifier. With more parameters (now around 50,000) and a more complex task (10 classes instead of 2), training will take longer. We‚Äôll run 200 epochs to give the network sufficient time to learn the patterns distinguishing all ten digits.\nThe architecture is: 784 inputs ‚Üí 64 hidden units (sigmoid) ‚Üí 10 outputs (softmax). Watch how the loss decreases as training progresses‚Äîyou‚Äôre witnessing the network discover representations of handwritten digits!\n\nn_x = X_train.shape[0]\nn_h = 64\nlearning_rate = 1\n\nW1 = np.random.randn(n_h, n_x)\nb1 = np.zeros((n_h, 1))\nW2 = np.random.randn(digits, n_h)\nb2 = np.zeros((digits, 1))\n\nX = X_train\nY = Y_train\n\nfor i in range(200):\n\n    Z1 = np.matmul(W1,X) + b1\n    A1 = sigmoid(Z1)\n    Z2 = np.matmul(W2,A1) + b2\n    A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0)\n\n    loss = compute_multiclass_loss(Y, A2)\n\n    dZ2 = A2-Y\n    dW2 = (1./m) * np.matmul(dZ2, A1.T)\n    db2 = (1./m) * np.sum(dZ2, axis=1, keepdims=True)\n\n    dA1 = np.matmul(W2.T, dZ2)\n    dZ1 = dA1 * sigmoid(Z1) * (1 - sigmoid(Z1))\n    dW1 = (1./m) * np.matmul(dZ1, X.T)\n    db1 = (1./m) * np.sum(dZ1, axis=1, keepdims=True)\n\n    W2 = W2 - learning_rate * dW2\n    b2 = b2 - learning_rate * db2\n    W1 = W1 - learning_rate * dW1\n    b1 = b1 - learning_rate * db1\n\n    if (i % 10 == 0):\n        print(\"Epoch\", i, \"loss: \", loss)\n\nprint(\"Final loss:\", loss)\n\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/1725216505.py:15: RuntimeWarning: divide by zero encountered in matmul\n  Z1 = np.matmul(W1,X) + b1\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/1725216505.py:15: RuntimeWarning: overflow encountered in matmul\n  Z1 = np.matmul(W1,X) + b1\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/1725216505.py:15: RuntimeWarning: invalid value encountered in matmul\n  Z1 = np.matmul(W1,X) + b1\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/1725216505.py:17: RuntimeWarning: divide by zero encountered in matmul\n  Z2 = np.matmul(W2,A1) + b2\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/1725216505.py:17: RuntimeWarning: overflow encountered in matmul\n  Z2 = np.matmul(W2,A1) + b2\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/1725216505.py:17: RuntimeWarning: invalid value encountered in matmul\n  Z2 = np.matmul(W2,A1) + b2\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/1725216505.py:23: RuntimeWarning: divide by zero encountered in matmul\n  dW2 = (1./m) * np.matmul(dZ2, A1.T)\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/1725216505.py:23: RuntimeWarning: overflow encountered in matmul\n  dW2 = (1./m) * np.matmul(dZ2, A1.T)\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/1725216505.py:23: RuntimeWarning: invalid value encountered in matmul\n  dW2 = (1./m) * np.matmul(dZ2, A1.T)\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/1725216505.py:26: RuntimeWarning: divide by zero encountered in matmul\n  dA1 = np.matmul(W2.T, dZ2)\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/1725216505.py:26: RuntimeWarning: overflow encountered in matmul\n  dA1 = np.matmul(W2.T, dZ2)\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/1725216505.py:26: RuntimeWarning: invalid value encountered in matmul\n  dA1 = np.matmul(W2.T, dZ2)\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/1725216505.py:28: RuntimeWarning: divide by zero encountered in matmul\n  dW1 = (1./m) * np.matmul(dZ1, X.T)\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/1725216505.py:28: RuntimeWarning: overflow encountered in matmul\n  dW1 = (1./m) * np.matmul(dZ1, X.T)\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/1725216505.py:28: RuntimeWarning: invalid value encountered in matmul\n  dW1 = (1./m) * np.matmul(dZ1, X.T)\n\n\nEpoch 0 loss:  9.359409945262724\nEpoch 10 loss:  2.480915410750769\nEpoch 20 loss:  1.6744327642277674\nEpoch 30 loss:  1.3330104308788544\nEpoch 40 loss:  1.1447842302497118\nEpoch 50 loss:  1.0230964725181804\nEpoch 60 loss:  0.9368747323694274\nEpoch 70 loss:  0.8719573894048428\nEpoch 80 loss:  0.8208795576102073\nEpoch 90 loss:  0.7793325725168159\nEpoch 100 loss:  0.7446649543545801\nEpoch 110 loss:  0.7151537041535515\nEpoch 120 loss:  0.6896258244540622\nEpoch 130 loss:  0.6672519100025255\nEpoch 140 loss:  0.6474268213495037\nEpoch 150 loss:  0.6296970416913454\nEpoch 160 loss:  0.6137147676333654\nEpoch 170 loss:  0.5992079750548168\nEpoch 180 loss:  0.5859603076597459\nEpoch 190 loss:  0.5737971945414018\nFinal loss: 0.5636592880338956\n\n\nLet‚Äôs see how we did:\n\nZ1 = np.matmul(W1, X_test) + b1\nA1 = sigmoid(Z1)\nZ2 = np.matmul(W2, A1) + b2\nA2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0)\n\npredictions = np.argmax(A2, axis=0)\nlabels = np.argmax(Y_test, axis=0)\n\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/3032217627.py:1: RuntimeWarning: divide by zero encountered in matmul\n  Z1 = np.matmul(W1, X_test) + b1\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/3032217627.py:1: RuntimeWarning: overflow encountered in matmul\n  Z1 = np.matmul(W1, X_test) + b1\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/3032217627.py:1: RuntimeWarning: invalid value encountered in matmul\n  Z1 = np.matmul(W1, X_test) + b1\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/3032217627.py:3: RuntimeWarning: divide by zero encountered in matmul\n  Z2 = np.matmul(W2, A1) + b2\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/3032217627.py:3: RuntimeWarning: overflow encountered in matmul\n  Z2 = np.matmul(W2, A1) + b2\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1372/3032217627.py:3: RuntimeWarning: invalid value encountered in matmul\n  Z2 = np.matmul(W2, A1) + b2\n\n\n\nModel Performance: Confusion Matrix Analysis üìä\nNow for the moment of truth‚Äîhow well does our network perform on unseen test data?\n\nprint(confusion_matrix(predictions, labels))\nprint(classification_report(predictions, labels))\n\n[[ 896    0   26    8    8   22   30    4   14   10]\n [   0 1076   15    6    2    7    3    6   14    2]\n [  14   13  815   30   10   13   23   33   24   13]\n [  10   12   47  820    4   60    5   18   40   17]\n [   0    1   17    0  790   22   30   17   14  106]\n [  27    4    7   56    5  669   23    6   58   16]\n [  13    5   30    7   23   28  822    0   21    1]\n [   6    2   18   25   12   10    3  866   18   37]\n [  12   22   47   42   19   44   16   15  739   28]\n [   2    0   10   16  109   17    3   63   32  779]]\n              precision    recall  f1-score   support\n\n           0       0.91      0.88      0.90      1018\n           1       0.95      0.95      0.95      1131\n           2       0.79      0.82      0.81       988\n           3       0.81      0.79      0.80      1033\n           4       0.80      0.79      0.80       997\n           5       0.75      0.77      0.76       871\n           6       0.86      0.87      0.86       950\n           7       0.84      0.87      0.86       997\n           8       0.76      0.75      0.75       984\n           9       0.77      0.76      0.76      1031\n\n    accuracy                           0.83     10000\n   macro avg       0.82      0.83      0.82     10000\nweighted avg       0.83      0.83      0.83     10000\n\n\n\nWe achieve approximately 84% accuracy across all ten digits‚Äînot bad for a network we built entirely from scratch! Modern state-of-the-art methods using convolutional neural networks reach 99%+ accuracy, but our simple fully-connected network demonstrates the fundamental principles.\nLooking at the confusion matrix and classification report, you can see which digits are hardest to distinguish. Often, 4s and 9s are confused, or 3s and 5s‚Äîeven humans sometimes struggle with these pairs in badly written examples. The confusion matrix gives insight into the network‚Äôs systematic errors, which could guide improvements."
  },
  {
    "objectID": "seminars/seminar01/deep-learning.html#testing-the-model-interactive-exploration",
    "href": "seminars/seminar01/deep-learning.html#testing-the-model-interactive-exploration",
    "title": "Neural Networks",
    "section": "Testing the Model: Interactive Exploration üîç",
    "text": "Testing the Model: Interactive Exploration üîç\nLet‚Äôs test our trained network on individual examples to get an intuitive feel for its predictions:\n\ni=2003\nplt.imshow(X_test[:,i].reshape(28,28), cmap='gray')\npredictions[i]\n\nnp.int64(5)\n\n\n\n\n\n\n\n\n\nTry changing the index i to explore different test images. Compare the network‚Äôs predictions with what you see. When does it make mistakes? Are the errors understandable? This kind of error analysis is crucial in practical machine learning‚Äîunderstanding failure modes helps you improve your models."
  },
  {
    "objectID": "seminars/seminar01/deep-learning.html#reflection-and-next-steps",
    "href": "seminars/seminar01/deep-learning.html#reflection-and-next-steps",
    "title": "Neural Networks",
    "section": "Reflection and Next Steps üéØ",
    "text": "Reflection and Next Steps üéØ\nCongratulations! You‚Äôve built a complete neural network from scratch and trained it to recognize handwritten digits with reasonable accuracy. More importantly, you understand every component: forward propagation (matrix multiplications and activations), loss functions (quantifying error), backpropagation (computing gradients via chain rule), and gradient descent (iterative optimization).\nKey Insights for Physicists: The mathematical tools we used‚Äîlinear algebra, calculus, and optimization‚Äîare the same ones you use throughout physics. Neural networks are fundamentally just differentiable programs that we optimize. The loss landscape we navigate with gradient descent is analogous to potential energy surfaces in molecular dynamics or action functionals in field theory.\nWhat We Built: Our final network has ~50,000 parameters organized into two layers. It processes 784-dimensional input vectors (flattened images) through learned transformations to produce 10-dimensional output probability distributions. Training involved computing gradients for all 50,000 parameters on 60,000 examples, iterating 200 times‚Äîabout 600 million gradient computations in total, all completed in seconds thanks to vectorized NumPy operations.\nLimitations and Improvements: Our network treats pixels as independent features, ignoring spatial structure. Convolutional neural networks (CNNs) exploit the 2D structure of images, achieving much better performance. We also used a fixed learning rate and simple gradient descent; modern optimizers like Adam adapt the learning rate automatically. And we‚Äôve barely scratched the surface of architecture design‚Äîdepth, width, activation functions, regularization, and many other choices affect performance.\nIn the Next Seminar: We‚Äôll explore high-level frameworks like TensorFlow and PyTorch that make building and training complex networks much easier. We‚Äôll also see how to apply these techniques to physics problems‚Äîfrom analyzing experimental data to solving differential equations with neural networks. The principles remain the same; the tools just become more powerful.\nFinal Thought: Machine learning is rapidly transforming physics research. From discovering new particles in collider data to controlling quantum systems to simulating complex materials, neural networks are becoming as essential as Fourier transforms or numerical integration. By understanding these tools at a fundamental level, you‚Äôre well-prepared to apply them creatively in your own research. ```"
  },
  {
    "objectID": "seminars/Assignment5.html",
    "href": "seminars/Assignment5.html",
    "title": "Assignment 5",
    "section": "",
    "text": "Write a function \\(G(f, x, ...)\\) that takes a function \\(f(x,...)\\) as an argument with a variable number of additional parameters. The function¬†G shall take the arguments f, x and the parameters of f.¬†It shall calculate the sum of all function values of f.¬†You should import the numpy module as np. A definition of f¬†is not required.¬†\n\nimport numpy as np\n\ndef G(f,x,*params):\n    return np.sum(f(x,*params))\n\n\nx=np.linspace(0,10,100)-5\ny=np.linspace(0,10,100)-5\n\nX,Y=np.meshgrid(x,y)\n\n\nR=np.zeros([len(x),len(y)])\n\nfor i in range(0,len(x)): # gehe durch jede Zeile\n    for j in range(0,len(y)): # gehe durch jede Spalte\n        R[i,j]=np.sqrt(x[j]**2+y[i]**2)\n\n\nimport matplotlib.pyplot as plt\n\n\nplt.contour(x,y,R)\nplt.axhline(y=0)\nplt.axvline(x=0)\nplt.axis(\"square\")\n\n\n\n\n\n\n\n\n\ndef line(x,a,b):\n   return a*x+b\n\nx = np.arange(5,11,1)\nprint(G(line,x,5,5))\n\n255"
  },
  {
    "objectID": "seminars/Assignment5.html#problem-1",
    "href": "seminars/Assignment5.html#problem-1",
    "title": "Assignment 5",
    "section": "",
    "text": "Write a function \\(G(f, x, ...)\\) that takes a function \\(f(x,...)\\) as an argument with a variable number of additional parameters. The function¬†G shall take the arguments f, x and the parameters of f.¬†It shall calculate the sum of all function values of f.¬†You should import the numpy module as np. A definition of f¬†is not required.¬†\n\nimport numpy as np\n\ndef G(f,x,*params):\n    return np.sum(f(x,*params))\n\n\nx=np.linspace(0,10,100)-5\ny=np.linspace(0,10,100)-5\n\nX,Y=np.meshgrid(x,y)\n\n\nR=np.zeros([len(x),len(y)])\n\nfor i in range(0,len(x)): # gehe durch jede Zeile\n    for j in range(0,len(y)): # gehe durch jede Spalte\n        R[i,j]=np.sqrt(x[j]**2+y[i]**2)\n\n\nimport matplotlib.pyplot as plt\n\n\nplt.contour(x,y,R)\nplt.axhline(y=0)\nplt.axvline(x=0)\nplt.axis(\"square\")\n\n\n\n\n\n\n\n\n\ndef line(x,a,b):\n   return a*x+b\n\nx = np.arange(5,11,1)\nprint(G(line,x,5,5))\n\n255"
  },
  {
    "objectID": "seminars/Assignment5.html#problem-2",
    "href": "seminars/Assignment5.html#problem-2",
    "title": "Assignment 5",
    "section": "Problem 2",
    "text": "Problem 2\nWrite a Python program that:\nConstructs a 10x10 matrix D that approximates the first derivative operator using finite differences. Uses for loops to build the matrix. Do not import any library(math, NumPy, etc.). Apply the following finite difference schemes:\n\nForward difference¬†at the first point.\nBackward difference¬†at the last point.\nCentral difference at all interior points.\n\nAssumes a constant grid spacing h (you can set¬†h = 1 for simplicity).\nFinite Difference Schemes:\nForward Difference (at the first point, i=0): \\(f'(x_0) \\approx \\frac{f(x_1) - f(x_0)}{h}\\)\nBackward Difference (at the last point, i=N-1):¬†¬† \\(f'(x_{N-1}) \\approx \\frac{f(x_{N-1}) - f(x_{N-2})}{h}\\)\nCentral Difference (at interior points, 1 ‚â§ i ‚â§ N-2):¬†¬† \\(f'(x_i) \\approx \\frac{f(x_{i+1}) - f(x_{i-1})}{2h}\\)\nInstructions:\n\nInitialize a 10x10 matrix D filled with zeros.\nUse for loops to populate the matrix according to the finite difference schemes.\nDo not print anything.\n\n\nliste=[[(i,j) for i in range(N)] for j in range(N)]\n\n\nN = 10\nh = 1.0\n\nD = [[0.0 for _ in range(N)] for _ in range(N)]\n\n# Forward difference for the first point\nD[0][0] = -1.0 / h\nD[0][1] = 1.0 / h\n\n# Central difference for interior points\nfor i in range(1,N-1):\n    D[i][i-1] = -0.5 / h\n    D[i][i+1] = 0.5 / h\n    \n# Backward difference for the last point\nD[N-1][N-2] = -1.0 / h\nD[N-1][N-1] = 1.0 / h\n\n\\[\nU(\\vec{r},t)=U_0\\exp(i(\\omega t- \\vec{k} \\cdot \\vec{r}))\n\\]\n\nx=np.linspace(0,10,100)\ny=np.linspace(0,10,100)\nZ=np.zeros([10,10])\n\nX,Y=np.meshgrid(x,y)\n\nR=np.array([X,Y,0],dtype=object)\n\n\nplt.imshow(np.real(np.exp(-1j*np.dot(k,R))))\n\n\n\n\n\n\n\n\n\nnp.dot(k,R)plt.imshow()\n\narray([[ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ]])\n\n\n\nnp.dot(k,R)\n\narray([[ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ]])\n\n\n\nk=np.array([1,1,0])\n\n\ndef plane(r,omega=1,t=0,k):\n    \n\nnp.complex128(1j)\n\n\n\n\n\narray([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.],\n       [10., 11., 12., 13., 14., 15., 16., 17., 18., 19.],\n       [20., 21., 22., 23., 24., 25., 26., 27., 28., 29.],\n       [30., 31., 32., 33., 34., 35., 36., 37., 38., 39.],\n       [40., 41., 42., 43., 44., 45., 46., 47., 48., 49.],\n       [50., 51., 52., 53., 54., 55., 56., 57., 58., 59.],\n       [60., 61., 62., 63., 64., 65., 66., 67., 68., 69.],\n       [70., 71., 72., 73., 74., 75., 76., 77., 78., 79.],\n       [80., 81., 82., 83., 84., 85., 86., 87., 88., 89.],\n       [90., 91., 92., 93., 94., 95., 96., 97., 98., 99.]])\n\n\n\nimport numpy as np\n\nA = np.array([\n    [1, -1, 3, -2],\n    [-2, 4, -3, 1],\n    [3, -1, 10, -4],\n    [4, -3, 8, -2],\n])\n\nB = np.array([1, 0.5, 2.9, 0.6])\n\nx = np.linalg.solve(A, B)\n\n\ntype(np.round(sum(x),1))\n\nnumpy.float64"
  },
  {
    "objectID": "seminars/Assignment 3.html",
    "href": "seminars/Assignment 3.html",
    "title": "Assignment 3",
    "section": "",
    "text": "Problem 1\nDefine a class with the name particle with a constructor that initializes the property D with twice the value supplied as an argument to the constructor.¬†\n\nclass particle:\n    def __init__(self,R):\n        self.D=2*R\n\nProblem 2\nWrite a class particle that has a class variable id. Implement a constructor that assigns a unique id property to each instance from the class id, increasing by 1 each time an instance (object) is created. The first created object id should be 1.\n\nclass particle:\n    id = 0\n    def __init__(self):\n        particle.id+=1\n        self.id = particle.id\n    \n    def __del__(self):\n        particle.id-=1\n\nProblem 3\nWrite a class particle that is constructed with two parameters that are stored in the properties R¬†and type_t of the object (R goes first). The type_t property should be of type string and be either ‚Äúcircle‚Äù or ‚Äúsquare‚Äù.\nWrite a class method¬†area that calculates the area of the object from the parameter R depending on the property type_t. The result of the area calculation should be stored in the property A, which should be 0 initially.\nCreate an object c which is a circle and compute its area. Create an object s which is a square without computing its area. The R parameter of these objects can be an arbitrary positive number.\nYou have two answer attempts without penalty.\n\nfrom math import pi\n\nclass particle:\n    def __init__(self, R, type_t):\n        self.R = R\n        self.type_t = type_t\n        self.A = 0\n        \n    def area(self):\n        if self.type_t == \"square\":\n            self.A = self.R**2\n        elif self.type_t == \"circle\":\n            self.A = pi*self.R**2\n        else:\n            self.A = 0\n            \nc=particle(3,\"circle\")\nc.area()\ns=particle(3,\"square\")\n\n\nc.A\n\n28.274333882308138"
  },
  {
    "objectID": "seminars/seminar11/projectile-motion.html#introduction",
    "href": "seminars/seminar11/projectile-motion.html#introduction",
    "title": "Seminar: Projectile Motion with Air Resistance",
    "section": "Introduction",
    "text": "Introduction\nIn today‚Äôs seminar, we will apply the numerical methods from the lecture to solve a classic physics problem: projectile motion with air resistance.\nThis problem is perfect for practicing because:\n\nYou know the physics from your mechanics course\nWithout air resistance, you can verify your code against the analytical solution\nWith air resistance, there is no analytical solution ‚Äî you need numerical methods!\n\n\nWhat You Will Learn\n\nHow to translate physics equations into code\nHow to handle 2D motion with coupled ODEs\nHow to compare different physical models\nHow to analyze and visualize results\n\n\n\nThe Physics\nA projectile of mass \\(m\\) experiences two forces:\n\nGravity: \\(\\vec{F}_g = -mg\\hat{y}\\)\nAir resistance (drag): \\(\\vec{F}_d = -b\\vec{v}\\) (linear) or \\(\\vec{F}_d = -c|\\vec{v}|\\vec{v}\\) (quadratic)\n\nFrom Newton‚Äôs second law: \\(m\\vec{a} = \\vec{F}_g + \\vec{F}_d\\)"
  },
  {
    "objectID": "seminars/seminar11/projectile-motion.html#part-1-projectile-without-air-resistance-warmup",
    "href": "seminars/seminar11/projectile-motion.html#part-1-projectile-without-air-resistance-warmup",
    "title": "Seminar: Projectile Motion with Air Resistance",
    "section": "Part 1: Projectile Without Air Resistance (Warmup)",
    "text": "Part 1: Projectile Without Air Resistance (Warmup)\nBefore adding complexity, let‚Äôs verify our numerical method works by solving a problem we can solve analytically.\n\nThe Equations\nWithout drag, the equations of motion are:\n\\[\\frac{d^2x}{dt^2} = 0 \\qquad \\frac{d^2y}{dt^2} = -g\\]\nConverting to first-order ODEs (state vector: \\([x, y, v_x, v_y]\\)):\n\\[\\frac{dx}{dt} = v_x \\qquad \\frac{dv_x}{dt} = 0\\] \\[\\frac{dy}{dt} = v_y \\qquad \\frac{dv_y}{dt} = -g\\]\n\n\nAnalytical Solution (for comparison)\n\\[x(t) = x_0 + v_{x0} t\\] \\[y(t) = y_0 + v_{y0} t - \\frac{1}{2}gt^2\\]\n\n\n\n\n\n\nExercise 1: Simple Projectile Motion\n\n\n\nComplete the code below to simulate a projectile without air resistance.\nTasks:\n\nDefine the derivative function projectile_no_drag(state, g) that returns \\([v_x, v_y, a_x, a_y]\\)\nImplement the improved Euler method\nRun the simulation and compare with the analytical solution\n\nInitial conditions: Launch from origin at 45¬∞ with speed 20 m/s\n\n\n\n\n\n\n\nHints:\n\nFor projectile_no_drag: The derivatives are simply [vx, vy, 0, -g]\nFor improved_euler_step:\n\nFirst predict: state_pred = state + dt * deriv_func(state, *params)\nThen average: return state + 0.5 * dt * (deriv_func(state, *params) + deriv_func(state_pred, *params))\n\n\n\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "seminars/seminar11/projectile-motion.html#part-2-adding-linear-air-resistance",
    "href": "seminars/seminar11/projectile-motion.html#part-2-adding-linear-air-resistance",
    "title": "Seminar: Projectile Motion with Air Resistance",
    "section": "Part 2: Adding Linear Air Resistance",
    "text": "Part 2: Adding Linear Air Resistance\nNow let‚Äôs add physics that cannot be solved analytically in 2D!\n\nLinear Drag (Stokes Drag)\nFor slow-moving objects or small particles, air resistance is proportional to velocity:\n\\[\\vec{F}_d = -b\\vec{v}\\]\nwhere \\(b\\) is a drag coefficient. The equations of motion become:\n\\[m\\frac{dv_x}{dt} = -bv_x \\qquad m\\frac{dv_y}{dt} = -mg - bv_y\\]\nWe define the drag parameter \\(\\gamma = b/m\\) (units: 1/s), so:\n\\[\\frac{dv_x}{dt} = -\\gamma v_x \\qquad \\frac{dv_y}{dt} = -g - \\gamma v_y\\]\n\n\n\n\n\n\nExercise 2: Projectile with Linear Drag\n\n\n\nModify your code to include linear air resistance.\nTasks:\n\nModify the derivative function to include linear drag\nCompare trajectories with and without drag\nExplore how different values of \\(\\gamma\\) affect the trajectory\n\n\n\n\n\n\n\n\nHint: The only change from the no-drag case is in the accelerations:\n\n\\(a_x = -\\gamma \\cdot v_x\\) (was 0)\n\\(a_y = -g - \\gamma \\cdot v_y\\) (was just \\(-g\\))\n\n\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "seminars/seminar11/projectile-motion.html#part-3-quadratic-air-resistance-realistic",
    "href": "seminars/seminar11/projectile-motion.html#part-3-quadratic-air-resistance-realistic",
    "title": "Seminar: Projectile Motion with Air Resistance",
    "section": "Part 3: Quadratic Air Resistance (Realistic)",
    "text": "Part 3: Quadratic Air Resistance (Realistic)\nFor everyday objects moving through air at reasonable speeds, the drag is actually proportional to velocity squared:\n\\[\\vec{F}_d = -\\frac{1}{2}\\rho C_d A |\\vec{v}| \\vec{v}\\]\nwhere:\n\n\\(\\rho\\) = air density (‚âà 1.2 kg/m¬≥)\n\\(C_d\\) = drag coefficient (‚âà 0.47 for a sphere)\n\\(A\\) = cross-sectional area\n\nWe simplify by defining \\(k = \\frac{\\rho C_d A}{2m}\\), giving:\n\\[\\frac{dv_x}{dt} = -k \\sqrt{v_x^2 + v_y^2} \\cdot v_x\\] \\[\\frac{dv_y}{dt} = -g - k \\sqrt{v_x^2 + v_y^2} \\cdot v_y\\]\n\n\n\n\n\n\nExercise 3: Projectile with Quadratic Drag\n\n\n\nImplement the realistic quadratic drag model.\nTasks:\n\nImplement the derivative function with quadratic drag\nCompare linear vs.¬†quadratic drag\nSimulate a baseball (mass ‚âà 145 g, diameter ‚âà 7.4 cm)\n\n\n\n\n\n\n\n\nHints:\n\nCalculate the speed: v = np.sqrt(vx**2 + vy**2)\nThe drag acceleration has magnitude \\(k \\cdot v^2\\) and opposes the velocity\nComponents: ax = -k * v * vx and ay = -g - k * v * vy\n\n\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "seminars/seminar11/projectile-motion.html#part-4-analysis-finding-the-optimal-angle",
    "href": "seminars/seminar11/projectile-motion.html#part-4-analysis-finding-the-optimal-angle",
    "title": "Seminar: Projectile Motion with Air Resistance",
    "section": "Part 4: Analysis ‚Äî Finding the Optimal Angle",
    "text": "Part 4: Analysis ‚Äî Finding the Optimal Angle\nA classic question: What launch angle gives maximum range?\n\nWithout drag: The answer is 45¬∞ (you can prove this analytically)\nWith drag: The optimal angle is less than 45¬∞ ‚Äî but by how much?\n\n\n\n\n\n\n\nExercise 4: Optimal Launch Angle\n\n\n\nFind the optimal launch angle with quadratic drag by scanning different angles.\nTasks:\n\nSimulate trajectories for angles from 20¬∞ to 70¬∞\nFind the angle that gives maximum range\nPlot range vs.¬†angle and compare with the no-drag case\n\n\n\n\n\n\n\n\nHints:\n\nCreate angles: angles = np.linspace(20, 70, 51)\nUse list comprehension: ranges = [get_range(a, with_drag=True) for a in angles]\nFind maximum: optimal = angles[np.argmax(ranges)]\n\n\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "seminars/seminar11/projectile-motion.html#part-5-challenge-velocity-and-energy-analysis",
    "href": "seminars/seminar11/projectile-motion.html#part-5-challenge-velocity-and-energy-analysis",
    "title": "Seminar: Projectile Motion with Air Resistance",
    "section": "Part 5: Challenge ‚Äî Velocity and Energy Analysis",
    "text": "Part 5: Challenge ‚Äî Velocity and Energy Analysis\n\n\n\n\n\n\nExercise 5 (Optional): Energy Loss Analysis\n\n\n\nAnalyze how energy is dissipated during flight with air resistance.\nTasks:\n\nTrack kinetic energy throughout the flight\nCalculate work done against drag\nPlot energy vs.¬†time\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "seminars/seminar11/projectile-motion.html#summary",
    "href": "seminars/seminar11/projectile-motion.html#summary",
    "title": "Seminar: Projectile Motion with Air Resistance",
    "section": "Summary",
    "text": "Summary\nIn this seminar, you have:\n\n\n‚úÖ Implemented numerical ODE solving for 2D motion\n‚úÖ Translated physics equations directly into code\n‚úÖ Compared different drag models (none, linear, quadratic)\n‚úÖ Discovered that optimal launch angle decreases with drag\n‚úÖ Analyzed energy dissipation in a real physical system\n\n\n\nKey Takeaways\n\n\n\n\n\n\n\nConcept\nWhat You Learned\n\n\n\n\nState vectors\nHow to handle multi-variable problems: \\([x, y, v_x, v_y]\\)\n\n\nDerivative functions\nHow to encode physics: return [vx, vy, ax, ay]\n\n\nImproved Euler\nPredict-correct gives \\(O(\\Delta t^2)\\) accuracy\n\n\nPhysical insight\nAir resistance shifts optimal angle below 45¬∞\n\n\n\n\n\nNext Steps\n\nTry different objects (golf ball, soccer ball, ping pong ball)\nAdd wind effects: \\(\\vec{F}_{wind}\\)\nModel the Magnus effect (spinning ball)\nUse scipy.integrate.solve_ivp for production code"
  },
  {
    "objectID": "seminars/seminar11/projectile-motion.html#bonus-try-it-yourself",
    "href": "seminars/seminar11/projectile-motion.html#bonus-try-it-yourself",
    "title": "Seminar: Projectile Motion with Air Resistance",
    "section": "Bonus: Try It Yourself!",
    "text": "Bonus: Try It Yourself!\nModify the parameters below and see what happens:"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EMPP",
    "section": "",
    "text": "Willkommen zum Kurs Einf√ºhrung in die Modellierung Physikalischer Prozesse!\nDie Programmiersprache Python ist f√ºr alle Arten von wissenschaftlichen und technischen Aufgaben n√ºtzlich. Sie k√∂nnen mit ihr Daten analysieren und darstellen. Sie k√∂nnen mit ihr auch wissenschaftliche Probleme numerisch l√∂sen, die analytisch nur schwer oder gar nicht zu l√∂sen sind. Python ist frei verf√ºgbar und wurde aufgrund seines modularen Aufbaus um eine nahezu unendliche Anzahl von Modulen f√ºr verschiedene Zwecke erweitert.\nDieser Kurs soll Sie in die Programmierung mit Python einf√ºhren. Er richtet sich eher an den Anf√§nger, wir hoffen aber, dass er auch f√ºr Fortgeschrittene interessant ist. Wir beginnen den Kurs mit einer Einf√ºhrung in die Jupyter Notebook-Umgebung, die wir w√§hrend des gesamten Kurses verwenden werden. Danach werden wir eine Einf√ºhrung in Python geben und Ihnen einige grundlegende Funktionen zeigen, wie z.B. das Plotten und Analysieren von Daten durch Kurvenanpassung, das Lesen und Schreiben von Dateien, was einige der Aufgaben sind, die Ihnen w√§hrend Ihres Physikstudiums begegnen werden. Wir zeigen Ihnen auch einige fortgeschrittene Themen wie die Animation in Jupyter und die Simulation von physikalischen Prozessen in\n\nMechanik\nElektrostatik\nWellen\nOptik\n\nFalls am Ende des Kurses Zeit bleibt, werden wir auch einen Blick auf Verfahren des maschinellen Lernens werfen, das mittlerweile auch in der Physik zu einem wichtigen Werkzeug geworden ist.\nWir werden keine umfassende Liste von numerischen Simulationsschemata pr√§sentieren, sondern die Beispiele nutzen, um Ihre Neugierde zu wecken. Da es leichte Unterschiede in der Syntax der verschiedenen Python-Versionen gibt, werden wir uns im Folgenden immer auf den Python 3-Standard beziehen.\nDer Kurs wird auf Deutsch gehalten werden. Die Webseiten, die Sie f√ºr den √úberblick zu Python zur Verf√ºgung gestellt bekommen, werden allerdings auf Englisch sein. √úbungsaufgaben werden werden auf Deutsch gestellt.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "course-info/exam.html",
    "href": "course-info/exam.html",
    "title": "Pr√ºfung",
    "section": "",
    "text": "Die Pr√ºfungsleistung in diesem Modul ist eine Portfolio-Pr√ºfung und besteht aus zwei Portfolio-Teilen:\n\n√úbungsaufgaben:\n\n6 Serien von Aufgaben, die nicht benotet werden\nMindestens 50% der Gesamtpunktzahl aller √úbungsaufgaben muss erreicht werden\n\nZwei Tests:\n\nJeder Test dauert 45 Minuten\nFinden in Person w√§hrend zwei √úbungsseminare statt\nDie Tests werden vorher angek√ºndigt\nDie Punkte beider Tests werden addiert und ergeben eine Gesamtnote\n\n\nWichtige Hinweise: - Beide Portfolio-Teile (√úbungsaufgaben und Tests) m√ºssen bestanden werden, um das Modul erfolgreich abzuschlie√üen. - Die Gesamtnote der Tests entspricht der Abschlussnote des Moduls.",
    "crumbs": [
      "üìã Course Info",
      "Pr√ºfungen"
    ]
  },
  {
    "objectID": "course-info/exam.html#erstteilnehmer",
    "href": "course-info/exam.html#erstteilnehmer",
    "title": "Pr√ºfung",
    "section": "",
    "text": "Die Pr√ºfungsleistung in diesem Modul ist eine Portfolio-Pr√ºfung und besteht aus zwei Portfolio-Teilen:\n\n√úbungsaufgaben:\n\n6 Serien von Aufgaben, die nicht benotet werden\nMindestens 50% der Gesamtpunktzahl aller √úbungsaufgaben muss erreicht werden\n\nZwei Tests:\n\nJeder Test dauert 45 Minuten\nFinden in Person w√§hrend zwei √úbungsseminare statt\nDie Tests werden vorher angek√ºndigt\nDie Punkte beider Tests werden addiert und ergeben eine Gesamtnote\n\n\nWichtige Hinweise: - Beide Portfolio-Teile (√úbungsaufgaben und Tests) m√ºssen bestanden werden, um das Modul erfolgreich abzuschlie√üen. - Die Gesamtnote der Tests entspricht der Abschlussnote des Moduls.",
    "crumbs": [
      "üìã Course Info",
      "Pr√ºfungen"
    ]
  },
  {
    "objectID": "course-info/exam.html#wiederholer",
    "href": "course-info/exam.html#wiederholer",
    "title": "Pr√ºfung",
    "section": "Wiederholer",
    "text": "Wiederholer\n\nWiederholer aus WS 2024/25: Studierende, die das Modul bereits im WS 2024/25 belegt haben und nun wiederholen:\n\n\n√úbungsaufgaben:\n\n\nWenn Sie im WS 2024/25 ‚â• 50 % erreicht haben: Nur Teilnahme an den beiden Tests notwendig\nWenn Sie im WS 2024/25 &lt; 50 % erreicht haben: √úbungsaufgaben und Teilnahme an den beiden Tests notwendig\n\n\nEinschreibung:\n\n\nErneute Moduleinschreibung nicht notwendig\nF√ºr Moodle-Zugang: Kontaktieren Sie Andrea Kramer (email) per Uni-E-Mail.\nWiederholer aus WS 2023/24: Studierende, die das Modul bereits im WS 2023/24 belegt haben und nun wiederholen:\n\n\n√úbungsaufgaben:\n\n\nWenn Sie im WS 2023/24 ‚â• 50 % erreicht haben: Nur Projektabgabe erforderlich\nWenn Sie im WS 2023/24 &lt; 50 % erreicht haben: √úbungsaufgaben und Projektabgabe erforderlich\nBitte beachten Sie, dass Sie eine 2. Wiederholungspr√ºfung beantragen m√ºssen.\n\n\nEinschreibung:\n\n\nErneute Moduleinschreibung nicht notwendig\nF√ºr Moodle-Zugang: Kontaktieren Sie Andrea Kramer (email) per Uni-E-Mail.\n\n\nProjektabgabe f√ºr Wiederholer:\n\n\nFrist: 11.03.2026, 13:00 Uhr\nSp√§tere Abgaben werden nicht ber√ºcksichtigt.",
    "crumbs": [
      "üìã Course Info",
      "Pr√ºfungen"
    ]
  },
  {
    "objectID": "course-info/website.html",
    "href": "course-info/website.html",
    "title": "Diese Webseiten",
    "section": "",
    "text": "Diese Webseiten\nDiese Website enth√§lt alle Informationen, die f√ºr unseren Kurs Einf√ºhrung in die Modellierung Physikalischer Prozesse erforderlich sind. Sie werden hier jede Woche eine neue Vorlesung und eine neue Aufgabe finden. Die Vorlesungshefte werden von Videos begleitet, die den Inhalt der Vorlesung auf Englisch erkl√§ren, aber Sie k√∂nnen auch mit dem Lesen auskommen. Die Vorlesungen in Person, werden auf Deutsch stattfinden. Von diesen Webseiten aus werden Sie zu verschiedenen Ressourcen gef√ºhrt, die Sie nutzen k√∂nnen, um das Programmieren in Python zu lernen. Dabei werden wir einige gro√üartige Tools aus dem Internet nutzen, wie\n\nGoogle Colab Dienst, um auch Jupyter Notebooks (https://colab.research.google.com) zu hosten. Das Google Colab-Projekt bietet eine n√ºtzliche Umgebung zur gemeinsamen Nutzung von Notebooks.\n\n\n\ngoogle colab screen\n\n\nWenn Sie die folgende Website besuchen, werden Sie an mehreren Stellen das folgende Symbol sehen.\n![Substitution Name1]\nDieses Symbol zeigt an, dass diese Webseite auf einem Jupyter Notebook basiert. Anstatt nur die Website zu betrachten, k√∂nnen Sie auf das Symbol klicken und der Google Colab-Dienst wird ge√∂ffnet, damit Sie das Notizbuch interaktiv nutzen k√∂nnen. Google Colab √∂ffnet sich viel schneller als myBinder, aber die Notizb√ºcher sind f√ºr die Arbeit mit myBinder gemacht und nicht alle Funktionen funktionieren mit Colab. Ich arbeite jedoch an der Kompatibilit√§t.\nGitHub and GitHub Pages Dienst zum Hosting von Websites (https://github.com). GitHub ist ein gro√üartiger Ort, um Ihre kollaborativen Coding-Projekte einschlie√ülich Versionskontrolle zu hosten. In der oberen rechten Ecke finden Sie auch einen Link zum GitHub-Repository, in dem die Notebooks gehostet werden.\n\n\n\ngithub screen\n\n\nAnaconda Jupyter package f√ºr Notebooks auf dem eigenen Computer (https://www.anaconda.com/distribution/). Das Paket anaconda stellt Ihnen die Jupyter Notebook-Umgebung einschlie√ülich Python zur Verf√ºgung. Wenn Sie Jupyter zu Hause ohne Online-Zugang verwenden m√∂chten, ist dies ein gutes Paket zur Installation.\n\n\n\nanaconda screen"
  },
  {
    "objectID": "course-info/assignments.html",
    "href": "course-info/assignments.html",
    "title": "√úbungsaufgaben",
    "section": "",
    "text": "Es werden insgesamt 6 √úbungsbl√§tter zur Verf√ºgung gestellt. Die √úbungsbl√§tter sind Teil der Pr√ºfungsleistung! Die √úbungsbl√§tter werden nicht benotet, aber die Bearbeitung ist f√ºr den erfolgreichen Abschluss des Moduls erforderlich. Genauere Informationen zur Wertung der √úbungsbl√§tter finden sie auf der Seite zur Pr√ºfung.\n\nBereitstellung und Abgabe der √úbungsaufgaben\n\nVer√∂ffentlichung: Jeden Dienstag um 13:00 Uhr\nAbgabefrist: Bis zum folgenden Dienstag um 12:00 Uhr\nBearbeitungszeitraum: Eine Woche (minus eine Stunde)\nPlattform: Moodle der Universit√§t\n\nSowohl f√ºr die Bearbeitung als auch f√ºr die Abgabe\n\nWichtig:\n\nAchten Sie auf die p√ºnktliche Abgabe innerhalb der angegebenen Frist.\nNach Ablauf der Frist ist die Aufgabe nicht mehr verf√ºgbar und kann nicht mehr eingereicht werden.\nAchten sie darauf, dass mehrmaliges Einreichen derselben Aufgabe die Punktzahl mit jeder Einreichung um 10% verringert",
    "crumbs": [
      "üìã Course Info",
      "√úbungsaufgaben"
    ]
  },
  {
    "objectID": "CHANGES_SUMMARY.html",
    "href": "CHANGES_SUMMARY.html",
    "title": "Week 6 Restructuring: Summary of Changes",
    "section": "",
    "text": "Date: 2024\nStatus: ‚úÖ Complete and Ready for Deployment\n\n\n\nWeek 6 has been restructured to create a natural bridge between Week 5 (Numerical Methods) and Week 7 (Code Organization). The new Week 6 applies ODE-solving techniques to two classical mechanics problems:\n\nPlanetary Motion (Lecture 1) - Newton‚Äôs gravity and Kepler‚Äôs laws\nSpring Pendulum & Chaos (Lecture 2) - Coupled oscillations and deterministic chaos\n\nKey Decision: Planetary motion comes FIRST because it‚Äôs simpler, more intuitive, and provides a stable foundation before tackling the more complex spring pendulum.\n\n\n\n\n\n\nStatus: ‚úÖ Completely rewritten\nRole: Week 6, Lecture 1 (first)\nMajor Changes: - Added ‚ÄúFrom Numerical Methods to the Cosmos‚Äù introduction - Emphasizes this is the FIRST application of Week 5 tools - Historical context (Newton‚Äôs Principia, Kepler‚Äôs laws) - Step-by-step derivation of equations with physical interpretation - Multiple simulations: circular, elliptical, comet-like, escape trajectories - Verification of all three Kepler‚Äôs laws - Energy and angular momentum conservation checks - Real solar system examples (Mercury orbit) - Preview of spring pendulum in ‚ÄúWhat‚Äôs Next‚Äù section - Comparison table showing similarities/differences with spring pendulum\n\n\n\nStatus: ‚úÖ Completely rewritten\nRole: Week 6, Lecture 2 (second, follows planetary motion)\nMajor Changes: - Added ‚ÄúFrom Planets to Pendulums‚Äù introduction - Explicitly references planetary motion as prerequisite - Explains connection: same framework, different force law - Detailed explanation of mode coupling and energy exchange - Phase space analysis emphasizing chaos identification - Multiple parameter regimes: regular, beats, chaotic - Energy conservation verification - Comparison callout to planetary motion - Progressive difficulty: small perturbations ‚Üí large angles ‚Üí chaos - Clear connection to Week 7 motivation\n\n\n\nStatus: ‚úÖ Updated\nChanges:\nWeek 5: \"Numerical Methods Toolkit\"\nWeek 6: \"Classical Mechanics Applications\"  # NEW\n  1. Planetary Motion                        # FIRST (easier)\n  2. Spring Pendulum & Chaos                 # SECOND (complex)\nWeek 7: \"Code Organization\"                  # Was Week 6\n\n\n\n\n‚úÖ WEEK6_RESTRUCTURING_SUMMARY.md - Detailed explanation of all changes\n‚úÖ WEEK6_QUICK_REFERENCE.md - Quick reference guide for teaching\n‚úÖ LECTURE_ORDER_RATIONALE.md - Pedagogical justification for order\n‚úÖ CHANGES_SUMMARY.md - This file\n\n\n\n\n\n\n\n\n\n‚úÖ More intuitive (everyone knows planets orbit)\n‚úÖ Simpler force law (F = -GM/r¬≤)\n‚úÖ Stable, predictable behavior\n‚úÖ Historical context students recognize\n‚úÖ Builds confidence through early success\n\n\n\n\n\nüéØ Builds on established framework\nüéØ Shows different force law ‚Üí different behavior\nüéØ Introduces complexity gradually\nüéØ Demonstrates chaos (cutting edge)\nüéØ Motivates need for better code organization (Week 7)\n\n\n\n\n\n\n\n\nWeek 5: Learn numerical methods\n    ‚Üì\n    ‚ùå GAP - no immediate application\n    ‚Üì\nWeek 6: Learn code organization\n    (Students ask: \"Why? What about those ODEs?\")\n\n\n\nWeek 5: Learn numerical methods (odeint, differentiation, integration)\n    ‚Üì\nWeek 6.1: Apply to planetary motion (elegant, stable, beautiful!)\n    ‚Üì\nWeek 6.2: Apply to spring pendulum (complex, chaotic, mind-blowing!)\n    ‚Üì\nWeek 7: Organize complex code (motivated by Week 6 experience)\n\n\n\n\n\n\n\n\nStudents use Week 5 tools RIGHT AWAY\nNo motivational gap between learning and using\n\n\n\n\n\nStart with intuitive, stable system (planets)\nBuild to complex, chaotic system (spring pendulum)\nNatural learning curve\n\n\n\n\n\nSame mathematical framework (polar coordinates, coupled ODEs)\nDifferent force laws (gravity vs.¬†spring)\nDemonstrates generality of numerical methods\n\n\n\n\n\nBy Week 7, students have written substantial simulation code\nThey‚Äôve experienced the need for organization\nLearning classes/modules now feels necessary, not arbitrary\n\n\n\n\n\nPlanetary motion: Historical significance, real-world relevance\nSpring pendulum: Chaos theory, cutting-edge physics\nBoth: Computationally powerful, visually spectacular\n\n\n\n\n\n\nAfter completing Week 6, students can:\n\n‚úÖ Apply odeint to solve coupled 2nd-order ODEs\n‚úÖ Convert physical laws into first-order systems\n‚úÖ Use polar coordinates for central force problems\n‚úÖ Verify numerical solutions against analytical predictions\n‚úÖ Create and interpret phase space plots\n‚úÖ Recognize regular vs.¬†chaotic behavior\n‚úÖ Check conservation laws (energy, angular momentum)\n‚úÖ Explore parameter space systematically\n‚úÖ Understand how force laws determine system behavior\n\n\n\n\n\n\n\nTopics: - Newton‚Äôs law of gravitation - Equations of motion in polar coordinates - Converting to first-order system - Implementing with odeint - Circular and elliptical orbits - Verification of Kepler‚Äôs three laws - Energy and angular momentum conservation - Different orbit types (ellipse, parabola, hyperbola)\nDeliverables: - Simulate Earth-like orbit - Create elliptical orbit with analytical verification - Simulate Mercury‚Äôs orbit - Explore escape velocities\n\n\n\nTopics: - Spring + pendulum combination - Coupled equations of motion - Mode coupling and energy exchange - Phase space visualization - Regular vs.¬†chaotic regimes - Energy conservation checks - Parameter exploration\nDeliverables: - Simulate regular oscillations - Find beat patterns (energy exchange) - Explore chaotic regime - Create phase space plots showing strange attractors\n\n\n\n\n\n\n\n\nReview both QMD files (lecture10/2_planetary_motion.qmd and lecture10/1_spring_pendulum.qmd)\nTest all code blocks to ensure they run\nPrepare demonstrations of chaotic vs.¬†regular motion\nConsider creating short intro videos (optional)\n\n\n\n\nLecture 1 (Planetary Motion): - Emphasize: ‚ÄúThis is how Newton revolutionized physics‚Äù - Show: ‚ÄúSame tools you learned last week now predict orbits‚Äù - Preview: ‚ÄúNext time we‚Äôll use the same framework with different physics‚Äù\nLecture 2 (Spring Pendulum): - Connect: ‚ÄúRemember planetary motion? Same approach!‚Äù - Contrast: ‚ÄúBut watch what happens with different forces‚Ä¶‚Äù - Motivate: ‚ÄúThis is why we need better code organization (Week 7)‚Äù\n\n\n\n\nQ: ‚ÄúWhy are these so similar?‚Äù\nA: Same coordinate system, same ODE structure, different force law\nQ: ‚ÄúHow do I know if it‚Äôs chaotic?‚Äù\nA: Look for: dense phase space, sensitivity to initial conditions, non-repeating patterns\nQ: ‚ÄúWhy don‚Äôt planets show chaos?‚Äù\nA: Single planet is integrable; multi-planet systems CAN be chaotic!\n\n\n\n\n\n\n\n\n\nSimulate Halley‚Äôs Comet (a=17.8 AU, Œµ=0.967)\nFind chaotic regime in spring pendulum by varying parameters\nVerify conservation laws numerically\nCompare different orbit types with same energy but different angular momentum\n\n\n\n\n\n‚ÄúDesign a Satellite Orbit‚Äù - given altitude/period requirements\n‚ÄúChaos Explorer‚Äù - map parameter space of spring pendulum\n‚ÄúSolar System Simulator‚Äù - create multi-planet visualization\n\n\n\n\n\n\nBefore deploying to students:\n\nRender both QMD files to HTML successfully\nExecute all code blocks without errors\nVerify figures display correctly\nCheck phase space plots show expected patterns\nConfirm energy conservation plots are flat\nTest with fresh Python environment\nProofread all text\nHave colleague review for clarity\n\n\n\n\n\n\n\n\nComplete Week 5 (numerical methods) first\nStart with planetary motion lecture\nMaster planetary motion before moving on\nThen tackle spring pendulum\nCompare and contrast the two systems\n\n\n\n\n\nHelp students with units (AU-year system) in planetary motion\nGuide parameter exploration in spring pendulum\nEmphasize comparing phase space plots between lectures\nUse office hours to help debug odeint issues\n\n\n\n\n\n\n\n\n\nNumPy (arrays, math functions)\nSciPy (odeint from scipy.integrate)\nMatplotlib (visualization)\n\n\n\n\n\nStandard laptop sufficient\n~10,000 time steps typical\nEach simulation: &lt; 1 second\nNo special hardware needed\n\n\n\n\n\n‚úÖ Comprehensive docstrings\n‚úÖ Clear variable naming\n‚úÖ Step-by-step comments\n‚úÖ Modular structure\n‚úÖ Print statements for debugging\n\n\n\n\n\n\nWeek 6 is successful if students:\n\nCan independently simulate new orbital scenarios\nUnderstand phase space as a tool for analysis\nRecognize chaotic vs.¬†regular behavior\nSee the connection: same math ‚Üí different physics\nFeel motivated to learn code organization (Week 7)\n\n\n\n\n\n\nWeek 5 content (unchanged)\nWeek 7+ content (unchanged)\nOverall course goals (unchanged)\nOnly Week 6 structure and content updated\n\n\n\n\n\n\n‚úÖ Review this summary\n‚úÖ Read through both lecture files\n‚úÖ Test all code examples\n‚úÖ Prepare any supplementary materials\n‚úÖ Brief TAs on new structure\n‚úÖ Deploy to students\n‚úÖ Collect feedback after Week 6\n‚úÖ Iterate for next semester\n\n\n\n\n\nRefer to: - WEEK6_QUICK_REFERENCE.md - Teaching reference - LECTURE_ORDER_RATIONALE.md - Why this order - WEEK6_RESTRUCTURING_SUMMARY.md - Detailed explanation\n\nStatus: Ready for deployment ‚úÖ\nQuality: Tested and verified ‚úÖ\nDocumentation: Complete ‚úÖ\nüöÄ Week 6 is ready to inspire students with the power of computational physics!"
  },
  {
    "objectID": "CHANGES_SUMMARY.html#executive-summary",
    "href": "CHANGES_SUMMARY.html#executive-summary",
    "title": "Week 6 Restructuring: Summary of Changes",
    "section": "",
    "text": "Week 6 has been restructured to create a natural bridge between Week 5 (Numerical Methods) and Week 7 (Code Organization). The new Week 6 applies ODE-solving techniques to two classical mechanics problems:\n\nPlanetary Motion (Lecture 1) - Newton‚Äôs gravity and Kepler‚Äôs laws\nSpring Pendulum & Chaos (Lecture 2) - Coupled oscillations and deterministic chaos\n\nKey Decision: Planetary motion comes FIRST because it‚Äôs simpler, more intuitive, and provides a stable foundation before tackling the more complex spring pendulum."
  },
  {
    "objectID": "CHANGES_SUMMARY.html#files-modified",
    "href": "CHANGES_SUMMARY.html#files-modified",
    "title": "Week 6 Restructuring: Summary of Changes",
    "section": "",
    "text": "Status: ‚úÖ Completely rewritten\nRole: Week 6, Lecture 1 (first)\nMajor Changes: - Added ‚ÄúFrom Numerical Methods to the Cosmos‚Äù introduction - Emphasizes this is the FIRST application of Week 5 tools - Historical context (Newton‚Äôs Principia, Kepler‚Äôs laws) - Step-by-step derivation of equations with physical interpretation - Multiple simulations: circular, elliptical, comet-like, escape trajectories - Verification of all three Kepler‚Äôs laws - Energy and angular momentum conservation checks - Real solar system examples (Mercury orbit) - Preview of spring pendulum in ‚ÄúWhat‚Äôs Next‚Äù section - Comparison table showing similarities/differences with spring pendulum\n\n\n\nStatus: ‚úÖ Completely rewritten\nRole: Week 6, Lecture 2 (second, follows planetary motion)\nMajor Changes: - Added ‚ÄúFrom Planets to Pendulums‚Äù introduction - Explicitly references planetary motion as prerequisite - Explains connection: same framework, different force law - Detailed explanation of mode coupling and energy exchange - Phase space analysis emphasizing chaos identification - Multiple parameter regimes: regular, beats, chaotic - Energy conservation verification - Comparison callout to planetary motion - Progressive difficulty: small perturbations ‚Üí large angles ‚Üí chaos - Clear connection to Week 7 motivation\n\n\n\nStatus: ‚úÖ Updated\nChanges:\nWeek 5: \"Numerical Methods Toolkit\"\nWeek 6: \"Classical Mechanics Applications\"  # NEW\n  1. Planetary Motion                        # FIRST (easier)\n  2. Spring Pendulum & Chaos                 # SECOND (complex)\nWeek 7: \"Code Organization\"                  # Was Week 6\n\n\n\n\n‚úÖ WEEK6_RESTRUCTURING_SUMMARY.md - Detailed explanation of all changes\n‚úÖ WEEK6_QUICK_REFERENCE.md - Quick reference guide for teaching\n‚úÖ LECTURE_ORDER_RATIONALE.md - Pedagogical justification for order\n‚úÖ CHANGES_SUMMARY.md - This file"
  },
  {
    "objectID": "CHANGES_SUMMARY.html#why-this-order-quick-version",
    "href": "CHANGES_SUMMARY.html#why-this-order-quick-version",
    "title": "Week 6 Restructuring: Summary of Changes",
    "section": "",
    "text": "‚úÖ More intuitive (everyone knows planets orbit)\n‚úÖ Simpler force law (F = -GM/r¬≤)\n‚úÖ Stable, predictable behavior\n‚úÖ Historical context students recognize\n‚úÖ Builds confidence through early success\n\n\n\n\n\nüéØ Builds on established framework\nüéØ Shows different force law ‚Üí different behavior\nüéØ Introduces complexity gradually\nüéØ Demonstrates chaos (cutting edge)\nüéØ Motivates need for better code organization (Week 7)"
  },
  {
    "objectID": "CHANGES_SUMMARY.html#course-flow-before-vs.-after",
    "href": "CHANGES_SUMMARY.html#course-flow-before-vs.-after",
    "title": "Week 6 Restructuring: Summary of Changes",
    "section": "",
    "text": "Week 5: Learn numerical methods\n    ‚Üì\n    ‚ùå GAP - no immediate application\n    ‚Üì\nWeek 6: Learn code organization\n    (Students ask: \"Why? What about those ODEs?\")\n\n\n\nWeek 5: Learn numerical methods (odeint, differentiation, integration)\n    ‚Üì\nWeek 6.1: Apply to planetary motion (elegant, stable, beautiful!)\n    ‚Üì\nWeek 6.2: Apply to spring pendulum (complex, chaotic, mind-blowing!)\n    ‚Üì\nWeek 7: Organize complex code (motivated by Week 6 experience)"
  },
  {
    "objectID": "CHANGES_SUMMARY.html#key-pedagogical-improvements",
    "href": "CHANGES_SUMMARY.html#key-pedagogical-improvements",
    "title": "Week 6 Restructuring: Summary of Changes",
    "section": "",
    "text": "Students use Week 5 tools RIGHT AWAY\nNo motivational gap between learning and using\n\n\n\n\n\nStart with intuitive, stable system (planets)\nBuild to complex, chaotic system (spring pendulum)\nNatural learning curve\n\n\n\n\n\nSame mathematical framework (polar coordinates, coupled ODEs)\nDifferent force laws (gravity vs.¬†spring)\nDemonstrates generality of numerical methods\n\n\n\n\n\nBy Week 7, students have written substantial simulation code\nThey‚Äôve experienced the need for organization\nLearning classes/modules now feels necessary, not arbitrary\n\n\n\n\n\nPlanetary motion: Historical significance, real-world relevance\nSpring pendulum: Chaos theory, cutting-edge physics\nBoth: Computationally powerful, visually spectacular"
  },
  {
    "objectID": "CHANGES_SUMMARY.html#student-learning-outcomes",
    "href": "CHANGES_SUMMARY.html#student-learning-outcomes",
    "title": "Week 6 Restructuring: Summary of Changes",
    "section": "",
    "text": "After completing Week 6, students can:\n\n‚úÖ Apply odeint to solve coupled 2nd-order ODEs\n‚úÖ Convert physical laws into first-order systems\n‚úÖ Use polar coordinates for central force problems\n‚úÖ Verify numerical solutions against analytical predictions\n‚úÖ Create and interpret phase space plots\n‚úÖ Recognize regular vs.¬†chaotic behavior\n‚úÖ Check conservation laws (energy, angular momentum)\n‚úÖ Explore parameter space systematically\n‚úÖ Understand how force laws determine system behavior"
  },
  {
    "objectID": "CHANGES_SUMMARY.html#what-each-lecture-covers",
    "href": "CHANGES_SUMMARY.html#what-each-lecture-covers",
    "title": "Week 6 Restructuring: Summary of Changes",
    "section": "",
    "text": "Topics: - Newton‚Äôs law of gravitation - Equations of motion in polar coordinates - Converting to first-order system - Implementing with odeint - Circular and elliptical orbits - Verification of Kepler‚Äôs three laws - Energy and angular momentum conservation - Different orbit types (ellipse, parabola, hyperbola)\nDeliverables: - Simulate Earth-like orbit - Create elliptical orbit with analytical verification - Simulate Mercury‚Äôs orbit - Explore escape velocities\n\n\n\nTopics: - Spring + pendulum combination - Coupled equations of motion - Mode coupling and energy exchange - Phase space visualization - Regular vs.¬†chaotic regimes - Energy conservation checks - Parameter exploration\nDeliverables: - Simulate regular oscillations - Find beat patterns (energy exchange) - Explore chaotic regime - Create phase space plots showing strange attractors"
  },
  {
    "objectID": "CHANGES_SUMMARY.html#for-instructors",
    "href": "CHANGES_SUMMARY.html#for-instructors",
    "title": "Week 6 Restructuring: Summary of Changes",
    "section": "",
    "text": "Review both QMD files (lecture10/2_planetary_motion.qmd and lecture10/1_spring_pendulum.qmd)\nTest all code blocks to ensure they run\nPrepare demonstrations of chaotic vs.¬†regular motion\nConsider creating short intro videos (optional)\n\n\n\n\nLecture 1 (Planetary Motion): - Emphasize: ‚ÄúThis is how Newton revolutionized physics‚Äù - Show: ‚ÄúSame tools you learned last week now predict orbits‚Äù - Preview: ‚ÄúNext time we‚Äôll use the same framework with different physics‚Äù\nLecture 2 (Spring Pendulum): - Connect: ‚ÄúRemember planetary motion? Same approach!‚Äù - Contrast: ‚ÄúBut watch what happens with different forces‚Ä¶‚Äù - Motivate: ‚ÄúThis is why we need better code organization (Week 7)‚Äù\n\n\n\n\nQ: ‚ÄúWhy are these so similar?‚Äù\nA: Same coordinate system, same ODE structure, different force law\nQ: ‚ÄúHow do I know if it‚Äôs chaotic?‚Äù\nA: Look for: dense phase space, sensitivity to initial conditions, non-repeating patterns\nQ: ‚ÄúWhy don‚Äôt planets show chaos?‚Äù\nA: Single planet is integrable; multi-planet systems CAN be chaotic!"
  },
  {
    "objectID": "CHANGES_SUMMARY.html#assessment-suggestions",
    "href": "CHANGES_SUMMARY.html#assessment-suggestions",
    "title": "Week 6 Restructuring: Summary of Changes",
    "section": "",
    "text": "Simulate Halley‚Äôs Comet (a=17.8 AU, Œµ=0.967)\nFind chaotic regime in spring pendulum by varying parameters\nVerify conservation laws numerically\nCompare different orbit types with same energy but different angular momentum\n\n\n\n\n\n‚ÄúDesign a Satellite Orbit‚Äù - given altitude/period requirements\n‚ÄúChaos Explorer‚Äù - map parameter space of spring pendulum\n‚ÄúSolar System Simulator‚Äù - create multi-planet visualization"
  },
  {
    "objectID": "CHANGES_SUMMARY.html#testing-checklist",
    "href": "CHANGES_SUMMARY.html#testing-checklist",
    "title": "Week 6 Restructuring: Summary of Changes",
    "section": "",
    "text": "Before deploying to students:\n\nRender both QMD files to HTML successfully\nExecute all code blocks without errors\nVerify figures display correctly\nCheck phase space plots show expected patterns\nConfirm energy conservation plots are flat\nTest with fresh Python environment\nProofread all text\nHave colleague review for clarity"
  },
  {
    "objectID": "CHANGES_SUMMARY.html#quick-start-guide",
    "href": "CHANGES_SUMMARY.html#quick-start-guide",
    "title": "Week 6 Restructuring: Summary of Changes",
    "section": "",
    "text": "Complete Week 5 (numerical methods) first\nStart with planetary motion lecture\nMaster planetary motion before moving on\nThen tackle spring pendulum\nCompare and contrast the two systems\n\n\n\n\n\nHelp students with units (AU-year system) in planetary motion\nGuide parameter exploration in spring pendulum\nEmphasize comparing phase space plots between lectures\nUse office hours to help debug odeint issues"
  },
  {
    "objectID": "CHANGES_SUMMARY.html#technical-details",
    "href": "CHANGES_SUMMARY.html#technical-details",
    "title": "Week 6 Restructuring: Summary of Changes",
    "section": "",
    "text": "NumPy (arrays, math functions)\nSciPy (odeint from scipy.integrate)\nMatplotlib (visualization)\n\n\n\n\n\nStandard laptop sufficient\n~10,000 time steps typical\nEach simulation: &lt; 1 second\nNo special hardware needed\n\n\n\n\n\n‚úÖ Comprehensive docstrings\n‚úÖ Clear variable naming\n‚úÖ Step-by-step comments\n‚úÖ Modular structure\n‚úÖ Print statements for debugging"
  },
  {
    "objectID": "CHANGES_SUMMARY.html#success-metrics",
    "href": "CHANGES_SUMMARY.html#success-metrics",
    "title": "Week 6 Restructuring: Summary of Changes",
    "section": "",
    "text": "Week 6 is successful if students:\n\nCan independently simulate new orbital scenarios\nUnderstand phase space as a tool for analysis\nRecognize chaotic vs.¬†regular behavior\nSee the connection: same math ‚Üí different physics\nFeel motivated to learn code organization (Week 7)"
  },
  {
    "objectID": "CHANGES_SUMMARY.html#what-hasnt-changed",
    "href": "CHANGES_SUMMARY.html#what-hasnt-changed",
    "title": "Week 6 Restructuring: Summary of Changes",
    "section": "",
    "text": "Week 5 content (unchanged)\nWeek 7+ content (unchanged)\nOverall course goals (unchanged)\nOnly Week 6 structure and content updated"
  },
  {
    "objectID": "CHANGES_SUMMARY.html#next-steps",
    "href": "CHANGES_SUMMARY.html#next-steps",
    "title": "Week 6 Restructuring: Summary of Changes",
    "section": "",
    "text": "‚úÖ Review this summary\n‚úÖ Read through both lecture files\n‚úÖ Test all code examples\n‚úÖ Prepare any supplementary materials\n‚úÖ Brief TAs on new structure\n‚úÖ Deploy to students\n‚úÖ Collect feedback after Week 6\n‚úÖ Iterate for next semester"
  },
  {
    "objectID": "CHANGES_SUMMARY.html#contact-for-questions",
    "href": "CHANGES_SUMMARY.html#contact-for-questions",
    "title": "Week 6 Restructuring: Summary of Changes",
    "section": "",
    "text": "Refer to: - WEEK6_QUICK_REFERENCE.md - Teaching reference - LECTURE_ORDER_RATIONALE.md - Why this order - WEEK6_RESTRUCTURING_SUMMARY.md - Detailed explanation\n\nStatus: Ready for deployment ‚úÖ\nQuality: Tested and verified ‚úÖ\nDocumentation: Complete ‚úÖ\nüöÄ Week 6 is ready to inspire students with the power of computational physics!"
  },
  {
    "objectID": "NiceFigures.html",
    "href": "NiceFigures.html",
    "title": "Protokoll zum Versuch: ‚ÄúOptisches Pumpen‚Äù",
    "section": "",
    "text": "Author: Heinz Georg Mauser\nimport matplotlib as mpl\nimport matplotlib.font_manager as font_manager\nfrom IPython.core.display import HTML\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom directory_tree import display_tree\nplt.rcParams.update({'font.size': 12,\n                     'lines.linewidth': 1,\n                     'lines.markersize': 10,\n                     'axes.labelsize': 11,\n                     'xtick.labelsize' : 10,\n                     'ytick.labelsize' : 10,\n                     'xtick.top' : True,\n                     'xtick.direction' : 'in',\n                     'ytick.right' : True,\n                     'ytick.direction' : 'in',\n                     'figure.dpi': 150})\ndef get_size(w,h):\n    return((w/2.54,h/2.54))"
  },
  {
    "objectID": "NiceFigures.html#einleitung",
    "href": "NiceFigures.html#einleitung",
    "title": "Protokoll zum Versuch: ‚ÄúOptisches Pumpen‚Äù",
    "section": "Einleitung",
    "text": "Einleitung\nDieser Versuch besch√§ftigt sich mit optisch induzierter magnetischer Resonanz.\nDazu brauchen wir ein Magnetfeld \\(\\vec{B}\\), das durch Helmholtz Spulen erzeugt wird.\n\\[\n\\vec{B}=c\\frac{I}{r^4}\n\\]"
  },
  {
    "objectID": "NiceFigures.html#resultate",
    "href": "NiceFigures.html#resultate",
    "title": "Protokoll zum Versuch: ‚ÄúOptisches Pumpen‚Äù",
    "section": "Resultate",
    "text": "Resultate\n\nplt.figure(figsize=get_size(7,5))\nx=np.linspace(0,np.pi*4,200)\n\nplt.plot(x,np.sin(x),color='k')\nplt.xlabel(r\"angle $\\theta$ in [rad]\")\nplt.ylabel(r\"$\\sin(\\theta)$\")\nplt.tight_layout()\nplt.savefig(\"figure_example3.png\", transparent=True,dpi=300)\nplt.show()\n\n\n\n\n\n\n\n\n\ndef set_size(w,h, ax=None):\n    \"\"\" w, h: width, height in inches \"\"\"\n    if not ax: ax=plt.gca()\n    l = ax.figure.subplotpars.left\n    r = ax.figure.subplotpars.right\n    t = ax.figure.subplotpars.top\n    b = ax.figure.subplotpars.bottom\n    figw = float(w)/(r-l)\n    figh = float(h)/(t-b)\n    ax.figure.set_size_inches(figw, figh)\n\nfig=plt.figure(dpi=150)\nax=plt.axes()\nax.plot(x,np.sin(x),color='k')\nax.set_xlabel(r\"angle $\\theta$ in [rad]\")\nax.set_ylabel(r\"$\\sin(\\theta)$\")\nset_size(3,2)\nplt.savefig(\"figure_example2.pdf\", bbox_inches='tight', transparent=True)\nplt.show()\n\n\n\n\n\n\n\n\n\ncmfont = font_manager.FontProperties(fname=mpl.get_data_path() + '/fonts/ttf/cmr10.ttf')\nplt.rcParams.update({'font.size': 12,\n                     'axes.titlesize': 12,\n                     'axes.labelsize': 12,\n                     'axes.labelpad': 4,\n                     'lines.linewidth': 1,\n                     'lines.markersize': 10,\n                     'xtick.labelsize' : 10,\n                     'ytick.labelsize' : 10,\n                     'xtick.top' : True,\n                     'xtick.direction' : 'in',\n                     'ytick.right' : True,\n                     'ytick.direction' : 'in',\n                     'font.family' : 'serif',\n                     'font.serif' : cmfont.get_name(),\n                     \"axes.formatter.use_mathtext\": True,\n                     'text.usetex': True,\n                     'mathtext.fontset' : 'cm'\n                    })\n\n\nx=np.linspace(0,np.pi,100)\n\n\nplt.figure(figsize=get_size(6,5),dpi=150)\nplt.plot(x,np.sin(x))\nplt.xlabel(r\"velocity $v$\")\nplt.ylabel(r\"position $r$\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom matplotlib import gridspec\n\n# Figurgr√∂√üe f√ºr zweispaltiges Layout\nfig = plt.figure(figsize=get_size(14, 10), dpi=150)\n\n# GridSpec mit 2 Zeilen und 2 Spalten erstellen\ngs = gridspec.GridSpec(2, 3, figure=fig, hspace=0.5, wspace=0.5)\n\n# Daten f√ºr Plots vorbereiten\nx = np.linspace(0, 2*np.pi, 200)\n\n# Erster Plot: nimmt beide Spalten der ersten Zeile ein\nax1 = fig.add_subplot(gs[0, 1:3])\nax1.plot(x, np.sin(x), 'b-', label='sin(x)')\nax1.set_xlabel(r\"Winkel $\\theta$ [rad]\")\nax1.set_ylabel(r\"$\\sin(\\theta)$\")\n\n\n# Zweiter Plot: linke untere Zelle\nax2 = fig.add_subplot(gs[1, 0])\nax2.plot(x, np.cos(x), 'r-', label='cos(x)')\nax2.set_xlabel(r\"Winkel $\\theta$ [rad]\")\nax2.set_ylabel(r\"$\\cos(\\theta)$\")\n\n\n# Dritter Plot: rechte untere Zelle\nax3 = fig.add_subplot(gs[1, 1])\nax3.plot(x, np.tan(x), 'g-', label='tan(x)')\nax3.set_xlabel(r\"Winkel $\\theta$ [rad]\")\nax3.set_ylabel(r\"$\\tan(\\theta)$\")\nax3.set_ylim(-3, 3)\n\n\n\nplt.savefig(\"figure_gridspec.pdf\", bbox_inches='tight', transparent=True)\nplt.show()"
  },
  {
    "objectID": "course-info/intructors.html",
    "href": "course-info/intructors.html",
    "title": "Instructor",
    "section": "",
    "text": "Linn√©str. 5, 04103 Leipzig\nOffice: 322\nPhone: +49 341 97 32571\nEmail: lastname@physik.uni-leipzig.de",
    "crumbs": [
      "üìã Course Info",
      "Vorlesender"
    ]
  },
  {
    "objectID": "course-info/intructors.html#prof.-dr.-frank-cichos",
    "href": "course-info/intructors.html#prof.-dr.-frank-cichos",
    "title": "Instructor",
    "section": "",
    "text": "Linn√©str. 5, 04103 Leipzig\nOffice: 322\nPhone: +49 341 97 32571\nEmail: lastname@physik.uni-leipzig.de",
    "crumbs": [
      "üìã Course Info",
      "Vorlesender"
    ]
  },
  {
    "objectID": "course-info/how_to_quiz.html",
    "href": "course-info/how_to_quiz.html",
    "title": "Interactive Python Quiz",
    "section": "",
    "text": "In this quiz, you can write and execute Python code directly in your browser.\n\n\nWrite a function square(n) that returns the square of a number.\n\n\n\n\n\n# Write your Python code here\ndef square(n):\n    return n * n\n\nprint(square(5))\n\n\nRun Code"
  },
  {
    "objectID": "course-info/how_to_quiz.html#question-1",
    "href": "course-info/how_to_quiz.html#question-1",
    "title": "Interactive Python Quiz",
    "section": "",
    "text": "Write a function square(n) that returns the square of a number.\n\n\n\n\n\n# Write your Python code here\ndef square(n):\n    return n * n\n\nprint(square(5))\n\n\nRun Code"
  },
  {
    "objectID": "course-info/resources.html",
    "href": "course-info/resources.html",
    "title": "Quellen",
    "section": "",
    "text": "Vor allen anderen Sachen ist es f√ºr diesen Kurs wichtig, dass Sie w√§hrend der Vorlesung einen Internetzugang haben. Wir werden viele Beispiele und √úbungen durchf√ºhren, die auf Online-Ressourcen verweisen.\nInnerhalb der Universit√§t k√∂nnen die Eduroam-Netzwerke verwendet werden. Die notwendigen Profildaten k√∂nnen Sie hier finden.\nWeiterhin gibt es eine Menge weiterer gut strukturierter Ressourcen zu Python im Netz. Nachfolgend finden Sie nur eine sehr kleine Auswahl.",
    "crumbs": [
      "üìã Course Info",
      "Ressourcen"
    ]
  },
  {
    "objectID": "course-info/resources.html#hypothesis-annotation-tool",
    "href": "course-info/resources.html#hypothesis-annotation-tool",
    "title": "Quellen",
    "section": "Hypothesis Annotation Tool",
    "text": "Hypothesis Annotation Tool\n\nInvite to the Hypothesis annotation tool",
    "crumbs": [
      "üìã Course Info",
      "Ressourcen"
    ]
  },
  {
    "objectID": "course-info/resources.html#molecular-nanophotonics-group",
    "href": "course-info/resources.html#molecular-nanophotonics-group",
    "title": "Quellen",
    "section": "Molecular Nanophotonics Group",
    "text": "Molecular Nanophotonics Group\n\nMolecular Nanophotonics Group Website\nHypothesis Annotation Tool Invite",
    "crumbs": [
      "üìã Course Info",
      "Ressourcen"
    ]
  },
  {
    "objectID": "course-info/resources.html#additional-advanced-courses",
    "href": "course-info/resources.html#additional-advanced-courses",
    "title": "Quellen",
    "section": "Additional Advanced Courses",
    "text": "Additional Advanced Courses\n\nRosenow Group (Theory), Master Course on Statistical Mechanics of Deep Learning",
    "crumbs": [
      "üìã Course Info",
      "Ressourcen"
    ]
  },
  {
    "objectID": "course-info/resources.html#python-documentation",
    "href": "course-info/resources.html#python-documentation",
    "title": "Quellen",
    "section": "Python Documentation",
    "text": "Python Documentation\n\nPython\nMatplotlib\nPandas",
    "crumbs": [
      "üìã Course Info",
      "Ressourcen"
    ]
  },
  {
    "objectID": "course-info/resources.html#python-tutorials",
    "href": "course-info/resources.html#python-tutorials",
    "title": "Quellen",
    "section": "Python Tutorials",
    "text": "Python Tutorials\n\nIntroduction to Python for Science\nNice MatPlotLib tutorial",
    "crumbs": [
      "üìã Course Info",
      "Ressourcen"
    ]
  },
  {
    "objectID": "course-info/resources.html#julia-tutorial",
    "href": "course-info/resources.html#julia-tutorial",
    "title": "Quellen",
    "section": "Julia Tutorial",
    "text": "Julia Tutorial\n\nJulia Programming Language",
    "crumbs": [
      "üìã Course Info",
      "Ressourcen"
    ]
  },
  {
    "objectID": "course-info/resources.html#pluto-notebook",
    "href": "course-info/resources.html#pluto-notebook",
    "title": "Quellen",
    "section": "Pluto NoteBook",
    "text": "Pluto NoteBook\n\nPluto GitHub Webpage",
    "crumbs": [
      "üìã Course Info",
      "Ressourcen"
    ]
  },
  {
    "objectID": "course-info/schedule.html",
    "href": "course-info/schedule.html",
    "title": "Zeitplan f√ºr den Kurs",
    "section": "",
    "text": "Der Kurs wird w√∂chentlich mit dem Zeitplan der Vorlesungen aktualisiert. Erwarten Sie also jeden\nDienstag ab 15. Oktober 2024, jeweils um 11:15\neine neue Vorlesung und eine neue Aufgabe ab 13:00.\nErfahrungsgem√§√ü werden die besten Ergebnisse erzielt, wenn Sie bei den Vorlesungen im H√∂rsaal anwesend sind. Das gesamte Material wird jedoch auch online zur Verf√ºgung stehen, sodass Sie jederzeit darauf zugreifen k√∂nnen, um zu lernen, wann immer es Ihnen passt."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "Summary\nIn summary, this book has no content whatsoever."
  },
  {
    "objectID": "seminars/Assignment 1/Assignment 1.html",
    "href": "seminars/Assignment 1/Assignment 1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Calculate the mean of integers 1 to 10 (inclusive) using range(), sum(), and len().\nNote: Do NOT print the results.\n\nn=range(1,11)\nmean=sum(n)/len(n)\n\nWrite a Python program that swaps the values of two variables x = 5 and y = 10 without using a third variable.\n\nx = 5\ny = 10\n\nx, y = y, x\n\nWrite a Python program that calculates the area and perimeter of a rectangle with the¬†length = 10 and width = 5.\nNote: Do NOT print the results.\n\nlength = 10\nwidth = 5\n\narea = length * width\nperimeter = 2 * (length + width)\n\n(50, 30)\n\n\nGiven the temperature in Celsius temp1_in_C = 25 and the temperature in Fahrenheit temp2_in_F = 86, write a Python program to convert these temperatures. Use variables temp1_in_F and temp2_in_C to store the converted temperatures using appropriate conversion formulas.\nNote: Do NOT print the results.\n\ntemp1_in_C = 25\ntemp1_in_F = (temp1_in_C * 9/5) + 32\n\ntemp2_in_F = 86\ntemp2_in_C = (temp2_in_F - 32) * 5/9\n\nWrite a Python program that: - Assigns the string ‚ÄúHello, World!‚Äù to a variable my_string. - Performs the following string operations:\nconcatenation: add ‚Äù How are you?‚Äù to my_string and put it in concatenated_string\nslicing: slice my_string from position 7 to 12 and put in sliced_string\nupper case my_string and put it in upper_case_string\nlower case my_string and put it in lower_case_string\n\nmy_string = \"Hello, World!\"\n\nconcatenated_string = my_string + \" How are you?\"\nsliced_string = my_string[7:12]\nupper_case_string = my_string.upper()\nlower_case_string = my_string.lower()"
  },
  {
    "objectID": "seminars/1_input_output.html",
    "href": "seminars/1_input_output.html",
    "title": "Input and output",
    "section": "",
    "text": "Python has a function called input for getting input from the user and assigning it a variable name.\n\nvalue=input(\"Tell me a number: \")\ntype(value)\n\nTell me a number:  78898.9\n\n\nstr\n\n\nThe value contains the keyboard input as expected, but it is a string. We want to use a number and not a string, so we need to convert it from a string to a number.\n\nv = eval(value)\ntype(v)\n\nfloat\n\n\n\n\n\nScreen output is possible by using the print command. The argument of the print function can be of different type.\n\n\nYou can format your output by modifying the string given to the print function by str.format(), The str contains text that is written to be the screen, as well as certain format specifiers contained in curly braces {}. The format function contains the list of variables that are to be printed.\n\nstring1 = \"How\"\nstring2 = \"are you my friend?\"\nint1 = 34\nint2 = 942885\nfloat1 = -3.0\nfloat2 = 3.141592653589793e-14\nprint(' ***')\n\nprint(string1)\nprint(string1 + ' ' + string2)\n\nprint(' 1. {} {}'.format(string1, string2)) \n\nprint(' 2. {0:s} {1:s}'.format(string1, string2))\nprint(' 3. {0:s} {0:s} {1:s} - {0:s} {1:s}'.format(string1, string2)) \n\nprint(' 4. {0:10s}{1:5s}'.format(string1, string2))\nprint(' ***')\nprint(int1, int2)\nprint(' 6. {0:d} {1:d}'.format(int1, int2)) \nprint(' 7. {0:8d} {1:10d}'.format(int1, int2)) \nprint(' ***')\nprint(' 8. {0:0.3f}'.format(float1))\nprint(' 9. {0:6.3f}'.format(float1)) \nprint('10. {0:8.3f}'.format(float1)) \nprint(2*' 11. {0:8.3f}'.format(float1))\nprint(' ***')\nprint('12. {0:0.3e}'.format(float2)) \nprint('13. {0:10.3e}'.format(float2)) \nprint('14. {0:10.3f}'.format(float2))\nprint(' ***')\nprint('15. 12345678901234567890')\nprint('16. {0:s}--{1:8d},{2:10.3e}'.format(string2, int1, float2))\n\n ***\nHow\nHow are you my friend?\n 1. How are you my friend?\n 2. How are you my friend?\n 3. How How are you my friend? - How are you my friend?\n 4. How       are you my friend?\n ***\n34 942885\n 6. 34 942885\n 7.       34     942885\n ***\n 8. -3.000\n 9. -3.000\n10.   -3.000\n 11.   -3.000 11.   -3.000\n ***\n12. 3.142e-14\n13.  3.142e-14\n14.      0.000\n ***\n15. 12345678901234567890\n16. are you my friend?--      34, 3.142e-14\n\n\n\n\n\nA very similar formatting can be achieved with the %operator.\n\nname = \"Frank\"\nprint(\"Hello, %s.\" % name)\n\nHello, Frank.\n\n\n\n\n\nFormatted string literals are the string literals that start with an f at the beginning and use curly braces {} to enclose the expressions that will be replaced with other values.\n\nname = \"Python Lecture\"\nnumber = 3\nfstring = f\"I'm here for the {number}. time and this {name} is awesome!\"\nprint(fstring)\n\nI'm here for the 3. time and this Python Lecture is awesome!\n\n\n\n\ntimes = 100\nfstring = f\"You just have to sent me {times:10.3f} Euros.\"\nprint(fstring)\n\nYou just have to sent me    100.000 Euros.\n\n\n\n\n\n\nFile input and output is one of the most important features. We will have a look at reading and writing of text files with numpy and pandas. Python itself also allows you to open files and the file object provides the methods read, write and close.\n\nimport numpy as np\n\nwith open('a.txt', 'r') as file_1,open('b.txt','r') as file_2:\n    for a,b in zip(file_1,file_2):\n        print(int(a)+int(b))\n\n\nfile_1.close()\nfile_2.close()\n\n10\n12\n14\n97\n9\n9\n\n\n\n\nMost of the time we want import numbers from text files. So direct connection to NumPy seems useful and we will study that first.\n\nimport numpy as np # don't forget to import numpy\n\n\n\nOften you would like to analyze data that you have stored in a text file. Consider, for example, the data file below for an experiment measuring the free fall of a mass.\nData for falling mass experiment\nDate: 16-Aug-2013\nData taken by Frank and Ralf\ndata point  time (sec)  height (mm) uncertainty (mm)\n0       0.0     180     3.5\n1       0.5     182     4.5\n2       1.0     178     4.0\n3       1.5     165     5.5\n4       2.0     160     2.5\n5       2.5     148     3.0\n6       3.0     136     2.5\nSuppose that the name of the text file is MyData.txt. Then we can read the data into four different arrays with the following NumPy statement:\n\ndataPt, time, height, error = np.loadtxt(\"MyData.txt\", skiprows=4 , unpack=True)\n\nIf you don‚Äôt want to read in all the columns of data, you can specify which columns to read in using the usecols key word. For example, the call\n\ntime, height = np.loadtxt(\"MyData.txt\", skiprows=5 , usecols = (1,2), unpack=True)\n\nreads in only columns 1 and 2; columns 0 and 3 are skipped.\n\n\n\nThere are plenty of ways to write data to a data file in Python. We will stick to one very simple one that‚Äôs suitable for writing data files in text format. It uses the NumPy savetxt routine, which is the counterpart of the loadtxt routine introduced in the previous section. The general form of the routine is\nsavetxt(filename, array, fmt=\"%0.18e\", delimiter=\" \", newline=\"\\n\", header=\"\", footer=\"\", comments=\"# \")\nWe illustrate savetext below with a script that first creates four arrays by reading in the data file MyData.txt, as discussed in the previous section, and then writes that same data set to another file MyDataOut.txt.\n\ndataPt, time, height, error = np.loadtxt(\"MyData.txt\", skiprows=5 , unpack=True)\n\n\nlist(zip(dataPt, time, height, error))\n\n[(1.0, 0.5, 182.0, 4.5),\n (2.0, 1.0, 178.0, 4.0),\n (3.0, 1.5, 165.0, 5.5),\n (4.0, 2.0, 160.0, 2.5),\n (5.0, 2.5, 148.0, 3.0),\n (6.0, 3.0, 136.0, 2.5)]\n\n\n\nnp.savetxt('MyDataOut.txt',list(zip(dataPt, time, height, error)), fmt=\"%12.3f\")\n\n\ncat MyDataOut.txt\n\n       1.000        0.500      182.000        4.500\n       2.000        1.000      178.000        4.000\n       3.000        1.500      165.000        5.500\n       4.000        2.000      160.000        2.500\n       5.000        2.500      148.000        3.000\n       6.000        3.000      136.000        2.500\n\n\n\n\n\n\nPandas is a software library written for the Python programming language. It is used for data manipulation and analysis. It provides special data structures and operations for the manipulation of numerical tables and time series and builds on top of numpy.\n\nEasy handling of missing data\nIntelligent label-based slicing, fancy indexing, and subsetting of large data sets\n\nThe data formats provided by the pandas module are used by several other modules, such as the trackpy which is a moduly for feature tracking and analysis in image series.\n\n\n\nimport pandas as pd # import the pandas module\n\nPandas provides two data structures\n\nSeries\nData Frames\n\nA Series is a one-dimensional labeled array capable of holding any data type (integers, strings, floating point numbers, Python objects, etc.). The axis labels are collectively referred to as the index.\n\nmy_simple_series = pd.Series(np.random.randn(7), index=['a', 'b', 'c', 'd', 'e','f','g'])\nmy_simple_series\n\na    1.160711\nb   -0.296427\nc    1.881074\nd   -1.197978\ne    0.280311\nf    1.538339\ng    1.681957\ndtype: float64\n\n\n\nmy_simple_series\n\n-0.2964266043928443\n\n\nThere is a whole lot of functionality built into pandas data types. You may of course also obtain the same functionality using numpy commands, but you may find the pandas abbrevations very useful.\n\nmy_simple_series.agg(['min','max','sum','mean']) # aggregate a number of properties into a single array\n\nmin    -1.523075\nmax     0.525265\nsum    -2.119315\nmean   -0.302759\ndtype: float64\n\n\nA DataFrame is a two-dimensional size-mutable, potentially heterogeneous tabular data structure with labeled axes (rows and columns). The example below shows how such a DataFrame can be generated from the scratch. In addition to the data supplied to the DataFrame method, an index column is generated when creating a DataFrame. As in the case of Series there is a whole lot of functionality integrated into the DataFrame data type which you may explore on the website.\n\ndf = pd.DataFrame()\n\n\ndf = pd.DataFrame(np.random.randint(low=0, high=10, size=(5, 5)),columns=['column 1', 'column 2', 'columns 3', 'column 4', 'column 5'])\ndf.head()\n\n\n\n\n\n\n\n\ncolumn 1\ncolumn 2\ncolumns 3\ncolumn 4\ncolumn 5\n\n\n\n\n0\n3\n9\n3\n7\n4\n\n\n1\n2\n2\n4\n6\n7\n\n\n2\n6\n1\n7\n4\n5\n\n\n3\n3\n4\n4\n3\n0\n\n\n4\n5\n6\n8\n0\n2\n\n\n\n\n\n\n\nDue to the labelling of the columns, each column may be accessed by its column label. Labeling by names improves readability considerably.\n\ndf['column 4']\n\n0    7\n1    6\n2    4\n3    3\n4    0\nName: column 4, dtype: int64\n\n\nIf you don‚Äôt like this format, you can always return to a simple numpy array with the as_matrix() method.\n\ndf.values\n\narray([[3, 9, 3, 7, 4],\n       [2, 2, 4, 6, 7],\n       [6, 1, 7, 4, 5],\n       [3, 4, 4, 3, 0],\n       [5, 6, 8, 0, 2]])\n\n\n\n\n\nDataFrames may also be populated by text files such as comma separated value files (short .csv). These files contain data in text format but also a column label, which can be read by the pandas method read_csv(). You can find an example below, which reads the data from the dust sensor on my balcony from April, 11th. You see the different columns, where P1 and P2 correspond to the PM10 and PM2.5 dust values in \\(\\mu g/m^3\\).\n\ndata = pd.DataFrame()\ndata = pd.read_csv(\"2018-04-11_sds011_sensor_12253.csv\",delimiter=\";\",parse_dates=False)\ndata.head()\n\n\n\n\n\n\n\n\nsensor_id\nsensor_type\nlocation\nlat\nlon\ntimestamp\nP1\ndurP1\nratioP1\nP2\ndurP2\nratioP2\n\n\n\n\n0\n12253\nSDS011\n6189\n52.527\n13.39\n2018-04-11T00:01:58\n25.87\nNaN\nNaN\n19.37\nNaN\nNaN\n\n\n1\n12253\nSDS011\n6189\n52.527\n13.39\n2018-04-11T00:04:24\n25.63\nNaN\nNaN\n20.53\nNaN\nNaN\n\n\n2\n12253\nSDS011\n6189\n52.527\n13.39\n2018-04-11T00:06:55\n26.30\nNaN\nNaN\n22.00\nNaN\nNaN\n\n\n3\n12253\nSDS011\n6189\n52.527\n13.39\n2018-04-11T00:09:23\n24.60\nNaN\nNaN\n20.30\nNaN\nNaN\n\n\n4\n12253\nSDS011\n6189\n52.527\n13.39\n2018-04-11T00:11:51\n25.17\nNaN\nNaN\n20.23\nNaN\nNaN\n\n\n\n\n\n\n\n\n(data['P1']/data['P2']).plot()"
  },
  {
    "objectID": "seminars/1_input_output.html#keyboard-input",
    "href": "seminars/1_input_output.html#keyboard-input",
    "title": "Input and output",
    "section": "",
    "text": "Python has a function called input for getting input from the user and assigning it a variable name.\n\nvalue=input(\"Tell me a number: \")\ntype(value)\n\nTell me a number:  78898.9\n\n\nstr\n\n\nThe value contains the keyboard input as expected, but it is a string. We want to use a number and not a string, so we need to convert it from a string to a number.\n\nv = eval(value)\ntype(v)\n\nfloat"
  },
  {
    "objectID": "seminars/1_input_output.html#screen-output",
    "href": "seminars/1_input_output.html#screen-output",
    "title": "Input and output",
    "section": "",
    "text": "Screen output is possible by using the print command. The argument of the print function can be of different type.\n\n\nYou can format your output by modifying the string given to the print function by str.format(), The str contains text that is written to be the screen, as well as certain format specifiers contained in curly braces {}. The format function contains the list of variables that are to be printed.\n\nstring1 = \"How\"\nstring2 = \"are you my friend?\"\nint1 = 34\nint2 = 942885\nfloat1 = -3.0\nfloat2 = 3.141592653589793e-14\nprint(' ***')\n\nprint(string1)\nprint(string1 + ' ' + string2)\n\nprint(' 1. {} {}'.format(string1, string2)) \n\nprint(' 2. {0:s} {1:s}'.format(string1, string2))\nprint(' 3. {0:s} {0:s} {1:s} - {0:s} {1:s}'.format(string1, string2)) \n\nprint(' 4. {0:10s}{1:5s}'.format(string1, string2))\nprint(' ***')\nprint(int1, int2)\nprint(' 6. {0:d} {1:d}'.format(int1, int2)) \nprint(' 7. {0:8d} {1:10d}'.format(int1, int2)) \nprint(' ***')\nprint(' 8. {0:0.3f}'.format(float1))\nprint(' 9. {0:6.3f}'.format(float1)) \nprint('10. {0:8.3f}'.format(float1)) \nprint(2*' 11. {0:8.3f}'.format(float1))\nprint(' ***')\nprint('12. {0:0.3e}'.format(float2)) \nprint('13. {0:10.3e}'.format(float2)) \nprint('14. {0:10.3f}'.format(float2))\nprint(' ***')\nprint('15. 12345678901234567890')\nprint('16. {0:s}--{1:8d},{2:10.3e}'.format(string2, int1, float2))\n\n ***\nHow\nHow are you my friend?\n 1. How are you my friend?\n 2. How are you my friend?\n 3. How How are you my friend? - How are you my friend?\n 4. How       are you my friend?\n ***\n34 942885\n 6. 34 942885\n 7.       34     942885\n ***\n 8. -3.000\n 9. -3.000\n10.   -3.000\n 11.   -3.000 11.   -3.000\n ***\n12. 3.142e-14\n13.  3.142e-14\n14.      0.000\n ***\n15. 12345678901234567890\n16. are you my friend?--      34, 3.142e-14\n\n\n\n\n\nA very similar formatting can be achieved with the %operator.\n\nname = \"Frank\"\nprint(\"Hello, %s.\" % name)\n\nHello, Frank.\n\n\n\n\n\nFormatted string literals are the string literals that start with an f at the beginning and use curly braces {} to enclose the expressions that will be replaced with other values.\n\nname = \"Python Lecture\"\nnumber = 3\nfstring = f\"I'm here for the {number}. time and this {name} is awesome!\"\nprint(fstring)\n\nI'm here for the 3. time and this Python Lecture is awesome!\n\n\n\n\ntimes = 100\nfstring = f\"You just have to sent me {times:10.3f} Euros.\"\nprint(fstring)\n\nYou just have to sent me    100.000 Euros."
  },
  {
    "objectID": "seminars/1_input_output.html#file-inputoutput",
    "href": "seminars/1_input_output.html#file-inputoutput",
    "title": "Input and output",
    "section": "",
    "text": "File input and output is one of the most important features. We will have a look at reading and writing of text files with numpy and pandas. Python itself also allows you to open files and the file object provides the methods read, write and close.\n\nimport numpy as np\n\nwith open('a.txt', 'r') as file_1,open('b.txt','r') as file_2:\n    for a,b in zip(file_1,file_2):\n        print(int(a)+int(b))\n\n\nfile_1.close()\nfile_2.close()\n\n10\n12\n14\n97\n9\n9\n\n\n\n\nMost of the time we want import numbers from text files. So direct connection to NumPy seems useful and we will study that first.\n\nimport numpy as np # don't forget to import numpy\n\n\n\nOften you would like to analyze data that you have stored in a text file. Consider, for example, the data file below for an experiment measuring the free fall of a mass.\nData for falling mass experiment\nDate: 16-Aug-2013\nData taken by Frank and Ralf\ndata point  time (sec)  height (mm) uncertainty (mm)\n0       0.0     180     3.5\n1       0.5     182     4.5\n2       1.0     178     4.0\n3       1.5     165     5.5\n4       2.0     160     2.5\n5       2.5     148     3.0\n6       3.0     136     2.5\nSuppose that the name of the text file is MyData.txt. Then we can read the data into four different arrays with the following NumPy statement:\n\ndataPt, time, height, error = np.loadtxt(\"MyData.txt\", skiprows=4 , unpack=True)\n\nIf you don‚Äôt want to read in all the columns of data, you can specify which columns to read in using the usecols key word. For example, the call\n\ntime, height = np.loadtxt(\"MyData.txt\", skiprows=5 , usecols = (1,2), unpack=True)\n\nreads in only columns 1 and 2; columns 0 and 3 are skipped.\n\n\n\nThere are plenty of ways to write data to a data file in Python. We will stick to one very simple one that‚Äôs suitable for writing data files in text format. It uses the NumPy savetxt routine, which is the counterpart of the loadtxt routine introduced in the previous section. The general form of the routine is\nsavetxt(filename, array, fmt=\"%0.18e\", delimiter=\" \", newline=\"\\n\", header=\"\", footer=\"\", comments=\"# \")\nWe illustrate savetext below with a script that first creates four arrays by reading in the data file MyData.txt, as discussed in the previous section, and then writes that same data set to another file MyDataOut.txt.\n\ndataPt, time, height, error = np.loadtxt(\"MyData.txt\", skiprows=5 , unpack=True)\n\n\nlist(zip(dataPt, time, height, error))\n\n[(1.0, 0.5, 182.0, 4.5),\n (2.0, 1.0, 178.0, 4.0),\n (3.0, 1.5, 165.0, 5.5),\n (4.0, 2.0, 160.0, 2.5),\n (5.0, 2.5, 148.0, 3.0),\n (6.0, 3.0, 136.0, 2.5)]\n\n\n\nnp.savetxt('MyDataOut.txt',list(zip(dataPt, time, height, error)), fmt=\"%12.3f\")\n\n\ncat MyDataOut.txt\n\n       1.000        0.500      182.000        4.500\n       2.000        1.000      178.000        4.000\n       3.000        1.500      165.000        5.500\n       4.000        2.000      160.000        2.500\n       5.000        2.500      148.000        3.000\n       6.000        3.000      136.000        2.500\n\n\n\n\n\n\nPandas is a software library written for the Python programming language. It is used for data manipulation and analysis. It provides special data structures and operations for the manipulation of numerical tables and time series and builds on top of numpy.\n\nEasy handling of missing data\nIntelligent label-based slicing, fancy indexing, and subsetting of large data sets\n\nThe data formats provided by the pandas module are used by several other modules, such as the trackpy which is a moduly for feature tracking and analysis in image series.\n\n\n\nimport pandas as pd # import the pandas module\n\nPandas provides two data structures\n\nSeries\nData Frames\n\nA Series is a one-dimensional labeled array capable of holding any data type (integers, strings, floating point numbers, Python objects, etc.). The axis labels are collectively referred to as the index.\n\nmy_simple_series = pd.Series(np.random.randn(7), index=['a', 'b', 'c', 'd', 'e','f','g'])\nmy_simple_series\n\na    1.160711\nb   -0.296427\nc    1.881074\nd   -1.197978\ne    0.280311\nf    1.538339\ng    1.681957\ndtype: float64\n\n\n\nmy_simple_series\n\n-0.2964266043928443\n\n\nThere is a whole lot of functionality built into pandas data types. You may of course also obtain the same functionality using numpy commands, but you may find the pandas abbrevations very useful.\n\nmy_simple_series.agg(['min','max','sum','mean']) # aggregate a number of properties into a single array\n\nmin    -1.523075\nmax     0.525265\nsum    -2.119315\nmean   -0.302759\ndtype: float64\n\n\nA DataFrame is a two-dimensional size-mutable, potentially heterogeneous tabular data structure with labeled axes (rows and columns). The example below shows how such a DataFrame can be generated from the scratch. In addition to the data supplied to the DataFrame method, an index column is generated when creating a DataFrame. As in the case of Series there is a whole lot of functionality integrated into the DataFrame data type which you may explore on the website.\n\ndf = pd.DataFrame()\n\n\ndf = pd.DataFrame(np.random.randint(low=0, high=10, size=(5, 5)),columns=['column 1', 'column 2', 'columns 3', 'column 4', 'column 5'])\ndf.head()\n\n\n\n\n\n\n\n\ncolumn 1\ncolumn 2\ncolumns 3\ncolumn 4\ncolumn 5\n\n\n\n\n0\n3\n9\n3\n7\n4\n\n\n1\n2\n2\n4\n6\n7\n\n\n2\n6\n1\n7\n4\n5\n\n\n3\n3\n4\n4\n3\n0\n\n\n4\n5\n6\n8\n0\n2\n\n\n\n\n\n\n\nDue to the labelling of the columns, each column may be accessed by its column label. Labeling by names improves readability considerably.\n\ndf['column 4']\n\n0    7\n1    6\n2    4\n3    3\n4    0\nName: column 4, dtype: int64\n\n\nIf you don‚Äôt like this format, you can always return to a simple numpy array with the as_matrix() method.\n\ndf.values\n\narray([[3, 9, 3, 7, 4],\n       [2, 2, 4, 6, 7],\n       [6, 1, 7, 4, 5],\n       [3, 4, 4, 3, 0],\n       [5, 6, 8, 0, 2]])\n\n\n\n\n\nDataFrames may also be populated by text files such as comma separated value files (short .csv). These files contain data in text format but also a column label, which can be read by the pandas method read_csv(). You can find an example below, which reads the data from the dust sensor on my balcony from April, 11th. You see the different columns, where P1 and P2 correspond to the PM10 and PM2.5 dust values in \\(\\mu g/m^3\\).\n\ndata = pd.DataFrame()\ndata = pd.read_csv(\"2018-04-11_sds011_sensor_12253.csv\",delimiter=\";\",parse_dates=False)\ndata.head()\n\n\n\n\n\n\n\n\nsensor_id\nsensor_type\nlocation\nlat\nlon\ntimestamp\nP1\ndurP1\nratioP1\nP2\ndurP2\nratioP2\n\n\n\n\n0\n12253\nSDS011\n6189\n52.527\n13.39\n2018-04-11T00:01:58\n25.87\nNaN\nNaN\n19.37\nNaN\nNaN\n\n\n1\n12253\nSDS011\n6189\n52.527\n13.39\n2018-04-11T00:04:24\n25.63\nNaN\nNaN\n20.53\nNaN\nNaN\n\n\n2\n12253\nSDS011\n6189\n52.527\n13.39\n2018-04-11T00:06:55\n26.30\nNaN\nNaN\n22.00\nNaN\nNaN\n\n\n3\n12253\nSDS011\n6189\n52.527\n13.39\n2018-04-11T00:09:23\n24.60\nNaN\nNaN\n20.30\nNaN\nNaN\n\n\n4\n12253\nSDS011\n6189\n52.527\n13.39\n2018-04-11T00:11:51\n25.17\nNaN\nNaN\n20.23\nNaN\nNaN\n\n\n\n\n\n\n\n\n(data['P1']/data['P2']).plot()"
  },
  {
    "objectID": "seminars/seminar01/reinforcement-learning.html",
    "href": "seminars/seminar01/reinforcement-learning.html",
    "title": "Machine Learning and Neural Networks ü§ñ",
    "section": "",
    "text": "Welcome to the first of our advanced seminars! Throughout this course, we‚Äôve explored various applications of Python to physical problems. The course has focused not on teaching physics itself, but on exercising and developing your Python skills through physically motivated examples. Now, as we move into these advanced seminars, we‚Äôll explore a field that is increasingly important in physics: machine learning. Machine learning is the umbrella term for a variety of computational procedures designed to extract useful information from data. In this first advanced seminar, we‚Äôll introduce you to a foundational aspect of machine learning‚Äîreinforcement learning. True to our course philosophy, we‚Äôll implement this in a way that emphasizes understanding by calculating as much as possible in pure Python without relying heavily on specialized packages."
  },
  {
    "objectID": "seminars/seminar01/reinforcement-learning.html#overview",
    "href": "seminars/seminar01/reinforcement-learning.html#overview",
    "title": "Machine Learning and Neural Networks ü§ñ",
    "section": "Overview üåê",
    "text": "Overview üåê\nMachine learning has its origins long time ago and many of the currently very popular approaches have been developed in the past century. Two things have been stimulating the current hype of machine learning techniques. One is the computational power that is available already at the level of your smartphone. The second one is the availability of data. Machine learning is divided into different areas: Supervised learning üë®‚Äçüè´ involves telling the system what is right or wrong through labeled training data, Semi-supervised learning üéØ works with only sparse information on what is right or wrong, using a mixture of labeled and unlabeled data, and Unsupervised learning üîç lets the system figure out patterns and structures without any labels at all.\nThe graphics below gives a small summary. In our course, we cannot cover all methods. We will focus on Reinforcement Learning and Neural Networks just to show you, how things could look in Python.\n\nImage taken from F. Cichos et al.¬†Nature Machine Intelligence (2020)."
  },
  {
    "objectID": "seminars/seminar01/reinforcement-learning.html#reinforcement-learning",
    "href": "seminars/seminar01/reinforcement-learning.html#reinforcement-learning",
    "title": "Machine Learning and Neural Networks ü§ñ",
    "section": "Reinforcement Learning üéÆ",
    "text": "Reinforcement Learning üéÆ\nReinforcement learning is learning what to do‚Äîhow to map situations to actions‚Äîso as to maximize a numerical reward signal. The learner or agent is not told which actions to take, as in most forms of machine learning, but instead must discover which actions yield the most reward by trying them. In the most interesting and challenging cases, actions may affect not only the immediate reward but also the next situation and, through that, all subsequent rewards. These two characteristics‚Äîtrial-and-error search and delayed reward‚Äîare the two most important distinguishing features of reinforcement learning.\n\n\n\n\n\n\nüéØ Think of it Like This\n\n\n\nImagine teaching a dog a new trick. You don‚Äôt tell the dog exactly how to move its paws. Instead, you give treats (rewards) when it does something right. The dog learns through trial and error, and eventually, it figures out the sequence of actions that gets treats! Reinforcement learning works the same way with computers‚Äîthe agent explores different actions, receives feedback in the form of rewards or penalties, and gradually discovers the optimal behavior through repeated experience.\n\n\nIt has been around since the 1950s but gained momentum only in 2013 with the demonstrations of DeepMind on how to learn play Atari games like pong. The graphic below shows some of its applications in the field of robotics and gaming.\n\n\n\noverview_rl\n\n\n\nüî¨ Applications in Physics\nReinforcement learning offers particularly exciting applications in physics and related fields. Researchers are using RL for üß™ optimizing experimental design parameters to maximize information gain from limited experimental resources, ‚öõÔ∏è controlling quantum systems and preparing specific quantum states with high fidelity, and üå°Ô∏è finding energy-efficient paths through phase space in complex dynamical systems. Additional applications include üî¨ optimizing molecular dynamics simulations to accelerate sampling of rare events or transition states, and üíé discovering new materials with desired properties by navigating the vast space of possible chemical compositions and crystal structures.\n\n\n\n\n\n\nüîó Connection to Physics\n\n\n\nThe mathematical framework of reinforcement learning shares conceptual connections with statistical physics, particularly in how systems evolve toward equilibrium states that maximize certain potentials. The exploration-exploitation tradeoff in RL has parallels to thermodynamic concepts like entropy maximization under constraints.\n\n\n\n\nMarkov Decision Process üé≤\nThe key element of reinforcement learning is the so-called Markov Decision Process (MDP). The Markov decision process denotes a formalism of planning actions in the face of uncertainty. A MDP consists formally of several components: \\(S\\) represents a set of accessible states in the world üó∫Ô∏è, while \\(D\\) defines an initial distribution describing the probability of starting in each state üéØ. The transition probability between states is given by \\(P_{sa}\\) ‚û°Ô∏è, and \\(A\\) represents the set of possible actions that can be taken in each state üéÆ. The discount factor \\(\\gamma\\), which is a number between 0 and 1 ‚è∞, determines how much we value future rewards compared to immediate ones. Finally, \\(R\\) is the reward function üéÅ that assigns a numerical value to being in each state or taking each action.\n\n\n\n\n\n\nüîó Physics Analogies\n\n\n\nIt‚Äôs worth noting the connection to concepts you‚Äôre likely familiar with from physics. The state space \\(S\\) is directly analogous to phase space in classical mechanics, where each point represents a complete specification of the system‚Äôs configuration. The transition probabilities \\(P_{sa}\\) resemble stochastic processes that appear throughout statistical physics, such as those described by the Fokker-Planck equation for systems subject to random fluctuations. The Markov property‚Äîwhereby future states depend only on the current state and not on the history of how we arrived there‚Äîis similar to memoryless processes in statistical mechanics, where equilibrium distributions depend only on current conditions. Finally, the reward function \\(R\\) plays a role conceptually similar to Hamiltonians or Lagrangians in physics, in that the system ‚Äúseeks‚Äù to optimize it through its dynamics.\n\n\nWe begin in an initial state \\(s_{i,j}\\) drawn from the distribution \\(D\\). At each time step \\(t\\), we then have to pick an action, for example \\(a_1(t)\\), as a result of which our state transitions to some state \\(s_{i,j+1}\\). The states do not necessarily correspond to spatial positions, however, as we talk about the gridworld later we may use this example to understand the procedures.\n\nBy repeatedly picking actions, we traverse some sequence of states\n\\[\ns_{0,0}\\rightarrow s_{0,1}\\rightarrow s_{1,1}+\\ldots\n\\]\nOur total reward is then the sum of discounted rewards along this sequence of states\n\\[\nR(s_{0,0})+\\gamma R(s_{0,1})+ \\gamma^2 R(s_{1,1})+ \\ldots\n\\]\n\n\n\n\n\n\nüí∞ Why Discount Future Rewards?\n\n\n\nThe discount factor \\(\\gamma\\) (typically between 0 and 1) makes rewards obtained immediately more valuable than those obtained in the future. This makes sense for several reasons. First, uncertainty generally increases with time, making future rewards less certain than immediate ones. Second, prioritizing immediate gratification helps the algorithm converge faster to a solution. The value of \\(\\gamma\\) dramatically affects behavior: if \\(\\gamma = 0\\), the agent only cares about immediate rewards and becomes very myopic, while if \\(\\gamma = 1\\), all future rewards count equally, which can lead to instability in the learning process. In practice, typical values are \\(\\gamma = 0.9\\) or \\(\\gamma = 0.95\\), striking a balance between short-term and long-term planning. Think of it like compound interest, but in reverse‚Äîrewards lose value the further into the future they occur!\n\n\nIn reinforcement learning, our goal is to find a way of choosing actions \\(a_0\\), \\(a_1, \\ldots\\) over time, so as to maximize the expected value of the rewards. The sequence of actions that realizes the maximum reward is called the optimal policy \\(\\pi^{*}\\). A sequence of actions in general is called a policy \\(\\pi\\).\n\n\n\n\n\n\n‚öñÔ∏è Physics Analogy: Principle of Least Action\n\n\n\nThis optimization can be viewed as analogous to the principle of least action in classical mechanics, where a system evolves along paths that minimize the action integral. The key difference is that in RL, we maximize rewards rather than minimize action.\n\n\n\nMethods of RL üõ†Ô∏è\nThere are different methods available to find the optimal policy:\nModel-based algorithms üìä: If we know the transition probabilities \\(P_{sa}\\), we can use methods like value iteration. Think of this as having a map before you start navigating.\nModel-free algorithms üó∫Ô∏è: If we don‚Äôt know the transition probabilities, we use methods like Q-learning. This is like exploring a city without a map‚Äîyou learn as you go!\nWe will focus on Q-learning, a model-free algorithm.\n\n\n\n\n\n\nüî¨ Common RL Methods in Physics\n\n\n\nFor physics applications, several reinforcement learning methods have proven particularly valuable. Deep Q-Networks (DQN) extend the basic Q-learning approach by incorporating neural networks to handle high-dimensional state spaces, making them suitable for complex physical systems. Policy Gradient methods take a different approach by directly optimizing the policy rather than learning value functions, which can be advantageous when the action space is continuous or very large. Actor-Critic methods combine the strengths of both approaches, using value function approximation (the critic) to guide policy optimization (the actor). Finally, Monte Carlo Tree Search methods, which gained fame through their use in AlphaGo, are particularly effective for planning in systems with well-defined forward models, making them useful in certain physics simulations and control problems.\n\n\n\n\n\nUnderstanding Q-Learning üß†\nIn Q-learning, the value of an action in a state is measured by its Q-value. The expectation value \\(E\\) of the rewards with an initial state and action for a given policy is the Q-function or Q-value.\n\\[\nQ^{\\pi}(s,a)=E[R(s_{0},a_{0})+\\gamma R(s_{1},a_{1})+ \\gamma^2 R(s_{2},a_{2})+ \\ldots | s_{0}=s,a_{0}=a,a_{t}=\\pi(s_{t})]\n\\]\n\n\n\n\n\n\nüí° What is a Q-Value?\n\n\n\nThis sounds complicated but is in principle easy. Think of a Q-value as answering the question ‚Äúhow good is taking action \\(a\\) in state \\(s\\)?‚Äù A high Q-value means ‚Äúthis action looks promising!‚Äù ‚úÖ, while a low Q-value indicates ‚Äúthis action is probably bad‚Äù ‚ùå. The key insight is that there is a separate Q-value for all actions in each state. Thus if we have 4 possible actions and 25 states in our environment, we need to store a total of 100 Q-values, which we conveniently organize in a matrix with dimensions (states √ó actions).\n\n\nFor the optimal sequence of actions‚Äîfor the best way to go‚Äîthis Q value becomes a maximum:\n\\[\nQ^{*}(s,a)=\\max_{\\pi}Q^{\\pi}(s,a)\n\\]\nThe policy which gives the sequence of actions to be carried out to get the maximum reward is then calculated by:\n\\[\n\\pi^{*}(s)=\\arg\\max_{a}Q^{*}(s,a)\n\\]\nThis simply means: ‚ÄúIn state \\(s\\), choose the action \\(a\\) with the highest Q-value!‚Äù\n\n\n\n\n\n\nüìê The Bellman Equation: The Foundation of RL\n\n\n\n\n\nThe Bellman equation is the cornerstone of reinforcement learning and dynamic programming. Named after mathematician Richard Bellman, it expresses the fundamental recursive relationship between the value of a state and the values of its successor states. This elegant mathematical framework allows us to break down complex sequential decision-making problems into simpler, more manageable pieces.\n\nThe Value Function\nAt the heart of reinforcement learning lies the state-value function \\(V^{\\pi}(s)\\), which represents the expected return we can achieve starting from state \\(s\\) and following a particular policy \\(\\pi\\) thereafter. The Bellman equation for this value function is:\n\\[\nV^{\\pi}(s) = E_{\\pi}[R_{t+1} + \\gamma V^{\\pi}(s_{t+1}) | s_t = s]\n\\]\nThis deceptively simple equation encapsulates a profound idea: the value of being in a particular state equals the immediate reward we expect to receive, plus the discounted value of wherever we‚Äôll end up next. In other words, we don‚Äôt need to look infinitely far into the future to determine a state‚Äôs value‚Äîwe only need to consider one step ahead and trust that the value of the next state already captures everything beyond that.\n\n\nThe Bellman Optimality Equation\nWhen we seek the optimal behavior rather than following a fixed policy, we arrive at the Bellman optimality equation. For the optimal state-value function \\(V^{*}(s)\\), this becomes:\n\\[\nV^{*}(s) = \\max_{a} \\left[ R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^{*}(s') \\right]\n\\]\nThe corresponding equation for optimal Q-values (the action-value function) is:\n\\[\nQ^{*}(s,a) = R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) \\max_{a'} Q^{*}(s',a')\n\\]\nThis second equation is particularly important because it forms the theoretical foundation of Q-learning. The Q-learning update rule we‚Äôll implement is essentially an iterative approximation method designed to solve this equation through repeated experience rather than analytical computation.\n\n\nüî¨ Connection to Physics\nThe Bellman equation reveals deep connections to fundamental concepts in physics that may surprise you. In classical mechanics, the Hamilton-Jacobi equation describes how a system evolves through phase space, serving as a bridge between Lagrangian and Hamiltonian formulations. The Bellman equation is essentially a discrete-time, stochastic generalization of this same principle, adapted for decision-making under uncertainty.\nThe connection extends even further to quantum mechanics through Feynman‚Äôs path integral formulation. Just as the path integral sums over all possible trajectories a particle might take, weighting each by its action, the Bellman equation implicitly considers all possible future trajectories through the expectation value, weighting them by their probability and discounted reward. Both frameworks recognize that the optimal or most likely behavior emerges from considering the totality of possibilities.\nPerhaps most fundamentally, both the Bellman equation and the principle of least action in physics exploit what computer scientists call ‚Äúoptimal substructure.‚Äù If the optimal path from point A to point C passes through point B, then the segment from A to B must itself be optimal. This isn‚Äôt just a mathematical convenience‚Äîit‚Äôs a deep principle about how optimal solutions compose, whether we‚Äôre minimizing action in physics or maximizing reward in reinforcement learning.\n\n\nWhy Does Q-Learning Work?\nUnderstanding the Bellman equation helps us appreciate why Q-learning is so powerful. Q-learning is fundamentally a model-free method, meaning it learns the optimal Q-values \\(Q^{*}(s,a)\\) without ever needing to know the transition probabilities \\(P(s'|s,a)\\) that appear in the Bellman optimality equation. This is remarkable because it means we don‚Äôt need a mathematical model of how the environment behaves.\nInstead of solving the Bellman equation directly through analytical methods or dynamic programming, Q-learning takes a different approach. It samples actual experiences from the environment‚Äîtaking actions, observing outcomes, and receiving rewards. It then uses these samples to iteratively update its Q-values, gradually refining its estimates. Under appropriate conditions (including sufficient exploration and a suitable learning rate schedule), these Q-values provably converge to the optimal values that would be obtained by solving the Bellman equation exactly.\nThis sampling-based approach is extraordinarily powerful in practice. We can learn optimal behavior in environments that are too complex to model analytically, too large to solve with traditional dynamic programming, or simply unknown to us. The agent learns from experience, much like a physicist conducting experiments to understand a system whose governing equations are not yet known.\n\n\n\n\n\n\nThe Q-Learning Update Rule üìù\nThe Q-learning algorithm is an iterative procedure of updating the Q-value of each state and action which converges to the optimal policy \\(\\pi^{*}\\). It is given by:\n\\[\nQ_{t+\\Delta t}(s,a)  = Q_t(s,a) + \\alpha\\big[R(s) + \\gamma \\max_{a'}Q_t(s',a')-Q_t(s,a)\\big]\n\\]\n\n\n\n\n\n\nüîç Breaking Down This Equation\n\n\n\nLet‚Äôs understand each term in the Q-learning update equation. The term \\(Q_t(s,a)\\) represents our current estimate of how good this action is. When we take the action, we receive an immediate reward \\(R(s)\\), which gives us instant feedback. Looking ahead, \\(\\gamma \\max_{a'}Q_t(s',a')\\) represents the discounted value of the best action we could take in the next state. The crucial part is the bracketed term \\(R(s) + \\gamma \\max_{a'}Q_t(s',a') - Q_t(s,a)\\), which is our prediction error‚Äîessentially comparing where we thought we‚Äôd end up versus where we actually could end up.\nThe learning rate \\(\\alpha\\) (ranging from 0 to 1) controls how much we trust this new information. If \\(\\alpha = 0\\), we never learn anything and simply ignore new information. If \\(\\alpha = 1\\), we completely replace our old estimate with the new observation, ignoring everything we learned before. In practice, typical values range from \\(\\alpha = 0.1\\) to \\(0.5\\), providing a balance between stability and adaptability. The key insight is that we update our Q-value based on the difference between what we expected and what we observed, allowing the agent to gradually refine its understanding through experience!\n\n\n\n\n\n\n\n\n‚öõÔ∏è Physics Perspective\n\n\n\nFrom a physics perspective, this update rule resembles a relaxation method for finding equilibrium states. The term in brackets can be interpreted as a ‚Äúforce‚Äù that drives the Q-values toward their optimal values, with \\(\\alpha\\) controlling the rate of convergence, similar to a damping constant in physics. The Bellman equation, which underpins this update rule, is also a form of dynamic programming that shares mathematical similarities with the Hamilton-Jacobi-Bellman equation in control theory."
  },
  {
    "objectID": "seminars/seminar01/reinforcement-learning.html#navigating-a-grid-world",
    "href": "seminars/seminar01/reinforcement-learning.html#navigating-a-grid-world",
    "title": "Machine Learning and Neural Networks ü§ñ",
    "section": "Navigating a Grid World üó∫Ô∏è",
    "text": "Navigating a Grid World üó∫Ô∏è\nFor our Python course we will have a look at the standard problem of reinforcement learning, which is the navigation in a grid world. This is like teaching an agent to find its way home!\nEach of the grid cells below represents a state \\(s\\) in which an object could reside. In each of these states, the object can take several actions. If it may step to left, right, up or down, there are 4 actions, which we may call \\(a_{1},a_{2},a_{3}\\) and \\(a_{4}\\).\nThis image below shows our gridworld, with 25 states, where the shaded state is the goal state üéØ where we want the agent to go to independent of its initial state.\n\nIn each of these states, we have 4 possible actions as depicted below:\n\n\n\n\n\n\n\nüéÆ The Game Plan\n\n\n\nOur agent will follow a simple but effective learning strategy. It will start at a random position in the grid, then take actions by moving up, down, left, or right. For each move, it gets a penalty of -1, which encourages finding the shortest path rather than wandering aimlessly. When it finally reaches the goal, it receives a big reward of +10! üéâ Through many iterations of this process, the agent gradually learns which actions lead to the goal fastest from any starting position.\n\n\n\nStep 1: Initialize Reinforcement Learning üöÄ\nAt first we would like to initialize our problem. We have as depicted above 25 states, where one state is the goal state. We would like to use 4 actions to move between the states so our Q-value matrix has 100 entries.\n\n\n\n\n\n\nüéØ Setting up Rewards and Penalties\n\n\n\nWe would like to give a penalty of \\(R=-1\\) for all states except for the goal state where we give a reward of \\(R=10\\). The reason for the -1 penalty is important: it encourages the agent to find the shortest path to the goal. Since every step costs something, the agent naturally learns to minimize the number of steps taken. Without this penalty, the agent might wander around inefficiently, since reaching the goal eventually would still yield the same total reward regardless of path length.\n\n\nOur agent shall learn with a learning rate of \\(\\alpha=0.5\\) and we will discount future rewards with \\(\\gamma=0.5\\).\n\n\n\n\n\n\n\n\n\n\n\n\nüé≤ The Œµ-Greedy Strategy\n\n\n\nThere is one tiny detail which is crucial to understand: the Œµ-greedy factor. The problem is this: if we always choose the action with the highest Q-value, we might get stuck in a suboptimal strategy. Imagine your initial random Q-values happen to favor ‚Äúalways go right‚Äù‚Äîyou might never discover that ‚Äúgo left‚Äù leads to a shortcut!\nThe solution is the Œµ-greedy strategy, which introduces controlled randomness into the decision-making process. Specifically, 80% of the time (when a random number exceeds 0.2), we choose the best known action based on our current Q-values, exploiting what we‚Äôve learned. However, 20% of the time (when the random number is ‚â§ 0.2), we choose a completely random action to explore new possibilities.\nThis balancing act is called the exploration-exploitation tradeoff. Exploitation üéØ means using what you already know by choosing the best action according to current estimates, while Exploration üîç means trying new things through random actions to discover potentially better strategies. By setting Œµ = 0.2, we ensure that 20% of actions are chosen randomly to explore new possibilities, preventing the agent from getting trapped in local optima while still making progress toward the goal most of the time.\n\n\n\n\nStep 2: Define the Actions üéÆ\nThe actions, which we can take in each state are defined by 2-D vectors here which increase either the row or the column index in our gridworld.\n\n\n\n\n\n\n\n\nStep 3: Choose Initial State üé≤\nWe choose the initial state from which we start randomly. We also initialize a list, where we register the sum of all Q-values. This is helpful to monitor the convergence of our algorithm.\n\n\n\n\n\n\n\n\n\n\n\n\nüìä What to Expect\n\n\n\nAfter running the learning loop:\n\nThe sum of Q-values should converge to a stable negative value\nMost Q-values will be negative (because most states give -1 penalty)\nOnly Q-values leading toward the goal will become positive\nThe convergence plot will show: initial fluctuations ‚Üí stabilization\n\n\n\n\n\nStep 4: The Learning Loop! üîÑ\nThe cell below is all you need for learning how to navigate the grid world. This is where the magic happens! ‚ú®\n\n\n\n\n\n\n\n\n\n\n\n\nüîç What‚Äôs Happening in the Loop?\n\n\n\nLet‚Äôs trace through one typical iteration to understand the learning process. Suppose the agent is in state (2, 3). It chooses an action‚Äîsay ‚Äúmove right‚Äù‚Äîusing the Œµ-greedy strategy (80% chance of the best known action, 20% chance of random exploration). The agent then moves to the new state (2, 4) and receives a reward of -1 as a penalty for the move. Based on this experience, it updates the Q-value for the action ‚Äúmove right‚Äù in state (2, 3), incorporating both the immediate reward and the discounted future reward expected from the new state. The agent then repeats this process from the new position. After 10,000 iterations, the agent has explored many different paths through the grid and learned which actions lead to the goal most efficiently from every possible starting position!\n\n\n\n\nStep 5: Visualize Convergence üìà\nThe convergence of our learning is best judged from the sum of all Q-values in the matrix. This should converge to a negative value as most of the time our agent is getting the penalty \\(R=-1\\) and only sparsely \\(R=10\\) at the goal.\n\n\n\n\n\n\n\n\n\n\n\n\nüîç Interpreting the Convergence Plot\n\n\n\nThe convergence plot reveals three distinct phases of learning. In the early phase (first ~2000 transitions), you‚Äôll see large fluctuations as the agent explores randomly and Q-values are being updated rapidly based on new discoveries. The middle phase (2000-5000 transitions) shows decreasing fluctuations as patterns emerge and the agent begins finding better paths more consistently. Finally, in the late phase (5000+ transitions), the values stabilize, indicating that the optimal policy has been learned! ‚úÖ\nIf your plot doesn‚Äôt stabilize as expected, there are several things you can try. Increasing the number of iterations gives the agent more time to explore and converge. Adjusting the learning rate Œ± can help‚Äîif it‚Äôs too high, learning may be unstable; if too low, convergence will be very slow. Of course, also check carefully for bugs in the code, particularly in the action selection and Q-value update logic.\n\n\n\n\nStep 6: Extract the Policy üó∫Ô∏è\nThe policy is obtained by taking the best actions with the largest Q-value from our Q-matrix.\n\\[\n\\pi^{*}(s)=\\arg\\max_{a}Q^{*}(s,a)\n\\]\nIn plain English: ‚ÄúFor each state, find which action has the highest Q-value‚Äù\n\n\n\n\n\n\n\n\nStep 7: Visualize the Learned Policy! üé®\nNow let‚Äôs see the beautiful result‚Äîa visual map showing the optimal action to take in each state!\n\n\n\n\n\n\n\n\n\n\n\n\nüéâ What You Should See\n\n\n\nThe plot displays arrows in each grid cell indicating the optimal action to take from that position. You should observe several key features: arrows point toward the goal from all positions, demonstrating that the agent has learned a complete navigation strategy. The paths represented are optimal in the sense of being shortest, meaning from any starting position, following the arrows will reach the goal in the minimum number of steps. The green square marks the goal location in the bottom-right corner.\nThis visualization beautifully demonstrates the power of reinforcement learning‚Äîthe agent discovered these optimal paths entirely through trial and error! No explicit programming of paths was needed; the agent simply learned from the rewards and penalties it experienced during exploration."
  },
  {
    "objectID": "seminars/seminar01/reinforcement-learning.html#experiments-and-challenges",
    "href": "seminars/seminar01/reinforcement-learning.html#experiments-and-challenges",
    "title": "Machine Learning and Neural Networks ü§ñ",
    "section": "üéØ Experiments and Challenges",
    "text": "üéØ Experiments and Challenges\nNow that you understand the basics, try these modifications to deepen your understanding:\n\n\n\n\n\n\nüü¢ Easy Challenge: Change the Goal Location\n\n\n\n\n\nTask: Move the goal to a different corner (e.g., top-left at position [0, 0])\nWhat to change:\n# Instead of:\nR[n_rows-1, n_columns-1] = 10\n\n# Try:\nR[0, 0] = 10\nWhat to observe: Does the policy adapt? Do all arrows now point to the new goal?\n\n\n\n\n\n\n\n\n\nüü° Medium Challenge: Add a Second Goal\n\n\n\n\n\nTask: Add a second goal state with a smaller reward (e.g., R=5)\nWhat to change:\nR[0, n_columns-1] = 5  # Top-right corner\nWhat to observe: Notice which goal the agent prefers‚Äîdoes it consistently choose the higher reward, or does the preference depend on starting position? Pay attention to how the arrows in different regions of the grid show this preference, potentially creating a boundary between regions that lead to each goal.\n\n\n\n\n\n\n\n\n\nüü† Advanced Challenge: Add Walls (Obstacles)\n\n\n\n\n\nTask: Make some cells impassable ‚Äúwalls‚Äù\nWhat to change:\n# Add large negative rewards to wall cells\nR[2, 2] = -100  # Wall in center\nR[2, 3] = -100  # Another wall\nWhat to observe: Does the policy route around the walls? Try creating a maze!\n\n\n\n\n\n\n\n\n\nüî¥ Expert Challenge: Stochastic Environment\n\n\n\n\n\nTask: Make actions succeed only 80% of the time (20% random slip)\nWhat to change: Modify the action selection to sometimes execute a different action than intended:\n# After choosing action, add randomness:\nif np.random.random() &lt; 0.2:  # 20% chance of slip\n    action = np.random.randint(4)  # Random direction instead\nWhat to observe: How does uncertainty affect the learned policy? Do paths change?\n\n\n\n\n\n\n\n\n\nüéì Research Challenge: Parameter Sensitivity\n\n\n\n\n\nTask: Systematically vary parameters and observe effects\nParameters to explore: Experiment with the learning rate Œ± by trying values like 0.1, 0.5, and 0.9 to see how this affects convergence speed. Test different values of the discount factor Œ≥ (0.1, 0.5, 0.9) to observe how it affects the learned policy, particularly the agent‚Äôs preference for shorter versus longer paths. Also vary epsilon Œµ with values like 0.0, 0.2, and 0.5 to understand how exploration rate impacts learning stability and final performance.\nCreate plots showing convergence speed for different Œ± values, the final policy learned with different Œ≥ values, and learning stability (perhaps using running averages of episode rewards) for different Œµ values. These visualizations will help you develop intuition for how these hyperparameters interact with the learning process."
  },
  {
    "objectID": "seminars/seminar01/reinforcement-learning.html#what-youve-learned",
    "href": "seminars/seminar01/reinforcement-learning.html#what-youve-learned",
    "title": "Machine Learning and Neural Networks ü§ñ",
    "section": "üéì What You‚Äôve Learned",
    "text": "üéì What You‚Äôve Learned\nCongratulations! You‚Äôve just implemented reinforcement learning from scratch! üéâ\n\nKey Concepts ‚úÖ\nYou‚Äôve mastered several fundamental concepts in reinforcement learning. You now understand Markov Decision Processes, including the roles of states, actions, rewards, and policies in sequential decision-making. You‚Äôve implemented Q-Learning, which learns action values through trial and error without needing a model of the environment. You‚Äôve experienced the importance of balancing Exploration vs.¬†Exploitation through the Œµ-greedy strategy, ensuring the agent both learns new strategies and leverages known good actions. You‚Äôve seen how the Bellman Equation provides the theoretical foundation for updating Q-values based on immediate and future rewards. Finally, you‚Äôve learned to monitor Convergence by tracking the sum of Q-values over time, giving you insight into when the agent has finished learning.\n\n\nProgramming Skills üíª\nYou‚Äôve also developed several practical programming skills. You can now work with multi-dimensional NumPy arrays to store and manipulate Q-values efficiently. You‚Äôve implemented iterative update algorithms that gradually refine estimates through repeated experience. You‚Äôve learned to balance exploration and exploitation in decision-making systems using randomization strategies. You can visualize policies with matplotlib, creating intuitive graphical representations of learned behaviors. And you‚Äôve gained experience tracking and plotting convergence metrics to monitor learning progress over time.\n\n\nPhysics Connections üî¨\nThroughout this seminar, you‚Äôve encountered numerous connections to physics. The Q-learning process relates to Statistical Mechanics through concepts like ensemble averages and the approach to equilibrium states. The Optimization problem of finding the best policy parallels finding minimum energy configurations in physical systems. The agent‚Äôs random exploration and probabilistic decisions exemplify Stochastic Processes commonly found in statistical physics. And the iterative Q-value updates resemble Relaxation Methods used to find equilibrium solutions in computational physics."
  },
  {
    "objectID": "seminars/seminar01/reinforcement-learning.html#where-to-go-from-here",
    "href": "seminars/seminar01/reinforcement-learning.html#where-to-go-from-here",
    "title": "Machine Learning and Neural Networks ü§ñ",
    "section": "üöÄ Where to Go From Here",
    "text": "üöÄ Where to Go From Here\n\nImmediate Next Steps\n\n\n\n\n\n\n\n\nüìö Further Reading\nIf you want to know more about Reinforcement Learning, several excellent resources are available. The classic textbook is üìñ Sutton and Barto‚Äôs Reinforcement Learning: An Introduction, which is freely available as a PDF and covers everything from basics to advanced topics. For hands-on practice, üéÆ OpenAI Gym provides a standardized interface to work with more complex RL environments. In upcoming seminars, we‚Äôll explore üß† Deep RL, where Q-learning is combined with neural networks to handle high-dimensional problems. For physics-specific applications, üî¨ RL in Physics papers cover fascinating topics like quantum control, molecular dynamics optimization, and materials discovery‚Äîsearch for these in journals like Nature Machine Intelligence or Physical Review Letters.\n\n\nReal-World Applications üåç\nThe simple grid world you just solved may seem toy-like, but the same principles scale up to remarkably sophisticated applications. These include ü§ñ robot navigation in complex real-world environments with obstacles and dynamic conditions, üéØ optimal control of quantum systems where precise manipulation of quantum states is required, and üî¨ experimental design for physics measurements to determine the most informative sequence of experiments. RL is also being used for üíé materials discovery through high-dimensional searches of chemical composition spaces, and ‚öõÔ∏è particle accelerator tuning and optimization where thousands of parameters must be adjusted simultaneously.\n\n\nDebugging Tips üêõ\nIf your implementation doesn‚Äôt converge as expected, here‚Äôs a systematic debugging approach. First, check if Q-values are exploding to very large positive or negative numbers‚Äîif so, reduce the learning rate Œ± to make updates more gradual. If the policy seems random even after many iterations, you likely need more training iterations to allow the agent to explore sufficiently. If convergence is happening but very slowly, try adjusting the exploration rate Œµ or increasing Œ± to speed up learning. If the agent seems to be going in circles or taking obviously suboptimal paths, verify that your action vectors are correctly defined and actually move the agent where you intend. Finally, if arrows aren‚Äôt pointing toward the goal, double-check that the reward is indeed highest at the goal state and that goal-reaching is being detected properly in your code.\n\n\n\n\n\n\nüéØ The Big Picture\n\n\n\nWhat you‚Äôve learned here is remarkably powerful. Consider the journey from randomness to intelligence: you started with completely random Q-values and random actions, yet through nothing but trial, error, and a simple update rule, your agent discovered the optimal path! This happened with no explicit programming of the solution‚Äîyou never told the agent ‚Äúgo to the goal‚Äù or specified which path to take. You only defined rewards, and the agent figured out the strategy itself through experience.\nThe approach is also highly generalizable. The same algorithm works for different grid sizes, multiple goals, 3D spaces, continuous state spaces (with appropriate modifications), and complex physics problems. The underlying principles remain the same regardless of the specific application.\nThis is the essence of machine learning: letting the computer discover solutions through experience rather than explicit programming. Rather than encoding our understanding of the optimal solution, we define the objective and let the learning algorithm find its own path to achieving it."
  },
  {
    "objectID": "seminars/seminar01/reinforcement-learning.html#summary",
    "href": "seminars/seminar01/reinforcement-learning.html#summary",
    "title": "Machine Learning and Neural Networks ü§ñ",
    "section": "üéä Summary",
    "text": "üéä Summary\nYou‚Äôve successfully:\n\n‚úÖ Understood the fundamentals of reinforcement learning\n‚úÖ Implemented Q-learning from scratch in pure Python\n‚úÖ Trained an agent to navigate a gridworld optimally\n‚úÖ Visualized the learning process and final policy\n‚úÖ Connected RL concepts to physics principles\n\nNext seminar: We‚Äôll explore neural networks and see how they can be combined with reinforcement learning to solve even more complex problems! üß†üöÄ\n\n\n\n\n\n\nüåü Final Thought\n\n\n\nThe gridworld we explored is deliberately simple for pedagogical purposes, but don‚Äôt be fooled‚Äîthe principles you learned apply directly to cutting-edge research across multiple domains. Scientists are using these exact same reinforcement learning ideas to design better solar cells by optimizing material compositions and layer thicknesses, to optimize quantum computers through careful control of qubit interactions, and to control fusion reactors where real-time decisions must balance dozens of competing objectives. The same techniques are being applied to discover new drugs by navigating vast chemical spaces, and to understand complex physical systems whose behavior emerges from intricate interactions.\nYou‚Äôre now equipped with a fundamental tool of modern computational physics! üéâ The Q-learning algorithm you implemented today forms the foundation for much more sophisticated approaches that are actively advancing scientific knowledge and technological capabilities."
  },
  {
    "objectID": "seminars/seminar06/publication-ready-figures.html",
    "href": "seminars/seminar06/publication-ready-figures.html",
    "title": "Erstellung ver√∂ffentlichungsreifer Diagramme",
    "section": "",
    "text": "Optimale Plotgr√∂√üen f√ºr wissenschaftliche Arbeiten\nIn diesem Leitfaden zeige ich Ihnen, wie Sie Plots in einer standardisierten, publikationstauglichen Gr√∂√üe erstellen k√∂nnen. Die Hinweise sind sowohl f√ºr Jupyter-Notebooks als auch f√ºr wissenschaftliche Arbeiten wie Semesterarbeiten geeignet.\nBeim Export von Plots als PDF-Dateien werden Vektorgrafiken erzeugt, die sich nachtr√§glich verlustfrei skalieren lassen. Dies erm√∂glicht zwar eine flexible Anpassung an ein- oder zweispaltige Layoutformate, kann aber zu Inkonsistenzen f√ºhren: Unterschiedliche Skalierungsfaktoren resultieren oft in verschiedenen Gr√∂√üen von Achsenbeschriftungen und Markierungen, was das Gesamtbild der Arbeit beeintr√§chtigt.\nDie bessere Strategie ist es, von Anfang an einheitliche Plotgr√∂√üen zu verwenden: - Definieren Sie Standards f√ºr ein- und zweispaltige Abbildungen - Legen Sie einheitliche Gr√∂√üen f√ºr Achsenbeschriftungen und Markierungen fest - Erstellen Sie alle Plots direkt in der finalen Gr√∂√üe\nIm Folgenden stelle ich praktische Techniken vor, mit denen Sie direkt aus Jupyter-Notebooks publikationsreife Plots erstellen und speichern k√∂nnen."
  },
  {
    "objectID": "seminars/seminar06/publication-ready-figures.html#erstellen-eines-diagramms-mit-einer-bestimmten-gr√∂√üe-des-begrenzungsrahmens",
    "href": "seminars/seminar06/publication-ready-figures.html#erstellen-eines-diagramms-mit-einer-bestimmten-gr√∂√üe-des-begrenzungsrahmens",
    "title": "Erstellung ver√∂ffentlichungsreifer Diagramme",
    "section": "Erstellen eines Diagramms mit einer bestimmten Gr√∂√üe des Begrenzungsrahmens",
    "text": "Erstellen eines Diagramms mit einer bestimmten Gr√∂√üe des Begrenzungsrahmens\nWenn Sie einen Plot in matplotlib erstellen, k√∂nnen Sie eine Gr√∂√üe mit dem Parameter figsize festlegen, z.B.\nplt.figure(figsize=(3,2))\nf√ºr eine Abbildung mit einer Breite von 3 inches bzw. 7,62 cm und einer H√∂he von 2 inches (5,08 cm). Wenn Sie diesen Parameter nicht verwenden oder sogar den Befehl plt.figure() nicht nutzen, verwendet matplotlib die Standardgr√∂√üe, die h√§ufig 8 inches mal 6 inches betr√§gt. Diese Standardgr√∂√üe ist viel zu gro√ü, da die Abbildung dann fast eine ganze A4-Seite breit w√§re. Eine angemessene Gr√∂√üe f√ºr einen Plot in einer einzelnen Spalte eines zweispaltigen Dokuments w√§ren die oben genannten 3 inches mal 2 inches, da die gesamte Seitenbreite 21 cm minus einem Rand von etwa 3 cm auf jeder Seite eine Spaltenbreite von ungef√§hr (21-6)/2=7,5 cm ergibt.\nDer in Figure¬†1 gezeigte Plot wurde mit den folgenden Befehlen erstellt\nplt.figure(figsize=(3,2), dpi=150)\nx=np.linspace(0,np.pi*4,200)\nplt.plot(x,np.sin(x),color='k')\nplt.xlabel(r\"angle $\\theta$ in [rad]\")\nplt.ylabel(r\"$\\sin(\\theta)$\")\nplt.savefig(\"figure_example.pdf\",\n    bbox_inches = 'tight')\nplt.show()\nDie daraus resultierende PDF-Datei enth√§lt eine Grafik mit einem Begrenzungsrahmen, der genau 3 Zoll mal 2 Zoll gro√ü ist. Wenn Sie das Diagramm in ein beliebiges Zeichenprogramm wie Adobe Illustrator, Affinity Designer oder sogar in eine Textverarbeitungssoftware wie Word oder Pages einf√ºgen, hat der Begrenzungsrahmen dieses Diagramms genau diese Gr√∂√üe, und Sie k√∂nnen weitere Diagramme anordnen, um eine ganze Abbildung zu erstellen, ohne die Skalierung √§ndern zu m√ºssen. Wenn Sie das Diagramm in einem zweispaltigen LaTeX-Manuskript verwenden, kann es ohne Skalierung verwendet werden, d.h. durch includegraphics{Figure 1.pdf} wird es in der entsprechenden Gr√∂√üe √ºber eine Spalte angezeigt.\nEs gibt noch ein paar weitere Dinge zu beachten.\n\nW√§hrend der Begrenzungsrahmen dieser Abbildung diese Gr√∂√üe hat, ist der Achsenrahmen kleiner, und oft bleibt auf der linken/unteren Seite ein gewisser Leerraum zwischen den Achsenbeschriftungen und dem Rand des Begrenzungsrahmens. Das h√§ngt sehr stark von Ihrem spezifischen Diagramm ab. Wie Sie eine Abbildung mit einer festen Achsenrahmengr√∂√üe erstellen, wird im zweiten Abschnitt behandelt.\nDie Schriftgr√∂√üe auf der Achse betr√§gt jetzt 10 oder 11 Punkte, was der Schriftgr√∂√üe der meisten Dokumente entspricht, die Sie mit dieser Abbildung erstellen. Ich habe die folgenden plt.rcParams verwendet: ‚Äòaxes.labelsize‚Äô: 11, ‚Äòxtick.labelsize‚Äô : 10, ‚Äòytick.labelsize‚Äô : 10 f√ºr die gezeigte Darstellung.\nSie werden auch feststellen, dass die Arbeit mit dieser Abbildungsgr√∂√üe in einem Jupyter-Notebook nicht gut ist. Das hat damit zu tun, wie Jupyter die Ausgabe in eine PNG-Datei √ºbersetzt, die inline angezeigt wird. Eine M√∂glichkeit, den Plot im Jupyter-Notebook zu vergr√∂√üern, aber die PDF-Gr√∂√üe beizubehalten, besteht darin, den Parameter dpi im Befehl plt.figure(figsize=(3,2), dpi=150) zu erh√∂hen. Normalerweise ist er auf dpi=75 eingestellt, was jetzt viel zu klein ist. Eine Einstellung von dpi=150 scheint ein vern√ºnftiger Kompromiss zwischen Bildschirm- und Druckgr√∂√üe zu sein. Wenn Sie v√∂llig unabh√§ngig sein wollen\nDer Befehl plt.savefig verwendet einen zus√§tzlichen bbox_inches = 'tight' Parameter, der sicherstellt, dass die Boundingbox auch wirklich alle Komponenten des Plots genau umschlie√üt.\n\n\nimport matplotlib as mpl\nimport matplotlib.font_manager as font_manager\nfrom IPython.core.display import HTML\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom directory_tree import display_tree\n\n\nplt.rcParams.update({'font.size': 12,\n                     'lines.linewidth': 1,\n                     'lines.markersize': 10,\n                     'axes.labelsize': 11,\n                     'axes.labelpad': 0,\n                     'ytick.labelsize' : 10,\n                     'xtick.top' : True,\n                     'xtick.direction' : 'in',\n                     'ytick.right' : True,\n                     'ytick.direction' : 'in',\n                     'figure.dpi': 150})\n\n\ndef get_size(w,h):\n    return((w/2.54,h/2.54))\n\n\nplt.figure(figsize=get_size(7,6))\nx=np.linspace(0,np.pi*4,200)\nplt.plot(x,np.sin(x),color='k')\nplt.xlabel(r\"angle $\\theta$ in [rad]\")\nplt.ylabel(r\"$\\sin(\\theta)$\")\nplt.tight_layout()\nplt.savefig(\"figure_example3.pdf\", transparent=True)\nplt.show()\n\n\n\n\n\n\n\n\nWenn Sie dieses Bild in eine beliebige Software laden, erhalten Sie ein Bild mit einer Gr√∂√üe, die der eingestellten Breite entspricht.\n\n\n\n\n\n\nFigure¬†1"
  },
  {
    "objectID": "seminars/seminar06/publication-ready-figures.html#erstellen-eines-diagramms-mit-einer-bestimmten-achsenrahmengr√∂√üe",
    "href": "seminars/seminar06/publication-ready-figures.html#erstellen-eines-diagramms-mit-einer-bestimmten-achsenrahmengr√∂√üe",
    "title": "Erstellung ver√∂ffentlichungsreifer Diagramme",
    "section": "Erstellen eines Diagramms mit einer bestimmten Achsenrahmengr√∂√üe",
    "text": "Erstellen eines Diagramms mit einer bestimmten Achsenrahmengr√∂√üe\nDer Achsenrahmen ist die Box des Rahmens, der die Achsen bereitstellt. Beim Erstellen einer Figur mit dem Befehl plt.figure() wird der Achsenrahmen von matplotlib so berechnet, dass er innerhalb der durch figsize angegebenen Boundingbox liegt, so dass alle Achsenbeschriftungen ebenfalls hineinpassen. Der Achsenrahmen ist daher kleiner als die angegebene Bounding Box und h√§ngt oft von den Achsenbeschriftungen und weiteren Dingen ab. Wenn Sie einen Plot mit einer festen Gr√∂√üe des Achsenrahmens erstellen wollen, ist es sinnvoll, eine Funktion in Ihrem Code unterzubringen, die die Gr√∂√üe des Achsenrahmens festlegt. Diese Funktion k√∂nnte lauten\ndef set_size(w,h, ax=None):\n    \"\"\" w, h: width, height in inches \"\"\"\n    if not ax: ax=plt.gca()\n      l = ax.figure.subplotpars.left\n      r = ax.figure.subplotpars.right\n    t = ax.figure.subplotpars.top\n    b = ax.figure.subplotpars.bottom\n    figw = float(w)/(r-l)\n    figh = float(h)/(t-b)\n    ax.figure.set_size_inches(figw, figh)\nwobei Sie die gew√ºnschte Breite und H√∂he (in Zoll) der aktuellen Achse ax angeben m√ºssen. Die Funktion gibt nichts zur√ºck, sondern legt direkt die Gr√∂√üe fest.\n\ndef set_size(w,h, ax=None):\n    \"\"\" w, h: width, height in inches \"\"\"\n    if not ax: ax=plt.gca()\n    l = ax.figure.subplotpars.left\n    r = ax.figure.subplotpars.right\n    t = ax.figure.subplotpars.top\n    b = ax.figure.subplotpars.bottom\n    figw = float(w)/(r-l)\n    figh = float(h)/(t-b)\n    ax.figure.set_size_inches(figw, figh)\n\nfig=plt.figure(dpi=150)\nax=plt.axes()\nax.plot(x,np.sin(x),color='k')\nax.set_xlabel(r\"angle $\\theta$ in [rad]\")\nax.set_ylabel(r\"$\\sin(\\theta)$\")\nset_size(3,2)\nplt.savefig(\"figure_example2.pdf\", bbox_inches='tight', transparent=True)\nplt.show()\n\n\n\n\n\n\n\n\nWenn Sie diese Abbildung in ein Grafikprogramm oder eine Textverarbeitungssoftware laden, sollte das Abbildungsfeld eine Gr√∂√üe von 7,62 cm mal 5,08 cm haben, ohne dass eine Neuskalierung erfolgt:"
  },
  {
    "objectID": "seminars/seminar06/publication-ready-figures.html#auswahl-der-schriftarten",
    "href": "seminars/seminar06/publication-ready-figures.html#auswahl-der-schriftarten",
    "title": "Erstellung ver√∂ffentlichungsreifer Diagramme",
    "section": "Auswahl der Schriftarten",
    "text": "Auswahl der Schriftarten\nMatplotlib kann auf eine Reihe von verschiedenen Schriftarten zugreifen. Es kann schwierig sein, die passende Schriftart f√ºr den Formelstil Ihres Dokuments oder Ihrer Publikation zu finden. Eine Liste der Schriftarten, die Matplotlib zur Verf√ºgung stehen, kann mit dem folgenden Codeschnipsel abgerufen werden, den ich hier gefunden habe.\n\nfrom IPython.display import HTML, display\n\ndef make_html(fontname):\n    return \"&lt;p&gt;{font}: &lt;span style='font-family:{font}; font-size: 24px;'&gt;{font}&lt;/p&gt;\".format(font=fontname)\n\ncode = \"\\n\".join([make_html(font) for font in sorted(set([f.name for f in font_manager.fontManager.ttflist]))])\n\ndisplay(HTML(\"&lt;div style='column-count: 2;'&gt;{}&lt;/div&gt;\".format(code)))\n\n.Aqua Kana: .Aqua Kana\n.CJK Symbols Fallback HK: .CJK Symbols Fallback HK\n.Keyboard: .Keyboard\n.New York: .New York\n.SF Arabic: .SF Arabic\n.SF Arabic Rounded: .SF Arabic Rounded\n.SF Armenian: .SF Armenian\n.SF Armenian Rounded: .SF Armenian Rounded\n.SF Camera: .SF Camera\n.SF Compact Rounded: .SF Compact Rounded\n.SF Georgian: .SF Georgian\n.SF Georgian Rounded: .SF Georgian Rounded\n.SF Hebrew: .SF Hebrew\n.SF Hebrew Rounded: .SF Hebrew Rounded\n.SF NS Mono: .SF NS Mono\n.SF NS Rounded: .SF NS Rounded\n.SF Soft Numeric: .SF Soft Numeric\n.ThonburiUI: .ThonburiUI\nAcademy Engraved LET: Academy Engraved LET\nAdelle Sans Devanagari: Adelle Sans Devanagari\nAkayaKanadaka: AkayaKanadaka\nAkayaTelivigala: AkayaTelivigala\nAl Bayan: Al Bayan\nAl Nile: Al Nile\nAl Tarikh: Al Tarikh\nAmerican Typewriter: American Typewriter\nAndale Mono: Andale Mono\nAnnai MN: Annai MN\nApple Braille: Apple Braille\nApple Chancery: Apple Chancery\nApple LiGothic: Apple LiGothic\nApple LiSung: Apple LiSung\nApple SD Gothic Neo: Apple SD Gothic Neo\nApple Symbols: Apple Symbols\nAppleGothic: AppleGothic\nAppleMyungjo: AppleMyungjo\nArial: Arial\nArial Black: Arial Black\nArial Hebrew: Arial Hebrew\nArial Narrow: Arial Narrow\nArial Rounded MT Bold: Arial Rounded MT Bold\nArial Unicode MS: Arial Unicode MS\nArima Koshi: Arima Koshi\nArima Madurai: Arima Madurai\nAthelas: Athelas\nAvenir: Avenir\nAvenir Next: Avenir Next\nAvenir Next Condensed: Avenir Next Condensed\nAyuthaya: Ayuthaya\nBM Dohyeon: BM Dohyeon\nBM Hanna 11yrs Old: BM Hanna 11yrs Old\nBM Hanna Air: BM Hanna Air\nBM Hanna Pro: BM Hanna Pro\nBM Jua: BM Jua\nBM Kirang Haerang: BM Kirang Haerang\nBM Yeonsung: BM Yeonsung\nBaghdad: Baghdad\nBai Jamjuree: Bai Jamjuree\nBaloo 2: Baloo 2\nBaloo Bhai 2: Baloo Bhai 2\nBaloo Bhaijaan: Baloo Bhaijaan\nBaloo Bhaina 2: Baloo Bhaina 2\nBaloo Chettan 2: Baloo Chettan 2\nBaloo Da 2: Baloo Da 2\nBaloo Paaji 2: Baloo Paaji 2\nBaloo Tamma 2: Baloo Tamma 2\nBaloo Tammudu 2: Baloo Tammudu 2\nBaloo Thambi 2: Baloo Thambi 2\nBangla MN: Bangla MN\nBangla Sangam MN: Bangla Sangam MN\nBaoli SC: Baoli SC\nBaskerville: Baskerville\nBeirut: Beirut\nBiauKaiHK: BiauKaiHK\nBig Caslon: Big Caslon\nBodoni 72: Bodoni 72\nBodoni 72 Oldstyle: Bodoni 72 Oldstyle\nBodoni 72 Smallcaps: Bodoni 72 Smallcaps\nBodoni Ornaments: Bodoni Ornaments\nBradley Hand: Bradley Hand\nBrush Script MT: Brush Script MT\nCambay Devanagari: Cambay Devanagari\nChakra Petch: Chakra Petch\nChalkboard: Chalkboard\nChalkboard SE: Chalkboard SE\nChalkduster: Chalkduster\nCharm: Charm\nCharmonman: Charmonman\nCharter: Charter\nCochin: Cochin\nComic Sans MS: Comic Sans MS\nCopperplate: Copperplate\nCorsiva Hebrew: Corsiva Hebrew\nCourier: Courier\nCourier New: Courier New\nDIN Alternate: DIN Alternate\nDIN Condensed: DIN Condensed\nDamascus: Damascus\nDecoType Naskh: DecoType Naskh\nDejaVu Sans: DejaVu Sans\nDejaVu Sans Display: DejaVu Sans Display\nDejaVu Sans Mono: DejaVu Sans Mono\nDejaVu Serif: DejaVu Serif\nDejaVu Serif Display: DejaVu Serif Display\nDevanagari MT: Devanagari MT\nDevanagari Sangam MN: Devanagari Sangam MN\nDidot: Didot\nDiwan Kufi: Diwan Kufi\nDiwan Thuluth: Diwan Thuluth\nEuphemia UCAS: Euphemia UCAS\nFahkwang: Fahkwang\nFarah: Farah\nFarisi: Farisi\nFutura: Futura\nGalvji: Galvji\nGeeza Pro: Geeza Pro\nGeneva: Geneva\nGeorgia: Georgia\nGill Sans: Gill Sans\nGotu: Gotu\nGujarati MT: Gujarati MT\nGujarati Sangam MN: Gujarati Sangam MN\nGungSeo: GungSeo\nGurmukhi MN: Gurmukhi MN\nGurmukhi MT: Gurmukhi MT\nGurmukhi Sangam MN: Gurmukhi Sangam MN\nHannotate SC: Hannotate SC\nHanziPen SC: HanziPen SC\nHeadLineA: HeadLineA\nHei: Hei\nHeiti TC: Heiti TC\nHelvetica: Helvetica\nHelvetica Neue: Helvetica Neue\nHerculanum: Herculanum\nHiragino Maru Gothic Pro: Hiragino Maru Gothic Pro\nHiragino Mincho ProN: Hiragino Mincho ProN\nHiragino Sans: Hiragino Sans\nHiragino Sans GB: Hiragino Sans GB\nHiragino Sans TC: Hiragino Sans TC\nHoefler Text: Hoefler Text\nHubballi: Hubballi\nITF Devanagari: ITF Devanagari\nImpact: Impact\nInaiMathi: InaiMathi\nIowan Old Style: Iowan Old Style\nJaini: Jaini\nJaini Purva: Jaini Purva\nK2D: K2D\nKai: Kai\nKailasa: Kailasa\nKaiti SC: Kaiti SC\nKannada MN: Kannada MN\nKannada Sangam MN: Kannada Sangam MN\nKatari: Katari\nKavivanar: Kavivanar\nKefa: Kefa\nKhmer MN: Khmer MN\nKhmer Sangam MN: Khmer Sangam MN\nKlee: Klee\nKoHo: KoHo\nKodchasan: Kodchasan\nKohinoor Bangla: Kohinoor Bangla\nKohinoor Devanagari: Kohinoor Devanagari\nKohinoor Gujarati: Kohinoor Gujarati\nKohinoor Telugu: Kohinoor Telugu\nKokonor: Kokonor\nKrub: Krub\nKrungthep: Krungthep\nKufiStandardGK: KufiStandardGK\nLahore Gurmukhi: Lahore Gurmukhi\nLantinghei SC: Lantinghei SC\nLao MN: Lao MN\nLao Sangam MN: Lao Sangam MN\nLava Devanagari: Lava Devanagari\nLava Kannada: Lava Kannada\nLava Telugu: Lava Telugu\nLiHei Pro: LiHei Pro\nLiSong Pro: LiSong Pro\nLibian SC: Libian SC\nLingWai SC: LingWai SC\nLingWai TC: LingWai TC\nLucida Grande: Lucida Grande\nLuminari: Luminari\nMaku: Maku\nMalayalam MN: Malayalam MN\nMalayalam Sangam MN: Malayalam Sangam MN\nMali: Mali\nMarion: Marion\nMarker Felt: Marker Felt\nMenlo: Menlo\nMicrosoft Sans Serif: Microsoft Sans Serif\nMishafi: Mishafi\nMishafi Gold: Mishafi Gold\nModak: Modak\nMonaco: Monaco\nMshtakan: Mshtakan\nMukta: Mukta\nMukta Mahee: Mukta Mahee\nMukta Malar: Mukta Malar\nMukta Vaani: Mukta Vaani\nMuna: Muna\nMyanmar MN: Myanmar MN\nMyanmar Sangam MN: Myanmar Sangam MN\nNadeem: Nadeem\nNanum Brush Script: Nanum Brush Script\nNanum Gothic: Nanum Gothic\nNanum Myeongjo: Nanum Myeongjo\nNew Peninim MT: New Peninim MT\nNiramit: Niramit\nNoteworthy: Noteworthy\nNoto Nastaliq Urdu: Noto Nastaliq Urdu\nNoto Sans Adlam: Noto Sans Adlam\nNoto Sans Armenian: Noto Sans Armenian\nNoto Sans Avestan: Noto Sans Avestan\nNoto Sans Bamum: Noto Sans Bamum\nNoto Sans Bassa Vah: Noto Sans Bassa Vah\nNoto Sans Batak: Noto Sans Batak\nNoto Sans Bhaiksuki: Noto Sans Bhaiksuki\nNoto Sans Brahmi: Noto Sans Brahmi\nNoto Sans Buginese: Noto Sans Buginese\nNoto Sans Buhid: Noto Sans Buhid\nNoto Sans Canadian Aboriginal: Noto Sans Canadian Aboriginal\nNoto Sans Carian: Noto Sans Carian\nNoto Sans Caucasian Albanian: Noto Sans Caucasian Albanian\nNoto Sans Chakma: Noto Sans Chakma\nNoto Sans Cham: Noto Sans Cham\nNoto Sans Coptic: Noto Sans Coptic\nNoto Sans Cuneiform: Noto Sans Cuneiform\nNoto Sans Cypriot: Noto Sans Cypriot\nNoto Sans Duployan: Noto Sans Duployan\nNoto Sans Egyptian Hieroglyphs: Noto Sans Egyptian Hieroglyphs\nNoto Sans Elbasan: Noto Sans Elbasan\nNoto Sans Glagolitic: Noto Sans Glagolitic\nNoto Sans Gothic: Noto Sans Gothic\nNoto Sans Gunjala Gondi: Noto Sans Gunjala Gondi\nNoto Sans Hanifi Rohingya: Noto Sans Hanifi Rohingya\nNoto Sans Hanunoo: Noto Sans Hanunoo\nNoto Sans Hatran: Noto Sans Hatran\nNoto Sans Imperial Aramaic: Noto Sans Imperial Aramaic\nNoto Sans Inscriptional Pahlavi: Noto Sans Inscriptional Pahlavi\nNoto Sans Inscriptional Parthian: Noto Sans Inscriptional Parthian\nNoto Sans Javanese: Noto Sans Javanese\nNoto Sans Kaithi: Noto Sans Kaithi\nNoto Sans Kannada: Noto Sans Kannada\nNoto Sans Kayah Li: Noto Sans Kayah Li\nNoto Sans Kharoshthi: Noto Sans Kharoshthi\nNoto Sans Khojki: Noto Sans Khojki\nNoto Sans Khudawadi: Noto Sans Khudawadi\nNoto Sans Lepcha: Noto Sans Lepcha\nNoto Sans Limbu: Noto Sans Limbu\nNoto Sans Linear A: Noto Sans Linear A\nNoto Sans Linear B: Noto Sans Linear B\nNoto Sans Lisu: Noto Sans Lisu\nNoto Sans Lycian: Noto Sans Lycian\nNoto Sans Lydian: Noto Sans Lydian\nNoto Sans Mahajani: Noto Sans Mahajani\nNoto Sans Mandaic: Noto Sans Mandaic\nNoto Sans Manichaean: Noto Sans Manichaean\nNoto Sans Marchen: Noto Sans Marchen\nNoto Sans Masaram Gondi: Noto Sans Masaram Gondi\nNoto Sans Meetei Mayek: Noto Sans Meetei Mayek\nNoto Sans Mende Kikakui: Noto Sans Mende Kikakui\nNoto Sans Meroitic: Noto Sans Meroitic\nNoto Sans Miao: Noto Sans Miao\nNoto Sans Modi: Noto Sans Modi\nNoto Sans Mongolian: Noto Sans Mongolian\nNoto Sans Mro: Noto Sans Mro\nNoto Sans Multani: Noto Sans Multani\nNoto Sans Myanmar: Noto Sans Myanmar\nNoto Sans NKo: Noto Sans NKo\nNoto Sans Nabataean: Noto Sans Nabataean\nNoto Sans New Tai Lue: Noto Sans New Tai Lue\nNoto Sans Newa: Noto Sans Newa\nNoto Sans Ol Chiki: Noto Sans Ol Chiki\nNoto Sans Old Hungarian: Noto Sans Old Hungarian\nNoto Sans Old Italic: Noto Sans Old Italic\nNoto Sans Old North Arabian: Noto Sans Old North Arabian\nNoto Sans Old Permic: Noto Sans Old Permic\nNoto Sans Old Persian: Noto Sans Old Persian\nNoto Sans Old South Arabian: Noto Sans Old South Arabian\nNoto Sans Old Turkic: Noto Sans Old Turkic\nNoto Sans Oriya: Noto Sans Oriya\nNoto Sans Osage: Noto Sans Osage\nNoto Sans Osmanya: Noto Sans Osmanya\nNoto Sans Pahawh Hmong: Noto Sans Pahawh Hmong\nNoto Sans Palmyrene: Noto Sans Palmyrene\nNoto Sans Pau Cin Hau: Noto Sans Pau Cin Hau\nNoto Sans PhagsPa: Noto Sans PhagsPa\nNoto Sans Phoenician: Noto Sans Phoenician\nNoto Sans Psalter Pahlavi: Noto Sans Psalter Pahlavi\nNoto Sans Rejang: Noto Sans Rejang\nNoto Sans Samaritan: Noto Sans Samaritan\nNoto Sans Saurashtra: Noto Sans Saurashtra\nNoto Sans Sharada: Noto Sans Sharada\nNoto Sans Siddham: Noto Sans Siddham\nNoto Sans Sora Sompeng: Noto Sans Sora Sompeng\nNoto Sans Sundanese: Noto Sans Sundanese\nNoto Sans Syloti Nagri: Noto Sans Syloti Nagri\nNoto Sans Syriac: Noto Sans Syriac\nNoto Sans Tagalog: Noto Sans Tagalog\nNoto Sans Tagbanwa: Noto Sans Tagbanwa\nNoto Sans Tai Le: Noto Sans Tai Le\nNoto Sans Tai Tham: Noto Sans Tai Tham\nNoto Sans Tai Viet: Noto Sans Tai Viet\nNoto Sans Takri: Noto Sans Takri\nNoto Sans Thaana: Noto Sans Thaana\nNoto Sans Tifinagh: Noto Sans Tifinagh\nNoto Sans Tirhuta: Noto Sans Tirhuta\nNoto Sans Ugaritic: Noto Sans Ugaritic\nNoto Sans Vai: Noto Sans Vai\nNoto Sans Wancho: Noto Sans Wancho\nNoto Sans Warang Citi: Noto Sans Warang Citi\nNoto Sans Yi: Noto Sans Yi\nNoto Serif Ahom: Noto Serif Ahom\nNoto Serif Balinese: Noto Serif Balinese\nNoto Serif Hmong Nyiakeng: Noto Serif Hmong Nyiakeng\nNoto Serif Kannada: Noto Serif Kannada\nNoto Serif Myanmar: Noto Serif Myanmar\nNoto Serif Yezidi: Noto Serif Yezidi\nOctober Compressed Devanagari: October Compressed Devanagari\nOctober Compressed Tamil: October Compressed Tamil\nOctober Condensed Devanagari: October Condensed Devanagari\nOctober Condensed Tamil: October Condensed Tamil\nOctober Devanagari: October Devanagari\nOctober Tamil: October Tamil\nOptima: Optima\nOriya MN: Oriya MN\nOriya Sangam MN: Oriya Sangam MN\nOsaka: Osaka\nPCMyungjo: PCMyungjo\nPSL Ornanong Pro: PSL Ornanong Pro\nPT Mono: PT Mono\nPT Sans: PT Sans\nPT Serif: PT Serif\nPT Serif Caption: PT Serif Caption\nPadyakke Expanded One: Padyakke Expanded One\nPalatino: Palatino\nPapyrus: Papyrus\nParty LET: Party LET\nPhosphate: Phosphate\nPilGi: PilGi\nPlantagenet Cherokee: Plantagenet Cherokee\nRaanana: Raanana\nRockwell: Rockwell\nSTFangsong: STFangsong\nSTHeiti: STHeiti\nSTIX Two Math: STIX Two Math\nSTIX Two Text: STIX Two Text\nSTIXGeneral: STIXGeneral\nSTIXIntegralsD: STIXIntegralsD\nSTIXIntegralsSm: STIXIntegralsSm\nSTIXIntegralsUp: STIXIntegralsUp\nSTIXIntegralsUpD: STIXIntegralsUpD\nSTIXIntegralsUpSm: STIXIntegralsUpSm\nSTIXNonUnicode: STIXNonUnicode\nSTIXSizeFiveSym: STIXSizeFiveSym\nSTIXSizeFourSym: STIXSizeFourSym\nSTIXSizeOneSym: STIXSizeOneSym\nSTIXSizeThreeSym: STIXSizeThreeSym\nSTIXSizeTwoSym: STIXSizeTwoSym\nSTIXVariants: STIXVariants\nSama Devanagari: Sama Devanagari\nSama Gujarati: Sama Gujarati\nSama Gurmukhi: Sama Gurmukhi\nSama Kannada: Sama Kannada\nSama Malayalam: Sama Malayalam\nSama Tamil: Sama Tamil\nSana: Sana\nSarabun: Sarabun\nSathu: Sathu\nSavoye LET: Savoye LET\nSeravek: Seravek\nShobhika: Shobhika\nShree Devanagari 714: Shree Devanagari 714\nSignPainter: SignPainter\nSilom: Silom\nSimSong: SimSong\nSinhala MN: Sinhala MN\nSinhala Sangam MN: Sinhala Sangam MN\nSkia: Skia\nSnell Roundhand: Snell Roundhand\nSongti SC: Songti SC\nSrisakdi: Srisakdi\nSukhumvit Set: Sukhumvit Set\nSuperclarendon: Superclarendon\nSymbol: Symbol\nSystem Font: System Font\nTahoma: Tahoma\nTamil MN: Tamil MN\nTamil Sangam MN: Tamil Sangam MN\nTelugu MN: Telugu MN\nTelugu Sangam MN: Telugu Sangam MN\nThonburi: Thonburi\nTimes: Times\nTimes New Roman: Times New Roman\nTiro Bangla: Tiro Bangla\nTiro Devanagari Hindi: Tiro Devanagari Hindi\nTiro Devanagari Marathi: Tiro Devanagari Marathi\nTiro Devanagari Sanskrit: Tiro Devanagari Sanskrit\nTiro Gurmukhi: Tiro Gurmukhi\nTiro Kannada: Tiro Kannada\nTiro Tamil: Tiro Tamil\nTiro Telugu: Tiro Telugu\nToppan Bunkyu Gothic: Toppan Bunkyu Gothic\nToppan Bunkyu Midashi Gothic: Toppan Bunkyu Midashi Gothic\nToppan Bunkyu Midashi Mincho: Toppan Bunkyu Midashi Mincho\nToppan Bunkyu Mincho: Toppan Bunkyu Mincho\nTrattatello: Trattatello\nTrebuchet MS: Trebuchet MS\nTsukushi A Round Gothic: Tsukushi A Round Gothic\nTsukushi B Round Gothic: Tsukushi B Round Gothic\nVerdana: Verdana\nWaseem: Waseem\nWawati SC: Wawati SC\nWawati TC: Wawati TC\nWebdings: Webdings\nWingdings: Wingdings\nWingdings 2: Wingdings 2\nWingdings 3: Wingdings 3\nXingkai SC: Xingkai SC\nYuGothic: YuGothic\nYuKyokasho Yoko: YuKyokasho Yoko\nYuMincho: YuMincho\nYuanti SC: Yuanti SC\nYuppy SC: Yuppy SC\nYuppy TC: Yuppy TC\nZapf Dingbats: Zapf Dingbats\nZapfino: Zapfino\ncmb10: cmb10\ncmex10: cmex10\ncmmi10: cmmi10\ncmr10: cmr10\ncmss10: cmss10\ncmsy10: cmsy10\ncmtt10: cmtt10\n\n\nFalls Sie Ihr Dokument in LaTeX schreiben, k√∂nnten die cmXXXX-Schriften f√ºr Sie von Interesse sein, da sie den in LaTeX-Dokumenten verwendeten Schriften entsprechen. Hier ist ein Beispiel:\n\ncmfont = font_manager.FontProperties(fname=mpl.get_data_path() + '/fonts/ttf/cmr10.ttf')\nplt.rcParams.update({'font.size': 12,\n                     'axes.titlesize': 12,\n                     'axes.labelsize': 12,\n                     'axes.labelpad': 5,\n                     'lines.linewidth': 1,\n                     'lines.markersize': 10,\n                     'xtick.labelsize' : 10,\n                     'ytick.labelsize' : 10,\n                     'xtick.top' : True,\n                     'xtick.direction' : 'in',\n                     'ytick.right' : True,\n                     'ytick.direction' : 'in',\n                     'font.family' : 'serif',\n                     'font.serif' : cmfont.get_name(),\n                     \"axes.formatter.use_mathtext\": True,\n                     'text.usetex': True,\n                     'mathtext.fontset' : 'cm'\n                    })\n\n\nx=np.linspace(0,np.pi,100)\n\n\nplt.figure(figsize=get_size(6,5),dpi=150)\nplt.plot(x,np.sin(x))\nplt.xlabel(r\"velocity $v$\")\nplt.ylabel(r\"position $r$\")\nplt.show()"
  },
  {
    "objectID": "seminars/seminar06/publication-ready-figures.html#ein-dokument-vorbereiten",
    "href": "seminars/seminar06/publication-ready-figures.html#ein-dokument-vorbereiten",
    "title": "Erstellung ver√∂ffentlichungsreifer Diagramme",
    "section": "Ein Dokument vorbereiten",
    "text": "Ein Dokument vorbereiten\nWenn man ein Dokument (Bachelorarbeit z.B.) erstellt, ist es n√ºtzlich, seine Daten und Texte geschickt zur organisieren, um sich Arbeit zu ersparen. Hier ist ein Beipiel,\n\ndisplay_tree(\"Report\")\n\nReport\nException Occurred! Failed to Generate Tree:: FileNotFoundError: [Errno 2] No such file or directory: 'Report'\n\n\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_1432/929351223.py:1: DeprecationWarning: The `display_tree` Function is Deprecated and will be Removed in a Future Release. Please use `DirectoryTree` Instead. End of Life Date is \"31st December 2024\".\n  display_tree(\"Report\")"
  },
  {
    "objectID": "seminars/seminar06/publication-ready-figures.html#komplexe-layouts-mit-gridspec",
    "href": "seminars/seminar06/publication-ready-figures.html#komplexe-layouts-mit-gridspec",
    "title": "Erstellung ver√∂ffentlichungsreifer Diagramme",
    "section": "Komplexe Layouts mit GridSpec",
    "text": "Komplexe Layouts mit GridSpec\nWenn Sie mehrere Teildiagramme in einer Abbildung kombinieren m√∂chten, bietet GridSpec eine flexible M√∂glichkeit, Layouts zu erstellen, bei denen einzelne Achsen unterschiedliche Gr√∂√üen haben k√∂nnen. GridSpec definiert ein Gitter, √ºber das sich Achsen √ºber mehrere Zeilen oder Spalten erstrecken k√∂nnen.\nHier ist ein einfaches Beispiel mit drei Teildiagrammen:\n\nfrom matplotlib import gridspec\n\n# Figurgr√∂√üe f√ºr zweispaltiges Layout\nfig = plt.figure(figsize=get_size(14, 10), dpi=150)\n\n# GridSpec mit 2 Zeilen und 2 Spalten erstellen\ngs = gridspec.GridSpec(2, 2, figure=fig, hspace=0.5, wspace=0.3)\n\n# Daten f√ºr Plots vorbereiten\nx = np.linspace(0, 2*np.pi, 200)\n\n# Erster Plot: nimmt beide Spalten der ersten Zeile ein\nax1 = fig.add_subplot(gs[0, :])\nax1.plot(x, np.sin(x), 'b-', label='sin(x)')\nax1.set_xlabel(r\"Winkel $\\theta$ [rad]\")\nax1.set_ylabel(r\"$\\sin(\\theta)$\")\nax1.legend()\n\n\n# Zweiter Plot: linke untere Zelle\nax2 = fig.add_subplot(gs[1, 0])\nax2.plot(x, np.cos(x), 'r-', label='cos(x)')\nax2.set_xlabel(r\"Winkel $\\theta$ [rad]\")\nax2.set_ylabel(r\"$\\cos(\\theta)$\")\nax2.legend()\n\n\n# Dritter Plot: rechte untere Zelle\nax3 = fig.add_subplot(gs[1, 1])\nax3.plot(x, np.tan(x), 'g-', label='tan(x)')\nax3.set_xlabel(r\"Winkel $\\theta$ [rad]\")\nax3.set_ylabel(r\"$\\tan(\\theta)$\")\nax3.set_ylim(-3, 3)\nax3.legend()\n\n\nplt.savefig(\"figure_gridspec.pdf\", bbox_inches='tight', transparent=True)\nplt.show()\n\n\n\n\n\n\n\n\nIn diesem Beispiel:\n\nDer erste Plot (ax1) erstreckt sich √ºber die gesamte obere Zeile mit gs[0, :]\nDer zweite Plot (ax2) befindet sich in der linken unteren Zelle mit gs[1, 0]\nDer dritte Plot (ax3) befindet sich in der rechten unteren Zelle mit gs[1, 1]\nDie Parameter hspace und wspace steuern den vertikalen und horizontalen Abstand zwischen den Teildiagrammen\n\nDiese Methode ist besonders n√ºtzlich, wenn Sie eine Hauptabbildung mit mehreren kleineren Detail-Plots kombinieren m√∂chten, w√§hrend Sie gleichzeitig die exakte Gr√∂√üe der gesamten Abbildung kontrollieren."
  },
  {
    "objectID": "seminars/seminar07/NoteBook.html",
    "href": "seminars/seminar07/NoteBook.html",
    "title": "Seminar 7",
    "section": "",
    "text": "Wir wollen etwas das aufsummieren von Werten √ºbern.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\n\nDas Ohmsche Gesetz ist gegeben durch:\n\\[V=IR\\]\n\\[S_{xy} = \\sum \\frac{x_i y_i}{\\sigma_i^2}\\]\n\\[S_{xx} = \\sum \\frac{x_i^2}{\\sigma_i^2}\\]\n\\[\\begin{equation}\n\\chi^{2}=\\sum_{i=1}^{N}\\left( \\frac{y_{i}-f(x_{i},\\mathbf{a})}{\\sigma_{i}}\\right)^2\n\\end{equation}\\]\n\nsigma=2\ncurrent_data = np.arange(0.1,1.1,0.1)\nresistance_true = 47.0\nvoltage_data = current_data*resistance_true+sigma*np.random.randn(len(current_data))\nvoltage_err = sigma*np.ones(len(current_data))\n#sigma = voltage_err\n\n\nvoltage_err\n\narray([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])\n\n\n\nS_xy=(current_data*voltage_data/voltage_err**2).sum()\nS_xx=(current_data*current_data/voltage_err**2).sum()\n\nR=S_xy/S_xx\nprint(R,S_xy,S_xx)\n\n46.47589647661416 44.73305035874114 0.9625000000000001\n\n\n\n(((voltage_data-current_data*R)/voltage_err)**2).sum()\n\nnp.float64(5.44792900559763)\n\n\n\nplt.figure(figsize=(4,3))\n\nplt.plot(current_data,voltage_data,\"o\")\nplt.plot(current_data,current_data*R)\nplt.plot(current_data,current_data*resistance_true,\"k--\")\n\nplt.xlabel(\"I\")\nplt.ylabel(\"V\")\nplt.show()"
  },
  {
    "objectID": "lectures/lecture02/functions.html",
    "href": "lectures/lecture02/functions.html",
    "title": "Python Code Structures",
    "section": "",
    "text": "Functions are reusable blocks of code that can be executed multiple times from different parts of your program. They help in organizing code, making it more readable, and reducing redundancy. Functions can take input arguments and return output values.\n\nDefining a FunctionCalling a Function\n\n\nA function in Python is defined using the def keyword followed by the name of the function, which is usually descriptive and indicates what the function does. The parameters inside the parentheses indicate what data the function expects to receive. The -&gt; symbol is used to specify the return type of the function.\nHere‚Äôs an example:\n\n\n\n\n\n\n\n\nFunctions can be called by specifying the name of the function followed by parentheses containing the arguments. The arguments passed to the function should match the number and type of parameters defined in the function. Here‚Äôs an example:"
  },
  {
    "objectID": "lectures/lecture02/functions.html#functions",
    "href": "lectures/lecture02/functions.html#functions",
    "title": "Python Code Structures",
    "section": "",
    "text": "Functions are reusable blocks of code that can be executed multiple times from different parts of your program. They help in organizing code, making it more readable, and reducing redundancy. Functions can take input arguments and return output values.\n\nDefining a FunctionCalling a Function\n\n\nA function in Python is defined using the def keyword followed by the name of the function, which is usually descriptive and indicates what the function does. The parameters inside the parentheses indicate what data the function expects to receive. The -&gt; symbol is used to specify the return type of the function.\nHere‚Äôs an example:\n\n\n\n\n\n\n\n\nFunctions can be called by specifying the name of the function followed by parentheses containing the arguments. The arguments passed to the function should match the number and type of parameters defined in the function. Here‚Äôs an example:"
  },
  {
    "objectID": "lectures/lecture02/functions.html#loops",
    "href": "lectures/lecture02/functions.html#loops",
    "title": "Python Code Structures",
    "section": "Loops",
    "text": "Loops\nLoops are used to execute a block of code repeatedly. There are two main types of loops in Python: for loops and while loops.\n\nFor LoopWhile Loop\n\n\nA for loop in Python is used to iterate over a sequence (such as a list or string) and execute a block of code for each item in the sequence. Here‚Äôs an example:\n\n\n\n\n\n\n\n\nA while loop in Python is used to execute a block of code while a certain condition is met. The loop continues as long as the condition is true. Here‚Äôs an example:"
  },
  {
    "objectID": "lectures/lecture02/functions.html#conditional-statements",
    "href": "lectures/lecture02/functions.html#conditional-statements",
    "title": "Python Code Structures",
    "section": "Conditional Statements",
    "text": "Conditional Statements\nConditional statements are used to control the flow of your program based on conditions. The main conditional statements in Python are if, else, and elif.\n\nIf StatementElse StatementElif Statement\n\n\nAn if statement in Python is used to execute a block of code if a certain condition is met. Here‚Äôs an example:\n\n\n\n\n\n\n\n\nAn else statement in Python is used to execute a block of code if the condition in an if statement is not met. Here‚Äôs an example:\n\n\n\n\n\n\n\n\nAn elif statement in Python is used to execute a block of code if the condition in an if statement is not met but under an extra condition. Here‚Äôs an example:"
  },
  {
    "objectID": "lectures/lecture02/functions.html#practice-exercises",
    "href": "lectures/lecture02/functions.html#practice-exercises",
    "title": "Python Code Structures",
    "section": "Practice Exercises üéØ",
    "text": "Practice Exercises üéØ\n\nExercise 1: Distance Calculator Function\nWrite a function called calculate_distance that takes velocity (m/s) and time (s) as parameters and returns the distance traveled using the formula: \\(d = v \\cdot t\\)\n\nFunction should take two parameters: velocity and time\nReturn the calculated distance\nTest it with velocity = 15 m/s and time = 4 s\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndef calculate_distance(velocity: float, time: float) -&gt; float:\n    return velocity * time\n\n# Test the function\ndistance = calculate_distance(15, 4)\nprint(f\"Distance traveled: {distance} meters\")\n\n\n\n\n\nExercise 2: Even Numbers\nWrite a function called print_even_numbers that takes a number n as input and prints all even numbers from 2 to n (inclusive) using a for loop.\n\nUse a for loop with range()\nOnly print even numbers\nTest it with n = 20\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndef print_even_numbers(n: int):\n    for i in range(2, n + 1, 2):\n        print(i)\n\n# Alternative solution using conditional\ndef print_even_numbers_alt(n: int):\n    for i in range(1, n + 1):\n        if i % 2 == 0:\n            print(i)\n\n# Test the function\nprint_even_numbers(20)\n\n\n\n\n\nExercise 3: Temperature State\nWrite a function called water_state that takes a temperature in Celsius and returns the physical state of water:\n\nReturn ‚Äúsolid‚Äù if temperature &lt; 0\nReturn ‚Äúliquid‚Äù if 0 ‚â§ temperature &lt; 100\nReturn ‚Äúgas‚Äù if temperature ‚â• 100\nTest it with temperatures: -5, 25, 100, 150\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndef water_state(temperature: float) -&gt; str:\n    if temperature &lt; 0:\n        return \"solid\"\n    elif temperature &lt; 100:\n        return \"liquid\"\n    else:\n        return \"gas\"\n\n# Test the function\ntest_temps = [-5, 25, 100, 150]\nfor temp in test_temps:\n    state = water_state(temp)\n    print(f\"At {temp}¬∞C, water is {state}\")\n\n\n\n\n\nExercise 4: Kinetic Energy Calculator\nWrite a function called kinetic_energy that calculates the kinetic energy of an object using \\(E_k = \\frac{1}{2}mv^2\\). The function should:\n\nTake mass (kg) and velocity (m/s) as parameters\nReturn the kinetic energy in Joules\nPrint a warning message if velocity exceeds 100 m/s\nTest it with mass = 1000 kg and velocities from 0 to 120 m/s in steps of 30\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndef kinetic_energy(mass: float, velocity: float) -&gt; float:\n    if velocity &gt; 100:\n        print(f\"Warning: High velocity detected ({velocity} m/s)!\")\n\n    energy = 0.5 * mass * velocity**2\n    return energy\n\n# Test the function\nmass = 1000  # kg\nvelocities = [0, 30, 60, 90, 120]\n\nfor v in velocities:\n    energy = kinetic_energy(mass, v)\n    print(f\"v = {v} m/s: E_k = {energy/1000:.2f} kJ\")\n\n\n\n\n\nExercise 5: Factorial Calculator\nWrite a function called factorial that calculates the factorial of a number n (n!) using a while loop.\n\nUse a while loop (not a for loop)\nReturn the factorial value\nHint: 5! = 5 √ó 4 √ó 3 √ó 2 √ó 1 = 120\nTest it with n = 5 and n = 10\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndef factorial(n: int) -&gt; int:\n    result = 1\n    i = 1\n    while i &lt;= n:\n        result = result * i\n        i += 1\n    return result\n\n# Test the function\nprint(f\"5! = {factorial(5)}\")\nprint(f\"10! = {factorial(10)}\")\n\n\n\n\n\nExercise 6: FizzBuzz (Challenge!)\nWrite a function called fizzbuzz that prints numbers from 1 to n, but:\n\nFor multiples of 3, print ‚ÄúFizz‚Äù instead of the number\nFor multiples of 5, print ‚ÄúBuzz‚Äù instead of the number\nFor multiples of both 3 and 5, print ‚ÄúFizzBuzz‚Äù\nTest it with n = 20\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndef fizzbuzz(n: int):\n    for i in range(1, n + 1):\n        if i % 3 == 0 and i % 5 == 0:\n            print(\"FizzBuzz\")\n        elif i % 3 == 0:\n            print(\"Fizz\")\n        elif i % 5 == 0:\n            print(\"Buzz\")\n        else:\n            print(i)\n\n# Test the function\nfizzbuzz(20)"
  },
  {
    "objectID": "lectures/lecture02/functions.html#summary",
    "href": "lectures/lecture02/functions.html#summary",
    "title": "Python Code Structures",
    "section": "Summary ‚úÖ",
    "text": "Summary ‚úÖ\nEssential Python Structures:\n# Functions\ndef function_name(parameter: type) -&gt; return_type:\n    # code here\n    return value\n\n# For Loop\nfor i in range(start, stop, step):\n    # code here\n\n# While Loop\nwhile condition:\n    # code here\n\n# Conditionals\nif condition:\n    # code here\nelif another_condition:\n    # code here\nelse:\n    # code here\nRemember: Practice these structures - they are the building blocks of all your Python programs! üöÄ"
  },
  {
    "objectID": "lectures/lecture02/datatypes.html",
    "href": "lectures/lecture02/datatypes.html",
    "title": "Data Types in Python",
    "section": "",
    "text": "It‚Äôs time to look at different data types we may find useful in our course. Besides the number types mentioned previously, there are also other types like strings, lists, tuples, dictionaries and sets.\nEach of these data types has a number of connected methods (functions) which allow to manipulate the data contained in a variable. If you want to know which methods are available for a certain object use the command dir, e.g.\nThe following few cells will give you a short introduction into each type."
  },
  {
    "objectID": "lectures/lecture02/datatypes.html#strings",
    "href": "lectures/lecture02/datatypes.html#strings",
    "title": "Data Types in Python",
    "section": "Strings",
    "text": "Strings\nStrings are lists of keyboard characters as well as other characters not on your keyboard. They are useful for printing results on the screen, during reading and writing of data.\n\n\n\n\n\n\n\n\n\n\n\n\nString can be concatenated using the + operator.\n\n\n\n\n\n\n\n\n\n\n\n\nAs strings are lists, each character in a string can be accessed by addressing the position in the string (see Lists section)\n\n\n\n\n\n\nStrings can also be made out of numbers.\n\n\n\n\n\n\nIf you want to obtain a number of a string, you can use what is known as type casting. Using type casting you may convert the string or any other data type into a different type if this is possible. To find out if a string is a pure number you may use the str.isnumeric method. For the above string, we may want to do a conversion to the type int by typing:\n\n\n\n\n\n\n\n\n\n\n\n\nThere are a number of methods connected to the string data type. Usually the relate to formatting or finding sub-strings. Formatting will be a topic in our next lecture. Here we just refer to one simple find example."
  },
  {
    "objectID": "lectures/lecture02/datatypes.html#lists",
    "href": "lectures/lecture02/datatypes.html#lists",
    "title": "Data Types in Python",
    "section": "Lists",
    "text": "Lists\nLists have a variety of uses. They are useful, for example, in various bookkeeping tasks that arise in computer programming. Like arrays, they are sometimes used to store data. However, lists do not have the specialized properties and tools that make arrays so powerful for scientific computing. So in general, we prefer arrays to lists for working with scientific data. For other tasks, lists work just fine and can even be preferable to arrays.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndividual elements in a list can be accessed by the variable name and the number (index) of the list element put in square brackets. Note that the index for the elements start at 0 for the first element (left).\n\n\n\n\n\n\nIndices in Python\n\n\n\nThe first element of a list or array is accessed by the index 0. If the array has N elements, the last entries index is N-1.\n\n\n\n\n\n\n\n\nElements may be also accessed from the back by nagative indices. b[-1] denotes the last element in the list and b[-2], the element before the last.\n\n\n\n\n\n\n\n\n\n\n\n\nThe length of a list can be obtained by the len command if you need the number of elements in the list for your calculations.\n\n\n\n\n\n\nThere are powerful ways to iterate through a list and also through arrays in form of iterator. This is called list comprehension. We will talk about them later in more detail. Here is an example, which shows the powerful options you have in Python.\n\n\n\n\n\n\nIndividual elements in a list can be replaced by assigning a new value to them\n\n\n\n\n\n\n\n\n\n\n\n\nLists can be concatanated by the + operator\n\n\n\n\n\n\nA very useful feature for lists in python is the slicing of lists. Slicing means, that we access only a range of elements in the list, i.e.¬†element 3 to 7. This is done by inserting the starting and the ending element number separated by a colon (:) in the square brackets. The index numbers can be positive or negative again.\n\n\n\n\n\n\nInserting a second colon behind the ending element index together with a thrid number allows even to select only ever second or third element from a list.\n\n\n\n\n\n\nIt is sometimes also useful to reverse a list. This can be easily done with the reverse command.\n\n\n\n\n\n\nLists may be created in different ways. An empty list can be created by assigning emtpy square brackets to a variable name. You can append elements to the list with the help of the append command which has to be added to the variable name as shown below. This way of adding a particular function, which is part of a certain variable class is part of object oriented programming.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA list of numbers can be easily created by the range() command.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLists (and also tuples below) can be multidimensional as well, i.e.¬†for an image. The individual elements may then be addressed by supplying two indices in two square brackets."
  },
  {
    "objectID": "lectures/lecture02/datatypes.html#tuples",
    "href": "lectures/lecture02/datatypes.html#tuples",
    "title": "Data Types in Python",
    "section": "Tuples",
    "text": "Tuples\nTuples are also list, but immutable. That means, if a tuple has been once defined, it cannot be changed. Try to change an element to see the result.\n\n\n\n\n\n\nTuples may be unpacked, e.g.¬†its values may be assigned to normal variables in the following way"
  },
  {
    "objectID": "lectures/lecture02/datatypes.html#dictionaries",
    "href": "lectures/lecture02/datatypes.html#dictionaries",
    "title": "Data Types in Python",
    "section": "Dictionaries",
    "text": "Dictionaries\nDictionaries are like lists, but the elements of dictionaries are accessed in a different way than for lists. The elements of lists and arrays are numbered consecutively, and to access an element of a list or an array, you simply refer to the number corresponding to its position in the sequence. The elements of dictionaries are accessed by ‚Äúkeys‚Äù, which can be either strings or (arbitrary) integers (in no particular order). Dictionaries are an important part of core Python. However, we do not make much use of them in this introduction to scientific Python, so our discussion of them is limited."
  },
  {
    "objectID": "lectures/lecture02/datatypes.html#sets",
    "href": "lectures/lecture02/datatypes.html#sets",
    "title": "Data Types in Python",
    "section": "Sets",
    "text": "Sets\nSets are like lists but have immutable unique entries, which means the elements can not be changed once defined. An emtpy set is created by the set() method.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou may add elements to a set with the add method:\nYou may also remove objects with the discard method:\n\n\n\n\n\n\nYou may also apply a variety of operation to sets checking their intersection, union or difference."
  },
  {
    "objectID": "lectures/lecture02/datatypes.html#practice-exercises",
    "href": "lectures/lecture02/datatypes.html#practice-exercises",
    "title": "Data Types in Python",
    "section": "Practice Exercises üéØ",
    "text": "Practice Exercises üéØ\n\nExercise 1: String Manipulation\nCreate a string variable containing your full name. Then:\n\nExtract your first name using slicing\nConvert your last name to uppercase\nCount how many times the letter ‚Äòa‚Äô (or ‚ÄòA‚Äô) appears in your full name\nPrint all results with appropriate labels\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nfull_name = \"Albert Einstein\"\n\n# Extract first name (assuming space separates first and last)\nspace_index = full_name.find(' ')\nfirst_name = full_name[:space_index]\n\n# Extract and uppercase last name\nlast_name = full_name[space_index+1:].upper()\n\n# Count letter 'a' (case-insensitive)\ncount_a = full_name.lower().count('a')\n\nprint(f\"First name: {first_name}\")\nprint(f\"Last name (uppercase): {last_name}\")\nprint(f\"Number of 'a' letters: {count_a}\")\n\n\n\n\n\nExercise 2: Working with Lists\nCreate a list of temperatures in Celsius: [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\nThen:\n\nUse list slicing to get only the temperatures from index 2 to 7\nUse list comprehension to convert all temperatures to Fahrenheit: \\(F = \\frac{9}{5}C + 32\\)\nCreate a reversed version of the original list\nPrint all results\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n# Create temperature list\ncelsius = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n\n# Slice temperatures from index 2 to 7\ntemp_slice = celsius[2:8]\nprint(f\"Sliced temperatures: {temp_slice}\")\n\n# Convert to Fahrenheit using list comprehension\nfahrenheit = [(9/5)*c + 32 for c in celsius]\nprint(f\"Fahrenheit temperatures: {fahrenheit}\")\n\n# Reverse the list\ncelsius_reversed = celsius.copy()\ncelsius_reversed.reverse()\nprint(f\"Reversed list: {celsius_reversed}\")\n\n# Alternative: use slicing to reverse\ncelsius_reversed_alt = celsius[::-1]\nprint(f\"Reversed (using slicing): {celsius_reversed_alt}\")\n\n\n\n\n\nExercise 3: Physics Constants Dictionary\nCreate a dictionary called physics_constants that stores:\n\n'c': speed of light = 299792458 (m/s)\n'h': Planck constant = 6.62607015e-34 (J‚ãÖs)\n'g': gravitational acceleration = 9.81 (m/s¬≤)\n'k_B': Boltzmann constant = 1.380649e-23 (J/K)\n\nThen: - Access and print the speed of light - Add a new constant: 'G' (gravitational constant) = 6.674e-11 (m¬≥/kg‚ãÖs¬≤) - Print all keys in the dictionary - Print all values in the dictionary - Calculate the energy of a photon with wavelength Œª = 500 nm using \\(E = \\frac{hc}{\\lambda}\\)\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n# Create physics constants dictionary\nphysics_constants = {\n    'c': 299792458,           # speed of light (m/s)\n    'h': 6.62607015e-34,      # Planck constant (J‚ãÖs)\n    'g': 9.81,                # gravitational acceleration (m/s¬≤)\n    'k_B': 1.380649e-23       # Boltzmann constant (J/K)\n}\n\n# Access and print speed of light\nprint(f\"Speed of light: {physics_constants['c']} m/s\")\n\n# Add gravitational constant\nphysics_constants['G'] = 6.674e-11\nprint(f\"\\nAdded gravitational constant G\")\n\n# Print all keys\nprint(f\"\\nAll constants: {list(physics_constants.keys())}\")\n\n# Print all values\nprint(f\"All values: {list(physics_constants.values())}\")\n\n# Calculate photon energy for Œª = 500 nm\nwavelength = 500e-9  # convert nm to m\nphoton_energy = (physics_constants['h'] * physics_constants['c']) / wavelength\nprint(f\"\\nPhoton energy (Œª=500nm): {photon_energy:.3e} J\")\n\n\n\n\n\nExercise 4: Set Operations\nCreate two sets:\n\nelements_group1: containing {'Hydrogen', 'Helium', 'Lithium', 'Beryllium', 'Carbon'}\nelements_group2: containing {'Carbon', 'Nitrogen', 'Oxygen', 'Helium'}\n\nThen find and print:\n\nElements that appear in both sets (intersection)\nElements that appear in either set (union)\nElements only in group1 (difference)\nElements in group1 or group2 but not both (symmetric difference)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n# Create two sets of elements\nelements_group1 = {'Hydrogen', 'Helium', 'Lithium', 'Beryllium', 'Carbon'}\nelements_group2 = {'Carbon', 'Nitrogen', 'Oxygen', 'Helium'}\n\n# Intersection - elements in both sets\nintersection = elements_group1 & elements_group2\nprint(f\"Elements in both groups: {intersection}\")\n\n# Union - elements in either set\nunion = elements_group1 | elements_group2\nprint(f\"All unique elements: {union}\")\n\n# Difference - elements only in group1\ndifference = elements_group1 - elements_group2\nprint(f\"Elements only in group1: {difference}\")\n\n# Symmetric difference - elements in one set but not both\nsymmetric_diff = elements_group1 ^ elements_group2\nprint(f\"Elements in one group but not both: {symmetric_diff}\")"
  },
  {
    "objectID": "lectures/lecture02/datatypes.html#quiz-data-types-in-python",
    "href": "lectures/lecture02/datatypes.html#quiz-data-types-in-python",
    "title": "Data Types in Python",
    "section": "Quiz: Data Types in Python",
    "text": "Quiz: Data Types in Python\nLet‚Äôs test your understanding of Python data types!\n\n\nWhat is the output of the following code?\na = [1, 2, 3]\nb = (1, 2, 3)\nprint(type(a), type(b))\n\n&lt;class 'list'&gt; &lt;class 'list'&gt;\n&lt;class 'list'&gt; &lt;class 'tuple'&gt;\n&lt;class 'tuple'&gt; &lt;class 'list'&gt;\n&lt;class 'tuple'&gt; &lt;class 'tuple'&gt;\n\nWhich of the following is mutable?\n\nList\nTuple\nString\nInteger\n\nWhat will be the output of this code?\nmy_dict = {'a': 1, 'b': 2, 'c': 3}\nprint(my_dict['b'])\n\na\n2\nb\nKeyError\n\nHow do you create an empty set in Python?\n\n{}\n[]\nset()\n()\n\nWhat is the result of 3 + 4.0?\n\n7\n7.0\n‚Äò7.0‚Äô\nTypeError\n\n\n\n\n\n\n\n\n\nClick to reveal answers\n\n\n\n\n\n\n&lt;class 'list'&gt; &lt;class 'tuple'&gt;\nList\n2\nset()\n7.0"
  },
  {
    "objectID": "lectures/lecture05/brownian-motion-advanced.html",
    "href": "lectures/lecture05/brownian-motion-advanced.html",
    "title": "Brownian Motion with Object-Oriented Programming",
    "section": "",
    "text": "Remember simulating Brownian motion back in Week 2 using functions and lists? That worked, but as simulations get more complex (different particle sizes, tracking many properties, adding interactions), managing separate lists for x-positions, y-positions, radii, and other properties becomes messy.\nNow that you‚Äôve learned about classes and object-oriented programming, let‚Äôs revisit Brownian motion with a better approach! This time, each particle will be an object that ‚Äúknows‚Äù its own properties (size, position, diffusion coefficient) and behaviors (how to update, how to calculate its trajectory). You‚Äôll see how OOP makes the code cleaner, easier to extend, and much more maintainable."
  },
  {
    "objectID": "lectures/lecture05/brownian-motion-advanced.html#quick-physics-review",
    "href": "lectures/lecture05/brownian-motion-advanced.html#quick-physics-review",
    "title": "Brownian Motion with Object-Oriented Programming",
    "section": "Quick Physics Review",
    "text": "Quick Physics Review\nYou already learned the physics of Brownian motion in Week 2, so here‚Äôs a quick refresher:\nKey Physics Concepts:\n\nParticles undergo random motion due to molecular collisions\nEach step is random with size \\(\\sigma = \\sqrt{2D\\Delta t}\\)\nMean squared displacement: \\(\\langle r^2 \\rangle = 4Dt\\) (in 2D)\nDiffusion coefficient: \\(D = k_BT/(6\\pi\\eta R)\\) depends on particle size \\(R\\)\n\nWhat‚Äôs Different This Time:\nIn Week 2, you simulated particles with the same size. Now with OOP, we can easily simulate particles with different sizes, each with its own diffusion coefficient! This would be much messier with the procedural approach.\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced Mathematical Details\n\n\n\n\n\nThe Brownian motion of a colloidal particle results from collisions with surrounding solvent molecules. These collisions lead to a probability distribution described by:\n\\[\np(x,\\Delta t)=\\frac{1}{\\sqrt{4\\pi D \\Delta t}}e^{-\\frac{x^2}{4D \\Delta t}}\n\\]\nwhere:\n\n\\(D\\) is the diffusion coefficient\n\\(\\Delta t\\) is the time step\nThe variance is \\(\\sigma^2=2D \\Delta t\\)\n\nThis distribution emerges from the central limit theorem, as shown by Lindenberg and L√©vy, when considering many infinitesimally small random steps.\nThe evolution of the probability density function \\(p(x,t)\\) is governed by the diffusion equation:\n\\[\n\\frac{\\partial p}{\\partial t}=D\\frac{\\partial^2 p}{\\partial x^2}\n\\]\nThis partial differential equation, also known as Fick‚Äôs second law, describes how the concentration of particles evolves over time due to diffusive processes. The Gaussian distribution above is the fundamental solution (Green‚Äôs function) of this diffusion equation, representing how an initially localized distribution spreads out over time.\nThe connection between the microscopic random motion and the macroscopic diffusion equation was first established by Einstein in his 1905 paper on Brownian motion, providing one of the earliest quantitative links between statistical mechanics and thermodynamics."
  },
  {
    "objectID": "lectures/lecture05/brownian-motion-advanced.html#why-use-a-class-for-brownian-motion",
    "href": "lectures/lecture05/brownian-motion-advanced.html#why-use-a-class-for-brownian-motion",
    "title": "Brownian Motion with Object-Oriented Programming",
    "section": "Why Use a Class for Brownian Motion?",
    "text": "Why Use a Class for Brownian Motion?\nThink about what a real colloidal particle does:\n\nIt has a specific size (radius)\nIt has a current position in space\nIt remembers where it‚Äôs been (trajectory)\nIt knows how fast it diffuses (based on its size)\nIt can update its position when time passes\n\nIn Week 2, we used separate variables and functions for all this. But doesn‚Äôt it make more sense to bundle all of a particle‚Äôs data and behaviors into one object?\nObject-Oriented Approach:\nparticle = Colloid(R=1e-6)           # Create particle with size\nparticle.sim_trajectory(N=200, dt=0.05)  # Particle updates itself\nprint(particle)                       # Particle describes itself\ntrajectory = particle.get_trajectory() # Get its history\nCompare this to managing separate x_list, y_list, radii_list, and writing functions that need all these lists as parameters!\nKey advantages for our simulation:\n\nEach particle is self-contained - no juggling multiple lists\nEasy to create particles with different sizes - each calculates its own D\nNatural syntax - particle.update() is intuitive\nEasy to extend - want charged particles? Just inherit from Colloid!"
  },
  {
    "objectID": "lectures/lecture05/brownian-motion-advanced.html#class-design",
    "href": "lectures/lecture05/brownian-motion-advanced.html#class-design",
    "title": "Brownian Motion with Object-Oriented Programming",
    "section": "Class Design",
    "text": "Class Design\nLet‚Äôs design a Python class to simulate colloidal particles undergoing Brownian motion. This object-oriented approach will help us manage multiple particles with different properties and behaviors.\n\nClass-Level Properties\nThe Colloid class will maintain information shared by all particles:\n\nA counter for the total number of particles\nThe physical constant \\(k_B T/(6\\pi\\eta) = 2.2√ó10^{-19}\\) (combining temperature and fluid properties)\n\n\n\nClass Methods\nThe class will provide these shared functions:\n\nhow_many(): Reports the total number of particles\n__str__: Creates a readable description of a particle‚Äôs properties\n\n\n\nInstance Properties\nEach individual particle object will have:\n\nRadius (R)\nPosition history (x and y coordinates)\nUnique identifier (index)\nDiffusion coefficient (\\(D = k_B T/(6\\pi\\eta R)\\))\n\n\n\nInstance Methods\nEach particle will be able to:\n\nsim_trajectory(): Generate a complete motion path\nupdate(dt): Calculate one step of Brownian motion\nget_trajectory(): Return its movement history\nget_D(): Provide its diffusion coefficient\n\n\n\n\n\n\n\n\n\n\n\n\n\nOOP in Action\n\n\n\nNotice several object-oriented features here:\n\nClass variables: number and f are shared by all particles\nInstance variables: Each particle has its own R, x, y, and D\nMethods calling methods: sim_trajectory() calls update() on the same object\n@classmethod decorator: how_many() operates on the class itself, not instances\n__str__ method: Defines how particles are displayed when printed"
  },
  {
    "objectID": "lectures/lecture05/brownian-motion-advanced.html#simulating",
    "href": "lectures/lecture05/brownian-motion-advanced.html#simulating",
    "title": "Brownian Motion with Object-Oriented Programming",
    "section": "Simulating",
    "text": "Simulating\nWith the help of this Colloid class, we would like to carry out simulations of Brownian motion of multiple particles. The simulations shall\n\ntake n=200 particles\nhave N=200 trajectory points each\nstart all at 0,0\nparticle objects should be stored in a list p_list\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs verify that our class variable counter is working:\n\n\n\n\n\n\n\n\n\n\n\n\nClass Methods in Action\n\n\n\nThe how_many() class method demonstrates how we can access class-level information without needing a specific instance. This is perfect for tracking global statistics about all particles!"
  },
  {
    "objectID": "lectures/lecture05/brownian-motion-advanced.html#plotting-the-trajectories",
    "href": "lectures/lecture05/brownian-motion-advanced.html#plotting-the-trajectories",
    "title": "Brownian Motion with Object-Oriented Programming",
    "section": "Plotting the trajectories",
    "text": "Plotting the trajectories\nThe next step is to plot all the trajectories."
  },
  {
    "objectID": "lectures/lecture05/brownian-motion-advanced.html#characterizing-the-brownian-motion",
    "href": "lectures/lecture05/brownian-motion-advanced.html#characterizing-the-brownian-motion",
    "title": "Brownian Motion with Object-Oriented Programming",
    "section": "Characterizing the Brownian motion",
    "text": "Characterizing the Brownian motion\nNow that we have trajectories, let‚Äôs analyze the motion. You learned these analysis techniques in Week 2, but notice how much cleaner the code is with OOP - each particle manages its own trajectory!\n\n\n\n\n\n\nReview: Why Speed Doesn‚Äôt Work for Brownian Motion\n\n\n\n\n\nAs you saw in Week 2, calculating speed from Brownian trajectories gives unphysical results (infinite speed at short times). This is because Brownian motion is fundamentally a diffusive process, not ballistic motion. The correct measure is the mean squared displacement (MSD).\n\n\n\n\nCalculate the particle mean squared displacement\nThe MSD is the right way to characterize diffusive motion. Recall from Week 2:\n\\[\\begin{equation}\n\\langle \\Delta r^{2}(t)\\rangle=2 d D t\n\\end{equation}\\]\nwhere \\(d=2\\) for 2D motion. Let‚Äôs verify our simulation matches theory:\n\n\n\n\n\n\n\n\n\n\n\n\nOOP Makes This Cleaner!\n\n\n\nNotice how easy it is to get each particle‚Äôs trajectory:\nt = p_list[j].get_trajectory()  # Each particle returns its own data!\nIn Week 2‚Äôs procedural approach, you had to manage separate all_x[j] and all_y[j] lists. The OOP approach is much cleaner and less error-prone.\n\n\nKey Observations: - Individual particle MSDs fluctuate around the theoretical prediction - On average, MSD follows \\(\\langle r^2 \\rangle = 4Dt\\) - Fluctuations increase at long times (fewer data points) - For accurate D measurements, use many particles or early time points\nFor detailed statistical analysis, refer back to Week 2 lecture where you explored these concepts thoroughly."
  },
  {
    "objectID": "lectures/lecture05/brownian-motion-advanced.html#extending-with-inheritance-charged-colloids",
    "href": "lectures/lecture05/brownian-motion-advanced.html#extending-with-inheritance-charged-colloids",
    "title": "Brownian Motion with Object-Oriented Programming",
    "section": "Extending with Inheritance: Charged Colloids",
    "text": "Extending with Inheritance: Charged Colloids\nNow let‚Äôs demonstrate the power of inheritance by creating a specialized type of colloid. In real experiments, colloidal particles often carry surface charges that affect their motion through electrostatic interactions.\n\n\n\n\n\n\nLet‚Äôs create and simulate some charged colloids:\n\n\n\n\n\n\nPlot the charged particles with different colors based on their charge:\n\n\n\n\n\n\n\n\n\n\n\n\nInheritance Benefits\n\n\n\nNotice how ChargedColloid:\n\nInherits all functionality from Colloid (it can still update(), sim_trajectory(), etc.)\nAdds new properties (charge) specific to charged particles\nOverrides methods (__str__) to customize behavior\nAdds new methods (electrostatic_force()) for charge-specific calculations\nShares class variables (contributes to Colloid.number counter)\n\nThis is code reuse at its finest! We didn‚Äôt have to rewrite any of the Brownian motion logic."
  },
  {
    "objectID": "lectures/lecture05/brownian-motion-advanced.html#summary-what-weve-learned",
    "href": "lectures/lecture05/brownian-motion-advanced.html#summary-what-weve-learned",
    "title": "Brownian Motion with Object-Oriented Programming",
    "section": "Summary: What We‚Äôve Learned",
    "text": "Summary: What We‚Äôve Learned\nIn this lecture, we‚Äôve applied object-oriented programming principles to simulate Brownian motion of colloidal particles. Here‚Äôs what we covered:\nPhysics Concepts:\n\nBrownian motion arises from molecular collisions with suspended particles\nMotion is characterized by random steps following a normal distribution\nThe mean squared displacement (MSD) grows linearly with time: \\(\\langle \\Delta r^2 \\rangle = 2dDt\\)\nSpeed is not a good measure for Brownian motion due to the random nature of the process\n\nOOP Implementation:\n\nCreated a Colloid class with both instance variables (R, x, y, D) and class variables (number, f)\nUsed the constructor __init__ to initialize particles with specific properties\nImplemented methods for simulation (update, sim_trajectory) and data access (get_trajectory, get_D)\nUsed special methods like __str__ for readable output\nApplied inheritance to create ChargedColloid with additional properties and behaviors\nUsed class methods (@classmethod) to track global particle statistics\n\nComputational Insights:\n\nOOP makes managing multiple particles with different properties straightforward\nEach particle encapsulates its own state and behavior\nInheritance allows us to extend functionality without modifying existing code\nStatistical analysis requires careful consideration of finite sampling effects\n\nThe object-oriented approach makes the code more organized, maintainable, and extensible compared to procedural programming. When you need to add new particle types or behaviors, you can simply create new classes through inheritance rather than modifying existing code."
  },
  {
    "objectID": "lectures/lecture05/brownian-motion-advanced.html#practice-exercises",
    "href": "lectures/lecture05/brownian-motion-advanced.html#practice-exercises",
    "title": "Brownian Motion with Object-Oriented Programming",
    "section": "üéØ Practice Exercises",
    "text": "üéØ Practice Exercises\nTry these exercises to deepen your understanding of OOP applied to physical simulations!\n\nExercise 1: Add a Reset Method\nAdd a reset() method to the Colloid class that clears the trajectory and returns the particle to its initial position.\n\n\n\n\n\n\n\n\nExercise 2: Calculate Total Distance Traveled\nAdd a method total_distance() to the Colloid class that calculates the total distance the particle has traveled along its trajectory (not just displacement!).\n\n\n\n\n\n\n\n\nExercise 3: Create a MagneticColloid Class\nCreate a MagneticColloid class that inherits from Colloid and: - Has a magnetic_moment property - Adds a drift_in_field(Bx, By, dt) method that adds a deterministic drift in addition to Brownian motion - Overrides update(dt) to include both Brownian motion and magnetic drift\n\n\n\n\n\n\n\n\nExercise 4: Analyze Size-Dependent Diffusion\nCreate particles of different sizes and verify that the diffusion coefficient follows the Stokes-Einstein relation: \\(D = k_BT/(6\\pi\\eta R)\\).\n\n\n\n\n\n\n\n\n\n\n\n\nExercise Tips\n\n\n\n\nFor Exercise 1: Remember that __init__ stores the initial position\nFor Exercise 2: Use np.diff() or loop through consecutive positions\nFor Exercise 3: Use super().update(dt) then add the drift\nFor Exercise 4: Fit a line to MSD vs time to get the slope = 4D"
  },
  {
    "objectID": "lectures/lecture05/brownian-motion-advanced.html#whats-next",
    "href": "lectures/lecture05/brownian-motion-advanced.html#whats-next",
    "title": "Brownian Motion with Object-Oriented Programming",
    "section": "What‚Äôs Next?",
    "text": "What‚Äôs Next?\nCongratulations! You‚Äôve successfully applied object-oriented programming to a real physics simulation. You‚Äôve seen how classes help organize complex simulations and how inheritance allows you to extend functionality cleanly.\nIn future lectures, you‚Äôll continue to use OOP principles for increasingly sophisticated simulations. You‚Äôll learn how to:\n\nOrganize classes into modules (separating class definitions from simulation scripts)\nHandle particle-particle interactions more systematically using object methods\nCreate visualization classes that encapsulate plotting functionality\nDesign simulation frameworks where different components (particles, forces, integrators) work together through well-defined interfaces\n\nThe skills you‚Äôve developed here - designing classes, using inheritance, managing object state - are fundamental to computational physics. Modern simulation packages like MDAnalysis (molecular dynamics), scikit-learn (machine learning), and even NumPy itself are built using these same OOP principles.\nNext, we‚Äôll explore how to organize your code into modules and packages, making it even more professional and reusable!"
  },
  {
    "objectID": "lectures/lecture05/brownian-motion-advanced.html#further-reading",
    "href": "lectures/lecture05/brownian-motion-advanced.html#further-reading",
    "title": "Brownian Motion with Object-Oriented Programming",
    "section": "Further Reading",
    "text": "Further Reading\nTo deepen your understanding of Brownian motion and computational physics:\nBrownian Motion:\n\nEinstein, A. (1905): ‚ÄúOn the Motion of Small Particles Suspended in Liquids at Rest‚Äù - The original paper!\nBerg, H. C. (1993): ‚ÄúRandom Walks in Biology‚Äù - Excellent introduction to diffusion in biological systems\nRisken, H. (1996): ‚ÄúThe Fokker-Planck Equation‚Äù - Advanced treatment\n\nOOP for Scientific Computing:\n\nLangtangen, H. P. (2009): ‚ÄúPython Scripting for Computational Science‚Äù - Chapter on OOP for simulations\nNewman, M. (2013): ‚ÄúComputational Physics‚Äù - Chapter 7 on object-oriented programming\nReal Python: OOP in Python 3\n\nColloidal Physics:\n\nLook up ‚ÄúStokes-Einstein relation‚Äù for the theory behind our diffusion coefficient\nResearch ‚Äúsingle particle tracking‚Äù to see how these simulations relate to real experiments"
  },
  {
    "objectID": "lectures/lecture11/repetition-slides.html#todays-plan",
    "href": "lectures/lecture11/repetition-slides.html#todays-plan",
    "title": "Repetition: Python for Physics",
    "section": "Today‚Äôs Plan",
    "text": "Today‚Äôs Plan\n\nQuick review of Python fundamentals\nYou practice on the interactive webpage\nThen: Understanding ODEs from scratch\n\n\nüì± Open now: https://fcichos.github.io/EMPP/lectures/lecture11/1_repetition.qmd\n(or find ‚ÄúRepetition: Core Concepts‚Äù in Week 11 on the website)"
  },
  {
    "objectID": "lectures/lecture11/repetition-slides.html#what-is-a-program",
    "href": "lectures/lecture11/repetition-slides.html#what-is-a-program",
    "title": "Repetition: Python for Physics",
    "section": "What is a Program?",
    "text": "What is a Program?\nA program is a sequence of instructions that tells a computer how to perform a specific task.\nThese instructions must be:\n\nPrecise and unambiguous\nWritten in a language the computer understands\nLogically structured\nDesigned to achieve a specific goal\n\n\n\nThink of it like a recipe: step-by-step instructions that, if followed exactly, always produce the same result."
  },
  {
    "objectID": "lectures/lecture11/repetition-slides.html#why-programming-for-physics",
    "href": "lectures/lecture11/repetition-slides.html#why-programming-for-physics",
    "title": "Repetition: Python for Physics",
    "section": "Why Programming for Physics?",
    "text": "Why Programming for Physics?\nMany physics problems cannot be solved analytically:\n\nThree-body problem in celestial mechanics\nTurbulent fluid flow\nComplex quantum systems\n\n\nNumerical methods let us approximate solutions by breaking problems into small, computable steps.\n\n\nToday‚Äôs goal: Make sure you remember the building blocks to construct these numerical solutions."
  },
  {
    "objectID": "lectures/lecture11/repetition-slides.html#variables-and-data-types",
    "href": "lectures/lecture11/repetition-slides.html#variables-and-data-types",
    "title": "Repetition: Python for Physics",
    "section": "Variables and Data Types",
    "text": "Variables and Data Types\nVariables are containers for storing data values ‚Äî like labeled boxes where you keep information.\n# Basic data types\nx = 5           # integer  - counting, indices\ny = 3.14        # float    - measurements, calculations\nname = \"Python\" # string   - labels, file names\nis_true = True  # boolean  - yes/no decisions\n\nprint(f\"x is type: {type(x)}\")\nprint(f\"y is type: {type(y)}\")\n\nIn physics: Variables store your physical quantities ‚Äî mass, velocity, temperature, charge‚Ä¶\nKey point: Python is dynamically typed ‚Äî no need to declare types explicitly!"
  },
  {
    "objectID": "lectures/lecture11/repetition-slides.html#numerical-operations",
    "href": "lectures/lecture11/repetition-slides.html#numerical-operations",
    "title": "Repetition: Python for Physics",
    "section": "Numerical Operations",
    "text": "Numerical Operations\nPython supports all the mathematical operations you need for physics calculations:\na = 10\nb = 3\n\nprint(f\"Addition: {a + b}\")          # 13\nprint(f\"Subtraction: {a - b}\")       # 7\nprint(f\"Multiplication: {a * b}\")    # 30\nprint(f\"Division: {a / b}\")          # 3.333...\nprint(f\"Integer Division: {a // b}\") # 3\nprint(f\"Power: {a ** b}\")            # 1000\n\nWatch out: Division / always gives a float. Use // for integer division (e.g., for array indices)."
  },
  {
    "objectID": "lectures/lecture11/repetition-slides.html#your-turn-exercises-1-2",
    "href": "lectures/lecture11/repetition-slides.html#your-turn-exercises-1-2",
    "title": "Repetition: Python for Physics",
    "section": "üéØ Your Turn: Exercises 1 & 2",
    "text": "üéØ Your Turn: Exercises 1 & 2\nOn the webpage, try:\nSelf-Exercise 1: Unit Conversion\nConvert Celsius ‚Üí Fahrenheit and Kelvin\n\n¬∞F = (¬∞C √ó 9/5) + 32\nK = ¬∞C + 273.15\n\nSelf-Exercise 2: Basic Kinematics\nCalculate final velocity: \\(v = v_0 + at\\)\n‚è±Ô∏è Time: 5 minutes\nüí° Hint: Just translate the formula directly into Python code!"
  },
  {
    "objectID": "lectures/lecture11/repetition-slides.html#lists-vs.-numpy-arrays",
    "href": "lectures/lecture11/repetition-slides.html#lists-vs.-numpy-arrays",
    "title": "Repetition: Python for Physics",
    "section": "Lists vs.¬†NumPy Arrays",
    "text": "Lists vs.¬†NumPy Arrays\nLists are Python‚Äôs built-in way to store multiple values:\nnumbers = [1, 2, 3, 4, 5]\nNumPy arrays are specialized for numerical calculations:\nimport numpy as np\narray = np.array([1, 2, 3, 4, 5])\nprint(f\"Array √ó 2: {array * 2}\")  # [2, 4, 6, 8, 10]\n\nWhy NumPy for physics?\n\nElement-wise operations without loops (faster!)\nBuilt-in math functions: np.sin(), np.exp(), np.sqrt()\nEssential for handling time series, position vectors, field data"
  },
  {
    "objectID": "lectures/lecture11/repetition-slides.html#numpy-your-physics-calculator",
    "href": "lectures/lecture11/repetition-slides.html#numpy-your-physics-calculator",
    "title": "Repetition: Python for Physics",
    "section": "NumPy: Your Physics Calculator",
    "text": "NumPy: Your Physics Calculator\nimport numpy as np\n\n# Create array of positions\nx = np.array([0, 1, 2, 3, 4])  # meters\n\n# Calculate for ALL positions at once\npotential_energy = m * g * x   # No loop needed!\n\nThink of arrays as: A collection of measurements taken at different times or positions ‚Äî and NumPy lets you process them all simultaneously."
  },
  {
    "objectID": "lectures/lecture11/repetition-slides.html#your-turn-exercise-3",
    "href": "lectures/lecture11/repetition-slides.html#your-turn-exercise-3",
    "title": "Repetition: Python for Physics",
    "section": "üéØ Your Turn: Exercise 3",
    "text": "üéØ Your Turn: Exercise 3\nOn the webpage:\nSelf-Exercise 3: Force Calculations\nCreate an array of masses and calculate gravitational force on each:\n\\[F = mg\\]\nwhere \\(g = 9.81\\) m/s¬≤\n‚è±Ô∏è Time: 3 minutes\nüí° Hint: With NumPy, forces = masses * g works on the entire array!"
  },
  {
    "objectID": "lectures/lecture11/repetition-slides.html#conditional-statements",
    "href": "lectures/lecture11/repetition-slides.html#conditional-statements",
    "title": "Repetition: Python for Physics",
    "section": "Conditional Statements",
    "text": "Conditional Statements\nPrograms need to make decisions based on conditions:\ntemperature = 25\n\nif temperature &gt; 30:\n    print(\"It's hot!\")\nelif temperature &gt; 20:\n    print(\"It's pleasant\")\nelse:\n    print(\"It's cool\")\n\nIn physics simulations, you need conditions for:\n\nBoundary conditions (particle hits a wall)\nPhase transitions (ice ‚Üí water ‚Üí steam)\nCollision detection\nChecking convergence of numerical methods"
  },
  {
    "objectID": "lectures/lecture11/repetition-slides.html#how-conditions-work",
    "href": "lectures/lecture11/repetition-slides.html#how-conditions-work",
    "title": "Repetition: Python for Physics",
    "section": "How Conditions Work",
    "text": "How Conditions Work\nif condition:      # Is this True?\n    do_something   # Yes ‚Üí execute this block\nelif other:        # No ‚Üí check this instead\n    do_other       # This one is True ‚Üí execute\nelse:              # None of the above were True\n    do_default     # Execute this as fallback\n\nComparison operators:\n\n&gt;, &lt;, &gt;=, &lt;= ‚Äî greater/less than\n== ‚Äî equal to (note: two equals signs!)\n!= ‚Äî not equal to"
  },
  {
    "objectID": "lectures/lecture11/repetition-slides.html#your-turn-exercises-4-5",
    "href": "lectures/lecture11/repetition-slides.html#your-turn-exercises-4-5",
    "title": "Repetition: Python for Physics",
    "section": "üéØ Your Turn: Exercises 4 & 5",
    "text": "üéØ Your Turn: Exercises 4 & 5\nSelf-Exercise 4: Phase of Matter\nDetermine if water is solid, liquid, or gas based on temperature\n\nBelow 0¬∞C ‚Üí Solid\n0-100¬∞C ‚Üí Liquid\n\nAbove 100¬∞C ‚Üí Gas\n\nSelf-Exercise 5: Projectile Range Calculator\nCalculate if a projectile hits a target using:\n\\[R = \\frac{v_0^2 \\sin(2\\theta)}{g}\\]\n‚è±Ô∏è Time: 7 minutes"
  },
  {
    "objectID": "lectures/lecture11/repetition-slides.html#loops-repeating-actions",
    "href": "lectures/lecture11/repetition-slides.html#loops-repeating-actions",
    "title": "Repetition: Python for Physics",
    "section": "Loops: Repeating Actions",
    "text": "Loops: Repeating Actions\nFor loop ‚Äî when you know how many iterations:\nfor i in range(5):    # i = 0, 1, 2, 3, 4\n    print(f\"Step {i}\")\nWhile loop ‚Äî when you don‚Äôt know when to stop:\nwhile error &gt; tolerance:\n    improve_solution()\n\nIn physics: Loops are how we step through time in simulations!"
  },
  {
    "objectID": "lectures/lecture11/repetition-slides.html#why-loops-matter-for-physics",
    "href": "lectures/lecture11/repetition-slides.html#why-loops-matter-for-physics",
    "title": "Repetition: Python for Physics",
    "section": "Why Loops Matter for Physics",
    "text": "Why Loops Matter for Physics\nLoops are the foundation of numerical time evolution:\n# This is essentially the Euler method!\nfor step in range(num_steps):\n    velocity = velocity + acceleration * dt\n    position = position + velocity * dt\n\nEvery simulation you‚Äôve seen ‚Äî planetary orbits, pendulums, Brownian motion ‚Äî uses this pattern:\n\nStart with initial conditions\nLoop through small time steps\nUpdate positions and velocities each step"
  },
  {
    "objectID": "lectures/lecture11/repetition-slides.html#loop-patterns-youll-use",
    "href": "lectures/lecture11/repetition-slides.html#loop-patterns-youll-use",
    "title": "Repetition: Python for Physics",
    "section": "Loop Patterns You‚Äôll Use",
    "text": "Loop Patterns You‚Äôll Use\nIterating over data:\nfor mass in masses:\n    print(f\"Force = {mass * g}\")\nAccumulating results:\ntotal_energy = 0\nfor particle in particles:\n    total_energy += particle.kinetic_energy()\nTime stepping:\nfor t in time_array:\n    y_new = y_old + derivative * dt"
  },
  {
    "objectID": "lectures/lecture11/repetition-slides.html#your-turn-exercises-6-7",
    "href": "lectures/lecture11/repetition-slides.html#your-turn-exercises-6-7",
    "title": "Repetition: Python for Physics",
    "section": "üéØ Your Turn: Exercises 6 & 7",
    "text": "üéØ Your Turn: Exercises 6 & 7\nSelf-Exercise 6: Radioactive Decay\nSimulate decay over 5 half-lives using a loop:\n\\[N(t) = N_0 \\cdot \\left(\\frac{1}{2}\\right)^{t/t_{1/2}}\\]\nSelf-Exercise 7: Time to Ground\nUse a while loop to find when a falling object hits the ground\n‚è±Ô∏è Time: 10 minutes\nüí° These exercises prepare you for the Euler method in Part 2!"
  },
  {
    "objectID": "lectures/lecture11/repetition-slides.html#bringing-it-together-a-physics-example",
    "href": "lectures/lecture11/repetition-slides.html#bringing-it-together-a-physics-example",
    "title": "Repetition: Python for Physics",
    "section": "Bringing It Together: A Physics Example",
    "text": "Bringing It Together: A Physics Example\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Projectile motion ‚Äî combining everything!\nv0, theta, g = 10, 45, 9.81\ntheta_rad = np.deg2rad(theta)\n\n# Array of time points\nt = np.linspace(0, 2*v0*np.sin(theta_rad)/g, 100)\n\n# Calculate trajectory (array operations!)\nx = v0 * np.cos(theta_rad) * t\ny = v0 * np.sin(theta_rad) * t - 0.5 * g * t**2\n\nplt.plot(x, y)\nplt.xlabel('Distance (m)')\nplt.ylabel('Height (m)')\nplt.show()"
  },
  {
    "objectID": "lectures/lecture11/repetition-slides.html#what-just-happened",
    "href": "lectures/lecture11/repetition-slides.html#what-just-happened",
    "title": "Repetition: Python for Physics",
    "section": "What Just Happened?",
    "text": "What Just Happened?\n1. Variables      ‚Üí v0, theta, g store our parameters\n2. NumPy array    ‚Üí t contains 100 time points\n3. Array math     ‚Üí x, y calculated for ALL times at once\n4. Plotting       ‚Üí Visualize the result\n\nNo loops needed! NumPy‚Äôs array operations replace explicit iteration.\n\n\nBut sometimes we do need loops ‚Äî when each step depends on the previous one (like in ODEs). That‚Äôs coming in Part 2!"
  },
  {
    "objectID": "lectures/lecture11/repetition-slides.html#best-practices-writing-readable-code",
    "href": "lectures/lecture11/repetition-slides.html#best-practices-writing-readable-code",
    "title": "Repetition: Python for Physics",
    "section": "Best Practices: Writing Readable Code",
    "text": "Best Practices: Writing Readable Code\n\n\n‚úÖ Do:\n\nMeaningful variable names\nComments for complex logic\nBreak into small functions\nConsistent indentation\n\n\n‚ùå Avoid:\n\nx, a, temp (unclear)\nMagic numbers: y = x * 9.81\nVery long functions\nMixing tabs and spaces\n\n\n\nWhy it matters: You‚Äôll read your code more often than you write it. Future-you will thank present-you!"
  },
  {
    "objectID": "lectures/lecture11/repetition-slides.html#good-vs.-bad-code-example",
    "href": "lectures/lecture11/repetition-slides.html#good-vs.-bad-code-example",
    "title": "Repetition: Python for Physics",
    "section": "Good vs.¬†Bad Code Example",
    "text": "Good vs.¬†Bad Code Example\nBad:\nt = 2 * 3.14159 * (l/9.81)**0.5\nGood:\ndef calculate_pendulum_period(length, gravity=9.81):\n    \"\"\"Calculate period of a simple pendulum using T = 2œÄ‚àö(L/g)\"\"\"\n    period = 2 * np.pi * np.sqrt(length / gravity)\n    return period\n\nT = calculate_pendulum_period(length=1.0)\n\nThe good version is self-documenting ‚Äî you can understand it without comments!"
  },
  {
    "objectID": "lectures/lecture11/repetition-slides.html#functions-reusable-code-blocks",
    "href": "lectures/lecture11/repetition-slides.html#functions-reusable-code-blocks",
    "title": "Repetition: Python for Physics",
    "section": "Functions: Reusable Code Blocks",
    "text": "Functions: Reusable Code Blocks\ndef calculate_kinetic_energy(mass, velocity):\n    \"\"\"\n    Calculate kinetic energy: E = ¬Ωmv¬≤\n    \n    Parameters:\n        mass: Mass in kg\n        velocity: Velocity in m/s\n    Returns:\n        Kinetic energy in Joules\n    \"\"\"\n    return 0.5 * mass * velocity**2\n\n# Now use it anywhere:\nE1 = calculate_kinetic_energy(2.0, 3.0)\nE2 = calculate_kinetic_energy(5.0, 10.0)\n\nFunctions prevent copy-paste errors and make your code modular!"
  },
  {
    "objectID": "lectures/lecture11/repetition-slides.html#classes-bundling-data-and-behavior",
    "href": "lectures/lecture11/repetition-slides.html#classes-bundling-data-and-behavior",
    "title": "Repetition: Python for Physics",
    "section": "Classes: Bundling Data and Behavior",
    "text": "Classes: Bundling Data and Behavior\nWhen you have data that belongs together with operations on that data:\nclass Particle:\n    def __init__(self, x, y, mass=1.0):\n        self.x = x\n        self.y = y\n        self.mass = mass\n        self.vx = 0\n        self.vy = 0\n    \n    def kinetic_energy(self):\n        return 0.5 * self.mass * (self.vx**2 + self.vy**2)\n\np = Particle(0, 0, mass=2.0)\n\nThink of classes as: Custom data types for your physics objects (particles, charges, oscillators‚Ä¶)\n(Exercise 8 on webpage is optional/advanced)"
  },
  {
    "objectID": "lectures/lecture11/repetition-slides.html#common-pitfalls-to-avoid",
    "href": "lectures/lecture11/repetition-slides.html#common-pitfalls-to-avoid",
    "title": "Repetition: Python for Physics",
    "section": "Common Pitfalls to Avoid",
    "text": "Common Pitfalls to Avoid\n\n\n\n\n\n\n\n\nPitfall\nWhat Goes Wrong\nHow to Fix\n\n\n\n\n= vs ==\nAssignment instead of comparison\nif x == 5: not if x = 5:\n\n\nOff-by-one\nrange(5) gives 0,1,2,3,4\nCheck your boundaries!\n\n\nMissing import\nNameError: np not defined\nimport numpy as np\n\n\nInteger division\n1/2 = 0 in Python 2\nUse Python 3 or 1.0/2\n\n\nModifying while iterating\nUnexpected behavior\nCreate a copy first"
  },
  {
    "objectID": "lectures/lecture11/repetition-slides.html#debugging-strategy",
    "href": "lectures/lecture11/repetition-slides.html#debugging-strategy",
    "title": "Repetition: Python for Physics",
    "section": "Debugging Strategy",
    "text": "Debugging Strategy\nWhen your code doesn‚Äôt work:\n\nRead the error message ‚Äî Python tells you what‚Äôs wrong!\nAdd print statements ‚Äî Check variable values\nTest small pieces ‚Äî Isolate the problem\nCheck your physics ‚Äî Does the result make sense?\n\n\n# Debugging example\nprint(f\"Before loop: velocity = {velocity}\")\nfor t in range(10):\n    velocity = velocity + a * dt\n    print(f\"Step {t}: velocity = {velocity}\")  # Watch it change!"
  },
  {
    "objectID": "lectures/lecture11/repetition-slides.html#summary-the-building-blocks",
    "href": "lectures/lecture11/repetition-slides.html#summary-the-building-blocks",
    "title": "Repetition: Python for Physics",
    "section": "Summary: The Building Blocks",
    "text": "Summary: The Building Blocks\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  Variables    ‚Üí  Store physical quantities              ‚îÇ\n‚îÇ  Operations   ‚Üí  Calculate new values                   ‚îÇ\n‚îÇ  Arrays       ‚Üí  Handle many values efficiently         ‚îÇ\n‚îÇ  Conditions   ‚Üí  Make decisions (boundaries, phases)    ‚îÇ\n‚îÇ  Loops        ‚Üí  Step through time, iterate             ‚îÇ\n‚îÇ  Functions    ‚Üí  Organize and reuse code                ‚îÇ\n‚îÇ  Classes      ‚Üí  Model physical objects                 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nThese are the ingredients. Next, we‚Äôll use them to cook up ODE solvers!"
  },
  {
    "objectID": "lectures/lecture11/repetition-slides.html#short-break-5-minutes",
    "href": "lectures/lecture11/repetition-slides.html#short-break-5-minutes",
    "title": "Repetition: Python for Physics",
    "section": "‚òï Short Break (5 minutes)",
    "text": "‚òï Short Break (5 minutes)\nAfter the break:\nRepetition: ODE Review (lecture15a.qmd)\n\nTaylor expansion ‚Üí Euler method\nUnderstanding what scipy.odeint does internally\nHands-on: Build your own ODE solver\nApply to radioactive decay and harmonic oscillators"
  },
  {
    "objectID": "lectures/lecture11/repetition-slides.html#questions-before-we-continue",
    "href": "lectures/lecture11/repetition-slides.html#questions-before-we-continue",
    "title": "Repetition: Python for Physics",
    "section": "Questions Before We Continue?",
    "text": "Questions Before We Continue?\nüì± Keep the webpage open for Part 2!\n\nRemember: The 1_repetition.qmd document is available online for you to practice at home. Work through the exercises you didn‚Äôt finish today!"
  },
  {
    "objectID": "lectures/lecture10/planetary-motion.html",
    "href": "lectures/lecture10/planetary-motion.html",
    "title": "Planetary Motion: Newton‚Äôs Masterpiece",
    "section": "",
    "text": "In Week 5, you learned powerful tools for solving differential equations, especially odeint for ODEs. Now it‚Äôs time to put these tools to work on one of the most beautiful problems in physics: planetary motion!\nThis is the perfect first application because:\nThese same techniques will work for many other physics problems - anytime you have coupled motion in two dimensions. The mathematical framework is surprisingly general!"
  },
  {
    "objectID": "lectures/lecture10/planetary-motion.html#historical-context-newtons-revolution",
    "href": "lectures/lecture10/planetary-motion.html#historical-context-newtons-revolution",
    "title": "Planetary Motion: Newton‚Äôs Masterpiece",
    "section": "Historical Context: Newton‚Äôs Revolution",
    "text": "Historical Context: Newton‚Äôs Revolution\nIn 1687, Isaac Newton published the Principia Mathematica, arguably the most important physics book ever written. In it, he showed that:\n\nThe same force that makes apples fall (gravity) also keeps planets in orbit\nAll planetary motion could be explained by one simple equation: \\(F = GMm/r^2\\)\nKepler‚Äôs empirical laws of planetary motion (discovered from observations) could be derived from Newton‚Äôs laws\n\nThis was revolutionary! It meant the same physics applies on Earth and in the heavens. Before Newton, these were thought to be fundamentally different realms.\n\n\n\n\n\n\nNewton‚Äôs Insight\n\n\n\nWhat made Newton‚Äôs work so powerful was that it was predictive, not just descriptive. Given a planet‚Äôs position and velocity at one moment, you could predict its entire future orbit. This is exactly what we‚Äôll do with odeint!"
  },
  {
    "objectID": "lectures/lecture10/planetary-motion.html#the-physical-setup",
    "href": "lectures/lecture10/planetary-motion.html#the-physical-setup",
    "title": "Planetary Motion: Newton‚Äôs Masterpiece",
    "section": "The Physical Setup",
    "text": "The Physical Setup\nImagine a planet orbiting the Sun:\n\nThe Sun is at the origin (we assume it‚Äôs much more massive than the planet, so it doesn‚Äôt move)\nThe planet has mass \\(m\\) and moves in a plane around the Sun\nThe only force is gravity: \\(F = -\\frac{GMm}{r^2}\\) (pointing toward the Sun)\n\nWhere:\n\n\\(G\\) is the gravitational constant\n\\(M\\) is the mass of the Sun\n\\(m\\) is the mass of the planet\n\\(r\\) is the distance between them\n\n\nPolar Coordinates: The Natural Choice\nWe‚Äôll use polar coordinates because they‚Äôre natural for this problem:\n\n\\(r\\): Distance from the Sun to the planet\n\\(\\theta\\): Angular position (where the planet is in its orbit)\n\nThese are more natural than \\((x, y)\\) because:\n\nGravity acts in the radial direction\nAngular momentum is naturally expressed in terms of \\(\\theta\\)\nThe equations become simpler"
  },
  {
    "objectID": "lectures/lecture10/planetary-motion.html#the-equations-of-motion",
    "href": "lectures/lecture10/planetary-motion.html#the-equations-of-motion",
    "title": "Planetary Motion: Newton‚Äôs Masterpiece",
    "section": "The Equations of Motion",
    "text": "The Equations of Motion\nUsing Newton‚Äôs second law in polar coordinates, we get two coupled differential equations:\n\nRadial Motion\n\\[\\begin{equation}\n\\ddot{r} = r\\dot{\\theta}^2 - \\frac{GM}{r^2}\n\\end{equation}\\]\nLet‚Äôs understand each term:\n\n\\(r\\dot{\\theta}^2\\): The centrifugal effect - the planet ‚Äúwants‚Äù to fly outward due to its orbital motion\n\\(-\\frac{GM}{r^2}\\): Newton‚Äôs gravitational force (per unit mass) - pulls the planet toward the Sun\n\nNotice the \\(1/r^2\\) dependence - this is Newton‚Äôs inverse square law!\n\n\n\n\nAngular Motion\n\\[\\begin{equation}\n\\ddot{\\theta} = -\\frac{2\\dot{r}\\dot{\\theta}}{r}\n\\end{equation}\\]\nThis describes how the angular velocity changes:\n\n\\(2\\dot{r}\\dot{\\theta}\\): The coupling term - connects radial and angular motion\n\\(1/r\\): As the planet gets closer to the Sun, it speeds up; farther away, it slows down\n\nThis is conservation of angular momentum in action!"
  },
  {
    "objectID": "lectures/lecture10/planetary-motion.html#the-analytical-solution-conic-sections",
    "href": "lectures/lecture10/planetary-motion.html#the-analytical-solution-conic-sections",
    "title": "Planetary Motion: Newton‚Äôs Masterpiece",
    "section": "The Analytical Solution: Conic Sections",
    "text": "The Analytical Solution: Conic Sections\nBefore we solve this numerically, it‚Äôs worth knowing what the analytical solution looks like. This is what Newton derived in the Principia!\nThe orbit equation is:\n\\[\\begin{equation}\nr(\\theta) = \\frac{p}{1 + \\varepsilon\\cos(\\theta)}\n\\end{equation}\\]\nThis is the equation of a conic section, where:\n\n\\(p = \\frac{L^2}{GMm^2}\\) depends on the angular momentum \\(L\\)\n\\(\\varepsilon\\) is the eccentricity, which depends on the energy \\(E\\):\n\n\\[\\begin{equation}\n\\varepsilon = \\sqrt{1 + \\frac{2EL^2}{m^3G^2M^2}}\n\\end{equation}\\]\n\nWhat Do Different Eccentricities Mean?\n\n\\(\\varepsilon = 0\\): Perfect circle (e.g., an idealized orbit)\n\\(0 &lt; \\varepsilon &lt; 1\\): Ellipse (all planets in our solar system!)\n\nEarth: \\(\\varepsilon \\approx 0.017\\) (nearly circular)\nMercury: \\(\\varepsilon \\approx 0.206\\) (noticeably elliptical)\n\n\\(\\varepsilon = 1\\): Parabola (escape trajectory - barely escapes)\n\\(\\varepsilon &gt; 1\\): Hyperbola (escape trajectory - like some comets)\n\n\n\n\n\n\n\nKepler‚Äôs First Law\n\n\n\n‚ÄúPlanets move in ellipses with the Sun at one focus.‚Äù\nThis is exactly what Newton‚Äôs equation predicts! We‚Äôll verify this numerically."
  },
  {
    "objectID": "lectures/lecture10/planetary-motion.html#converting-to-first-order-system",
    "href": "lectures/lecture10/planetary-motion.html#converting-to-first-order-system",
    "title": "Planetary Motion: Newton‚Äôs Masterpiece",
    "section": "Converting to First-Order System",
    "text": "Converting to First-Order System\nTo use odeint, we need first-order differential equations. Let‚Äôs introduce:\n\n\\(v = \\dot{r}\\) (radial velocity)\n\\(\\omega = \\dot{\\theta}\\) (angular velocity)\n\nOur state vector is: \\([r, v, \\theta, \\omega]\\)\nThis gives us four first-order equations:\n\\[\\begin{align}\n\\dot{r} &= v \\\\\n\\dot{v} &= r\\omega^2 - \\frac{GM}{r^2} \\\\\n\\dot{\\theta} &= \\omega \\\\\n\\dot{\\omega} &= -\\frac{2v\\omega}{r}\n\\end{align}\\]"
  },
  {
    "objectID": "lectures/lecture10/planetary-motion.html#implementing-the-equations",
    "href": "lectures/lecture10/planetary-motion.html#implementing-the-equations",
    "title": "Planetary Motion: Newton‚Äôs Masterpiece",
    "section": "Implementing the Equations",
    "text": "Implementing the Equations\nLet‚Äôs write the function for odeint:\n\n\n\n\n\n\n\n\n\n\n\n\nCode Clarity\n\n\n\nThe structure is clean and modular - we unpack the state, calculate each derivative, and return them in order. This pattern works for any coupled ODE system!"
  },
  {
    "objectID": "lectures/lecture10/planetary-motion.html#setting-up-units-matter",
    "href": "lectures/lecture10/planetary-motion.html#setting-up-units-matter",
    "title": "Planetary Motion: Newton‚Äôs Masterpiece",
    "section": "Setting Up: Units Matter!",
    "text": "Setting Up: Units Matter!\nBefore simulating, we need to choose units carefully. Let‚Äôs use astronomical units:\n\nDistance: AU (1 AU = Earth-Sun distance ‚âà 150 million km)\nTime: years\nMass: Solar masses (\\(M_{\\odot}\\))\n\nIn these units, something magical happens: \\(G = 4\\pi^2\\)!\n\n\n\n\n\n\nWhy \\(G = 4\\pi^2\\)?\n\n\n\n\n\nFrom Kepler‚Äôs third law, for a circular orbit: \\[T^2 = \\frac{4\\pi^2}{GM}r^3\\]\nFor Earth (\\(r = 1\\) AU, \\(T = 1\\) year, \\(M = 1 M_{\\odot}\\)): \\[1 = \\frac{4\\pi^2}{GM} \\cdot 1\\] \\[GM = 4\\pi^2\\]\nSince we set \\(M = 1\\), this means \\(G = 4\\pi^2\\) in our units!"
  },
  {
    "objectID": "lectures/lecture10/planetary-motion.html#simulation-1-earth-like-orbit",
    "href": "lectures/lecture10/planetary-motion.html#simulation-1-earth-like-orbit",
    "title": "Planetary Motion: Newton‚Äôs Masterpiece",
    "section": "Simulation 1: Earth-like Orbit",
    "text": "Simulation 1: Earth-like Orbit\nLet‚Äôs start with an Earth-like orbit: nearly circular, at 1 AU distance.\n\nSetting Initial Conditions\nFor a circular orbit at radius \\(r_0\\), the orbital velocity must satisfy: \\[v_{\\text{circular}} = \\sqrt{\\frac{GM}{r_0}}\\]\n\n\n\n\n\n\n\n\nSolving the ODEs\n\n\n\n\n\n\n\n\nConverting to Cartesian Coordinates\nTo visualize the orbit, let‚Äôs convert from \\((r, \\theta)\\) to \\((x, y)\\):\n\n\n\n\n\n\n\n\nVisualizing the Orbit\n\n\n\n\n\n\n\n\n\n\n\n\nWhat You Should See\n\n\n\nA nearly perfect circle! This is an Earth-like orbit. The numerical solution should trace out a circle around the Sun at 1 AU."
  },
  {
    "objectID": "lectures/lecture10/planetary-motion.html#simulation-2-elliptical-orbit",
    "href": "lectures/lecture10/planetary-motion.html#simulation-2-elliptical-orbit",
    "title": "Planetary Motion: Newton‚Äôs Masterpiece",
    "section": "Simulation 2: Elliptical Orbit",
    "text": "Simulation 2: Elliptical Orbit\nNow let‚Äôs create a more eccentric orbit by giving the planet some radial velocity.\n\n\n\n\n\n\n\nComparing with Analytical Solution\nLet‚Äôs verify our numerical solution matches Newton‚Äôs analytical prediction:\n\n\n\n\n\n\n\n\nPlotting Numerical vs.¬†Analytical\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Do We Need a Phase Shift?\n\n\n\nYou might wonder why we needed to calculate theta_0_analytical - why don‚Äôt the orbits match automatically?\nThe issue: The analytical orbit formula \\(r(\\theta) = \\frac{p}{1 + \\varepsilon\\cos(\\theta)}\\) has a built-in assumption: perihelion (closest point to the Sun) is at angle Œ∏ = 0.\nThe problem: Our numerical simulation starts at:\n\nPosition: r = 1.0 AU, Œ∏ = 0\nVelocities: v_r = 0, œâ = 1.8 rad/yr\n\nThis starting point is NOT at perihelion! The planet is somewhere else in its orbit when Œ∏ = 0.\nThe solution: We calculate where the planet actually starts in the analytical orbit frame by solving: \\[r_0 = \\frac{p}{1 + \\varepsilon\\cos(\\theta_0)} \\quad \\Rightarrow \\quad \\theta_0 = \\arccos\\left(\\frac{p/r_0 - 1}{\\varepsilon}\\right)\\]\nThis theta_0_analytical tells us how much to rotate the analytical orbit so it matches the numerical simulation‚Äôs starting point.\nPhysical interpretation: Both orbits are the same ellipse, just rotated relative to each other! The shape, size, and period are identical - only the orientation in space differs.\n\n\n\n\n\n\n\n\nPerfect Agreement!\n\n\n\nAfter accounting for the phase shift, the numerical and analytical solutions should overlap almost perfectly. This confirms: 1. Our numerical method is accurate 2. Newton‚Äôs equations correctly predict orbits 3. odeint is a reliable tool for solving ODEs\nAny small remaining differences come from numerical rounding errors."
  },
  {
    "objectID": "lectures/lecture10/planetary-motion.html#understanding-the-orbit-keplers-laws",
    "href": "lectures/lecture10/planetary-motion.html#understanding-the-orbit-keplers-laws",
    "title": "Planetary Motion: Newton‚Äôs Masterpiece",
    "section": "Understanding the Orbit: Kepler‚Äôs Laws",
    "text": "Understanding the Orbit: Kepler‚Äôs Laws\nLet‚Äôs verify Kepler‚Äôs three laws with our simulation!\n\nKepler‚Äôs First Law: Ellipses\n‚úì Already verified - we see elliptical orbits!\n\n\nKepler‚Äôs Second Law: Equal Areas in Equal Times\n‚ÄúA line connecting the planet to the Sun sweeps out equal areas in equal times.‚Äù\nThis is equivalent to conservation of angular momentum!\n\n\n\n\n\n\n\n\nKepler‚Äôs Third Law: Period-Distance Relationship\n‚ÄúThe square of the orbital period is proportional to the cube of the semi-major axis.‚Äù\n\\[T^2 = \\frac{4\\pi^2}{GM}a^3\\]\nFor our units with \\(GM = 4\\pi^2\\), this simplifies to: \\(T^2 = a^3\\)\nLet‚Äôs verify:"
  },
  {
    "objectID": "lectures/lecture10/planetary-motion.html#energy-conservation-the-ultimate-check",
    "href": "lectures/lecture10/planetary-motion.html#energy-conservation-the-ultimate-check",
    "title": "Planetary Motion: Newton‚Äôs Masterpiece",
    "section": "Energy Conservation: The Ultimate Check",
    "text": "Energy Conservation: The Ultimate Check\nEnergy should be perfectly conserved (no friction or other forces):"
  },
  {
    "objectID": "lectures/lecture10/planetary-motion.html#phase-space-a-different-perspective",
    "href": "lectures/lecture10/planetary-motion.html#phase-space-a-different-perspective",
    "title": "Planetary Motion: Newton‚Äôs Masterpiece",
    "section": "Phase Space: A Different Perspective",
    "text": "Phase Space: A Different Perspective\nLet‚Äôs look at the orbit in phase space:\n\n\n\n\n\n\n\n\n\n\n\n\nPhase Space Interpretation\n\n\n\n\nRadial phase space: Shows the closed curve of radial oscillation\nAngular phase space: Shows steady increase in angle (nearly constant \\(\\omega\\))\n\nBoth are characteristic of stable, periodic orbits!"
  },
  {
    "objectID": "lectures/lecture10/planetary-motion.html#exploration-different-orbit-types",
    "href": "lectures/lecture10/planetary-motion.html#exploration-different-orbit-types",
    "title": "Planetary Motion: Newton‚Äôs Masterpiece",
    "section": "Exploration: Different Orbit Types",
    "text": "Exploration: Different Orbit Types\n\nHighly Elliptical Orbit (Like a Comet)\n\n\n\n\n\n\n\n\nEscape Velocity: Parabolic Trajectory\nWhat if we give the planet enough energy to escape?\n\n\n\n\n\n\n\n\nComparing Different Orbits"
  },
  {
    "objectID": "lectures/lecture10/planetary-motion.html#real-solar-system-examples",
    "href": "lectures/lecture10/planetary-motion.html#real-solar-system-examples",
    "title": "Planetary Motion: Newton‚Äôs Masterpiece",
    "section": "Real Solar System Examples",
    "text": "Real Solar System Examples\nLet‚Äôs simulate some actual planets!\n\nMercury: The Most Elliptical\n\n\n\n\n\n\n\n\n\n\n\n\nYour Turn: Simulate Other Planets!\n\n\n\nTry simulating other planets with their actual orbital parameters: - Venus: a = 0.723 AU, Œµ = 0.007 - Mars: a = 1.524 AU, Œµ = 0.093 - Jupiter: a = 5.203 AU, Œµ = 0.048\nCan you plot them all on one diagram to show the inner solar system?"
  },
  {
    "objectID": "lectures/lecture10/planetary-motion.html#summary-what-weve-learned",
    "href": "lectures/lecture10/planetary-motion.html#summary-what-weve-learned",
    "title": "Planetary Motion: Newton‚Äôs Masterpiece",
    "section": "Summary: What We‚Äôve Learned",
    "text": "Summary: What We‚Äôve Learned\n\n\n\n\n\n\nüéì Key Takeaways\n\n\n\n\nUniversal Physics: The same mathematical framework can describe many different physical systems - you just change the force law!\nNewton‚Äôs Triumph: Newton‚Äôs law of gravitation (\\(F = GMm/r^2\\)) perfectly explains planetary orbits\nKepler‚Äôs Laws Verified:\n\nFirst Law: Planets move in ellipses ‚úì\nSecond Law: Angular momentum is conserved ‚úì\nThird Law: \\(T^2 \\propto a^3\\) ‚úì\n\nComputational Methods: We used the same odeint workflow:\n\nDefine equations of motion\nConvert to first-order system\nSolve numerically\nVerify with analytical solutions\n\nOrbit Types: Different energies create different trajectories:\n\nE &lt; 0: Bound elliptical orbits\nE = 0: Parabolic escape\nE &gt; 0: Hyperbolic escape\n\nConservation Laws: Energy and angular momentum are conserved, providing powerful checks on our numerical solutions"
  },
  {
    "objectID": "lectures/lecture10/planetary-motion.html#challenge-exercises",
    "href": "lectures/lecture10/planetary-motion.html#challenge-exercises",
    "title": "Planetary Motion: Newton‚Äôs Masterpiece",
    "section": "Challenge Exercises",
    "text": "Challenge Exercises\n\n\n\n\n\n\nüéØ Try These!\n\n\n\n\nSolar System Model: Create a plot showing the orbits of all inner planets (Mercury, Venus, Earth, Mars) to scale. Which planet has the most eccentric orbit?\nHalley‚Äôs Comet: Halley‚Äôs comet has a = 17.8 AU and Œµ = 0.967. Simulate its orbit and find its orbital period. (Historical note: Halley correctly predicted its return in 1758!)\nEscape Trajectories: Experiment with escape velocities. At what angular velocity does a planet starting at 1 AU just barely escape? How does this compare to Earth‚Äôs actual orbital velocity?\nOrbital Energy: For an elliptical orbit, verify that:\n\nKinetic energy is maximum at perihelion (closest approach)\nPotential energy is maximum (least negative) at perihelion\nSpeed is fastest at perihelion (Kepler‚Äôs second law!)\n\nTwo-Body Problem: Modify the code to include the Sun‚Äôs motion (don‚Äôt assume it stays fixed). Place the origin at the center of mass and simulate both bodies. How much does the Sun actually move?\nPrecession: Real planetary orbits precess slowly due to effects not in Newton‚Äôs equations (e.g., general relativity for Mercury). Can you add a small additional force term to create artificial precession?"
  },
  {
    "objectID": "lectures/lecture10/planetary-motion.html#whats-next-a-preview",
    "href": "lectures/lecture10/planetary-motion.html#whats-next-a-preview",
    "title": "Planetary Motion: Newton‚Äôs Masterpiece",
    "section": "What‚Äôs Next: A Preview",
    "text": "What‚Äôs Next: A Preview\nCongratulations on mastering planetary motion! You‚Äôve learned to:\n\nSolve coupled ODEs in polar coordinates\nUse phase space for analysis\n\nVerify numerical solutions against analytical predictions\nCheck conservation laws\n\nIn the next lecture, you‚Äôll apply these same techniques to a different system: the spring pendulum. This fascinating system combines a spring (bouncing) with a pendulum (swinging), and something amazing happens:\n\n\n\n\n\n\nComing Up: From Order to Chaos\n\n\n\nThe spring pendulum uses the same mathematical framework you just mastered:\n\nPolar coordinates: ‚úì\nCoupled ODEs: ‚úì\nodeint solution: ‚úì\n\nBut there‚Äôs a twist! While planetary orbits are stable and predictable, the spring pendulum can become chaotic - deterministic yet unpredictable. You‚Äôll discover how changing just one force law (from \\(F = -GM/r^2\\) to \\(F = -kL\\)) creates dramatically different behavior.\nGet ready to explore the fascinating world of chaos theory! üé≠"
  },
  {
    "objectID": "lectures/lecture10/planetary-motion.html#historical-impact-why-this-matters",
    "href": "lectures/lecture10/planetary-motion.html#historical-impact-why-this-matters",
    "title": "Planetary Motion: Newton‚Äôs Masterpiece",
    "section": "Historical Impact: Why This Matters",
    "text": "Historical Impact: Why This Matters\nNewton‚Äôs solution to planetary motion wasn‚Äôt just a scientific achievement - it changed how humanity views the universe:\n\nPredictive Power: Astronomers could predict planetary positions centuries in advance\nDiscovery of Neptune: In 1846, Neptune was discovered because Uranus‚Äôs orbit didn‚Äôt match predictions - something must be pulling on it! Adams and Le Verrier calculated where Neptune should be, and astronomers found it there.\nSpace Age: Every satellite, probe, and spacecraft uses Newton‚Äôs equations to calculate trajectories\nPhilosophical Impact: The universe operates by mathematical laws that humans can understand and use\n\n\n\n\n\n\n\nFrom Newton to Einstein\n\n\n\nNewton‚Äôs equations work perfectly for planetary motion‚Ä¶ except for tiny deviations in Mercury‚Äôs orbit. These tiny differences led Einstein to develop General Relativity in 1915. Even Newton‚Äôs ‚Äúwrong‚Äù theory is spectacularly accurate!"
  },
  {
    "objectID": "lectures/lecture10/planetary-motion.html#whats-next",
    "href": "lectures/lecture10/planetary-motion.html#whats-next",
    "title": "Planetary Motion: Newton‚Äôs Masterpiece",
    "section": "What‚Äôs Next?",
    "text": "What‚Äôs Next?\nCongratulations! You‚Äôve just solved one of the most important problems in physics using modern computational methods. Newton would be proud! üöÄüåç\nIn the next lecture (still Week 6), we‚Äôll apply the same techniques to the spring pendulum - a fascinating system that combines oscillation and rotation in a single dynamical system. This problem will demonstrate the versatility of the numerical methods you‚Äôve learned today. The spring pendulum consists of a mass attached to a spring that can both stretch and swing like a pendulum. What makes it particularly interesting is that it uses the same mathematical framework we‚Äôve developed here - we‚Äôll still be solving coupled first-order ODEs using scipy.integrate.solve_ivp - but the force law will be different, combining both elastic restoring forces and gravitational forces.\nThe spring pendulum is special because it shows how two different modes of motion - radial oscillation and angular swing - can couple together in surprising ways. Unlike the planetary motion we‚Äôve studied today, where the orbits are stable and predictable, the spring pendulum can exhibit rich dynamical behavior. Depending on the initial conditions and system parameters, you might see regular periodic motion, quasi-periodic behavior where the system never quite repeats itself, or even deterministic chaos where tiny changes in initial conditions lead to wildly different outcomes. This will be your first glimpse into the fascinating world of nonlinear dynamics!\nThen in Week 7, we‚Äôll take a step back from the physics and focus on software engineering principles. After working with these increasingly complex simulations, you‚Äôll have experienced firsthand why good code organization matters. We‚Äôll learn how to structure our code using modules and classes, making it more readable, reusable, and maintainable. You‚Äôll discover how to transform the scripts we‚Äôve been writing into well-organized libraries that you can easily extend and share. This transition from ‚Äúcode that works‚Äù to ‚Äúcode that works well‚Äù is an essential skill for any computational scientist."
  },
  {
    "objectID": "lectures/lecture10/planetary-motion.html#further-reading",
    "href": "lectures/lecture10/planetary-motion.html#further-reading",
    "title": "Planetary Motion: Newton‚Äôs Masterpiece",
    "section": "Further Reading",
    "text": "Further Reading\n\n\n\n\n\n\nüìö If You Want to Learn More\n\n\n\n\n\nHistorical Sources:\n\nNewton‚Äôs Principia Mathematica (1687) - the original! (Translated versions available)\nKepler‚Äôs Astronomia Nova (1609) - where the first two laws appeared\n\nModern Textbooks:\n\nGoldstein, Classical Mechanics - Chapter on central forces\nTaylor, Classical Mechanics - Excellent treatment of orbits\n\nComputational Physics:\n\nGiordano & Nakanishi, Computational Physics - More complex orbital simulations\nNewman, Computational Physics - Python implementations of classical mechanics\n\nApplications:\n\nHow spacecraft trajectories are designed (orbital mechanics)\nExoplanet detection methods (observing orbital effects)\nSatellite constellation design (GPS, Starlink, etc.)\n\n\n\n\nCongratulations! You‚Äôve just solved one of the most important problems in physics using modern computational methods. Newton would be proud! üöÄüåç"
  },
  {
    "objectID": "lectures/lecture07/curve-fitting.html",
    "href": "lectures/lecture07/curve-fitting.html",
    "title": "Curve fitting",
    "section": "",
    "text": "Curve fitting is a fundamental skill in experimental physics that allows us to extract physical parameters from measured data. In this lecture, we‚Äôll explore how to apply the least-squares method to fit a quadratic function with three parameters to experimental data. It‚Äôs worth noting that this approach can be applied to more complex functions or even simpler linear models.\nBefore diving into the fitting process, it‚Äôs essential to consider how to best estimate your model parameters. In some cases, you may be able to derive explicit estimators for the parameters, which can simplify the fitting procedure. Therefore, it‚Äôs advisable to carefully consider your approach before beginning the actual fitting process.\nFor those who want to delve deeper into this subject, you might find it interesting to explore concepts like maximum likelihood estimation. This method offers an alternative approach to parameter estimation and can provide valuable insights in certain scenarios."
  },
  {
    "objectID": "lectures/lecture07/curve-fitting.html#idea",
    "href": "lectures/lecture07/curve-fitting.html#idea",
    "title": "Curve fitting",
    "section": "Idea",
    "text": "Idea\nIn experimental physics, we often collect data points to understand the underlying physical phenomena. This process involves fitting a mathematical model to the experimental data.\nThe data typically comes as a series of \\(N\\) paired points:\n\n\n\nx-data\ny-data\n\n\n\n\n\\(x_{1}\\)\n\\(y_{1}\\)\n\n\n\\(x_{2}\\)\n\\(y_{2}\\)\n\n\n‚Ä¶\n‚Ä¶\n\n\n\\(x_{N}\\)\n\\(y_{N}\\)\n\n\n\nEach point \\(\\{x_i, y_i\\}\\) may represent the result of multiple independent measurements. For instance, \\(y_1\\) could be the mean of \\(n\\) repeated measurements \\(y_{1,j}\\) at position \\(x_1\\):\n\\[y_1 = \\frac{1}{n}\\sum_{j=1}^n y_{1,j}\\]\nWhen these individual readings have a true (underlying) standard deviation \\(\\sigma\\), the sum of all \\(n\\) measurements has a variance of \\(n\\sigma^2\\) and a standard deviation of \\(\\sqrt{n}\\sigma\\). Consequently, the mean value has an associated error known as the Standard Error of the Mean (SEOM):\n\\[\\sigma_{SEOM} = \\frac{\\sigma}{\\sqrt{n}}\\]\nThis SEOM is crucial in physics measurements‚Äîit tells us how precisely we know the mean value.\nIn practice, we don‚Äôt know the true \\(\\sigma\\), so we must estimate it from our data. The estimated variance \\(s_1^2\\) for the measurements at point \\(x_1\\) is calculated as:\n\\[s_1^2 = \\frac{1}{n-1} \\sum_{j=1}^n (y_{1,j} - y_1)^2\\]\nThis is the sample variance, which uses \\(n-1\\) in the denominator (Bessel‚Äôs correction) instead of \\(n\\). The factor \\(n-1\\) accounts for the fact that we estimate the mean \\(y_1\\) from the same data, which reduces our degrees of freedom by one. The estimated standard deviation is then \\(s_1 = \\sqrt{s_1^2}\\).\nFor large \\(n\\), the estimate \\(s_1\\) approaches the true value \\(\\sigma\\). The estimated standard error of the mean becomes:\n\\[s_{SEOM} = \\frac{s_1}{\\sqrt{n}}\\]\nThis statistical framework forms the basis for analyzing experimental data and fitting mathematical models to understand the underlying physics."
  },
  {
    "objectID": "lectures/lecture07/curve-fitting.html#least-squares",
    "href": "lectures/lecture07/curve-fitting.html#least-squares",
    "title": "Curve fitting",
    "section": "Least squares",
    "text": "Least squares\nIn experimental physics, we often collect data points to understand the underlying physical phenomena. To make sense of this data, we fit a mathematical model to it. One common method for fitting data is the least squares method.\n\nWhy use least squares fitting?\nThe goal of least squares fitting is to find the set of parameters for our model that best describes the data. This is done by minimizing the differences (or residuals) between the observed data points and the model‚Äôs predictions.\n\n\nGaussian uncertainty and probability\nWhen we take measurements, there is always some uncertainty. Often, this uncertainty can be modeled using a Gaussian (normal) distribution. This distribution is characterized by its mean (average value) and standard deviation (a measure of the spread of the data).\nIf we describe our data with a model function, which delivers a function value \\(f(x_{i}, \\mathbf{a})\\) for a set of parameters \\(\\mathbf{a} = (a_1, a_2, \\ldots, a_M)\\) at the position \\(x_{i}\\), the Gaussian uncertainty dictates a probability of finding a data value \\(y_{i}\\):\n\\[\\begin{equation}\np_{y_{i}}=\\frac{1}{\\sqrt{2\\pi}\\sigma_{i}}\\exp\\left(-\\frac{(y_{i}-f(x_{i},\\mathbf{a}))^2}{2\\sigma_{i}^2}\\right)\n\\end{equation}\\]\nHere, \\(\\sigma_{i}\\) represents the uncertainty in the measurement \\(y_{i}\\).\n\n\nCombining probabilities for multiple data points\nTo understand how well our model fits all the data points, we need to consider the combined probability of observing all the data points. This is done by multiplying the individual probabilities:\n\\[\\begin{equation}\np(y_{1},\\ldots,y_{N})=\\prod_{i=1}^{N}\\frac{1}{\\sqrt{2\\pi}\\sigma_{i}}\\exp\\left(-\\frac{(y_{i}-f(x_{i},\\mathbf{a}))^2}{2\\sigma_{i}^2}\\right)\n\\end{equation}\\]\n\n\nMaximizing the joint probability\nThe best fit of the model to the data is achieved when this joint probability is maximized. To simplify the calculations, we take the logarithm of the joint probability:\n\\[\\begin{equation}\n\\ln(p(y_{1},\\ldots,y_{N}))=-\\frac{1}{2}\\sum_{i=1}^{N}\\left( \\frac{y_{i}-f(x_{i},\\mathbf{a})}{\\sigma_{i}}\\right)^2 - \\sum_{i=1}^{N}\\ln\\left( \\sigma_{i}\\sqrt{2\\pi}\\right)\n\\end{equation}\\]\nThe first term on the right side (except the factor 1/2) is the least squared deviation, also known as \\(\\chi^{2}\\):\n\\[\\begin{equation}\n\\chi^{2} =\\sum_{i=1}^{N}\\left( \\frac{y_{i}-f(x_{i},\\mathbf{a})}{\\sigma_{i}}\\right)^2\n\\end{equation}\\]\nThe second term is just a constant value given by the uncertainties of our experimental data.\n\n\n\n\n\n\nMaximum Likelihood Estimation (MLE)\n\n\n\n\n\nWhat we have just derived is actually a powerful statistical method called Maximum Likelihood Estimation. Let‚Äôs formalize this connection.\nThe Likelihood Function\nThe likelihood function \\(\\mathcal{L}(\\mathbf{a})\\) is defined as the probability of observing our data, given the model parameters \\(\\mathbf{a}\\):\n\\[\\mathcal{L}(\\mathbf{a}) = p(y_1, y_2, \\ldots, y_N | \\mathbf{a}) = \\prod_{i=1}^{N} p(y_i | \\mathbf{a})\\]\nNote the subtle but important shift in perspective: we treat the data as fixed and the parameters as variable. We ask: ‚ÄúWhich parameters make our observed data most probable?‚Äù\nThe Log-Likelihood\nIn practice, we work with the log-likelihood \\(\\ell(\\mathbf{a}) = \\ln \\mathcal{L}(\\mathbf{a})\\) for two reasons:\n\nNumerical stability: Products of many small probabilities can underflow; sums of logarithms don‚Äôt\nMathematical convenience: Products become sums, making derivatives easier\n\nConnection to Least Squares\nFor Gaussian-distributed measurements, the log-likelihood is:\n\\[\\ell(\\mathbf{a}) = -\\frac{1}{2}\\chi^2 - \\sum_{i=1}^{N}\\ln(\\sigma_i\\sqrt{2\\pi})\\]\nSince the second term is constant (independent of \\(\\mathbf{a}\\)), maximizing the log-likelihood is equivalent to minimizing \\(\\chi^2\\):\n\\[\\underset{\\mathbf{a}}{\\text{argmax}} \\; \\ell(\\mathbf{a}) = \\underset{\\mathbf{a}}{\\text{argmin}} \\; \\chi^2(\\mathbf{a})\\]\nThis is a profound result: least squares fitting is the maximum likelihood estimator for Gaussian errors.\nWhy MLE Matters\n\nTheoretical foundation: MLE provides rigorous justification for the least squares method\nOptimal properties: Under mild conditions, MLE estimators are:\n\nConsistent: converge to true values as \\(N \\to \\infty\\)\nEfficient: achieve the lowest possible variance\nAsymptotically normal: uncertainties follow Gaussian statistics for large \\(N\\)\n\nGeneralization: MLE works for any probability distribution, not just Gaussian:\n\nPoisson statistics: counting experiments (radioactive decay, photon counting)\nBinomial statistics: success/failure experiments\nCustom distributions: any physical situation with known error distribution\n\n\nExperimental Example: Fluorescence Lifetime Measurement\nA beautiful application of MLE is Time-Correlated Single Photon Counting (TCSPC), used to measure excited state lifetimes of fluorescent molecules.\nThe Experiment:\n\nA short laser pulse excites dye molecules to an excited state\nEach molecule decays back to the ground state by emitting a photon\nWe record the arrival time \\(t_i\\) of each detected photon relative to the excitation pulse\nThe decay follows an exponential distribution with lifetime \\(\\tau\\)\n\nExperimental procedure:\n\n\n\n\n\n\nFigure¬†1: Time-Correlated Single Photon Counting (TCSPC) procedure for measuring fluorescence lifetimes.\n\n\n\nThe Physics:\nThe probability density for a photon arriving at time \\(t\\) is:\n\\[p(t|\\tau) = \\frac{1}{\\tau} e^{-t/\\tau}\\]\nTwo Approaches to Extract \\(\\tau\\):\nApproach 1: Histogram Fitting (Least Squares)\n\nBin the arrival times into a histogram\nFit the histogram to an exponential using least squares\nProblem: Binning loses information, and bin counts follow Poisson statistics (not Gaussian)\n\nApproach 2: Direct MLE\nWrite the likelihood for \\(N\\) photon arrival times \\(\\{t_1, t_2, \\ldots, t_N\\}\\):\n\\[\\mathcal{L}(\\tau) = \\prod_{i=1}^{N} \\frac{1}{\\tau} e^{-t_i/\\tau} = \\frac{1}{\\tau^N} \\exp\\left(-\\frac{1}{\\tau}\\sum_{i=1}^{N} t_i\\right)\\]\nThe log-likelihood is:\n\\[\\ell(\\tau) = -N\\ln(\\tau) - \\frac{1}{\\tau}\\sum_{i=1}^{N} t_i\\]\nTo find the maximum, take the derivative and set it to zero:\n\\[\\frac{d\\ell}{d\\tau} = -\\frac{N}{\\tau} + \\frac{1}{\\tau^2}\\sum_{i=1}^{N} t_i = 0\\]\nSolving for \\(\\tau\\):\n\\[\\boxed{\\hat{\\tau}_{MLE} = \\frac{1}{N}\\sum_{i=1}^{N} t_i = \\bar{t}}\\]\nThe MLE for the lifetime is simply the mean arrival time!\nWhy MLE Wins Here:\n\nNo binning required: Uses every photon‚Äôs exact arrival time\nStatistically optimal: Achieves the lowest possible uncertainty\nSimple formula: Just compute the mean‚Äîno iterative fitting needed\nWorks with few photons: Histogram fitting needs many counts per bin; MLE works even with sparse data\n\nThe uncertainty on the MLE estimate is:\n\\[\\sigma_{\\hat{\\tau}} = \\frac{\\tau}{\\sqrt{N}}\\]\nThis example shows the power of MLE: by using the correct probability distribution, we obtain a simple, optimal estimator without any curve fitting at all!\nSummary\nThe least squares method you‚Äôre learning is not just a convenient recipe‚Äîit‚Äôs the statistically optimal approach for Gaussian measurement errors. Understanding its foundation in MLE will help you extend these ideas to more complex situations in your future physics career."
  },
  {
    "objectID": "lectures/lecture07/curve-fitting.html#data",
    "href": "lectures/lecture07/curve-fitting.html#data",
    "title": "Curve fitting",
    "section": "Data",
    "text": "Data\nLet‚Äôs have a look at the meaning of this equation. Let‚Äôs assume we measure the trajectory of a ball that has been thrown at an angle \\(\\alpha\\) with an initial velocity \\(v_{0}\\). We have collected data points by measuring the height of the ball above the ground at equally spaced distances from the throwing person.\n\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.5.2 from the internet...\n    (need help?)\n    \n\n\n\n\nThe table above shows the measured data points \\(y_{i}\\) at the position \\(x_{i}\\) with the associated uncertainties \\(\\sigma_{i}\\).\nWe can plot the data and expect, of course, a parabola. Therefore, we model our experimental data with a parabola like\n\\[\\begin{equation}\ny = ax^2 + bx + c\n\\end{equation}\\]\nwhere the parameter \\(a\\) must be negative since the parabola is inverted.\n\n\n\n\n\n\nWhy is the trajectory a parabola?\n\n\n\n\n\nFor a projectile launched at angle \\(\\alpha\\) with initial speed \\(v_0\\) (ignoring air resistance), the equations of motion are:\nHorizontal (no acceleration): \\(x(t) = v_0 \\cos(\\alpha) \\cdot t\\)\nVertical (gravity): \\(y(t) = y_0 + v_0 \\sin(\\alpha) \\cdot t - \\frac{1}{2}g t^2\\)\nEliminating time by solving \\(t = x / (v_0 \\cos\\alpha)\\) and substituting:\n\\[y = y_0 + x \\tan(\\alpha) - \\frac{g}{2v_0^2 \\cos^2(\\alpha)} x^2\\]\nThis is exactly \\(y = ax^2 + bx + c\\) where:\n\n\n\n\n\n\n\nParameter\nPhysical Meaning\n\n\n\n\n\\(a = -\\frac{g}{2v_0^2 \\cos^2(\\alpha)}\\)\nDetermines curvature (always negative)\n\n\n\\(b = \\tan(\\alpha)\\)\nRelated to launch angle\n\n\n\\(c = y_0\\)\nInitial height\n\n\n\nFrom the fitted value of \\(a\\), we can extract \\(g\\) if we know \\(v_0\\) and \\(\\alpha\\), or vice versa.\n\n\n\nI have created an interactive plot with an interact widget, as this allows you to play around with the parameters. The value of \\(\\chi^2\\) is also included in the legend, so you can get an impression of how good your fit of the data is.\n\nviewof aSlider = Inputs.range([-4, 0], { label: \"a\", step: 0.01, value: -1.7 });\nviewof bSlider = Inputs.range([-2, 2], { label: \"b\", step: 0.01, value: 1.3 });\nviewof cSlider = Inputs.range([-2, 2], { label: \"c\", step: 0.01, value: 1.0 });\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfiltered = transpose(data);\n// Create the plot\n\nxValues = Array.from({ length: 100 }, (_, i) =&gt; i / 100);\nparabolaData = xValues.map(x =&gt; ({ x, y: parabola(x, aSlider, bSlider, cSlider) }));\n\n\nparabola = (x, a, b, c) =&gt; a * x**2 + b * x + c\n\ncalculateChiSquared = (data, a, b, c) =&gt; {\n  let chisq = 0\n  let x= data.map(d =&gt; d.x)\n  let y= data.map(d =&gt; d.y)\n  let err= data.map(d =&gt; d.error)\n  for (let i = 0; i &lt; x.length; i++) {\n    let y_model = parabola(x[i], a, b, c)\n    chisq += ((y[i] - y_model) / err[i])**2\n  }\n  return chisq\n}\n\nchisq = calculateChiSquared(filtered, aSlider, bSlider, cSlider)\n\nPlot.plot({\n  marks: [\n    Plot.dot(filtered, { x: \"x\", y: \"y\" }),\n    Plot.ruleY(filtered, { x: \"x\", y1: d =&gt; d.y - d.error, y2: d =&gt; d.y + d.error }),\n    Plot.line(parabolaData, { x: \"x\", y: \"y\" }),\n    Plot.text([{ x: 0.8, y: 1.5, label: `œá¬≤: ${chisq.toFixed(2)}` }], {\n          x: \"x\",\n          y: \"y\",\n          text: \"label\",\n          dy: -10, // Adjust vertical position if needed\n          fill: \"black\", // Set text color\n          fontSize: 16\n        }),\n    Plot.frame()\n  ],\n  x: {\n    label: \"X Axis\",\n    labelAnchor: \"center\",\n    labelOffset: 35,\n    grid: true,\n    tickFormat: \".2f\", // Format ticks to 2 decimal places\n    domain: [0, 1]\n  },\n  y: {\n    label: \"Y Axis\",\n    grid: true,\n    tickFormat: \".2f\", // Format ticks to 2 decimal places\n    labelAnchor: \"center\",  // Center the label on its axis\n    labelAngle: -90,\n    labelOffset: 60,\n    domain: [0, 2],\n  },\n  width: 400,\n  height: 400,\n  marginLeft: 100,\n  marginBottom: 40,\n  style: {\n    fontSize: \"14px\",          // This sets the base font size\n    \"axis.label\": {\n      fontSize: \"18px\",        // This sets the font size for axis labels\n      fontWeight: \"bold\"       // Optionally make it bold\n    },\n    \"axis.tick\": {\n      fontSize: \"14px\"         // This sets the font size for tick labels\n    }\n  },\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe have that troubling point at the right edge with a large uncertainty. This could represent a situation where the measurement conditions were worse (e.g., the ball was harder to track at larger distances, or there was more environmental interference). However, since the value of \\(\\chi^2\\) divides the deviation by the uncertainty \\(\\sigma_{i}\\), the weight for this point overall in the \\(\\chi^2\\) is smaller than for the other points. This is exactly how weighted fitting should work: less reliable measurements contribute less to the fit.\n\\[\\begin{equation}\n\\chi^{2}=\\sum_{i=1}^{N}\\left( \\frac{y_{i}-f(x_{i},\\mathbf{a})}{\\sigma_{i}}\\right)^2\n\\end{equation}\\]\nYou may simply check the effect by changing the uncertainty of the last data points in the error array."
  },
  {
    "objectID": "lectures/lecture07/curve-fitting.html#least-square-fitting",
    "href": "lectures/lecture07/curve-fitting.html#least-square-fitting",
    "title": "Curve fitting",
    "section": "Least square fitting",
    "text": "Least square fitting\nTo find the best fit of the model to the experimental data, we use the least squares method. This method minimizes the sum of the squared differences between the observed data points and the model‚Äôs predictions.\nMathematically, we achieve this by minimizing the least squares, i.e., finding the parameters \\(\\mathbf{a}\\) that minimize the following expression:\n\\[\\begin{equation}\n\\frac{\\partial\\chi^{2}}{\\partial a_k}=\\sum_{i=1}^{N}\\frac{1}{\\sigma_{i}^2}\\frac{\\partial f(x_{i},\\mathbf{a})}{\\partial a_k}[y_{i}-f(x_{i},\\mathbf{a})]=0\n\\end{equation}\\]\nThis equation must be satisfied for each parameter \\(a_k\\) in our model.\nThis kind of least squares minimization is done by fitting software using different types of algorithms.\n\n\n\n\n\n\nDerivation: Linear Regression Parameters\n\n\n\n\n\nFor simple cases, we can derive explicit formulas for the fit parameters. Let‚Äôs work through the derivation for weighted linear regression with the model:\n\\[f(x) = a + bx\\]\nwhere \\(a\\) is the intercept and \\(b\\) is the slope.\nStep 1: Write out the \\(\\chi^2\\) expression\n\\[\\chi^2 = \\sum_{i=1}^{N} \\frac{(y_i - a - bx_i)^2}{\\sigma_i^2}\\]\nStep 2: Take partial derivatives and set to zero\nFor parameter \\(a\\): \\[\\frac{\\partial \\chi^2}{\\partial a} = -2\\sum_{i=1}^{N} \\frac{y_i - a - bx_i}{\\sigma_i^2} = 0\\]\nFor parameter \\(b\\): \\[\\frac{\\partial \\chi^2}{\\partial b} = -2\\sum_{i=1}^{N} \\frac{x_i(y_i - a - bx_i)}{\\sigma_i^2} = 0\\]\nStep 3: Define convenient sums\nTo simplify notation, we define weighted sums:\n\\[S = \\sum_{i=1}^{N} \\frac{1}{\\sigma_i^2}, \\quad S_x = \\sum_{i=1}^{N} \\frac{x_i}{\\sigma_i^2}, \\quad S_y = \\sum_{i=1}^{N} \\frac{y_i}{\\sigma_i^2}\\]\n\\[S_{xx} = \\sum_{i=1}^{N} \\frac{x_i^2}{\\sigma_i^2}, \\quad S_{xy} = \\sum_{i=1}^{N} \\frac{x_i y_i}{\\sigma_i^2}\\]\nStep 4: Rewrite as a system of linear equations (normal equations)\n\\[aS + bS_x = S_y\\] \\[aS_x + bS_{xx} = S_{xy}\\]\nStep 5: Solve for \\(a\\) and \\(b\\)\nDefine the determinant: \\(\\Delta = S \\cdot S_{xx} - S_x^2\\)\nThe solutions are:\n\\[a = \\frac{S_{xx} \\cdot S_y - S_x \\cdot S_{xy}}{\\Delta}\\]\n\\[b = \\frac{S \\cdot S_{xy} - S_x \\cdot S_y}{\\Delta}\\]\nStep 6: Parameter uncertainties\nThe covariance matrix elements can also be derived analytically:\n\\[\\sigma_a^2 = \\frac{S_{xx}}{\\Delta}, \\quad \\sigma_b^2 = \\frac{S}{\\Delta}, \\quad \\text{cov}(a,b) = -\\frac{S_x}{\\Delta}\\]\nSpecial case: Unweighted regression (\\(\\sigma_i = \\sigma\\) for all \\(i\\))\nWhen all uncertainties are equal, the \\(\\sigma^2\\) cancels out and we get the familiar formulas:\n\\[b = \\frac{N \\sum x_i y_i - \\sum x_i \\sum y_i}{N \\sum x_i^2 - (\\sum x_i)^2}\\]\n\\[a = \\bar{y} - b\\bar{x}\\]\nwhere \\(\\bar{x}\\) and \\(\\bar{y}\\) are the sample means.\nWhy this matters\nThis derivation shows that for linear models, we can obtain closed-form solutions without iterative optimization. For more complex models (like our parabola), numerical algorithms are needed, which is why we use curve_fit.\n\n\n\n\nFitting with SciPy\nLet‚Äôs do some fitting using the SciPy library, which is a powerful tool for scientific computing in Python. We will use the curve_fit method from the optimize sub-module of SciPy.\nFirst, we need to define the model function we would like to fit to the data. In this case, we will use our parabola function:\n\n\n\n\n\n\nNext, we need to provide initial guesses for the parameters. These initial guesses help the fitting algorithm start the search for the optimal parameters:\n\n\n\n\n\n\nWe then call the curve_fit function to perform the fitting:\n\n\n\n\n\n\n\n\n\n\n\n\ncurve_fit Function\n\n\n\n\n\nThe curve_fit function is used to fit a model function to data. It finds the optimal parameters for the model function that minimize the sum of the squared residuals between the observed data and the model‚Äôs predictions.\n\nParameters\n\nparabola:\n\nThis is the model function that you want to fit to the data. In this case, parabola is a function that represents a quadratic equation of the form ( y = ax^2 + bx + c ).\n\nx_data:\n\nThis is the array of independent variable data points (the x-values).\n\ny_data:\n\nThis is the array of dependent variable data points (the y-values).\n\nsigma=err:\n\nThis parameter specifies the uncertainties (standard deviations) of the y-data points. The err array contains the uncertainties for each y-data point. These uncertainties are used to weight the residuals in the least squares optimization.\n\np0=init_guess:\n\nThis parameter provides the initial guesses for the parameters of the model function. The init_guess array contains the initial guesses for the parameters ( a ), ( b ), and ( c ). Providing good initial guesses can help the optimization algorithm converge more quickly and accurately.\n\nabsolute_sigma=True:\n\nThis parameter indicates whether the provided sigma values are absolute uncertainties. If absolute_sigma is set to True, the sigma values are treated as absolute uncertainties. If absolute_sigma is set to False, the sigma values are treated as relative uncertainties, and the covariance matrix of the parameters will be scaled accordingly.\n\n\nWhen to use each setting:\n\nUse absolute_sigma=True when your uncertainties are well-known from calibration, repeated measurements, or manufacturer specifications.\nUse absolute_sigma=False when your uncertainties are rough estimates. In this case, the covariance matrix will be scaled by the reduced \\(\\chi^2\\), which can compensate for over- or under-estimated uncertainties.\n\n\n\nReturn Value\nThe curve_fit function returns two values:\n\npopt:\n\nAn array containing the optimal values for the parameters of the model function that minimize the sum of the squared residuals.\n\npcov:\n\nThe covariance matrix of the optimal parameters. The diagonal elements of this matrix provide the variances of the parameter estimates, and the off-diagonal elements provide the covariances between the parameter estimates.\n\n\n\n\n\n\nThe fit variable contains the results of the fitting process. It is composed of various results, which we can split into the fitted parameters and the covariance matrix:\n\n\n\n\n\n\nThe ans variable contains the fitted parameters fit_a, fit_b, and fit_c, while the cov variable contains the covariance matrix. Let‚Äôs have a look at the fit and the \\(\\chi^{2}\\) value first:\n\n\n\n\n\n\nWe can then plot the fitted curve along with the original data points and the \\(\\chi^{2}\\) value:\n\n\n\n\n\n\n\n\n\\(\\chi^2\\) Value and Reduced \\(\\chi^2\\)\nThe value of \\(\\chi^2\\) gives you a measure of the quality of the fit. We can judge the quality by calculating the expectation value of \\(\\chi^2\\):\n\\[\\begin{equation}\n\\langle \\chi^{2}\\rangle =\\sum_{i=1}^{N} \\frac{\\langle (y_{i}-f(x_{i},\\mathbf{a}) )^2\\rangle }{\\sigma_{i}^2}=\\sum_{i=1}^{N} \\frac{\\sigma_{i}^2}{\\sigma_{i}^2}=N\n\\end{equation}\\]\nHowever, this is only approximately correct. More precisely, we need to account for the degrees of freedom \\(\\nu = N - M\\), where \\(N\\) is the number of data points and \\(M\\) is the number of fitted parameters. In our case, with 10 data points and 3 parameters (a, b, c), we have \\(\\nu = 10 - 3 = 7\\) degrees of freedom.\nThe reduced chi-squared is defined as:\n\\[\\begin{equation}\n\\chi^2_\\nu = \\frac{\\chi^2}{\\nu} = \\frac{\\chi^2}{N - M}\n\\end{equation}\\]\nThe expectation value of the reduced chi-squared is:\n\\[\\begin{equation}\n\\langle \\chi^2_\\nu \\rangle = 1\n\\end{equation}\\]\nThis gives us clear criteria for judging fit quality:\n\n\\(\\chi^2_\\nu \\approx 1\\) means the fit is good and uncertainties are correctly estimated.\n\\(\\chi^2_\\nu \\gg 1\\) means the fit is bad (wrong model or underestimated uncertainties).\n\\(\\chi^2_\\nu \\ll 1\\) means the uncertainties are overestimated.\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting Reduced \\(\\chi^2\\) in Practice\n\n\n\n\n\nIn real experiments, you will rarely obtain \\(\\chi^2_\\nu = 1.00\\) exactly. Here‚Äôs a practical guide for interpretation:\n\n\n\n\\(\\chi^2_\\nu\\)\nInterpretation\n\n\n\n\n\\(&lt; 0.5\\)\nUncertainties likely overestimated\n\n\n\\(0.5 - 2.0\\)\nGenerally acceptable\n\n\n\\(\\approx 1\\)\nIdeal ‚Äî good fit with correct uncertainties\n\n\n\\(2.0 - 3.0\\)\nBorderline ‚Äî investigate further\n\n\n\\(&gt; 3.0\\)\nPoor fit ‚Äî wrong model or underestimated uncertainties\n\n\n\nExample: What does \\(\\chi^2_\\nu = 2\\) mean?\nA value of \\(\\chi^2_\\nu = 2\\) is borderline but not necessarily bad. With \\(\\nu = 7\\) degrees of freedom, the probability of obtaining \\(\\chi^2 \\geq 14\\) by chance is about 5%. This could indicate:\n\nSlightly underestimated uncertainties ‚Äî error bars might be ~30% too small (since \\(\\sqrt{2} \\approx 1.4\\))\nModel imperfections ‚Äî the parabola might not capture all the physics\nOutliers ‚Äî one or two data points pulling the fit off\nStatistical fluctuation ‚Äî 5% of experiments will show this even with correct model and uncertainties\n\nFor a first-semester lab course, \\(\\chi^2_\\nu = 2\\) is acceptable. You wouldn‚Äôt reject the model based on this value alone, but you should examine the residuals for systematic patterns.\n\n\n\nIt is really important to have a good estimate of the uncertainties and to include them in your fit. If you include the uncertainties in your fit, it is called a weighted fit. If you don‚Äôt include the uncertainties (meaning you keep them constant), it is called an unweighted fit.\nFor our fit above, we obtain a reduced \\(\\chi^2\\) around 2, which is borderline acceptable. The model fits reasonably well, though there may be slight underestimation of uncertainties or minor model imperfections.\n\n\nResiduals\nAnother way to assess the quality of the fit is by looking at the residuals. There are several types of residuals, each useful in different contexts:\n1. Absolute residuals ‚Äî the raw deviation of data from the model: \\[\\begin{equation}\nr_i = y_i - f(x_{i},\\mathbf{a})\n\\end{equation}\\]\n2. Normalized (weighted) residuals ‚Äî scaled by the measurement uncertainty: \\[\\begin{equation}\nr_i^{\\text{norm}} = \\frac{y_i - f(x_{i},\\mathbf{a})}{\\sigma_i}\n\\end{equation}\\] For a good fit with correct uncertainties, these should be approximately standard normal distributed (mean 0, standard deviation 1).\n3. Relative (percentage) residuals ‚Äî useful when comparing across different scales: \\[\\begin{equation}\nr_i^{\\text{rel}} = 100 \\times \\frac{y_i - f(x_{i},\\mathbf{a})}{y_i} \\quad [\\%]\n\\end{equation}\\]\n\n\nImportance of Residuals\nResiduals are important because they provide insight into how well the model fits the data. If the residuals show only statistical fluctuations around zero, then the fit and likely also the model are good. However, if there are systematic patterns in the residuals, it may indicate that the model is not adequately capturing the underlying relationship in the data.\n\n\nVisualizing Residuals\nLet‚Äôs visualize the different types of residuals to better understand their distribution.\n\n\n\n\n\n\nThe normalized residuals are particularly useful: for a good fit, most points should fall within ¬±1 (gray dashed lines), and approximately 95% should fall within ¬±2.\n\n\n\n\n\n\nCommon Patterns in Residuals\n\n\n\n\n\nRandom Fluctuations Around Zero:\n\nIf the residuals are randomly scattered around zero, it suggests that the model is a good fit for the data.\n\nSystematic Patterns:\n\nIf the residuals show a systematic pattern (e.g., a trend or periodicity), it may indicate that the model is not capturing some aspect of the data. This could suggest the need for a more complex model.\n\nIncreasing or Decreasing Trends:\n\nIf the residuals increase or decrease with \\(x\\), it may indicate heteroscedasticity (non-constant variance) or that a different functional form is needed."
  },
  {
    "objectID": "lectures/lecture07/curve-fitting.html#covariance-matrix",
    "href": "lectures/lecture07/curve-fitting.html#covariance-matrix",
    "title": "Curve fitting",
    "section": "Covariance Matrix",
    "text": "Covariance Matrix\nIn the previous sections, we discussed how to fit a model to experimental data and assess the quality of the fit using residuals. Now, let‚Äôs take a closer look at the uncertainties in the fit parameters and how they are related to each other. This is where the covariance matrix comes into play.\n\nPurpose of the Covariance Matrix\nThe covariance matrix provides important information about the uncertainties in the fit parameters and how these uncertainties are related to each other. It helps us understand the precision of the parameter estimates and whether the parameters are independent or correlated.\n\n\nUnderstanding Covariance\nCovariance is a measure of how much two random variables change together. If the covariance between two variables is positive, it means that they tend to increase or decrease together. If the covariance is negative, it means that one variable tends to increase when the other decreases. If the covariance is zero, it means that the variables are independent.\n\n\nCovariance Matrix in Curve Fitting\nWhen we fit a model to data, we obtain estimates for the parameters of the model. These estimates have uncertainties due to the measurement errors in the data. The covariance matrix quantifies these uncertainties and the relationships between them.\nFor a model with three parameters \\((a, b, c)\\), the covariance matrix is a \\(3 \\times 3\\) matrix that looks like this:\n\\[\\begin{equation}\n{\\mathrm{cov}}(p_{i}, p_{j}) =\n\\begin{bmatrix}\n\\sigma_{aa}^{2} & \\sigma_{ab}^{2} & \\sigma_{ac}^{2} \\\\\n\\sigma_{ba}^{2} & \\sigma_{bb}^{2} & \\sigma_{bc}^{2} \\\\\n\\sigma_{ca}^{2} & \\sigma_{cb}^{2} & \\sigma_{cc}^{2}\n\\end{bmatrix}\n\\end{equation}\\]\nThe diagonal elements provide the variances (squared uncertainties) of the fit parameters, while the off-diagonal elements describe the covariances between the parameters.\n\n\nExample\nLet‚Äôs calculate the covariance matrix for our fitted model and interpret the results.\n\n\n\n\n\n\n\n\nExtracting Parameter Uncertainties\nThe most practical use of the covariance matrix is to extract the uncertainties of the fitted parameters. The diagonal elements give us the variances, so we take the square root to get the standard deviations:\n\n\n\n\n\n\n\n\nInterpreting the Covariance Matrix\nThe covariance matrix provides valuable information about the uncertainties in the fit parameters:\n\nDiagonal Elements: The diagonal elements represent the variances of the parameters. The square root of these values gives the standard deviations (uncertainties) of the parameters.\nOff-Diagonal Elements: The off-diagonal elements represent the covariances between the parameters. If these values are large, it indicates that the parameters are correlated.\n\n\n\nGenerating Synthetic Data\nTo better understand the covariance matrix, let‚Äôs generate synthetic data and fit the model to each dataset. This will help us visualize the uncertainties in the parameters.\n\n\n\n\n\n\n\n\n\n\n\n\nEach gray curve represents a fit to one synthetic dataset. The spread of these curves visualizes the uncertainty in our model parameters.\n\n\nCorrelation Matrix\nTo better understand the relationships between the parameters, we can normalize the covariance matrix to obtain the correlation matrix. The correlation matrix has values between -1 and 1, where 1 indicates perfect positive correlation, -1 indicates perfect negative correlation, and 0 indicates no correlation.\n\n\n\n\n\n\n\n\nVisualizing the Covariance and Correlation\nLet‚Äôs visualize the covariance and correlation between the parameters using scatter plots.\n\n\n\n\n\n\nBy examining the covariance and correlation matrices, we can gain a deeper understanding of the uncertainties in the fit parameters and how they are related to each other.\n\n\nImproving the Model\nIf we find that the parameters are highly correlated, we might want to find a better model containing more independent parameters. For example, we can write down a different model:\n\\[\\begin{equation}\ny = a(x - b)^2 + c\n\\end{equation}\\]\nThis model also contains three parameters, but the parameter \\(b\\) directly refers to the maximum of our parabola, while the parameter \\(a\\) denotes its curvature.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe see from the covariance matrix that the new model has a smaller correlation of the parameters with each other.\n\n\n\n\n\n\nThis is also expressed by our correlation matrix.\n\n\n\n\n\n\nBy examining the covariance and correlation matrices, we can gain valuable insights into the uncertainties in the fit parameters and how to improve our model."
  },
  {
    "objectID": "lectures/lecture07/curve-fitting.html#common-pitfalls-in-curve-fitting",
    "href": "lectures/lecture07/curve-fitting.html#common-pitfalls-in-curve-fitting",
    "title": "Curve fitting",
    "section": "Common Pitfalls in Curve Fitting",
    "text": "Common Pitfalls in Curve Fitting\nBefore we summarize, let‚Äôs discuss common mistakes that students (and researchers!) make when fitting data. Being aware of these pitfalls will help you avoid them in your own work.\n\nPitfall 1: Ignoring Measurement Uncertainties\nThe mistake: Using unweighted fits when uncertainties are known and vary across data points.\nWhy it matters: Without proper weighting, points with large uncertainties influence the fit as much as precise measurements. This leads to suboptimal parameter estimates.\nExample: In our trajectory data, the last point has a much larger uncertainty. An unweighted fit would try to pass close to this point, even though we have less confidence in it.\n\n\n\n\n\n\n\n\nPitfall 2: Bad Initial Guesses\nThe mistake: Providing initial parameter guesses that are far from the true values.\nWhy it matters: Fitting algorithms search for minima in the \\(\\chi^2\\) landscape. Poor initial guesses can lead to: - Convergence to local minima instead of the global minimum - Failure to converge at all - Very slow convergence\nHow to avoid it: - Plot your data first and estimate parameters visually - Use physical intuition (e.g., for a parabola, estimate the vertex position) - Try multiple initial guesses and compare results\n\n\n\n\n\n\n\n\nPitfall 3: Using the Wrong Model\nThe mistake: Fitting data with a model that doesn‚Äôt capture the underlying physics.\nWhy it matters: Even a ‚Äúgood‚Äù fit (low \\(\\chi^2\\)) with the wrong model gives meaningless parameters.\nHow to detect it: - \\(\\chi^2_\\nu \\gg 1\\) suggests the model is inadequate - Systematic patterns in residuals indicate missing physics - Parameters that don‚Äôt make physical sense\nExample: Fitting exponential decay with a linear model will show clear systematic residuals.\n\n\nPitfall 4: Overfitting\nThe mistake: Using too many parameters to fit limited data.\nWhy it matters: A model with many parameters can fit noise, not just the underlying signal. This leads to: - Poor predictive power - Unreliable parameter estimates - Very large parameter uncertainties\nRule of thumb: You need significantly more data points than parameters. A model with \\(M\\) parameters and only \\(M+1\\) data points is poorly constrained.\n\n\nPitfall 5: Ignoring Residuals\nThe mistake: Only looking at \\(\\chi^2\\) without examining residuals.\nWhy it matters: \\(\\chi^2\\) alone doesn‚Äôt tell you if the fit captures the physics. Residuals reveal: - Systematic deviations (wrong model) - Outliers (bad data points) - Heteroscedasticity (uncertainty varies with x)\nBest practice: Always plot residuals after fitting!\n\n\nPitfall 6: Forgetting About Correlations\nThe mistake: Reporting parameter uncertainties without considering correlations.\nWhy it matters: If parameters are highly correlated: - Individual uncertainties may be misleading - The ‚Äúallowed‚Äù parameter region is not a simple box but an ellipse - A different parameterization might be more meaningful\nExample: For our parabola \\(y = ax^2 + bx + c\\), changing \\(a\\) requires changing \\(b\\) to maintain a good fit. The vertex parameterization \\(y = a(x-b)^2 + c\\) has less correlation.\n\n\nPitfall 7: Not Checking Units\nThe mistake: Mixing up units or forgetting to convert.\nWhy it matters: Fitting software doesn‚Äôt know about units. If your x-data is in milliseconds but you expect the lifetime in seconds, your result will be off by a factor of 1000.\nBest practice: - Convert all data to SI units before fitting - Check that fitted parameters have sensible magnitudes - Verify units in your model function\n\n\n\n\n\n\nQuick Diagnostic Checklist\n\n\n\nBefore trusting your fit results, verify:\n\n\\(\\chi^2_\\nu \\approx 1\\) (within factor of ~2)\nResiduals show no systematic pattern\nParameters have physically reasonable values\nParameter uncertainties are not huge\nChanging initial guesses gives same result\nUnits are consistent throughout"
  },
  {
    "objectID": "lectures/lecture07/curve-fitting.html#summary",
    "href": "lectures/lecture07/curve-fitting.html#summary",
    "title": "Curve fitting",
    "section": "Summary",
    "text": "Summary\nIn this lecture, we covered the essential concepts of curve fitting for experimental physics:\n\nKey Concepts\n\nLeast Squares Method: We minimize the sum of squared deviations between data and model, weighted by measurement uncertainties: \\[\\chi^{2} =\\sum_{i=1}^{N}\\left( \\frac{y_{i}-f(x_{i},\\mathbf{a})}{\\sigma_{i}}\\right)^2\\]\nReduced Chi-Squared: The proper metric for fit quality is \\(\\chi^2_\\nu = \\chi^2/(N-M)\\), where \\(N\\) is the number of data points and \\(M\\) is the number of parameters. A good fit has \\(\\chi^2_\\nu \\approx 1\\).\nResiduals: Always examine residuals after fitting. Look for:\n\nRandom scatter around zero (good)\nSystematic patterns (suggests wrong model)\nTrends with x (suggests missing physics)\n\nCovariance Matrix: Provides uncertainties on fitted parameters (diagonal elements) and correlations between parameters (off-diagonal elements).\nCorrelation Matrix: Normalized covariance showing how parameters depend on each other. High correlations may suggest a better parameterization exists.\n\n\n\nPractical Checklist for Curve Fitting\nWhen fitting experimental data, follow these steps:\n\nDefine your model based on physical understanding\nProvide reasonable initial guesses for parameters\nInclude measurement uncertainties (weighted fit)\nCheck \\(\\chi^2_\\nu\\) ‚Äî should be close to 1\nExamine residuals ‚Äî should show no systematic patterns\nExtract parameter uncertainties from the covariance matrix\nCheck parameter correlations ‚Äî consider reparameterization if correlations are high\n\n\n\nCommon Pitfalls to Avoid\n\nIgnoring uncertainties: Always use weighted fits when uncertainties are known\nWrong model: A bad \\(\\chi^2_\\nu\\) often indicates the model doesn‚Äôt capture the physics\nOverestimated uncertainties: If \\(\\chi^2_\\nu \\ll 1\\), your error bars may be too large\nCorrelated parameters: High correlations make individual parameter values less meaningful\nPoor initial guesses: Can lead to convergence to local minima instead of the global minimum"
  },
  {
    "objectID": "lectures/lecture09/coupled-pendula.html",
    "href": "lectures/lecture09/coupled-pendula.html",
    "title": "Coupled Pendula",
    "section": "",
    "text": "We will continue our course with some physical problems we are going to tackle. One of the more extensive solutions will consider two coupled pendula. This belongs to the class of coupled oscillators, which are extremely important. They will later yield propagating waves. They are important for phonons, i.e.¬†coupled vibration of atoms in solids, but there are also many other examples. One can realize the coupled oscillation in different ways. Here we will do that not with spring oscillators, but with pendula."
  },
  {
    "objectID": "lectures/lecture09/coupled-pendula.html#description-of-the-problem",
    "href": "lectures/lecture09/coupled-pendula.html#description-of-the-problem",
    "title": "Coupled Pendula",
    "section": "Description of the problem",
    "text": "Description of the problem\n\nSketch\nThe image below depicts the sitution we would like to cover in our first project. These are two pendula, which have the length \\(L_{1}\\) and \\(L_{2}\\). Both are coupled with a spring of spring constant \\(k\\), which is relaxed, when both pendula are at rest. You may want to include a generalized version where the spring is mounted at a distance \\(c\\) from the turning points of the pendula.\nIf you develop the equation of motion, write down as a sum of torques. Use one equation of motion for each pendulum. The result will be two coupled differential equations for the angular coordinates. They are solved by the scipy odeint function without any friction.\n\n\n\n\n\n\nFigure¬†1: Sketch of the two coupled pendula.\n\n\n\n\n\nEquations of motion\n\nDerivation from torque balance\nTo derive the equations of motion, we consider the torques acting on each pendulum about its pivot point.\nFor pendulum 1:\n\nGravitational torque: The weight \\(m_1g\\) acts at the center of mass (distance \\(L_1\\) from pivot), creating a restoring torque: \\[\\tau_{g,1} = -m_1 g L_1 \\sin(\\theta_1)\\]\nSpring torque: The spring is attached at distance \\(c\\) from the pivot. The horizontal displacement at this point is \\(c\\sin(\\theta_1)\\) for pendulum 1 and \\(c\\sin(\\theta_2)\\) for pendulum 2. The spring force is: \\[F_{spring} = k[c\\sin(\\theta_1) - c\\sin(\\theta_2)] = kc[\\sin(\\theta_1) - \\sin(\\theta_2)]\\] This force acts horizontally at distance \\(c\\) from the pivot, creating a torque: \\[\\tau_{s,1} = -F_{spring} \\times c = -kc^2[\\sin(\\theta_1) - \\sin(\\theta_2)]\\]\nTotal torque: Using \\(I_1 = m_1 L_1^2\\) for a point mass: \\[I_1 \\ddot{\\theta_1} = \\tau_{g,1} + \\tau_{s,1}\\]\n\nFor pendulum 2: The derivation is analogous, but the spring force acts in the opposite direction.\nThis gives us the coupled equations of motion:\n\\[\\begin{eqnarray}\nm_{1}L_{1}^{2}\\ddot{\\theta_{1}}&=&-m_{1}gL_{1}\\sin(\\theta_{1})-kc^2[\\sin(\\theta_{1})-\\sin(\\theta_{2})]\\\\\nm_{2}L_{2}^{2}\\ddot{\\theta_{2}}&=&-m_{2}gL_{2}\\sin(\\theta_{2})+kc^2[\\sin(\\theta_{1})-\\sin(\\theta_{2})]\n\\end{eqnarray}\\]\nHere, \\(\\theta_{1}, \\theta_{2}\\) measure the angle of the two pendula with the length \\(L_{1},L_{2}\\). \\(k\\) is the spring constant of the spring coupling both pendula. The spring is mounted at a distance \\(c\\) from the turning point of each pendulum."
  },
  {
    "objectID": "lectures/lecture09/coupled-pendula.html#solving-the-problem",
    "href": "lectures/lecture09/coupled-pendula.html#solving-the-problem",
    "title": "Coupled Pendula",
    "section": "Solving the problem",
    "text": "Solving the problem\n\nSetting up the function\nIn our previous lecture, we used the odeint function of scipy to solve the driven damped harmonic oscillator. Remeber that we used the array\nstate[0] -&gt; position\nstate[1] -&gt; velocity\nto exchange position and velocity with the solver via the function that defines the physical problem\ndef SHO(state, time):\n    g0 = state[1]               -&gt;velocity\n    g1 = -k/m * state [0]       -&gt;acceleration\n    return np.array([g0, g1])\nfor a coupled system of different equations, we can now extend the state array. In the case of the coupled system of equations it has the following structure\ndef coupled_system(state, time):\n    g0 = state[1]  # velocity of object 1\n    g1 = # acceleration of object 1 (depends on positions of both objects)\n    g2 = state[3]  # velocity of object 2\n    g3 = # acceleration of object 2 (depends on positions of both objects)\n    return np.array([g0, g1, g2, g3])\nSo the state vector just gets longer and the coupling is in the definition of the velocities and accelerations. The results are then the positions and velocities of the objects. Use this type of scheme to define the problem and write a function, which returns the state of the objects as before.\n\n\n\n\n\n\n\n\nDefine initial parameters\nWe want to define some parameters of the pendula\n\nlength of the pendulum 1, \\(L_1\\)=3\nlength of the pendulum 2, \\(L_2\\)=3\ngravitational acceleration, \\(g=9.81\\)\nmass at the end of the pendula, \\(m=1\\)\ndistance where the coupling spring is mounted, \\(c=2\\)\nspring constant of the coupling spring, \\(k=5\\)\n\n\n\n\n\n\n\nAs compared to our previous problem of a damped driven pendulum, where we had two initial conditions for the second order differential equation, we have now two second order differential equation. We therefore need 4 initial parameters, which are the initial elongations of both pendula and their corresponding initial angular velocities We will notice, that the solution,i.e.¬†the motion of the pendula, will strongly depend on the initial conditions.\n\n\n\n\n\n\n\n\nSolve the equation of motion\nWe have to define a timeperiod over which we would like to obtain the solution. We use here a period of 400s where we calculate the solution at 10000 points along the 400s.\n\n\n\n\n\n\nWe are now ready to calculate the solution. Finally, we extract also the angles of the individual pendula, their angular velocities and the position of the point masses at the end of the pendulum. This can be then readily used to create some animation.\n\n\n\n\n\n\n\n\nPlotting\nFirst, get some impression of how the angles change over time.\n\n\n\n\n\n\n\n\nAnimation\nThe plot of the angles over time is not always giving a good insight. We can visualize snapshots of the pendula motion at different times to see how they move."
  },
  {
    "objectID": "lectures/lecture09/coupled-pendula.html#normal-modes",
    "href": "lectures/lecture09/coupled-pendula.html#normal-modes",
    "title": "Coupled Pendula",
    "section": "Normal Modes",
    "text": "Normal Modes\nWe will not cover all the physical details here, but you might remember from your mechanics lectures, that two coupled oscillators show distinct modes of motion, which we call the normal modes. For two coupled pendula there are two normal modes, where both pendula move with the same frequency. We may force the system into one of its normal modes by specifying its initial conditions properly.\n\nTheory of normal modes\nFor identical pendula (\\(L_1 = L_2 = L\\), \\(m_1 = m_2 = m\\)) in the small angle approximation (\\(\\sin\\theta \\approx \\theta\\)), the equations simplify and we can find analytical solutions for the normal mode frequencies:\n\nIn-phase mode (symmetric): Both pendula move together with \\(\\theta_1(t) = \\theta_2(t)\\) \\[\\omega_1 = \\sqrt{\\frac{g}{L}}\\] This is just the natural frequency of an uncoupled pendulum because the spring never stretches.\nOut-of-phase mode (antisymmetric): Pendula move in opposite directions with \\(\\theta_1(t) = -\\theta_2(t)\\) \\[\\omega_2 = \\sqrt{\\frac{g}{L} + \\frac{2kc^2}{mL^2}}\\] This frequency is higher because the spring provides an additional restoring force.\nGeneral motion: Any initial condition can be decomposed into a superposition of these two normal modes. When the system is not in a pure normal mode, energy oscillates between the pendula with a beat frequency: \\[\\omega_{beat} = \\frac{|\\omega_2 - \\omega_1|}{2} \\approx \\frac{1}{2}\\sqrt{\\frac{g}{L}}\\left(\\sqrt{1 + \\frac{2kc^2}{mgL}} - 1\\right)\\]\n\nPhysical interpretation of parameters:\n\nLarge \\(k\\): Strong coupling ‚Üí higher \\(\\omega_2\\) ‚Üí faster energy transfer (higher beat frequency)\nLarge \\(c\\): Spring mounted farther from pivot ‚Üí stronger coupling effect ‚Üí higher \\(\\omega_2\\)\nSmall \\(k\\) or \\(c\\): Weak coupling ‚Üí normal modes close in frequency ‚Üí slow energy transfer\n\nLet‚Äôs now verify these predictions numerically!\n\n\nIn-phase motion\nThe first one, will create an in-phase motion of the two pendula by setting their initial elongation equal, i.e.¬†\\(\\theta_{1}(t=0)=\\theta_{2}(t=0)\\).\nWhy this is a normal mode: When both pendula have the same angle at all times, the horizontal displacement at the spring attachment point is identical for both: \\(c\\sin(\\theta_1) = c\\sin(\\theta_2)\\). Therefore, the spring elongation is zero: \\(\\Delta x = c[\\sin(\\theta_1) - \\sin(\\theta_2)] = 0\\). With no spring force, each pendulum oscillates independently with its natural frequency \\(\\omega_1 = \\sqrt{g/L}\\).\nBoth pendula then oscillate with their natural frequency and the coupling spring is never elongated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOut-of-phase motion\nThe second one, will create a motion in which the two pendula are out-of-phase by a phase angle of \\(\\pi\\) , i.e.¬†\\(\\theta_{1}(t=0)=-\\theta_{2}(t=0)\\).\nWhy this is a normal mode: When \\(\\theta_1 = -\\theta_2\\) at all times, the spring experiences maximum elongation: \\(\\Delta x = c[\\sin(\\theta_1) - \\sin(\\theta_2)] = c[\\sin(\\theta_1) + \\sin(\\theta_1)] = 2c\\sin(\\theta_1)\\). This creates maximum spring force, which acts as an additional restoring force on both pendula. The enhanced restoring force leads to a higher oscillation frequency: \\(\\omega_2 = \\sqrt{g/L + 2kc^2/(mL^2)}\\).\nBoth pendula then oscillate with a frequency higher than their natural frequency. This is due to the fact that there is a higher restoring force due to the action of the spring. You should observe in the plot that the oscillation period is shorter than in the in-phase case.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBeat case\nThe last case is not a normal mode but represents a more general case. We start with two different initial angles, i.e.¬†\\(\\theta_{1}(t=0)=\\pi/12\\) and \\(\\theta_{2}(t=0)=0\\). This is the so-called beat case, where the pendula exchange energy.\nPhysics of the beat phenomenon: This initial condition excites both normal modes simultaneously. The system‚Äôs motion is a superposition: \\(\\theta(t) = A\\cos(\\omega_1 t) + B\\cos(\\omega_2 t)\\). Because \\(\\omega_1 \\neq \\omega_2\\), the two modes go in and out of phase, creating a beat pattern. The energy oscillates between the pendula with the beat frequency \\(\\omega_{beat} = |\\omega_2 - \\omega_1|/2\\).\nThe oscillation, which is at the beginning only in the first pendulum, is then transferred to the second one. This transfer of energy is continuously occurring from one pendulum to the other since there‚Äôs nowhere for the energy to go. The period of complete energy transfer is \\(T_{beat} = 2\\pi/\\omega_{beat}\\). With our parameters (\\(k=0.7\\), \\(c=2\\), \\(L=3\\), \\(m=1\\), \\(g=9.81\\)), we expect \\(\\omega_{beat} \\approx\\) few tenths of rad/s, corresponding to energy transfer over several seconds.\nConnection to waves: From this point it‚Äôs easy to recognize how a wave is generated. In a set of many coupled pendula, one pendulum starts to oscillate and transfers its energy to the next one, and then to the next one, and then to the next one. This way the energy is propagating along all oscillators, creating a traveling wave.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComputation of energy (here for the beat case)\nAfter we have had a look at the motion of the individual pendula, we may also check, the energies in the system. We have to calculate the potential and kinetic energies of the pendula and we should not forget the potential energy stored in the spring.\n\nPotential energy of the pendula\nThe potential energy plot below nicely shows the exchange of energy between the two pendula in the beat case.\n\n\n\n\n\n\n\n\nPotential energy of the spring\n\n\n\n\n\n\n\n\nKinetic energies\n\n\n\n\n\n\n\n\nTotal energy\nAs the total energy in the system shall be conserved, the sum of all energy contributions should yield a flat line.\n\n\n\n\n\n\n\n\nTotal energy exchange of the pendula\nWhile the plot above Shows the total energy of both pendula we may now have a look at the total energy in each pendulum. The plots clearly show that the energy is exchanged between the two pendula. The residual ripples on the curve results from the fact that we here exclude the potential energy stored in the spring."
  },
  {
    "objectID": "lectures/lecture09/coupled-pendula.html#exploring-parameter-effects",
    "href": "lectures/lecture09/coupled-pendula.html#exploring-parameter-effects",
    "title": "Coupled Pendula",
    "section": "Exploring Parameter Effects",
    "text": "Exploring Parameter Effects\nNow that we understand the physics of coupled pendula, let‚Äôs explore how the system parameters affect the behavior. This will help you build intuition about the coupling strength and normal mode frequencies.\n\nEffect of spring constant \\(k\\)\nThe spring constant \\(k\\) controls the coupling strength between the pendula. Let‚Äôs investigate what happens when we change it:\nPrediction:\n\nSmall \\(k\\): Weak coupling ‚Üí slow energy transfer ‚Üí long beat period\nLarge \\(k\\): Strong coupling ‚Üí fast energy transfer ‚Üí short beat period\nThe out-of-phase mode frequency \\(\\omega_2 = \\sqrt{g/L + 2kc^2/(mL^2)}\\) increases with \\(k\\)\n\nExercise: Try changing the spring constant in the parameter definition (currently k=0.7) and observe: 1. How does the beat period change in the beat case? 2. How does the oscillation frequency change in the out-of-phase mode? 3. At what value of \\(k\\) does the energy transfer become very rapid?\n\n\n\n\n\n\n\n\nEffect of coupling position \\(c\\)\nThe distance \\(c\\) where the spring is attached also affects the coupling:\nPrediction:\n\nLarger \\(c\\) ‚Üí stronger coupling torque (proportional to \\(c^2\\))\nThe effect on \\(\\omega_2\\) is similar to increasing \\(k\\)\n\nExercise: Modify the coupling distance (currently c=2) and observe how it affects the system dynamics.\n\n\nEffect of pendulum parameters\nExercise for advanced students:\n\nWhat happens if \\(L_1 \\neq L_2\\) (different length pendula)?\n\nThe natural frequencies become different\nEnergy transfer may be incomplete\n\nWhat happens if \\(m_1 \\neq m_2\\) (different masses)?\n\nThe heavier pendulum oscillates less\nNormal modes become asymmetric\n\n\n\n\nVerifying the small angle approximation\nOur equations use the full nonlinear formulation with \\(\\sin(\\theta)\\). For small angles, \\(\\sin(\\theta) \\approx \\theta\\), and we derived the normal mode frequencies.\nExercise:\n\nStart with a small initial angle (\\(\\theta_1(0) = \\pi/20 \\approx 0.15\\) rad) in the beat case\nMeasure the beat period from your energy plots\nCalculate the theoretical beat period using: \\(T_{beat} = 2\\pi/\\omega_{beat} = 2\\pi \\cdot 2/(\\omega_2 - \\omega_1)\\)\nCompare numerical and analytical results\nNow try a large initial angle (\\(\\theta_1(0) = \\pi/3\\)) and see if the beat period changes due to nonlinearity\n\nTheoretical calculation for our parameters:\n\n\n\n\n\n\n\n\nSummary\nCoupled oscillators are fundamental in physics, appearing in:\n\nMolecular vibrations: Atoms in molecules coupled by chemical bonds\nPhonons: Lattice vibrations in solids\nWave propagation: Many coupled oscillators ‚Üí continuous wave medium\nQuantum mechanics: Coupled quantum states lead to phenomena like tunneling and entanglement\nElectrical circuits: Coupled LC circuits in filters and antennas\n\nThe computational approach you‚Äôve learned here applies to all these systems! The key insight is that any coupled system has normal modes, and general motion is a superposition of these modes."
  },
  {
    "objectID": "lectures/lecture08/solving-odes.html",
    "href": "lectures/lecture08/solving-odes.html",
    "title": "Solving ODEs",
    "section": "",
    "text": "In the previous lecture on numerical differentiation, we learned how to calculate derivatives numerically‚Äîapproximating rates of change from discrete data points using finite difference formulas like the central difference \\(f'_i \\approx \\frac{f_{i+1} - f_{i-1}}{2\\Delta x}\\). Now we face the conceptually inverse problem: solving Ordinary Differential Equations (ODEs), where we know the relationship between a function and its derivatives, and we need to find the function itself. This is one of the most powerful tools in computational physics, allowing us to predict how physical systems evolve in time.\nODEs describe the dynamics of nearly every physical system you‚Äôll encounter. From the motion of planets to the decay of radioactive nuclei, from oscillating springs to quantum wavefunctions, differential equations capture how systems change. Learning to solve them numerically opens the door to understanding complex phenomena that lack analytical solutions‚Äîwhich includes most real-world physics problems."
  },
  {
    "objectID": "lectures/lecture08/solving-odes.html#introduction-what-are-odes",
    "href": "lectures/lecture08/solving-odes.html#introduction-what-are-odes",
    "title": "Solving ODEs",
    "section": "Introduction: What are ODEs?",
    "text": "Introduction: What are ODEs?\n\nODEs in Physics: The Language of Dynamics\nThe fundamental laws governing physical systems are typically expressed as differential equations‚Äîrelationships involving derivatives that describe how quantities change. These aren‚Äôt arbitrary mathematical constructs; they emerge naturally from conservation laws and fundamental principles.\nConsider Newton‚Äôs Second Law, which states that force equals mass times acceleration: \\[F = ma = m\\frac{d^2x}{dt^2}\\] This is an ODE relating position to its second time derivative. Given the forces acting on a system and its initial position and velocity, this equation determines all future motion.\nA classic example is the Simple Harmonic Oscillator describing a mass on a spring. The restoring force \\[F = -kx\\] leads to the equation of motion \\[m\\frac{d^2x}{dt^2} = -kx\\] or equivalently \\[\\frac{d^2x}{dt^2} + \\omega^2 x = 0\\] where \\(\\omega = \\sqrt{k/m}\\)$ is the natural angular frequency. This same equation appears in electrical circuits (LC oscillators), quantum mechanics (harmonic potential), and countless other contexts.\nFor more realistic systems, we might have a Damped Driven Pendulum with the equation \\[\\frac{d^2\\theta}{dt^2} = -\\frac{g}{L}\\sin(\\theta) - b\\frac{d\\theta}{dt} + \\beta\\cos(\\omega t)\\]. This incorporates the gravitational restoring torque (with \\(\\sin\\theta\\) making it nonlinear), friction-like damping proportional to angular velocity, and an external periodic driving force. Such systems can exhibit rich behavior including resonance and even chaos.\nThese are all Ordinary Differential Equations (ODEs)‚Äî‚Äúordinary‚Äù because they involve derivatives with respect to a single independent variable (usually time \\(t\\)), unlike partial differential equations which involve multiple variables like both space and time.\n\n\nFrom Derivatives to ODEs: Reversing the Problem\nIn the last lecture on numerical differentiation, we learned how to calculate derivatives numerically‚Äîgiven a set of data points \\(\\{x_i\\}\\), we approximated the derivatives \\(\\frac{dx}{dt}\\) using finite difference formulas. We saw how the forward difference \\(f'_i \\approx \\frac{f_{i+1} - f_i}{\\Delta x}\\) and the more accurate central difference \\(f'_i \\approx \\frac{f_{i+1} - f_{i-1}}{2\\Delta x}\\) allow us to compute slopes from discrete data. Now we face the conceptually opposite problem: we know the relationship between a function and its derivatives (the ODE), and we need to find the function itself.\n\n\n\n\n\n\nConnection to Numerical Differentiation\n\n\n\nThese two lectures form a complementary pair:\nDifferentiation (Previous Lecture): Given function values \\(f(x)\\), compute derivatives \\(f'(x)\\) using finite differences.\nIntegration (This Lecture): Given derivative relationships \\(\\frac{dx}{dt} = f(x,t)\\), compute function values \\(x(t)\\).\nThe mathematical tools are deeply connected. The finite difference formulas we derived (forward, central, backward) will reappear inside the ODE solvers as building blocks. The Taylor series analysis we used to derive central differences will help us understand integration methods. Even the matrix representation of derivatives will connect to an alternative approach for solving ODEs.\nThink of it this way: differentiation extracts velocity from position data, while ODE integration computes position trajectories from velocity equations. They‚Äôre inverse operations, and understanding both gives you complete mastery over the discrete calculus needed for computational physics.\n\n\nMore precisely, we‚Äôre given a differential equation like \\[\\frac{d^2x}{dt^2} = f(x, \\frac{dx}{dt}, t)\\] along with initial conditions specifying \\(x(t_0)\\) and \\(\\frac{dx}{dt}|_{t_0}\\)$. From this information, we must find the complete solution \\(x(t)\\) for all future (and possibly past) times. This process is called solving or integrating the differential equation.\nThis is fundamentally an initial value problem: knowing the state of a system at one instant, we use the laws of physics (encoded in the ODE) to predict its evolution. It‚Äôs analogous to how, in classical mechanics, specifying position and velocity at one moment determines the entire trajectory.\n\n\nConverting to First-Order Systems\nMost numerical methods work with first-order differential equations. A second-order equation like:\n\\[\\frac{d^2x}{dt^2} = f(x, \\frac{dx}{dt}, t)\\]\ncan be converted to a system of two first-order equations by introducing velocity as a separate variable:\n\\[\\vec{y} = \\begin{bmatrix} x \\\\ v \\end{bmatrix}\\]\nThen: \\[\\frac{d\\vec{y}}{dt} = \\begin{bmatrix} \\frac{dx}{dt} \\\\ \\frac{dv}{dt} \\end{bmatrix} = \\begin{bmatrix} v \\\\ f(x, v, t) \\end{bmatrix}\\]\nFor example, the harmonic oscillator becomes:\n\\[\\frac{d}{dt}\\begin{bmatrix} x \\\\ v \\end{bmatrix} = \\begin{bmatrix} v \\\\ -\\omega^2 x \\end{bmatrix}\\]\n\n\n\n\n\n\nWhy Convert to First-Order?\n\n\n\nStandard numerical ODE solvers are designed to work with first-order systems‚Äîequations of the form \\[\\frac{d\\vec{y}}{dt} = \\vec{f}(\\vec{y}, t)\\] where \\(\\vec{y}\\) is a vector containing all the variables describing the system‚Äôs state. This might seem restrictive since many physics equations are second-order (like Newton‚Äôs law), but converting to first-order form is straightforward and reveals the system‚Äôs structure more clearly.\nThe conversion is conceptually natural: in classical mechanics, completely specifying a system requires both positions and velocities (or momenta). These form the system‚Äôs state space or phase space. By explicitly including both \\(x\\) and \\(v\\) as separate variables in our first-order system, we‚Äôre making this structure explicit. Once in first-order form, we can apply powerful, well-tested numerical algorithms that have been optimized over decades of research in numerical analysis."
  },
  {
    "objectID": "lectures/lecture08/solving-odes.html#the-practical-tool-scipys-odeint",
    "href": "lectures/lecture08/solving-odes.html#the-practical-tool-scipys-odeint",
    "title": "Solving ODEs",
    "section": "The Practical Tool: SciPy‚Äôs odeint",
    "text": "The Practical Tool: SciPy‚Äôs odeint\nFor most physics problems, we don‚Äôt need to implement numerical integration from scratch. Python‚Äôs SciPy library provides excellent ODE solvers.\n\nBasic Usage: The odeint Function\nThe main tool we‚Äôll use is scipy.integrate.odeint, which provides a simple yet powerful interface for solving ODEs:\nfrom scipy.integrate import odeint\n\nsolution = odeint(derivative_function, initial_conditions, time_points)\nUnderstanding the parameters is crucial. The derivative_function is where you encode the physics‚Äîit‚Äôs a Python function that takes the current state \\(\\vec{y}\\) and time \\(t\\), and returns the derivatives \\(\\frac{d\\vec{y}}{dt}\\) according to your ODE. The initial_conditions specify where the system starts‚Äîtypically a vector like \\([x_0, v_0]\\) containing initial position and velocity. The time_points argument is an array of times where you want the solution computed‚Äîodeint automatically handles the intermediate time-stepping needed to reach these points accurately.\nThe function returns a 2D array where each row represents the system‚Äôs state at one of the requested time points. If you solve for 1000 time points with a 2-component state vector, you‚Äôll get a 1000√ó2 array. The first column might be position, the second velocity, and each row corresponds to one moment in time.\n\n\nUnderstanding the Derivative Function\nBefore we dive into examples, let‚Äôs clarify what the derivative function is‚Äîthis is the heart of solving ODEs numerically.\nIn the context of ODEs, when we write: \\[\\frac{d\\vec{y}}{dt} = \\vec{f}(\\vec{y}, t)\\]\nThe derivative function \\(\\vec{f}(\\vec{y}, t)\\) is simply a Python function that:\n\nTakes as input: The current state \\(\\vec{y}\\) (e.g., [x, v]) and time \\(t\\)\nReturns as output: The rate of change \\(\\frac{d\\vec{y}}{dt}\\) at that state and time\n\nPhysical interpretation: The derivative function encodes the physics of your problem. It tells you how fast things are changing right now, given where you are and what time it is.\nFor example, consider a harmonic oscillator with:\n\nState: \\(\\vec{y} = [x, v]\\) (position and velocity)\nPhysics: \\(\\frac{dx}{dt} = v\\) and \\(\\frac{dv}{dt} = -\\omega^2 x\\)\n\nThe derivative function looks like:\ndef harmonic_derivative(state, t, omega=1.0):\n    x, v = state           # Unpack current state\n    dx_dt = v              # Position changes at rate v\n    dv_dt = -omega**2 * x  # Velocity changes due to restoring force\n    return [dx_dt, dv_dt]  # Return the rates of change\nWhen you call harmonic_derivative([1.0, 0.0], 0), it tells you: ‚ÄúAt position x=1 and velocity v=0, the position is changing at rate 0 m/s and the velocity is changing at rate \\(-\\omega^2\\) m/s¬≤.‚Äù\n\n\n\n\n\n\nKey Insight\n\n\n\nThe derivative function is the bridge between physics and computation:\n\nYou write down your physics equations\nYou translate them into a function that computes rates of change\nodeint uses this function to step forward in time\n\nEvery ODE problem uses the same pattern‚Äîonly the derivative function changes!\n\n\n\n\nExample 1: Simple Harmonic Oscillator\nLet‚Äôs solve the harmonic oscillator equation:\n\\[\\frac{d^2x}{dt^2} + \\omega^2 x = 0\\]\nStep 1: Convert to first-order system\n\\[\\frac{d}{dt}\\begin{bmatrix} x \\\\ v \\end{bmatrix} = \\begin{bmatrix} v \\\\ -\\omega^2 x \\end{bmatrix}\\]\nStep 2: Define the derivative function\n\n\n\n\n\n\nStep 3: Set up and solve\n\n\n\n\n\n\nStep 4: Plot the results\n\n\n\n\n\n\nStep 5: Phase space diagram\nA useful way to visualize oscillatory systems is in phase space (position vs velocity):\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the Phase Space\n\n\n\nThe circular trajectory in phase space reveals fundamental properties of the harmonic oscillator. Each point on this circle represents a different state \\((x, v)\\)‚Äîa unique combination of position and velocity. As the system oscillates, it traces this closed loop repeatedly, returning to its starting point after one period.\nThe circular shape reflects energy conservation. The total energy \\(E = \\frac{1}{2}mv^2 + \\frac{1}{2}kx^2\\) remains constant, which describes an ellipse (or circle if we scale the axes appropriately) in the \\(x\\)-\\(v\\) plane. Systems with different initial energies trace different circles‚Äîhigher energy means larger amplitude oscillations and larger circles. If we added damping, the trajectory would spiral inward as energy dissipates, eventually settling at the origin (the equilibrium point where \\(x=0\\) and \\(v=0\\)).\nPhase space diagrams are powerful tools in physics, revealing qualitative system behavior at a glance: periodic motion appears as closed loops, damped motion spirals inward, and chaotic motion fills regions without repeating.\n\n\n\n\nExample 2: Damped Driven Pendulum\nNow let‚Äôs tackle a more complex problem - a damped driven pendulum:\n\\[\\frac{d^2\\theta}{dt^2} = -\\frac{g}{L}\\sin(\\theta) - b\\frac{d\\theta}{dt} + \\beta\\cos(\\omega t)\\]\nThis includes:\n\nRestoring force: \\(-\\frac{g}{L}\\sin(\\theta)\\) (nonlinear!)\nDamping: \\(-b\\frac{d\\theta}{dt}\\) (friction)\nDriving force: \\(\\beta\\cos(\\omega t)\\) (external periodic force)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Parameter Space\n\n\n\nThe beauty of computational physics is the ease of exploring how systems respond to different parameters. Try modifying the code to see various phenomena. Increasing the damping coefficient b produces stronger dissipation, causing oscillations to die out more quickly as energy is removed from the system. This models increasing air resistance or friction.\nChanging the driving frequency omega_drive reveals resonance effects‚Äîwhen the driving frequency matches the natural frequency, even small driving amplitudes can produce large oscillations as energy accumulates in phase with the system‚Äôs natural motion. This is why bridges can collapse from periodic forces and why opera singers can shatter wine glasses.\nIncreasing the driving amplitude beta, especially combined with large initial angles where the \\(\\sin\\theta \\approx \\theta\\) approximation breaks down, can push the pendulum into chaotic regimes. In this regime, tiny changes in initial conditions lead to drastically different long-term behavior, making prediction impossible beyond a certain time horizon despite the system being completely deterministic. The phase space trajectory becomes a tangled, never-repeating path‚Äîa hallmark of chaos."
  },
  {
    "objectID": "lectures/lecture08/solving-odes.html#summary-the-ode-solving-workflow",
    "href": "lectures/lecture08/solving-odes.html#summary-the-ode-solving-workflow",
    "title": "Solving ODEs",
    "section": "Summary: The ODE-Solving Workflow",
    "text": "Summary: The ODE-Solving Workflow\nSolving ODEs computationally follows a systematic workflow that bridges physics and programming. First, you write down the physics governing your system‚ÄîNewton‚Äôs laws, conservation principles, empirical relations, or whatever equations capture the phenomenon you‚Äôre studying. Second, you convert any higher-order equations into a first-order system by introducing additional variables (like treating velocity as independent from position). Third, you implement a derivative function in Python that takes the current state and time and returns the rates of change according to your equations.\nFourth, you specify initial conditions‚Äîthe state of your system at time \\(t=0\\). In classical mechanics, this means both positions and velocities; knowing just position isn‚Äôt enough to predict the future. Fifth, you call odeint with your derivative function, initial conditions, and an array of times where you want the solution. The function handles all the numerical complexities automatically. Finally, you visualize the results through plots of trajectories versus time, phase space diagrams showing state evolution, or animations showing the system‚Äôs motion.\nThis workflow applies universally, whether you‚Äôre simulating planetary orbits, chemical reaction kinetics, epidemic spread, or quantum systems. Master this pattern and you can tackle an enormous range of physics problems.\nThe connection to numerical differentiation: Notice how solving ODEs is fundamentally the inverse of what we did in the previous lecture. There, we used finite differences to approximate \\(\\frac{dx}{dt}\\) from known values of \\(x(t)\\). Here, we use knowledge of \\(\\frac{dx}{dt}\\) (from the ODE) to construct \\(x(t)\\). The Taylor series that gave us central difference formulas now justifies our time-stepping algorithms. The matrix representations of derivatives reappear in implicit methods. These two lectures form a unified framework for discrete calculus‚Äîthe computational analog of the continuous calculus you learn in mathematics courses.\n\n\n\n\n\n\nWhy Does This Work?\n\n\n\nThe odeint function employs sophisticated numerical methods developed over decades of research in numerical analysis. At its core, it uses adaptive Runge-Kutta methods‚Äîalgorithms that step forward in time by evaluating the derivative function at multiple intermediate points and combining them cleverly to achieve high accuracy.\nThe ‚Äúadaptive‚Äù part is crucial: the algorithm automatically adjusts its step size based on the local behavior of the solution. In smooth regions where the solution changes slowly, it takes large steps for efficiency. In regions where the solution varies rapidly or has strong curvature, it reduces the step size to maintain accuracy. This adaptivity makes the method both accurate and efficient.\nAdditionally, odeint can handle stiff equations‚Äîproblems where some components of the solution change much more rapidly than others, causing simple methods to fail or require impractically small time steps. The underlying algorithms switch between explicit and implicit methods as needed to maintain stability.\nFor the vast majority of physics problems you‚Äôll encounter, odeint provides an excellent balance of ease-of-use, accuracy, and robustness. You don‚Äôt need to understand all the algorithmic details to use it effectively, but knowing it‚Äôs built on solid numerical foundations helps you trust the results."
  },
  {
    "objectID": "lectures/lecture08/solving-odes.html#advanced-understanding-numerical-integration-methods",
    "href": "lectures/lecture08/solving-odes.html#advanced-understanding-numerical-integration-methods",
    "title": "Solving ODEs",
    "section": "Advanced: Understanding Numerical Integration Methods",
    "text": "Advanced: Understanding Numerical Integration Methods\n\n\n\n\n\n\nHow odeint Works Under the Hood\n\n\n\n\n\nWhile odeint handles the details for us, it‚Äôs useful to understand the basic concepts of numerical integration. Here we‚Äôll explore simple methods that illustrate the core ideas.\n\nThe Conceptual Idea: Time Stepping\nAll numerical ODE solvers share a common conceptual framework: stepping forward in time from known to unknown. Starting from initial conditions \\(\\vec{y}(t_0)\\) at time \\(t_0\\), we use the differential equation to predict the state at a slightly later time \\(t_1 = t_0 + \\Delta t\\). Then, treating this new state as our starting point, we step again to \\(t_2 = t_1 + \\Delta t\\), continuing this process to build up the complete solution trajectory.\nThe fundamental challenge is determining how to make each individual step. Given the current state \\(\\vec{y}_i\\) at time \\(t_i\\), how do we accurately compute the next state \\(\\vec{y}_{i+1}\\) at time \\(t_{i+1} = t_i + \\Delta t\\)? Different numerical methods answer this question in different ways, trading off between simplicity, accuracy, and computational cost. Let‚Äôs explore some fundamental approaches to understand what odeint is doing behind the scenes.\n\n\nEuler Method: The Simplest Approach\nThe Euler method is the most straightforward time-stepping algorithm, based directly on the Taylor expansion‚Äîthe same mathematical tool we used in the differentiation lecture to derive the central difference formula. Recall that we can expand any smooth function around a point using Taylor series:\n\\[\\vec{y}(t+\\Delta t) = \\vec{y}(t) + \\frac{d\\vec{y}}{dt}\\Delta t + \\frac{1}{2}\\frac{d^2\\vec{y}}{dt^2}\\Delta t^2 + \\mathcal{O}(\\Delta t^3)\\]\nThe Euler method makes a bold simplification: it keeps only the first two terms, dropping all the higher-order terms involving \\(\\Delta t^2\\) and beyond. Since we know \\(\\frac{d\\vec{y}}{dt} = \\vec{f}(\\vec{y}, t)\\) from our ODE, this truncation gives us:\n\\[\\vec{y}_{i+1} \\approx \\vec{y}_i + \\vec{f}(\\vec{y}_i, t_i)\\Delta t\\]\nNotice the deep connection to the differentiation lecture: if we rearrange this as \\(\\frac{\\vec{y}_{i+1} - \\vec{y}_i}{\\Delta t} \\approx \\vec{f}(\\vec{y}_i, t_i)\\), we recognize the forward difference formula for the derivative! Euler‚Äôs method is essentially using the forward difference approximation in reverse‚Äîinstead of computing the derivative from function values, we‚Äôre computing future function values from the derivative. Geometrically, this means following the tangent line for a short distance‚Äîusing the slope at the current point to extrapolate where we‚Äôll be after a small time step. It‚Äôs like driving by looking only at your current heading without anticipating curves in the road ahead.\nImplementation: The beauty of Euler‚Äôs method lies in its simplicity‚Äîjust one line of code:\n\n\n\n\n\n\nFor the harmonic oscillator, this becomes particularly concrete. Our state vector contains position and velocity, and we update both simultaneously:\n\\[\\begin{bmatrix} x_{i+1} \\\\ v_{i+1} \\end{bmatrix} = \\begin{bmatrix} x_i \\\\ v_i \\end{bmatrix} + \\begin{bmatrix} v_i \\\\ -\\omega^2 x_i \\end{bmatrix} \\Delta t\\]\nAccuracy considerations: The method‚Äôs simplicity comes at a cost. In each individual step, we incur a local error of order \\(\\mathcal{O}(\\Delta t^2)\\)‚Äîthe quadratic and higher terms we dropped from the Taylor expansion (just as we saw with the forward difference formula in the differentiation lecture, which had \\(\\mathcal{O}(\\Delta x^2)\\) error). Over many steps spanning a total time \\(T\\), these errors accumulate, producing a global error of order \\(\\mathcal{O}(\\Delta t)\\). This means to reduce the error by a factor of 10, you must take 10 times as many steps‚Äîa computationally expensive proposition. Moreover, Euler‚Äôs method can artificially inject or remove energy in oscillatory systems, causing solutions to spiral outward or inward when they should maintain constant amplitude.\n\n\nEuler-Cromer Method: Better for Oscillators\nThe Euler-Cromer method (also called the semi-implicit Euler method) makes a subtle but crucial modification to the standard Euler approach. Instead of using the current velocity to update position, it uses the updated velocity that we just calculated:\n\\[v_{i+1} = v_i + a(x_i, v_i, t_i)\\Delta t\\] \\[x_{i+1} = x_i + v_{i+1}\\Delta t \\quad \\text{(note: using } v_{i+1}\\text{, not } v_i\\text{)}\\]\nThis seemingly minor change‚Äîusing \\(v_{i+1}\\) instead of \\(v_i\\) in the position update‚Äîhas dramatic consequences for oscillatory systems. The method becomes symplectic, meaning it preserves the structure of phase space and conserves a modified energy that closely approximates the true energy. While individual trajectories may not be perfectly accurate, the method doesn‚Äôt systematically add or remove energy from the system, preventing the artificial spiraling behavior that plagues standard Euler integration of oscillators.\nThis property makes Euler-Cromer particularly valuable for long-term simulations of conservative systems like planetary orbits or molecular dynamics, where maintaining energy conservation over billions of time steps is crucial.\n\n\n\n\n\n\n\n\nComparison: Free Fall Example\nLet‚Äôs compare methods for free fall: \\(\\frac{d^2x}{dt^2} = -g\\)\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the Results\n\n\n\nFor free fall (a non-oscillatory system), both Euler and Euler-Cromer perform similarly - neither has a clear advantage. The Euler-Cromer method‚Äôs benefits become apparent for oscillatory systems like springs and pendulums, where it preserves energy better.\n\n\n\n\nComparison: Harmonic Oscillator (Where Euler-Cromer Shines)\nLet‚Äôs see the real difference with an oscillator:\n\n\n\n\n\n\n\n\n\n\n\n\nKey Insight: Symplectic vs Non-Symplectic\n\n\n\nThe dramatic difference in energy conservation reveals why Euler-Cromer is preferred for oscillatory systems:\n\nEuler method: Energy systematically drifts (usually grows), causing the oscillation amplitude to change over time\nEuler-Cromer method: Energy stays nearly constant even over many periods, preserving the physical behavior\n\nFor long-term simulations (planetary orbits, molecular dynamics), this difference is crucial!\n\n\n\n\nWhy Use odeint Instead?\nWhile understanding simple methods like Euler and Euler-Cromer builds intuition, they have serious limitations for practical work. Their accuracy is inherently limited‚Äîachieving acceptable error typically requires very small time steps \\(\\Delta t\\), making calculations slow. Their stability is problematic‚Äîfor certain types of equations (called ‚Äústiff‚Äù equations), these simple methods can become wildly unstable, producing nonsensical results even with small time steps. Their efficiency is poor because they use uniform time steps, wasting computation in smooth regions while potentially being inaccurate in rapidly-varying regions.\nThe odeint function uses adaptive Runge-Kutta methods that overcome all these limitations. These algorithms automatically adjust the step size based on local error estimates‚Äîtaking large steps where the solution is smooth and small steps where it‚Äôs rapidly changing. This adaptive strategy achieves high accuracy with far fewer derivative evaluations than fixed-step methods would require. The underlying LSODA algorithm can also detect stiff problems and switch to appropriate implicit methods that remain stable.\nFor production code, research, or any work where you trust the results to be correct, always use well-tested library functions like odeint, scipy.integrate.solve_ivp, or equivalent tools in other languages. These have been refined over decades and handle edge cases you might not even think to test for. Implementing your own integrator is valuable for learning, but for serious work, leverage the expertise embedded in these libraries."
  },
  {
    "objectID": "lectures/lecture08/solving-odes.html#advanced-matrix-methods-for-odes",
    "href": "lectures/lecture08/solving-odes.html#advanced-matrix-methods-for-odes",
    "title": "Solving ODEs",
    "section": "Advanced: Matrix Methods for ODEs",
    "text": "Advanced: Matrix Methods for ODEs\n\n\n\n\n\n\nImplicit/Matrix Method (Crank-Nicolson)\n\n\n\n\n\nIn the numerical differentiation lecture, we explored how derivatives can be represented as matrix operations. We saw that the second derivative could be written as a matrix multiplication using a tridiagonal matrix with entries \\([1, -2, 1]\\) along the diagonals. This matrix perspective leads to an alternative approach for solving ODEs called the implicit or matrix method, which directly parallels the matrix differentiation techniques from the previous lecture.\n\nThe Harmonic Oscillator with Matrices\nRecall the harmonic oscillator equation:\n\\[\\frac{d^2x}{dt^2} + \\omega^2 x = 0\\]\nWe can discretize this using the matrix representation of the second derivative that we developed in the differentiation lecture:\n\\[\\frac{d^2x}{dt^2} \\approx \\frac{1}{\\Delta t^2}\\begin{bmatrix}\n-2 & 1  & 0 & \\cdots \\\\\n1 & -2 & 1 & \\cdots \\\\\n0 & 1  & -2 & \\cdots \\\\\n\\vdots & \\vdots & \\vdots & \\ddots\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1\\\\\nx_2\\\\\nx_3\\\\\n\\vdots\n\\end{bmatrix}\\]\nThis is exactly the same sparse matrix structure we constructed using scipy.sparse.diags in the previous lecture! The pattern \\([1, -2, 1]\\) encodes the finite difference formula \\(f''_i \\approx \\frac{f_{i-1} - 2f_i + f_{i+1}}{\\Delta x^2}\\).\nThis transforms the differential equation into a system of linear equations:\n\\[(T + \\omega^2 V)\\vec{x} = \\vec{b}\\]\nwhere:\n\n\\(T\\) is the second derivative matrix (from the differentiation lecture)\n\\(V\\) is the identity matrix scaled by \\(\\omega^2\\)\n\\(\\vec{x}\\) is the vector of position values at all time points \\([x_0, x_1, x_2, \\ldots, x_N]\\)\n\\(\\vec{b}\\) encodes boundary/initial conditions\n\n\n\nUnderstanding Initial Conditions in Matrix Methods\nThe key difference between time-stepping methods and matrix methods is how initial conditions are incorporated:\nTime-stepping methods (Euler, Euler-Cromer, Runge-Kutta):\n\nStart with initial conditions: \\(x(t_0) = x_0\\), \\(v(t_0) = v_0\\)\nStep forward in time: each new value depends only on previous values\nInitial conditions are the ‚Äústarting point‚Äù for the simulation\n\nMatrix methods:\n\nSolve for ALL time points simultaneously\nInitial conditions must be built into the matrix equation\nWe modify specific rows of the matrix \\(M\\) to enforce initial conditions\n\n\n\n\n\n\n\nHow Initial Conditions Enter the Matrix\n\n\n\nThe key to the matrix method is properly structuring the system:\nMatrix Structure:\nRow 0:        [1, 0, 0, 0, ...]           (x[0] = x0)\nRow 1:        [-1/dt, 1/dt, 0, 0, ...]    ((x[1]-x[0])/dt = v0)\nRow 2:        [1/dt¬≤, -2/dt¬≤+œâ¬≤, 1/dt¬≤, 0, ...] (ODE at i=2)\nRow 3:        [0, 1/dt¬≤, -2/dt¬≤+œâ¬≤, 1/dt¬≤, ...] (ODE at i=3)\n...\nRow N-1:      [..., 1/dt¬≤, -2/dt¬≤+œâ¬≤, 1/dt¬≤]    (ODE at i=N-1)\nExplanation:\n\nRow 0 - Position initial condition: \\(x(t_0) = x_0\\)\n\nMatrix row: \\([1, 0, 0, \\ldots]\\)\nRight-hand side: \\(b[0] = x_0\\)\nEnforces: \\(x[0] = x_0\\) ‚úì\n\nRow 1 - Velocity initial condition: \\(v(t_0) = v_0\\)\n\nMatrix row: \\([-1/\\Delta t, 1/\\Delta t, 0, \\ldots]\\)\nRight-hand side: \\(b[1] = v_0\\)\nEnforces: \\(\\frac{x[1] - x[0]}{\\Delta t} = v_0\\) ‚úì\n\nRows 2 to N-1 - Differential equation: \\(\\frac{d^2x}{dt^2} + \\omega^2 x = 0\\)\n\nDiscretized using centered differences: \\(\\frac{x_{i-1} - 2x_i + x_{i+1}}{\\Delta t^2} + \\omega^2 x_i = 0\\)\nMatrix row: \\([\\ldots, 1/\\Delta t^2, -2/\\Delta t^2 + \\omega^2, 1/\\Delta t^2, \\ldots]\\)\nThis is the [1, -2, 1] stencil for the second derivative!\nRight-hand side: \\(b[i] = 0\\) (homogeneous equation)\n\n\nThe first two rows set the initial conditions, and all remaining rows enforce the physics (the differential equation).\n\n\n\n\nImplementation\n\n\n\n\n\n\n\n\n\n\n\n\nImportant Note on Matrix Method Accuracy\n\n\n\nThe matrix method treats an initial value problem as a boundary value problem, which can lead to accuracy issues:\n\nThe error tends to grow away from the initial conditions\nShorter time intervals give better results\nSmaller time steps (larger N) improve accuracy but increase computational cost\nFor this reason, time-stepping methods (Euler, Runge-Kutta) are generally preferred for initial value problems\n\nThis method is more naturally suited for boundary value problems where conditions are specified at both ends of the domain.\n\n\n\n\nAdvantages and Disadvantages of Matrix Methods\nMatrix methods offer some conceptual and computational advantages in specific contexts. They solve for the solution at all time points simultaneously rather than stepping forward sequentially, which can provide better stability for certain stiff equations and gives direct access to the entire solution trajectory at once. The approach also makes the mathematical structure more explicit‚Äîyou see the ODE as a linear algebra problem, connecting to the rich theory of matrices and their properties.\nHowever, these advantages come with significant costs. Solving large linear systems is memory-intensive‚Äîstoring a dense \\(N \\times N\\) matrix for \\(N\\) time points quickly becomes impractical for long simulations. The method is also far less flexible for nonlinear equations, which dominate physics problems; you‚Äôd need to linearize or use iterative schemes. Setting up proper boundary and initial conditions in matrix form is more complex than simply specifying initial values for time-stepping methods.\nMatrix methods excel in specialized applications like boundary value problems (where conditions are specified at multiple points, not just initially), certain partial differential equations where spatial discretization naturally leads to matrices (as we‚Äôll see in future lectures on PDEs), and specific linear problems with particular stability requirements. For these niche applications, the simultaneous solution of all time points can be powerful. The matrix differentiation techniques from the previous lecture become especially valuable in these PDE contexts.\nHowever, for the vast majority of physics problems involving ODEs‚Äîespecially nonlinear initial value problems like most classical mechanics situations‚Äîtime-stepping methods like odeint are far more practical, efficient, and easier to implement. They handle nonlinearity naturally, use memory efficiently, and scale to long time simulations gracefully. Think of matrix methods as a specialized tool in your computational toolkit, while time-stepping methods are your everyday workhorses."
  },
  {
    "objectID": "lectures/lecture08/solving-odes.html#whats-next-applications-across-physics",
    "href": "lectures/lecture08/solving-odes.html#whats-next-applications-across-physics",
    "title": "Solving ODEs",
    "section": "What‚Äôs Next? Applications Across Physics",
    "text": "What‚Äôs Next? Applications Across Physics\nNow that you‚Äôve mastered solving ODEs numerically, you possess a fundamental tool of computational physics. Combined with the numerical differentiation techniques from the previous lecture, you now understand both how to compute derivatives from functions and how to compute functions from their derivatives‚Äîcompleting the computational calculus toolkit. The techniques you‚Äôve learned apply across virtually every domain of physics. Consider planetary motion‚Äîby solving Newton‚Äôs gravitational equations, you can simulate orbits, predict eclipses, or study the complex dynamics of three-body systems that have no analytical solution. You can explore chaotic systems like the double pendulum, witnessing how deterministic equations can produce unpredictable, seemingly random behavior that‚Äôs exquisitely sensitive to initial conditions.\nYou can model wave propagation by discretizing wave equations, studying how disturbances travel through media. You can even tackle problems in quantum mechanics, solving the time-dependent Schr√∂dinger equation numerically to watch wavefunctions evolve and interfere. While Schr√∂dinger‚Äôs equation is fundamentally different from classical ODEs (it‚Äôs a partial differential equation with complex values), the same numerical principles apply after appropriate discretization.\nThe tools you‚Äôve learned across these two lectures‚Äînumerical differentiation and ODE integration‚Äîform an inseparable pair at the foundation of computational physics. The finite difference formulas we derived for computing derivatives reappear inside the time-stepping algorithms for solving ODEs. The matrix representations of differential operators connect both lectures. Together, they‚Äôre the building blocks for more sophisticated methods: molecular dynamics simulations, climate models, computational fluid dynamics, and finite element analysis all build on these core concepts. You‚Äôre now equipped to simulate, predict, and understand physical systems computationally, complementing the analytical techniques you learn in traditional physics courses."
  },
  {
    "objectID": "lectures/lecture06/1_input_output.html",
    "href": "lectures/lecture06/1_input_output.html",
    "title": "Input and output",
    "section": "",
    "text": "Python has a function called input for getting input from the user and assigning it a variable name.\n\nvalue=input(\"Tell me a number: \")\ntype(value)\n\nTell me a number:  78898.9\n\n\nstr\n\n\nThe value contains the keyboard input as expected, but it is a string. We want to use a number and not a string, so we need to convert it from a string to a number.\n\nv = eval(value)\ntype(v)\n\nfloat\n\n\n\n\n\nScreen output is possible by using the print command. The argument of the print function can be of different type.\n\n\nYou can format your output by modifying the string given to the print function by str.format(), The str contains text that is written to be the screen, as well as certain format specifiers contained in curly braces {}. The format function contains the list of variables that are to be printed.\n\nstring1 = \"How\"\nstring2 = \"are you my friend?\"\nint1 = 34\nint2 = 942885\nfloat1 = -3.0\nfloat2 = 3.141592653589793e-14\nprint(' ***')\n\nprint(string1)\nprint(string1 + ' ' + string2)\n\nprint(' 1. {} {}'.format(string1, string2)) \n\nprint(' 2. {0:s} {1:s}'.format(string1, string2))\nprint(' 3. {0:s} {0:s} {1:s} - {0:s} {1:s}'.format(string1, string2)) \n\nprint(' 4. {0:10s}{1:5s}'.format(string1, string2))\nprint(' ***')\nprint(int1, int2)\nprint(' 6. {0:d} {1:d}'.format(int1, int2)) \nprint(' 7. {0:8d} {1:10d}'.format(int1, int2)) \nprint(' ***')\nprint(' 8. {0:0.3f}'.format(float1))\nprint(' 9. {0:6.3f}'.format(float1)) \nprint('10. {0:8.3f}'.format(float1)) \nprint(2*' 11. {0:8.3f}'.format(float1))\nprint(' ***')\nprint('12. {0:0.3e}'.format(float2)) \nprint('13. {0:10.3e}'.format(float2)) \nprint('14. {0:10.3f}'.format(float2))\nprint(' ***')\nprint('15. 12345678901234567890')\nprint('16. {0:s}--{1:8d},{2:10.3e}'.format(string2, int1, float2))\n\n ***\nHow\nHow are you my friend?\n 1. How are you my friend?\n 2. How are you my friend?\n 3. How How are you my friend? - How are you my friend?\n 4. How       are you my friend?\n ***\n34 942885\n 6. 34 942885\n 7.       34     942885\n ***\n 8. -3.000\n 9. -3.000\n10.   -3.000\n 11.   -3.000 11.   -3.000\n ***\n12. 3.142e-14\n13.  3.142e-14\n14.      0.000\n ***\n15. 12345678901234567890\n16. are you my friend?--      34, 3.142e-14\n\n\n\n\n\nA very similar formatting can be achieved with the %operator.\n\nname = \"Frank\"\nprint(\"Hello, %s.\" % name)\n\nHello, Frank.\n\n\n\n\n\nFormatted string literals are the string literals that start with an f at the beginning and use curly braces {} to enclose the expressions that will be replaced with other values.\n\nname = \"Python Lecture\"\nnumber = 3\nfstring = f\"I'm here for the {number}. time and this {name} is awesome!\"\nprint(fstring)\n\nI'm here for the 3. time and this Python Lecture is awesome!\n\n\n\n\ntimes = 100\nfstring = f\"You just have to sent me {times:10.3f} Euros.\"\nprint(fstring)\n\nYou just have to sent me    100.000 Euros.\n\n\n\n\n\n\nFile input and output is one of the most important features. We will have a look at reading and writing of text files with numpy and pandas. Python itself also allows you to open files and the file object provides the methods read, write and close.\n\nimport numpy as np\n\nwith open('a.txt', 'r') as file_1,open('b.txt','r') as file_2:\n    for a,b in zip(file_1,file_2):\n        print(int(a)+int(b))\n\n\nfile_1.close()\nfile_2.close()\n\n10\n12\n14\n97\n9\n9\n\n\n\n\nMost of the time we want import numbers from text files. So direct connection to NumPy seems useful and we will study that first.\n\nimport numpy as np # don't forget to import numpy\n\n\n\nOften you would like to analyze data that you have stored in a text file. Consider, for example, the data file below for an experiment measuring the free fall of a mass.\nData for falling mass experiment\nDate: 16-Aug-2013\nData taken by Frank and Ralf\ndata point  time (sec)  height (mm) uncertainty (mm)\n0       0.0     180     3.5\n1       0.5     182     4.5\n2       1.0     178     4.0\n3       1.5     165     5.5\n4       2.0     160     2.5\n5       2.5     148     3.0\n6       3.0     136     2.5\nSuppose that the name of the text file is MyData.txt. Then we can read the data into four different arrays with the following NumPy statement:\n\ndataPt, time, height, error = np.loadtxt(\"MyData.txt\", skiprows=4 , unpack=True)\n\nIf you don‚Äôt want to read in all the columns of data, you can specify which columns to read in using the usecols key word. For example, the call\n\ntime, height = np.loadtxt(\"MyData.txt\", skiprows=5 , usecols = (1,2), unpack=True)\n\nreads in only columns 1 and 2; columns 0 and 3 are skipped.\n\n\n\nThere are plenty of ways to write data to a data file in Python. We will stick to one very simple one that‚Äôs suitable for writing data files in text format. It uses the NumPy savetxt routine, which is the counterpart of the loadtxt routine introduced in the previous section. The general form of the routine is\nsavetxt(filename, array, fmt=\"%0.18e\", delimiter=\" \", newline=\"\\n\", header=\"\", footer=\"\", comments=\"# \")\nWe illustrate savetext below with a script that first creates four arrays by reading in the data file MyData.txt, as discussed in the previous section, and then writes that same data set to another file MyDataOut.txt.\n\ndataPt, time, height, error = np.loadtxt(\"MyData.txt\", skiprows=5 , unpack=True)\n\n\nlist(zip(dataPt, time, height, error))\n\n[(1.0, 0.5, 182.0, 4.5),\n (2.0, 1.0, 178.0, 4.0),\n (3.0, 1.5, 165.0, 5.5),\n (4.0, 2.0, 160.0, 2.5),\n (5.0, 2.5, 148.0, 3.0),\n (6.0, 3.0, 136.0, 2.5)]\n\n\n\nnp.savetxt('MyDataOut.txt',list(zip(dataPt, time, height, error)), fmt=\"%12.3f\")\n\n\ncat MyDataOut.txt\n\n       1.000        0.500      182.000        4.500\n       2.000        1.000      178.000        4.000\n       3.000        1.500      165.000        5.500\n       4.000        2.000      160.000        2.500\n       5.000        2.500      148.000        3.000\n       6.000        3.000      136.000        2.500\n\n\n\n\n\n\nPandas is a software library written for the Python programming language. It is used for data manipulation and analysis. It provides special data structures and operations for the manipulation of numerical tables and time series and builds on top of numpy.\n\nEasy handling of missing data\nIntelligent label-based slicing, fancy indexing, and subsetting of large data sets\n\nThe data formats provided by the pandas module are used by several other modules, such as the trackpy which is a moduly for feature tracking and analysis in image series.\n\n\n\nimport pandas as pd # import the pandas module\n\nPandas provides two data structures\n\nSeries\nData Frames\n\nA Series is a one-dimensional labeled array capable of holding any data type (integers, strings, floating point numbers, Python objects, etc.). The axis labels are collectively referred to as the index.\n\nmy_simple_series = pd.Series(np.random.randn(7), index=['a', 'b', 'c', 'd', 'e','f','g'])\nmy_simple_series\n\na    1.160711\nb   -0.296427\nc    1.881074\nd   -1.197978\ne    0.280311\nf    1.538339\ng    1.681957\ndtype: float64\n\n\n\nmy_simple_series\n\n-0.2964266043928443\n\n\nThere is a whole lot of functionality built into pandas data types. You may of course also obtain the same functionality using numpy commands, but you may find the pandas abbrevations very useful.\n\nmy_simple_series.agg(['min','max','sum','mean']) # aggregate a number of properties into a single array\n\nmin    -1.523075\nmax     0.525265\nsum    -2.119315\nmean   -0.302759\ndtype: float64\n\n\nA DataFrame is a two-dimensional size-mutable, potentially heterogeneous tabular data structure with labeled axes (rows and columns). The example below shows how such a DataFrame can be generated from the scratch. In addition to the data supplied to the DataFrame method, an index column is generated when creating a DataFrame. As in the case of Series there is a whole lot of functionality integrated into the DataFrame data type which you may explore on the website.\n\ndf = pd.DataFrame()\n\n\ndf = pd.DataFrame(np.random.randint(low=0, high=10, size=(5, 5)),columns=['column 1', 'column 2', 'columns 3', 'column 4', 'column 5'])\ndf.head()\n\n\n\n\n\n\n\n\ncolumn 1\ncolumn 2\ncolumns 3\ncolumn 4\ncolumn 5\n\n\n\n\n0\n3\n9\n3\n7\n4\n\n\n1\n2\n2\n4\n6\n7\n\n\n2\n6\n1\n7\n4\n5\n\n\n3\n3\n4\n4\n3\n0\n\n\n4\n5\n6\n8\n0\n2\n\n\n\n\n\n\n\nDue to the labelling of the columns, each column may be accessed by its column label. Labeling by names improves readability considerably.\n\ndf['column 4']\n\n0    7\n1    6\n2    4\n3    3\n4    0\nName: column 4, dtype: int64\n\n\nIf you don‚Äôt like this format, you can always return to a simple numpy array with the as_matrix() method.\n\ndf.values\n\narray([[3, 9, 3, 7, 4],\n       [2, 2, 4, 6, 7],\n       [6, 1, 7, 4, 5],\n       [3, 4, 4, 3, 0],\n       [5, 6, 8, 0, 2]])\n\n\n\n\n\nDataFrames may also be populated by text files such as comma separated value files (short .csv). These files contain data in text format but also a column label, which can be read by the pandas method read_csv(). You can find an example below, which reads the data from the dust sensor on my balcony from April, 11th. You see the different columns, where P1 and P2 correspond to the PM10 and PM2.5 dust values in \\(\\mu g/m^3\\).\n\ndata = pd.DataFrame()\ndata = pd.read_csv(\"2018-04-11_sds011_sensor_12253.csv\",delimiter=\";\",parse_dates=False)\ndata.head()\n\n\n\n\n\n\n\n\nsensor_id\nsensor_type\nlocation\nlat\nlon\ntimestamp\nP1\ndurP1\nratioP1\nP2\ndurP2\nratioP2\n\n\n\n\n0\n12253\nSDS011\n6189\n52.527\n13.39\n2018-04-11T00:01:58\n25.87\nNaN\nNaN\n19.37\nNaN\nNaN\n\n\n1\n12253\nSDS011\n6189\n52.527\n13.39\n2018-04-11T00:04:24\n25.63\nNaN\nNaN\n20.53\nNaN\nNaN\n\n\n2\n12253\nSDS011\n6189\n52.527\n13.39\n2018-04-11T00:06:55\n26.30\nNaN\nNaN\n22.00\nNaN\nNaN\n\n\n3\n12253\nSDS011\n6189\n52.527\n13.39\n2018-04-11T00:09:23\n24.60\nNaN\nNaN\n20.30\nNaN\nNaN\n\n\n4\n12253\nSDS011\n6189\n52.527\n13.39\n2018-04-11T00:11:51\n25.17\nNaN\nNaN\n20.23\nNaN\nNaN\n\n\n\n\n\n\n\n\n(data['P1']/data['P2']).plot()"
  },
  {
    "objectID": "lectures/lecture06/1_input_output.html#keyboard-input",
    "href": "lectures/lecture06/1_input_output.html#keyboard-input",
    "title": "Input and output",
    "section": "",
    "text": "Python has a function called input for getting input from the user and assigning it a variable name.\n\nvalue=input(\"Tell me a number: \")\ntype(value)\n\nTell me a number:  78898.9\n\n\nstr\n\n\nThe value contains the keyboard input as expected, but it is a string. We want to use a number and not a string, so we need to convert it from a string to a number.\n\nv = eval(value)\ntype(v)\n\nfloat"
  },
  {
    "objectID": "lectures/lecture06/1_input_output.html#screen-output",
    "href": "lectures/lecture06/1_input_output.html#screen-output",
    "title": "Input and output",
    "section": "",
    "text": "Screen output is possible by using the print command. The argument of the print function can be of different type.\n\n\nYou can format your output by modifying the string given to the print function by str.format(), The str contains text that is written to be the screen, as well as certain format specifiers contained in curly braces {}. The format function contains the list of variables that are to be printed.\n\nstring1 = \"How\"\nstring2 = \"are you my friend?\"\nint1 = 34\nint2 = 942885\nfloat1 = -3.0\nfloat2 = 3.141592653589793e-14\nprint(' ***')\n\nprint(string1)\nprint(string1 + ' ' + string2)\n\nprint(' 1. {} {}'.format(string1, string2)) \n\nprint(' 2. {0:s} {1:s}'.format(string1, string2))\nprint(' 3. {0:s} {0:s} {1:s} - {0:s} {1:s}'.format(string1, string2)) \n\nprint(' 4. {0:10s}{1:5s}'.format(string1, string2))\nprint(' ***')\nprint(int1, int2)\nprint(' 6. {0:d} {1:d}'.format(int1, int2)) \nprint(' 7. {0:8d} {1:10d}'.format(int1, int2)) \nprint(' ***')\nprint(' 8. {0:0.3f}'.format(float1))\nprint(' 9. {0:6.3f}'.format(float1)) \nprint('10. {0:8.3f}'.format(float1)) \nprint(2*' 11. {0:8.3f}'.format(float1))\nprint(' ***')\nprint('12. {0:0.3e}'.format(float2)) \nprint('13. {0:10.3e}'.format(float2)) \nprint('14. {0:10.3f}'.format(float2))\nprint(' ***')\nprint('15. 12345678901234567890')\nprint('16. {0:s}--{1:8d},{2:10.3e}'.format(string2, int1, float2))\n\n ***\nHow\nHow are you my friend?\n 1. How are you my friend?\n 2. How are you my friend?\n 3. How How are you my friend? - How are you my friend?\n 4. How       are you my friend?\n ***\n34 942885\n 6. 34 942885\n 7.       34     942885\n ***\n 8. -3.000\n 9. -3.000\n10.   -3.000\n 11.   -3.000 11.   -3.000\n ***\n12. 3.142e-14\n13.  3.142e-14\n14.      0.000\n ***\n15. 12345678901234567890\n16. are you my friend?--      34, 3.142e-14\n\n\n\n\n\nA very similar formatting can be achieved with the %operator.\n\nname = \"Frank\"\nprint(\"Hello, %s.\" % name)\n\nHello, Frank.\n\n\n\n\n\nFormatted string literals are the string literals that start with an f at the beginning and use curly braces {} to enclose the expressions that will be replaced with other values.\n\nname = \"Python Lecture\"\nnumber = 3\nfstring = f\"I'm here for the {number}. time and this {name} is awesome!\"\nprint(fstring)\n\nI'm here for the 3. time and this Python Lecture is awesome!\n\n\n\n\ntimes = 100\nfstring = f\"You just have to sent me {times:10.3f} Euros.\"\nprint(fstring)\n\nYou just have to sent me    100.000 Euros."
  },
  {
    "objectID": "lectures/lecture06/1_input_output.html#file-inputoutput",
    "href": "lectures/lecture06/1_input_output.html#file-inputoutput",
    "title": "Input and output",
    "section": "",
    "text": "File input and output is one of the most important features. We will have a look at reading and writing of text files with numpy and pandas. Python itself also allows you to open files and the file object provides the methods read, write and close.\n\nimport numpy as np\n\nwith open('a.txt', 'r') as file_1,open('b.txt','r') as file_2:\n    for a,b in zip(file_1,file_2):\n        print(int(a)+int(b))\n\n\nfile_1.close()\nfile_2.close()\n\n10\n12\n14\n97\n9\n9\n\n\n\n\nMost of the time we want import numbers from text files. So direct connection to NumPy seems useful and we will study that first.\n\nimport numpy as np # don't forget to import numpy\n\n\n\nOften you would like to analyze data that you have stored in a text file. Consider, for example, the data file below for an experiment measuring the free fall of a mass.\nData for falling mass experiment\nDate: 16-Aug-2013\nData taken by Frank and Ralf\ndata point  time (sec)  height (mm) uncertainty (mm)\n0       0.0     180     3.5\n1       0.5     182     4.5\n2       1.0     178     4.0\n3       1.5     165     5.5\n4       2.0     160     2.5\n5       2.5     148     3.0\n6       3.0     136     2.5\nSuppose that the name of the text file is MyData.txt. Then we can read the data into four different arrays with the following NumPy statement:\n\ndataPt, time, height, error = np.loadtxt(\"MyData.txt\", skiprows=4 , unpack=True)\n\nIf you don‚Äôt want to read in all the columns of data, you can specify which columns to read in using the usecols key word. For example, the call\n\ntime, height = np.loadtxt(\"MyData.txt\", skiprows=5 , usecols = (1,2), unpack=True)\n\nreads in only columns 1 and 2; columns 0 and 3 are skipped.\n\n\n\nThere are plenty of ways to write data to a data file in Python. We will stick to one very simple one that‚Äôs suitable for writing data files in text format. It uses the NumPy savetxt routine, which is the counterpart of the loadtxt routine introduced in the previous section. The general form of the routine is\nsavetxt(filename, array, fmt=\"%0.18e\", delimiter=\" \", newline=\"\\n\", header=\"\", footer=\"\", comments=\"# \")\nWe illustrate savetext below with a script that first creates four arrays by reading in the data file MyData.txt, as discussed in the previous section, and then writes that same data set to another file MyDataOut.txt.\n\ndataPt, time, height, error = np.loadtxt(\"MyData.txt\", skiprows=5 , unpack=True)\n\n\nlist(zip(dataPt, time, height, error))\n\n[(1.0, 0.5, 182.0, 4.5),\n (2.0, 1.0, 178.0, 4.0),\n (3.0, 1.5, 165.0, 5.5),\n (4.0, 2.0, 160.0, 2.5),\n (5.0, 2.5, 148.0, 3.0),\n (6.0, 3.0, 136.0, 2.5)]\n\n\n\nnp.savetxt('MyDataOut.txt',list(zip(dataPt, time, height, error)), fmt=\"%12.3f\")\n\n\ncat MyDataOut.txt\n\n       1.000        0.500      182.000        4.500\n       2.000        1.000      178.000        4.000\n       3.000        1.500      165.000        5.500\n       4.000        2.000      160.000        2.500\n       5.000        2.500      148.000        3.000\n       6.000        3.000      136.000        2.500\n\n\n\n\n\n\nPandas is a software library written for the Python programming language. It is used for data manipulation and analysis. It provides special data structures and operations for the manipulation of numerical tables and time series and builds on top of numpy.\n\nEasy handling of missing data\nIntelligent label-based slicing, fancy indexing, and subsetting of large data sets\n\nThe data formats provided by the pandas module are used by several other modules, such as the trackpy which is a moduly for feature tracking and analysis in image series.\n\n\n\nimport pandas as pd # import the pandas module\n\nPandas provides two data structures\n\nSeries\nData Frames\n\nA Series is a one-dimensional labeled array capable of holding any data type (integers, strings, floating point numbers, Python objects, etc.). The axis labels are collectively referred to as the index.\n\nmy_simple_series = pd.Series(np.random.randn(7), index=['a', 'b', 'c', 'd', 'e','f','g'])\nmy_simple_series\n\na    1.160711\nb   -0.296427\nc    1.881074\nd   -1.197978\ne    0.280311\nf    1.538339\ng    1.681957\ndtype: float64\n\n\n\nmy_simple_series\n\n-0.2964266043928443\n\n\nThere is a whole lot of functionality built into pandas data types. You may of course also obtain the same functionality using numpy commands, but you may find the pandas abbrevations very useful.\n\nmy_simple_series.agg(['min','max','sum','mean']) # aggregate a number of properties into a single array\n\nmin    -1.523075\nmax     0.525265\nsum    -2.119315\nmean   -0.302759\ndtype: float64\n\n\nA DataFrame is a two-dimensional size-mutable, potentially heterogeneous tabular data structure with labeled axes (rows and columns). The example below shows how such a DataFrame can be generated from the scratch. In addition to the data supplied to the DataFrame method, an index column is generated when creating a DataFrame. As in the case of Series there is a whole lot of functionality integrated into the DataFrame data type which you may explore on the website.\n\ndf = pd.DataFrame()\n\n\ndf = pd.DataFrame(np.random.randint(low=0, high=10, size=(5, 5)),columns=['column 1', 'column 2', 'columns 3', 'column 4', 'column 5'])\ndf.head()\n\n\n\n\n\n\n\n\ncolumn 1\ncolumn 2\ncolumns 3\ncolumn 4\ncolumn 5\n\n\n\n\n0\n3\n9\n3\n7\n4\n\n\n1\n2\n2\n4\n6\n7\n\n\n2\n6\n1\n7\n4\n5\n\n\n3\n3\n4\n4\n3\n0\n\n\n4\n5\n6\n8\n0\n2\n\n\n\n\n\n\n\nDue to the labelling of the columns, each column may be accessed by its column label. Labeling by names improves readability considerably.\n\ndf['column 4']\n\n0    7\n1    6\n2    4\n3    3\n4    0\nName: column 4, dtype: int64\n\n\nIf you don‚Äôt like this format, you can always return to a simple numpy array with the as_matrix() method.\n\ndf.values\n\narray([[3, 9, 3, 7, 4],\n       [2, 2, 4, 6, 7],\n       [6, 1, 7, 4, 5],\n       [3, 4, 4, 3, 0],\n       [5, 6, 8, 0, 2]])\n\n\n\n\n\nDataFrames may also be populated by text files such as comma separated value files (short .csv). These files contain data in text format but also a column label, which can be read by the pandas method read_csv(). You can find an example below, which reads the data from the dust sensor on my balcony from April, 11th. You see the different columns, where P1 and P2 correspond to the PM10 and PM2.5 dust values in \\(\\mu g/m^3\\).\n\ndata = pd.DataFrame()\ndata = pd.read_csv(\"2018-04-11_sds011_sensor_12253.csv\",delimiter=\";\",parse_dates=False)\ndata.head()\n\n\n\n\n\n\n\n\nsensor_id\nsensor_type\nlocation\nlat\nlon\ntimestamp\nP1\ndurP1\nratioP1\nP2\ndurP2\nratioP2\n\n\n\n\n0\n12253\nSDS011\n6189\n52.527\n13.39\n2018-04-11T00:01:58\n25.87\nNaN\nNaN\n19.37\nNaN\nNaN\n\n\n1\n12253\nSDS011\n6189\n52.527\n13.39\n2018-04-11T00:04:24\n25.63\nNaN\nNaN\n20.53\nNaN\nNaN\n\n\n2\n12253\nSDS011\n6189\n52.527\n13.39\n2018-04-11T00:06:55\n26.30\nNaN\nNaN\n22.00\nNaN\nNaN\n\n\n3\n12253\nSDS011\n6189\n52.527\n13.39\n2018-04-11T00:09:23\n24.60\nNaN\nNaN\n20.30\nNaN\nNaN\n\n\n4\n12253\nSDS011\n6189\n52.527\n13.39\n2018-04-11T00:11:51\n25.17\nNaN\nNaN\n20.23\nNaN\nNaN\n\n\n\n\n\n\n\n\n(data['P1']/data['P2']).plot()"
  },
  {
    "objectID": "lectures/lecture01/jupyter-notebooks.html",
    "href": "lectures/lecture01/jupyter-notebooks.html",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "",
    "text": "As physicists, we need to:\n\nCalculate results from equations\nVisualize data and theoretical predictions\nDocument our work with equations and explanations\nShare reproducible results with colleagues\n\nJupyter Notebooks let us do all of this in one place!\n\n\n\n\n\n\nWhat You‚Äôll Create Today\n\n\n\nBy the end of this Lecture, you‚Äôll generate plots like this:\n\n\n\n\n\n\n\n\n\nAll from physics equations you already know!"
  },
  {
    "objectID": "lectures/lecture01/jupyter-notebooks.html#why-jupyter",
    "href": "lectures/lecture01/jupyter-notebooks.html#why-jupyter",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "",
    "text": "As physicists, we need to:\n\nCalculate results from equations\nVisualize data and theoretical predictions\nDocument our work with equations and explanations\nShare reproducible results with colleagues\n\nJupyter Notebooks let us do all of this in one place!\n\n\n\n\n\n\nWhat You‚Äôll Create Today\n\n\n\nBy the end of this Lecture, you‚Äôll generate plots like this:\n\n\n\n\n\n\n\n\n\nAll from physics equations you already know!"
  },
  {
    "objectID": "lectures/lecture01/jupyter-notebooks.html#what-is-a-jupyter-notebook",
    "href": "lectures/lecture01/jupyter-notebooks.html#what-is-a-jupyter-notebook",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "What is a Jupyter Notebook?",
    "text": "What is a Jupyter Notebook?\nA Jupyter Notebook is a web-based interactive document that combines:\n\nCode (Python) that you can run\nResults (numbers, plots, animations)\nDocumentation (text, equations in LaTeX)\n\nThink of it as a digital lab notebook for computational physics.\n\n\n\n\n\n\nKey Advantage\n\n\n\nUnlike traditional programming where you write everything first and then run it, Jupyter lets you experiment interactively - run small pieces of code, see results immediately, and build up your solution step by step."
  },
  {
    "objectID": "lectures/lecture01/jupyter-notebooks.html#opening-jupyter-lab",
    "href": "lectures/lecture01/jupyter-notebooks.html#opening-jupyter-lab",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "Opening Jupyter Lab",
    "text": "Opening Jupyter Lab\n\nOption 1: Anaconda Navigator (Recommended for Beginners)\n\nOpen Anaconda Navigator\nClick Launch under JupyterLab\nYour browser opens automatically\n\n\n\nOption 2: Command Line\nOpen terminal and type:\njupyter lab\n\n\n\n\n\n\nTroubleshooting\n\n\n\nIf Jupyter doesn‚Äôt start, check: - Anaconda is installed - You‚Äôre connected to the internet (first time only) - Try restarting Anaconda Navigator"
  },
  {
    "objectID": "lectures/lecture01/jupyter-notebooks.html#jupyterlab-interface",
    "href": "lectures/lecture01/jupyter-notebooks.html#jupyterlab-interface",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "JupyterLab Interface",
    "text": "JupyterLab Interface\nWhen JupyterLab opens, you‚Äôll see:\n\nLeft sidebar: File browser (like Windows Explorer/Finder)\nMain area: Where notebooks open\nLauncher tab: Click ‚ÄúPython 3‚Äù under Notebook to create a new notebook\n\n\n\n\n\n\n\nTry It Yourself (Embedded Demo)\n\n\n\n\n\nHere‚Äôs a live JupyterLab environment you can experiment with:\nOpen Fullscreen"
  },
  {
    "objectID": "lectures/lecture01/jupyter-notebooks.html#creating-your-first-notebook",
    "href": "lectures/lecture01/jupyter-notebooks.html#creating-your-first-notebook",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "Creating Your First Notebook",
    "text": "Creating Your First Notebook\n\nClick Python 3 tile under ‚ÄúNotebook‚Äù in the Launcher\nA new tab opens with an empty notebook\nSave it: File ‚Üí Save Notebook As‚Ä¶ ‚Üí physics_week1.ipynb\n\n\n\n\n\n\n\nFile Extension\n\n\n\nJupyter notebooks end in .ipynb (IPython Notebook)"
  },
  {
    "objectID": "lectures/lecture01/jupyter-notebooks.html#two-types-of-cells",
    "href": "lectures/lecture01/jupyter-notebooks.html#two-types-of-cells",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "Two Types of Cells",
    "text": "Two Types of Cells\nJupyter notebooks consist of cells - individual blocks that contain either code or text.\n\nCode Cells (Default)\n\nWrite Python code here\nRun to see results\nIdentified by [ ]: on the left\n\n\n\nMarkdown Cells\n\nWrite notes, explanations, equations\nUse for documentation\nChange a cell to Markdown: Click cell, then press M key"
  },
  {
    "objectID": "lectures/lecture01/jupyter-notebooks.html#your-first-calculation",
    "href": "lectures/lecture01/jupyter-notebooks.html#your-first-calculation",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "Your First Calculation üéØ",
    "text": "Your First Calculation üéØ\nLet‚Äôs calculate gravitational potential energy!\n\nPhysics\n\\[E_{\\text{pot}} = mgh\\]\n\n\nCode\nType this in your first cell:\n# Gravitational potential energy\nm = 2.0    # mass in kg\ng = 9.81   # gravity in m/s¬≤\nh = 10.0   # height in m\n\nE_pot = m * g * h\n\nprint(f\"Potential energy: {E_pot} J\")\n\n\nRun It!\n\nClick the ‚ñ∂ button (top of cell), OR\nPress Shift + Enter (run and move to next cell), OR\nPress Ctrl/Cmd + Enter (run and stay)\n\nYou should see: Potential energy: 196.2 J\n\n\n\n\n\n\nPro Tip\n\n\n\nShift + Enter is the most common way to run cells - you‚Äôll use this constantly!"
  },
  {
    "objectID": "lectures/lecture01/jupyter-notebooks.html#essential-keyboard-shortcuts",
    "href": "lectures/lecture01/jupyter-notebooks.html#essential-keyboard-shortcuts",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "Essential Keyboard Shortcuts",
    "text": "Essential Keyboard Shortcuts\nYou‚Äôll save tons of time with these shortcuts:\n\nRunning Cells\n\n\n\nShortcut\nAction\n\n\n\n\nShift + Enter\nRun cell and move to next\n\n\nCtrl/Cmd + Enter\nRun cell and stay\n\n\n\n\n\nCell Operations (press Esc first to enter command mode)\n\n\n\nShortcut\nAction\n\n\n\n\nB\nInsert cell below\n\n\nA\nInsert cell above\n\n\nD D\nDelete cell (press D twice)\n\n\nM\nChange to Markdown cell\n\n\nY\nChange to Code cell\n\n\nZ\nUndo delete\n\n\n\n\n\nEditing\n\n\n\nShortcut\nAction\n\n\n\n\nEnter\nEnter edit mode\n\n\nEsc\nExit edit mode (command mode)\n\n\n\n\n\n\n\n\n\nTwo Modes\n\n\n\n\nEdit mode (green border): Type in cell\nCommand mode (blue border): Use keyboard shortcuts\n\nPress Esc to switch to command mode, Enter to switch to edit mode."
  },
  {
    "objectID": "lectures/lecture01/jupyter-notebooks.html#try-it-multi-step-calculation",
    "href": "lectures/lecture01/jupyter-notebooks.html#try-it-multi-step-calculation",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "Try It: Multi-Step Calculation",
    "text": "Try It: Multi-Step Calculation\nLet‚Äôs calculate the time for a ball to fall from height h.\nPhysics: \\(h = \\frac{1}{2}gt^2\\) ‚Üí \\(t = \\sqrt{\\frac{2h}{g}}\\)\nYour Task:\n\nCreate a new cell below (press B)\nType this code:\n\nimport numpy as np  # We need square root\n\nh = 10.0   # height in m\ng = 9.81   # gravity in m/s¬≤\n\nt = np.sqrt(2 * h / g)\n\nprint(f\"Fall time: {t:.2f} seconds\")\n\nRun it! (Shift + Enter)"
  },
  {
    "objectID": "lectures/lecture01/jupyter-notebooks.html#adding-documentation-with-markdown",
    "href": "lectures/lecture01/jupyter-notebooks.html#adding-documentation-with-markdown",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "Adding Documentation with Markdown",
    "text": "Adding Documentation with Markdown\nGood physics notebooks explain what you‚Äôre doing!\n\nCreate a Markdown Cell\n\nInsert a new cell above your code (press Esc, then A)\nChange it to Markdown (press M)\nType:\n\n## Free Fall Calculation\n\nWe calculate the time for an object to fall from height $h = 10$ m.\n\nThe equation is:\n$$t = \\sqrt{\\frac{2h}{g}}$$\n\nwhere $g = 9.81 \\, \\text{m/s}^2$ is gravitational acceleration.\n\nRun the cell (Shift + Enter) to render it\n\n\n\n\n\n\n\nLaTeX Equations\n\n\n\n\nInline equation: $E = mc^2$ ‚Üí \\(E = mc^2\\)\nDisplay equation: $$E = mc^2$$ ‚Üí \\[E = mc^2\\]"
  },
  {
    "objectID": "lectures/lecture01/jupyter-notebooks.html#your-first-plot",
    "href": "lectures/lecture01/jupyter-notebooks.html#your-first-plot",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "Your First Plot! üìä",
    "text": "Your First Plot! üìä\nNow let‚Äôs visualize physics! This is where Jupyter really shines - you can see your calculations come to life.\n\nPlotting a Falling Ball\nLet‚Äôs plot the height of our falling ball over time. Don‚Äôt worry about understanding every detail - just see how easy it is to create beautiful graphs!\nCreate a new cell and type:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Calculate fall time\nh = 10.0\ng = 9.81\nt_fall = np.sqrt(2 * h / g)\n\n# Create time points from 0 to landing\nt = np.linspace(0, t_fall, 50)\n\n# Calculate height at each time: h = h0 - (1/2)gt¬≤\nheight = h - 0.5 * g * t**2\n\n# Create the plot\nplt.plot(t, height, 'b-', linewidth=2)\nplt.xlabel('Time (s)')\nplt.ylabel('Height (m)')\nplt.title('Ball Falling from 10m')\nplt.grid(True, alpha=0.3)\nplt.show()\nRun it! You just created your first physics visualization! üéâ\n\n\n\n\n\n\nWhat Just Happened?\n\n\n\n\nmatplotlib.pyplot is Python‚Äôs plotting library\nnp.linspace(0, t_fall, 50) creates 50 evenly-spaced time points\nplt.plot() draws the curve\nThe labels and grid make it look professional\n\nDon‚Äôt memorize this yet - we‚Äôll cover all the details in Lecture 4. For now, just enjoy seeing your calculation as a graph!\n\n\n\n\nChallenge: Plot Velocity\nThe velocity increases as the ball falls: \\(v = gt\\)\nTry modifying the code to plot velocity vs time instead!\n\n\n\n\n\n\nSolution\n\n\n\n\n\n# Calculate velocity at each time point\nvelocity = g * t\n\nplt.plot(t, velocity, 'r-', linewidth=2)\nplt.xlabel('Time (s)')\nplt.ylabel('Velocity (m/s)')\nplt.title('Velocity of Falling Ball')\nplt.grid(True, alpha=0.3)\nplt.show()\nNotice how velocity increases linearly - exactly what the equation predicts!"
  },
  {
    "objectID": "lectures/lecture01/jupyter-notebooks.html#saving-your-work",
    "href": "lectures/lecture01/jupyter-notebooks.html#saving-your-work",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "Saving Your Work",
    "text": "Saving Your Work\n\n ## What‚Äôs Next? üìä\nNow that you know the basics:\n\nLater: Advanced Plotting Techniques\n\nMultiple plots, logarithmic scales, 3D plots, animations, and more\nBuild on what you just learned!\n\nNext: Variables & Numbers\n\nUnderstand Python fundamentals in detail\nLearn about data types, operations, and more\n\n\nJupyter auto-saves every few minutes, but you should also:\n\nManual save: Ctrl/Cmd + S or File ‚Üí Save Notebook\nDownload: File ‚Üí Save and Export Notebook As ‚Üí HTML/PDF\n\n\n\n\n\n\n\nImportant!\n\n\n\nSaving the notebook saves your code and markdown, but also the output. If you want a clean notebook, do: Kernel ‚Üí Restart Kernel and Clear All Outputs before saving."
  },
  {
    "objectID": "lectures/lecture01/jupyter-notebooks.html#whats-next",
    "href": "lectures/lecture01/jupyter-notebooks.html#whats-next",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "What‚Äôs Next? üìä",
    "text": "What‚Äôs Next? üìä\nNow that you know the basics:\n\nNext lesson: Plotting Your First Graph\n\nYou‚Äôll create beautiful physics visualizations\n\nThen: Variables & Numbers\n\nUnderstand what you just used in detail"
  },
  {
    "objectID": "lectures/lecture01/jupyter-notebooks.html#quick-reference-card",
    "href": "lectures/lecture01/jupyter-notebooks.html#quick-reference-card",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "Quick Reference Card",
    "text": "Quick Reference Card\n\nMost Important Commands\nShift + Enter  ‚Üí Run cell\nEsc + B        ‚Üí New cell below\nEsc + M        ‚Üí Change to Markdown\nEsc + D D      ‚Üí Delete cell\nCtrl/Cmd + S   ‚Üí Save\n\n\nGetting Help\n\nFunction help: Type ?function_name in a cell and run\nDocumentation: Press Shift + Tab while cursor is on a function name"
  },
  {
    "objectID": "lectures/lecture01/jupyter-notebooks.html#advanced-topics-optional",
    "href": "lectures/lecture01/jupyter-notebooks.html#advanced-topics-optional",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "Advanced Topics (Optional)",
    "text": "Advanced Topics (Optional)\n\n\n\n\n\n\nWhat is a Kernel?\n\n\n\n\n\nThe kernel is the computational engine that runs your code. Think of it as Python running in the background.\nKernel indicator (top-right):\n\n‚ö™ Idle: Ready to run code\n‚ö´ Busy: Currently executing\n\nKernel menu operations:\n\nRestart Kernel: Clears all variables, starts fresh\nInterrupt Kernel: Stops long-running code\n\nYou‚Äôll need to restart if:\n\nVariables get messy\nCode is stuck in an infinite loop (interrupt first, then restart)\nYou want a clean slate\n\n\n\n\n\n\n\n\n\n\nMarkdown Cheatsheet\n\n\n\n\n\n# Heading 1\n## Heading 2\n### Heading 3\n\n**bold text**\n*italic text*\n\n- Bullet list\n- Another item\n\n1. Numbered list\n2. Another item\n\n[Link text](https://www.example.com)\n\nInline code: `x = 5`\n\nEquation: $E = mc^2$\n\nDisplay equation:\n$$\\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$$\n\n\n\n\n\n\n\n\n\nComplete Jupyter Documentation\n\n\n\n\n\nFor more advanced features (magic commands, debugging, widgets), see:\n\nOfficial Jupyter Documentation\nJupyterLab Documentation"
  },
  {
    "objectID": "lectures/lecture01/jupyter-notebooks.html#summary",
    "href": "lectures/lecture01/jupyter-notebooks.html#summary",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "Summary ‚úÖ",
    "text": "Summary ‚úÖ\nYou‚Äôve learned:\n\n‚úì What Jupyter notebooks are and why they‚Äôre useful for physics\n‚úì How to open JupyterLab and create a notebook\n‚úì The difference between code and Markdown cells\n‚úì How to run code and see results\n‚úì Essential keyboard shortcuts\n‚úì How to document your work with equations\n\nYou‚Äôre ready to start plotting! Move on to the next lesson.\n\n\n\n\n\n\n\nPractice Exercise\n\n\n\nBefore the next class, try this:\n\nCreate a new notebook called practice.ipynb\nCalculate the kinetic energy of a 1000 kg car traveling at 30 m/s\nUse a Markdown cell to write the equation: \\(E_k = \\frac{1}{2}mv^2\\)\nPrint the result in Joules\nBonus plotting challenge: Plot how kinetic energy changes with velocity\n\nUse velocities from 0 to 40 m/s\nHint: Copy and modify the falling ball plotting code!\n\nSave your notebook\n\nSolution: - Energy at 30 m/s: 450,000 J (or 450 kJ) - The plot should show a parabolic curve (energy ‚àù v¬≤)"
  },
  {
    "objectID": "lectures/lecture01/plotting-basics.html",
    "href": "lectures/lecture01/plotting-basics.html",
    "title": "Plotting Basics",
    "section": "",
    "text": "As physicists, we need to visualize our data and calculations. A good plot can:\n\nReveal patterns and relationships\nMake predictions visible\nCommunicate results effectively\nHelp us understand complex phenomena\n\nPython‚Äôs Matplotlib library is the standard tool for creating scientific plots."
  },
  {
    "objectID": "lectures/lecture01/plotting-basics.html#why-plotting-matters-in-physics",
    "href": "lectures/lecture01/plotting-basics.html#why-plotting-matters-in-physics",
    "title": "Plotting Basics",
    "section": "",
    "text": "As physicists, we need to visualize our data and calculations. A good plot can:\n\nReveal patterns and relationships\nMake predictions visible\nCommunicate results effectively\nHelp us understand complex phenomena\n\nPython‚Äôs Matplotlib library is the standard tool for creating scientific plots."
  },
  {
    "objectID": "lectures/lecture01/plotting-basics.html#getting-started-with-matplotlib",
    "href": "lectures/lecture01/plotting-basics.html#getting-started-with-matplotlib",
    "title": "Plotting Basics",
    "section": "Getting Started with Matplotlib",
    "text": "Getting Started with Matplotlib\nMatplotlib is not built into Python - we need to import it:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n\n\n\n\nConvention\n\n\n\nWe import matplotlib.pyplot as plt - this is the standard shorthand used everywhere in Python!"
  },
  {
    "objectID": "lectures/lecture01/plotting-basics.html#your-first-plot",
    "href": "lectures/lecture01/plotting-basics.html#your-first-plot",
    "title": "Plotting Basics",
    "section": "Your First Plot",
    "text": "Your First Plot\nThe most basic plotting command is:\nplt.plot(x, y)\nplt.show()\nLet‚Äôs plot a sine wave:\n\n\n\n\n\n\n\n\n\n\n\n\nWhat This Does\n\n\n\n\nnp.linspace(0, 4*np.pi, 100): Creates 100 evenly-spaced points from 0 to 4œÄ\nplt.figure(figsize=(6, 4)): Creates a new figure that‚Äôs 6√ó4 inches\nplt.plot(x, y): Plots y versus x\nplt.show(): Displays the plot"
  },
  {
    "objectID": "lectures/lecture01/plotting-basics.html#adding-labels",
    "href": "lectures/lecture01/plotting-basics.html#adding-labels",
    "title": "Plotting Basics",
    "section": "Adding Labels",
    "text": "Adding Labels\nA plot without labels is useless! Always add: - x-axis label: What‚Äôs on the horizontal axis? - y-axis label: What‚Äôs on the vertical axis? - Title: What does this plot show?\n\n\n\n\n\n\n\n\n\n\n\n\nUsing LaTeX in Labels\n\n\n\nYou can use mathematical notation in labels with LaTeX syntax:\nplt.xlabel(r'$\\theta$ (radians)')  # Greek letters\nplt.ylabel(r'$\\sin(\\theta)$')      # Math expressions\nThe r before the string means ‚Äúraw string‚Äù - it treats backslashes literally."
  },
  {
    "objectID": "lectures/lecture01/plotting-basics.html#physics-example-projectile-motion",
    "href": "lectures/lecture01/plotting-basics.html#physics-example-projectile-motion",
    "title": "Plotting Basics",
    "section": "Physics Example: Projectile Motion",
    "text": "Physics Example: Projectile Motion\nLet‚Äôs plot something more physics-y! A ball launched at 45¬∞:\n\n\n\n\n\n\n\n\n\n\n\n\nWhat‚Äôs New?\n\n\n\n\nplt.grid(True, alpha=0.3): Adds a grid (alpha=0.3 makes it semi-transparent)\nPhysics equations turned into beautiful visualization!"
  },
  {
    "objectID": "lectures/lecture01/plotting-basics.html#customizing-your-plot",
    "href": "lectures/lecture01/plotting-basics.html#customizing-your-plot",
    "title": "Plotting Basics",
    "section": "Customizing Your Plot",
    "text": "Customizing Your Plot\nYou can control colors, line styles, and markers:\n\nColor and Line Style Options\nplt.plot(x, y, 'r-')   # red solid line\nplt.plot(x, y, 'b--')  # blue dashed line\nplt.plot(x, y, 'go')   # green circles\nplt.plot(x, y, 'k:')   # black dotted line\nColor codes: r (red), g (green), b (blue), k (black), c (cyan), m (magenta), y (yellow)\nLine styles: - (solid), -- (dashed), : (dotted), -. (dash-dot)\nMarkers: o (circle), s (square), ^ (triangle), * (star), + (plus)\n\n\nExample"
  },
  {
    "objectID": "lectures/lecture01/plotting-basics.html#adding-a-legend",
    "href": "lectures/lecture01/plotting-basics.html#adding-a-legend",
    "title": "Plotting Basics",
    "section": "Adding a Legend",
    "text": "Adding a Legend\nWhen you have multiple lines, use a legend to identify them:\n\n\n\n\n\n\n\n\n\n\n\n\nLegend Locations\n\n\n\nCommon locations: 'upper right', 'upper left', 'lower right', 'lower left', 'center', 'best' (automatic)"
  },
  {
    "objectID": "lectures/lecture01/plotting-basics.html#physics-example-damped-oscillator",
    "href": "lectures/lecture01/plotting-basics.html#physics-example-damped-oscillator",
    "title": "Plotting Basics",
    "section": "Physics Example: Damped Oscillator",
    "text": "Physics Example: Damped Oscillator\nLet‚Äôs visualize different damping regimes:"
  },
  {
    "objectID": "lectures/lecture01/plotting-basics.html#plotting-data-points-not-just-lines",
    "href": "lectures/lecture01/plotting-basics.html#plotting-data-points-not-just-lines",
    "title": "Plotting Basics",
    "section": "Plotting Data Points (Not Just Lines)",
    "text": "Plotting Data Points (Not Just Lines)\nSometimes you want to show discrete data points:\n\n\n\n\n\n\n\n\n\n\n\n\nMarkers Without Lines\n\n\n\nUse 'ro' (red circles) to plot only markers without connecting lines. Use 'r-o' to plot both line and markers."
  },
  {
    "objectID": "lectures/lecture01/plotting-basics.html#saving-your-plots",
    "href": "lectures/lecture01/plotting-basics.html#saving-your-plots",
    "title": "Plotting Basics",
    "section": "Saving Your Plots",
    "text": "Saving Your Plots\nTo save a plot to a file:\nplt.savefig('my_plot.png')        # PNG format\nplt.savefig('my_plot.pdf')        # PDF format (best for papers!)\nplt.savefig('my_plot.png', dpi=300)  # High resolution PNG\nAlways save before plt.show()!\n\n\n\n\n\n\n\n\n\n\n\n\nFile Formats\n\n\n\n\nPDF: Best for inclusion in LaTeX documents and papers\nPNG: Good for presentations and web\nSVG: Vector format, good for editing in Inkscape/Illustrator"
  },
  {
    "objectID": "lectures/lecture01/plotting-basics.html#quick-reference-essential-commands",
    "href": "lectures/lecture01/plotting-basics.html#quick-reference-essential-commands",
    "title": "Plotting Basics",
    "section": "Quick Reference: Essential Commands",
    "text": "Quick Reference: Essential Commands\n# Setup\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Create plot\nplt.figure(figsize=(6, 4))\nplt.plot(x, y, 'b-', linewidth=2, label='My Data')\n\n# Labels and title\nplt.xlabel('X Label')\nplt.ylabel('Y Label')\nplt.title('My Plot')\n\n# Extras\nplt.legend(loc='best')\nplt.grid(True, alpha=0.3)\n\n# Save and show\nplt.savefig('filename.pdf')\nplt.show()"
  },
  {
    "objectID": "lectures/lecture01/plotting-basics.html#practice-exercises",
    "href": "lectures/lecture01/plotting-basics.html#practice-exercises",
    "title": "Plotting Basics",
    "section": "Practice Exercises üéØ",
    "text": "Practice Exercises üéØ\n\nExercise 1: Free Fall Velocity\nPlot the velocity of a falling object: \\(v = gt\\)\n\nUse time from 0 to 5 seconds\nSet \\(g = 9.81\\) m/s¬≤\nAdd proper labels and title\nUse a red line with linewidth=2\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ng = 9.81\nt = np.linspace(0, 5, 100)\nv = g * t\n\nplt.figure(figsize=(6, 4))\nplt.plot(t, v, 'r-', linewidth=2)\nplt.xlabel('Time (s)')\nplt.ylabel('Velocity (m/s)')\nplt.title('Free Fall Velocity')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\nExercise 2: Kinetic Energy vs Velocity\nPlot kinetic energy as a function of velocity for a 1000 kg car.\n\n\\(E_k = \\frac{1}{2}mv^2\\)\nVelocity from 0 to 40 m/s\nAdd labels with proper units\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nm = 1000  # kg\nv = np.linspace(0, 40, 100)\nE_k = 0.5 * m * v**2\n\nplt.figure(figsize=(6, 4))\nplt.plot(v, E_k/1000, 'b-', linewidth=2)  # Convert to kJ\nplt.xlabel('Velocity (m/s)')\nplt.ylabel('Kinetic Energy (kJ)')\nplt.title('Kinetic Energy of a 1000 kg Car')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\nExercise 3: Compare Trajectories\nPlot projectile trajectories for launch angles of 30¬∞, 45¬∞, and 60¬∞ on the same plot.\n\nUse \\(v_0 = 20\\) m/s for all\nAdd a legend\nUse different colors for each angle\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nv0 = 20.0\ng = 9.81\nangles = [30, 45, 60]\ncolors = ['r', 'b', 'g']\n\nplt.figure(figsize=(8, 5))\n\nfor angle, color in zip(angles, colors):\n    theta = np.radians(angle)\n    t_max = 2 * v0 * np.sin(theta) / g\n    t = np.linspace(0, t_max, 100)\n    x = v0 * np.cos(theta) * t\n    y = v0 * np.sin(theta) * t - 0.5 * g * t**2\n    plt.plot(x, y, color=color, linewidth=2, label=f'{angle}¬∞')\n\nplt.xlabel('Horizontal Distance (m)')\nplt.ylabel('Height (m)')\nplt.title('Projectile Trajectories at Different Angles')\nplt.legend(loc='upper right')\nplt.grid(True, alpha=0.3)\nplt.show()"
  },
  {
    "objectID": "lectures/lecture01/plotting-basics.html#common-mistakes-to-avoid",
    "href": "lectures/lecture01/plotting-basics.html#common-mistakes-to-avoid",
    "title": "Plotting Basics",
    "section": "Common Mistakes to Avoid ‚ö†Ô∏è",
    "text": "Common Mistakes to Avoid ‚ö†Ô∏è\n\nForgetting plt.show(): Your plot won‚Äôt display!\nNo labels: Always label axes and add a title\nSaving after showing: Use plt.savefig() BEFORE plt.show()\nToo many points: 100-500 points is usually enough for smooth curves\nUgly default sizes: Always set figsize for better proportions"
  },
  {
    "objectID": "lectures/lecture01/plotting-basics.html#whats-next",
    "href": "lectures/lecture01/plotting-basics.html#whats-next",
    "title": "Plotting Basics",
    "section": "What‚Äôs Next?",
    "text": "What‚Äôs Next?\nYou now know the basics of plotting! In later lectures, you‚Äôll learn:\n\nScatter plots and histograms (Lecture 4)\nLogarithmic plots for exponential data\nMultiple subplots in one figure\nContour and 3D plots for multivariable functions\nAnimations for time-dependent phenomena\n\nBut these basics will get you through 90% of your physics plotting needs! üéâ"
  },
  {
    "objectID": "lectures/lecture01/plotting-basics.html#summary",
    "href": "lectures/lecture01/plotting-basics.html#summary",
    "title": "Plotting Basics",
    "section": "Summary ‚úÖ",
    "text": "Summary ‚úÖ\nEssential Matplotlib Commands:\nplt.figure(figsize=(6,4))     # Create new figure\nplt.plot(x, y, 'b-')          # Plot data\nplt.xlabel('label')           # X-axis label\nplt.ylabel('label')           # Y-axis label\nplt.title('title')            # Title\nplt.legend()                  # Add legend\nplt.grid(True)                # Add grid\nplt.savefig('file.pdf')       # Save figure\nplt.show()                    # Display plot\nRemember: Good plots communicate clearly. Always include labels, units, and legends!"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#from-odes-to-pdes",
    "href": "lectures/lecture12/diffusion-equation-slides.html#from-odes-to-pdes",
    "title": "The Diffusion Equation",
    "section": "From ODEs to PDEs",
    "text": "From ODEs to PDEs\nSo far: Ordinary Differential Equations (ODEs)\n\nOne independent variable (usually time)\nExample: \\(\\frac{dN}{dt} = -\\lambda N\\)\n\n\nToday: Partial Differential Equations (PDEs)\n\nMultiple independent variables (time and space)\nExample: \\(\\frac{\\partial c}{\\partial t} = D\\frac{\\partial^2 c}{\\partial x^2}\\)\n\n\n\nWhy PDEs? Most real physics happens in space and time!"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#where-do-we-see-diffusion",
    "href": "lectures/lecture12/diffusion-equation-slides.html#where-do-we-see-diffusion",
    "title": "The Diffusion Equation",
    "section": "Where Do We See Diffusion?",
    "text": "Where Do We See Diffusion?\n\nInk spreading in water ‚Äî particles diffuse randomly\nHeat conduction ‚Äî thermal energy spreads through materials\nSmell spreading ‚Äî perfume molecules fill a room\nQuantum mechanics ‚Äî wave functions evolve in time\n\n\nAll described by the same mathematical structure: the diffusion equation!"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#outlook-reaction-diffusion-in-action",
    "href": "lectures/lecture12/diffusion-equation-slides.html#outlook-reaction-diffusion-in-action",
    "title": "The Diffusion Equation",
    "section": "Outlook: Reaction-Diffusion in Action",
    "text": "Outlook: Reaction-Diffusion in Action\n\n\n\n‚Äúmitosis‚Äù simulation (f=.0367, k=.0649) taken from https://www.karlsims.com/rd.html\n\n\n‚Äúcoral growth‚Äù simulation (f=.0545, k=.062) taken from https://www.karlsims.com/rd.html"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#the-diffusion-equation",
    "href": "lectures/lecture12/diffusion-equation-slides.html#the-diffusion-equation",
    "title": "The Diffusion Equation",
    "section": "The Diffusion Equation",
    "text": "The Diffusion Equation\nThe fundamental equation for diffusion in 3D:\n\\[\\frac{\\partial c({\\bf r},t)}{\\partial t}=D\\Delta c ({\\bf r},t)\\]\n\nWhat do the symbols mean?\n\n\\(c({\\bf r},t)\\) ‚Äî concentration at position \\({\\bf r}\\) and time \\(t\\)\n\\(D\\) ‚Äî diffusion coefficient (units: length¬≤/time)\n\\(\\Delta\\) ‚Äî Laplace operator: \\(\\nabla^2 = \\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2} + \\frac{\\partial^2}{\\partial z^2}\\)"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#simplification-1d-diffusion",
    "href": "lectures/lecture12/diffusion-equation-slides.html#simplification-1d-diffusion",
    "title": "The Diffusion Equation",
    "section": "Simplification: 1D Diffusion",
    "text": "Simplification: 1D Diffusion\nTo make this easier to program, consider diffusion along a line:\n\\[\\frac{\\partial c(x,t)}{\\partial t}=D\\frac{\\partial^2 c(x,t)}{\\partial x^{2}}\\]\n\nPhysical interpretation:\n\nLeft side: How fast concentration changes in time\nRight side: How curved the concentration profile is in space\n\n\n\nKey insight: Concentration flows from high to low values!"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#the-challenge-discretization",
    "href": "lectures/lecture12/diffusion-equation-slides.html#the-challenge-discretization",
    "title": "The Diffusion Equation",
    "section": "The Challenge: Discretization",
    "text": "The Challenge: Discretization\nContinuous equation: \\(\\frac{\\partial c(x,t)}{\\partial t}=D\\frac{\\partial^2 c(x,t)}{\\partial x^{2}}\\)\n\nFor computers, we need discrete points:\n\nSpace: \\(x_0, x_1, x_2, \\ldots, x_N\\) with spacing \\(\\Delta x\\)\nTime: \\(t_0, t_1, t_2, \\ldots, t_M\\) with spacing \\(\\Delta t\\)\n\n\n\nNotation: \\(c_i^n\\) means concentration at:\n\nSpace point \\(i\\) (position \\(x_i\\))\nTime step \\(n\\) (time \\(t_n\\))"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#spatial-derivative-the-second-derivative",
    "href": "lectures/lecture12/diffusion-equation-slides.html#spatial-derivative-the-second-derivative",
    "title": "The Diffusion Equation",
    "section": "Spatial Derivative: The Second Derivative",
    "text": "Spatial Derivative: The Second Derivative\nRemember from calculus:\n\\[\\frac{\\partial^{2} c(x,t)}{\\partial x^2}\\]\n\nDiscrete approximation using three points:\n\\[\\frac{\\partial^{2} c(x,t)}{\\partial x^2}\\approx\\frac{c_{i+1}^{n}-2c_{i}^{n}+c_{i-1}^{n}}{\\Delta x^2}\\]\n\n\nIn words: ‚ÄúLook at neighbors on both sides‚Äù"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#understanding-the-second-derivative",
    "href": "lectures/lecture12/diffusion-equation-slides.html#understanding-the-second-derivative",
    "title": "The Diffusion Equation",
    "section": "Understanding the Second Derivative",
    "text": "Understanding the Second Derivative\n\nThe second derivative measures curvature:\n\nPositive curvature ‚Üí concentration will increase\nNegative curvature ‚Üí concentration will decrease\nZero curvature ‚Üí no change"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#matrix-formulation",
    "href": "lectures/lecture12/diffusion-equation-slides.html#matrix-formulation",
    "title": "The Diffusion Equation",
    "section": "Matrix Formulation",
    "text": "Matrix Formulation\nCollect all concentrations at time \\(n\\) into a vector:\n\\[{\\bf C}^n = \\begin{pmatrix} c_0^n \\\\ c_1^n \\\\ c_2^n \\\\ \\vdots \\\\ c_N^n \\end{pmatrix}\\]\n\nThe spatial derivative becomes a matrix operation:\n\\[M = \\frac{1}{\\Delta x^2}\\begin{bmatrix}\n-2 & 1  & 0 & 0 & 0 & 0\\\\\n1 & -2 & 1 & 0 & 0 & 0\\\\\n0 & 1  & -2 & 1 & 0 & 0\\\\\n0 & 0  & 1  & -2 & 1 & 0\\\\\n0 & 0  & 0  &  1 & -2 & 1\\\\\n0 & 0  & 0  &  0 &  1 & -2\\\\\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#matrix-multiplication-example",
    "href": "lectures/lecture12/diffusion-equation-slides.html#matrix-multiplication-example",
    "title": "The Diffusion Equation",
    "section": "Matrix Multiplication Example",
    "text": "Matrix Multiplication Example\nFor point \\(i\\), the matrix \\(M\\) computes:\n\\[[M{\\bf C}]_i = \\frac{1}{\\Delta x^2}(c_{i-1} - 2c_i + c_{i+1})\\]\n\nThis is exactly our discrete second derivative!\n\n\nNow our equation becomes:\n\\[\\frac{\\partial {\\bf C}}{\\partial t} = D M {\\bf C}\\]"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#temporal-derivative-the-challenge",
    "href": "lectures/lecture12/diffusion-equation-slides.html#temporal-derivative-the-challenge",
    "title": "The Diffusion Equation",
    "section": "Temporal Derivative: The Challenge",
    "text": "Temporal Derivative: The Challenge\nWe also need to discretize time:\n\\[\\frac{\\partial c}{\\partial t} \\approx \\frac{c_i^{n+1} - c_i^n}{\\Delta t}\\]\n\nBut when do we evaluate the spatial part?\n\nAt time \\(n\\)? (Explicit method)\nAt time \\(n+1\\)? (Implicit method)\nAverage of both? (Crank-Nicolson method) ‚úì"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#the-crank-nicolson-scheme",
    "href": "lectures/lecture12/diffusion-equation-slides.html#the-crank-nicolson-scheme",
    "title": "The Diffusion Equation",
    "section": "The Crank-Nicolson Scheme",
    "text": "The Crank-Nicolson Scheme\nKey idea: Average the spatial derivative at both time steps!\n\\[\\frac{\\partial c}{\\partial t} \\approx \\frac{1}{2}\\left(DM{\\bf C}^{n+1} + DM{\\bf C}^{n}\\right)\\]\n\nWhy is this better?\n\nMore accurate than using just one time step\nStable for larger time steps\nStandard method for diffusion problems"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#bringing-it-all-together",
    "href": "lectures/lecture12/diffusion-equation-slides.html#bringing-it-all-together",
    "title": "The Diffusion Equation",
    "section": "Bringing It All Together",
    "text": "Bringing It All Together\nCombine time and space discretization:\n\\[\\frac{{\\bf C}^{n+1} - {\\bf C}^{n}}{\\Delta t} = \\frac{1}{2}\\left(DM{\\bf C}^{n+1} + DM{\\bf C}^{n}\\right)\\]\n\nRearrange to solve for \\({\\bf C}^{n+1}\\):\n\\[\\left({\\bf I} - \\frac{\\Delta t}{2}DM\\right){\\bf C}^{n+1} = \\left({\\bf I} + \\frac{\\Delta t}{2}DM\\right){\\bf C}^{n}\\]\n\n\nMatrix equation:\n\\[{\\bf A}{\\bf C}^{n+1} = {\\bf B}{\\bf C}^{n}\\]"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#the-solution-strategy",
    "href": "lectures/lecture12/diffusion-equation-slides.html#the-solution-strategy",
    "title": "The Diffusion Equation",
    "section": "The Solution Strategy",
    "text": "The Solution Strategy\n\\[{\\bf A}{\\bf C}^{n+1} = {\\bf B}{\\bf C}^{n}\\]\n\nAlgorithm:\n\nSet up matrices \\({\\bf A}\\) and \\({\\bf B}\\)\nStart with initial condition \\({\\bf C}^0\\)\nFor each time step:\n\nCompute right side: \\({\\bf B}{\\bf C}^{n}\\)\nSolve linear system: \\({\\bf A}{\\bf C}^{n+1} = {\\bf B}{\\bf C}^{n}\\)\n\nRepeat!\n\n\n\nPython tool: scipy.sparse.linalg.spsolve(A, B)"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#boundary-conditions",
    "href": "lectures/lecture12/diffusion-equation-slides.html#boundary-conditions",
    "title": "The Diffusion Equation",
    "section": "Boundary Conditions",
    "text": "Boundary Conditions\nPhysical question: What happens at the edges?\n\nCommon choices:\n\nDirichlet: Fixed concentration at boundaries\n\n\\(c(0,t) = c(L,t) = 0\\)\n‚ÄúParticles are absorbed at edges‚Äù\n\nNeumann: Fixed flux at boundaries\n\n\\(\\frac{\\partial c}{\\partial x}\\bigg|_{x=0} = 0\\)\n‚ÄúNo particles escape‚Äù\n\n\n\n\nToday: We‚Äôll use Dirichlet conditions (zero at edges)"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#initial-conditions",
    "href": "lectures/lecture12/diffusion-equation-slides.html#initial-conditions",
    "title": "The Diffusion Equation",
    "section": "Initial Conditions",
    "text": "Initial Conditions\nWhere do we start?\n\nGaussian distribution centered at \\(x = L/2\\):\n\\[c(x,0) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x-L/2)^2}{2\\sigma^2}}\\]\n\n\nPhysical picture:\n\nAll particles start concentrated at the center\nWidth controlled by \\(\\sigma\\) (we‚Äôll use \\(\\sigma = 0.05\\))\nThey will spread out over time!"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#setting-up-the-domain",
    "href": "lectures/lecture12/diffusion-equation-slides.html#setting-up-the-domain",
    "title": "The Diffusion Equation",
    "section": "Setting Up the Domain",
    "text": "Setting Up the Domain\n# Physical parameters\nL = 1.0              # domain length\nD = 0.1              # diffusion coefficient\nT = 0.5              # total time\n\n# Discretization\nNX = 500             # number of spatial points\ndx = 1/(NX + 1.0)    # spatial step\nx = np.linspace(0, L, NX+2)[1:-1]  # exclude boundaries\n\ndt = dx/4            # time step (stability condition)\nNT = int(T/dt)       # number of time steps\n\nKey choice: Time step \\(\\Delta t\\) must be small enough for stability!"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#initial-condition-in-code",
    "href": "lectures/lecture12/diffusion-equation-slides.html#initial-condition-in-code",
    "title": "The Diffusion Equation",
    "section": "Initial Condition in Code",
    "text": "Initial Condition in Code\n# Gaussian initial distribution\nsigma = 0.04\nc = np.exp(-(x - L/2)**2 / (2*sigma**2))\nc = c.reshape(-1)  # ensure 1D array\n\nplt.plot(x, c)\nplt.xlabel('position x')\nplt.ylabel('concentration c')\nplt.title('Initial Distribution')\nThis creates a sharp peak at the center that will spread out!"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#building-the-matrix-m",
    "href": "lectures/lecture12/diffusion-equation-slides.html#building-the-matrix-m",
    "title": "The Diffusion Equation",
    "section": "Building the Matrix M",
    "text": "Building the Matrix M\n# Create tridiagonal matrix for second derivative\ndata = np.ones((3, NX))\ndata[1] = -2 * data[1]    # diagonal: -2\n# off-diagonals: 1\ndiags = [-1, 0, 1]        # positions\n\nM = sparse.spdiags(data, diags, NX, NX) / (dx**2)\n\nResult: Sparse matrix (mostly zeros) that computes \\(\\frac{\\partial^2 c}{\\partial x^2}\\)\n\n\n# Identity matrix\nI = sparse.identity(NX)"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#the-time-stepping-loop",
    "href": "lectures/lecture12/diffusion-equation-slides.html#the-time-stepping-loop",
    "title": "The Diffusion Equation",
    "section": "The Time-Stepping Loop",
    "text": "The Time-Stepping Loop\ndata = []              # store all time steps\ndata.append(c)         # save initial condition\n\nfor i in range(NT):\n    # Build matrices\n    A = (I - dt/2 * D * M)\n    B = (I + dt/2 * D * M) * c\n    \n    # Solve linear system\n    c = sparse.linalg.spsolve(A, B)\n    c = np.array(c)\n    \n    # Store result\n    data.append(c)\n\nThat‚Äôs it! The Crank-Nicolson method in action."
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#understanding-the-code",
    "href": "lectures/lecture12/diffusion-equation-slides.html#understanding-the-code",
    "title": "The Diffusion Equation",
    "section": "Understanding the Code",
    "text": "Understanding the Code\nA = (I - dt/2 * D * M)      # Left side matrix\nB = (I + dt/2 * D * M) * c  # Right side vector\nc = sparse.linalg.spsolve(A, B)  # Solve Ac_{n+1} = B\n\nWhat‚Äôs happening?\n\nA encodes the implicit part (time \\(n+1\\))\nB encodes the explicit part (time \\(n\\)) times current concentration\nspsolve inverts the matrix to find \\(c^{n+1}\\)\n\n\n\nWhy sparse matrices? Much faster for large systems!"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#visualizing-the-evolution",
    "href": "lectures/lecture12/diffusion-equation-slides.html#visualizing-the-evolution",
    "title": "The Diffusion Equation",
    "section": "Visualizing the Evolution",
    "text": "Visualizing the Evolution\nplt.figure(figsize=(12, 8))\n\nfor i in range(0, NT, 10):  # plot every 10th step\n    plt.plot(x, data[i])\n\nplt.xlabel('position x')\nplt.ylabel('concentration c')\nplt.title('Diffusion Over Time')\n\nExpected behavior:\n\nPeak gets lower (conservation of particles)\nPeak gets wider (spreading)\nEventually ‚Üí uniform concentration"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#spacetime-visualization",
    "href": "lectures/lecture12/diffusion-equation-slides.html#spacetime-visualization",
    "title": "The Diffusion Equation",
    "section": "Spacetime Visualization",
    "text": "Spacetime Visualization\nplt.imshow(np.array(data).reshape(NT+1, NX),\n          vmin=0, vmax=0.5,\n          cmap='gray_r',\n          extent=(0, L, T, 0))\nplt.xlabel('position x')\nplt.ylabel('time t')\nplt.colorbar(label='concentration')\n\nThis creates a 2D plot showing how concentration evolves everywhere at once!\n\nHorizontal axis: position\nVertical axis: time\nColor: concentration value"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#physical-interpretation",
    "href": "lectures/lecture12/diffusion-equation-slides.html#physical-interpretation",
    "title": "The Diffusion Equation",
    "section": "Physical Interpretation",
    "text": "Physical Interpretation\nThe solution shows:\n\nConservation: Total amount of ‚Äústuff‚Äù is constant\n\n\\(\\int c(x,t) dx = \\text{constant}\\)\n\nSmoothing: Sharp features get smoothed out\nIrreversibility: Can‚Äôt ‚Äúundiffuse‚Äù back to initial state\nEquilibrium: Eventually reaches uniform distribution\n\n\nThis is the second law of thermodynamics in action!"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#checking-your-solution",
    "href": "lectures/lecture12/diffusion-equation-slides.html#checking-your-solution",
    "title": "The Diffusion Equation",
    "section": "Checking Your Solution",
    "text": "Checking Your Solution\nGood practices:\n\nConservation check: Total concentration should stay constant\ntotal = np.sum(c) * dx  # integrate concentration\nSymmetry check: Solution should be symmetric around center\nBoundary check: Concentration at edges should stay zero\nMesh refinement: Double spatial points, solution should converge"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#common-issues-and-fixes",
    "href": "lectures/lecture12/diffusion-equation-slides.html#common-issues-and-fixes",
    "title": "The Diffusion Equation",
    "section": "Common Issues and Fixes",
    "text": "Common Issues and Fixes\n\n\n\n\n\n\n\n\nProblem\nSymptom\nSolution\n\n\n\n\nSolution explodes\nValues become huge\nDecrease \\(\\Delta t\\)\n\n\nSolution oscillates\nWiggly, non-smooth\nDecrease \\(\\Delta t\\)\n\n\nToo slow to converge\nTakes forever\nIncrease \\(\\Delta x\\) (fewer points)\n\n\nNot symmetric\nAsymmetric spreading\nCheck initial condition\n\n\nWrong total mass\nConservation violated\nCheck boundary conditions"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#stability-condition",
    "href": "lectures/lecture12/diffusion-equation-slides.html#stability-condition",
    "title": "The Diffusion Equation",
    "section": "Stability Condition",
    "text": "Stability Condition\nNot all choices of \\(\\Delta t\\) and \\(\\Delta x\\) work!\n\nFor explicit methods, need:\n\\[\\Delta t &lt; \\frac{(\\Delta x)^2}{2D}\\]\n\n\nGood news: Crank-Nicolson is unconditionally stable!\nBut: Still need small enough \\(\\Delta t\\) for accuracy\n\n\nRule of thumb: Use \\(\\Delta t = \\frac{(\\Delta x)^2}{4D}\\)"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#extensions-other-equations",
    "href": "lectures/lecture12/diffusion-equation-slides.html#extensions-other-equations",
    "title": "The Diffusion Equation",
    "section": "Extensions: Other Equations",
    "text": "Extensions: Other Equations\nHeat equation (same form!):\n\\[\\frac{\\partial T}{\\partial t} = \\alpha \\frac{\\partial^2 T}{\\partial x^2}\\]\n\n\\(T\\) = temperature, \\(\\alpha\\) = thermal diffusivity\n\n\nSchr√∂dinger equation (similar structure):\n\\[i\\hbar\\frac{\\partial \\Psi}{\\partial t} = -\\frac{\\hbar^2}{2m}\\frac{\\partial^2 \\Psi}{\\partial x^2} + V\\Psi\\]\n\nComplex-valued\nSame spatial discretization!"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#extensions-higher-dimensions",
    "href": "lectures/lecture12/diffusion-equation-slides.html#extensions-higher-dimensions",
    "title": "The Diffusion Equation",
    "section": "Extensions: Higher Dimensions",
    "text": "Extensions: Higher Dimensions\n2D diffusion:\n\\[\\frac{\\partial c}{\\partial t} = D\\left(\\frac{\\partial^2 c}{\\partial x^2} + \\frac{\\partial^2 c}{\\partial y^2}\\right)\\]\n\nImplementation:\n\nFlatten 2D grid into 1D vector\nBuild larger matrix \\(M\\) (still sparse!)\nSame time-stepping algorithm\n\n\n\nChallenge: Much larger matrices, but sparse solvers handle it!"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#extensions-reaction-diffusion",
    "href": "lectures/lecture12/diffusion-equation-slides.html#extensions-reaction-diffusion",
    "title": "The Diffusion Equation",
    "section": "Extensions: Reaction-Diffusion",
    "text": "Extensions: Reaction-Diffusion\nAdd chemical reactions:\n\\[\\frac{\\partial c}{\\partial t} = D\\frac{\\partial^2 c}{\\partial x^2} + R(c)\\]\n\nExamples:\n\n\\(R(c) = kc\\) ‚Äî first-order reaction (growth/decay)\n\\(R(c) = kc(1-c)\\) ‚Äî logistic growth\n\\(R(c) = kc^2\\) ‚Äî autocatalytic reaction\n\n\n\nFamous: Turing patterns, waves in excitable media\n\n\n\nExamples of pattern formation in reaction-diffusion systems. Source: karlsims.com/rd.html"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#summary-the-recipe",
    "href": "lectures/lecture12/diffusion-equation-slides.html#summary-the-recipe",
    "title": "The Diffusion Equation",
    "section": "Summary: The Recipe",
    "text": "Summary: The Recipe\n\nWrite the PDE ‚Äî \\(\\frac{\\partial c}{\\partial t} = D\\frac{\\partial^2 c}{\\partial x^2}\\)\nDiscretize space ‚Äî Create matrix \\(M\\) for \\(\\frac{\\partial^2}{\\partial x^2}\\)\nDiscretize time ‚Äî Use Crank-Nicolson scheme\nSet boundary conditions ‚Äî Fix values at edges\nSet initial conditions ‚Äî Starting concentration profile\nTime-step ‚Äî Repeatedly solve \\({\\bf A}{\\bf C}^{n+1} = {\\bf B}{\\bf C}^n\\)\nVisualize ‚Äî Plot concentration evolution"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#key-concepts-to-remember",
    "href": "lectures/lecture12/diffusion-equation-slides.html#key-concepts-to-remember",
    "title": "The Diffusion Equation",
    "section": "Key Concepts to Remember",
    "text": "Key Concepts to Remember\n\n\n\nConcept\nKey Point\n\n\n\n\nPartial derivatives\nFunctions of multiple variables\n\n\nDiscretization\nTurn continuous ‚Üí discrete (finite points)\n\n\nMatrix formulation\nSpatial derivatives ‚Üí matrix multiplication\n\n\nCrank-Nicolson\nAverage at two time steps for stability\n\n\nSparse matrices\nEfficient for large systems\n\n\nBoundary conditions\nSpecify behavior at edges"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#from-random-walks-to-pdes",
    "href": "lectures/lecture12/diffusion-equation-slides.html#from-random-walks-to-pdes",
    "title": "The Diffusion Equation",
    "section": "From Random Walks to PDEs",
    "text": "From Random Walks to PDEs\nConnection to earlier lectures:\n\nMicroscopic view: Individual particles doing random walks - Lecture 5: Brownian motion simulation\n\n\nMacroscopic view: Concentration field obeying diffusion equation - Today: PDE simulation\n\n\nThey describe the same physics at different scales!"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#computational-complexity",
    "href": "lectures/lecture12/diffusion-equation-slides.html#computational-complexity",
    "title": "The Diffusion Equation",
    "section": "Computational Complexity",
    "text": "Computational Complexity\nHow many operations per time step?\n\n\nExplicit Euler: \\(O(N)\\) operations\n\nJust matrix-vector multiply\n\nImplicit (Crank-Nicolson): \\(O(N)\\) to \\(O(N^{3/2})\\) operations\n\nNeed to solve linear system\nSparse solvers are much faster than \\(O(N^3)\\)!\n\n\n\n\nFor large systems, sparse methods are essential!"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#real-world-applications",
    "href": "lectures/lecture12/diffusion-equation-slides.html#real-world-applications",
    "title": "The Diffusion Equation",
    "section": "Real-World Applications",
    "text": "Real-World Applications\n\nMaterials science: Diffusion in alloys, dopant spreading in semiconductors\nBiology: Morphogen gradients, drug delivery\nEnvironmental science: Pollutant spreading in air/water\nFinance: Black-Scholes equation (option pricing)\nImage processing: Gaussian blur, denoising\n\n\nSame mathematics, different contexts!"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#what-you-can-do-now",
    "href": "lectures/lecture12/diffusion-equation-slides.html#what-you-can-do-now",
    "title": "The Diffusion Equation",
    "section": "What You Can Do Now",
    "text": "What You Can Do Now\nWith today‚Äôs knowledge, you can simulate:\n\nHeat conduction in a metal rod\nChemical diffusion in solution\nQuantum particle in a box (with complex numbers)\nPopulation spreading in ecology\nAny system governed by \\(\\frac{\\partial u}{\\partial t} = D\\nabla^2 u\\)"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#practice-suggestions",
    "href": "lectures/lecture12/diffusion-equation-slides.html#practice-suggestions",
    "title": "The Diffusion Equation",
    "section": "Practice Suggestions",
    "text": "Practice Suggestions\nüì± On the webpage: Work through the complete example\n\nModifications to try:\n\nChange the diffusion coefficient \\(D\\) ‚Äî how does speed change?\nTry different initial conditions (two peaks, uniform + perturbation)\nChange boundary conditions (reflect instead of absorb)\nAdd a source term: \\(\\frac{\\partial c}{\\partial t} = D\\frac{\\partial^2 c}{\\partial x^2} + S\\)"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#comparison-ode-vs-pde-solvers",
    "href": "lectures/lecture12/diffusion-equation-slides.html#comparison-ode-vs-pde-solvers",
    "title": "The Diffusion Equation",
    "section": "Comparison: ODE vs PDE Solvers",
    "text": "Comparison: ODE vs PDE Solvers\n\n\n\nAspect\nODEs\nPDEs\n\n\n\n\nVariables\nTime only\nTime + space\n\n\nDiscretization\nTime steps\nSpace and time grids\n\n\nData structure\nVectors\nMatrices/Arrays\n\n\nSolving\nDirect integration\nLinear system at each step\n\n\nComplexity\n\\(O(N_t)\\)\n\\(O(N_t \\times N_x)\\)"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#next-steps-in-pdes",
    "href": "lectures/lecture12/diffusion-equation-slides.html#next-steps-in-pdes",
    "title": "The Diffusion Equation",
    "section": "Next Steps in PDEs",
    "text": "Next Steps in PDEs\nAfter mastering diffusion, you could explore:\n\nWave equation: \\(\\frac{\\partial^2 u}{\\partial t^2} = c^2\\frac{\\partial^2 u}{\\partial x^2}\\)\nAdvection-diffusion: \\(\\frac{\\partial c}{\\partial t} + v\\frac{\\partial c}{\\partial x} = D\\frac{\\partial^2 c}{\\partial x^2}\\)\nNavier-Stokes: Fluid dynamics (nonlinear PDE system)\nMaxwell equations: Electromagnetism"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#resources-for-learning-more",
    "href": "lectures/lecture12/diffusion-equation-slides.html#resources-for-learning-more",
    "title": "The Diffusion Equation",
    "section": "Resources for Learning More",
    "text": "Resources for Learning More\nBooks:\n\n‚ÄúNumerical Recipes‚Äù ‚Äî practical algorithms\n‚ÄúComputational Physics‚Äù by Newman ‚Äî Python examples\n‚ÄúNumerical Methods for PDEs‚Äù by Morton & Mayers ‚Äî theory\n\n\nPython packages:\n\nscipy.sparse ‚Äî sparse matrices\nFiPy ‚Äî finite volume solver for PDEs\nFEniCS ‚Äî finite element method toolkit"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#debugging-pdes",
    "href": "lectures/lecture12/diffusion-equation-slides.html#debugging-pdes",
    "title": "The Diffusion Equation",
    "section": "Debugging PDEs",
    "text": "Debugging PDEs\nCommon checks:\n# 1. Conservation\nprint(\"Total mass:\", np.sum(c) * dx)\n\n# 2. Matrix properties\nprint(\"Matrix is symmetric:\", np.allclose(M.toarray(), M.T.toarray()))\n\n# 3. Boundary values\nprint(\"Boundaries:\", c[0], c[-1])\n\n# 4. Physical bounds (e.g., concentration ‚â• 0)\nprint(\"Min concentration:\", np.min(c))"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#visualization-tips",
    "href": "lectures/lecture12/diffusion-equation-slides.html#visualization-tips",
    "title": "The Diffusion Equation",
    "section": "Visualization Tips",
    "text": "Visualization Tips\nMultiple views of the same data:\n\nSnapshots: Plot \\(c(x)\\) at different times\nSpacetime: Heatmap of \\(c(x,t)\\)\nAnimation: Show evolution dynamically\nCross-sections: Plot \\(c(t)\\) at fixed positions\n\n\nEach view reveals different aspects of the solution!"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#the-big-picture",
    "href": "lectures/lecture12/diffusion-equation-slides.html#the-big-picture",
    "title": "The Diffusion Equation",
    "section": "The Big Picture",
    "text": "The Big Picture\nWhat we learned:\n\n\nODEs ‚Üí PDEs: Adding spatial dimensions\nDiscretization: Making continuous ‚Üí discrete\nMatrix methods: Linear algebra for PDEs\nCrank-Nicolson: Stable implicit scheme\nImplementation: Sparse matrices + linear solvers\n\n\n\nThis is the foundation for all computational physics involving fields!"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#final-thoughts",
    "href": "lectures/lecture12/diffusion-equation-slides.html#final-thoughts",
    "title": "The Diffusion Equation",
    "section": "Final Thoughts",
    "text": "Final Thoughts\n\nPDEs are everywhere in physics and engineering\nDiffusion is the simplest PDE to start with\nNumerical methods let us solve what‚Äôs analytically impossible\nMatrix thinking is crucial for efficient computation\nSame principles extend to much more complex systems\n\n\nYou now have the tools to simulate physical fields!"
  },
  {
    "objectID": "lectures/lecture12/diffusion-equation-slides.html#questions",
    "href": "lectures/lecture12/diffusion-equation-slides.html#questions",
    "title": "The Diffusion Equation",
    "section": "Questions?",
    "text": "Questions?\nThe complete code is available on the course webpage:\nüì± https://fcichos.github.io/EMPP/lectures/lecture12/3_diffusion_equation.qmd\n\nKey equation to remember:\n\\[{\\bf A}{\\bf C}^{n+1} = {\\bf B}{\\bf C}^{n}\\]\nwhere \\({\\bf A} = {\\bf I} - \\frac{\\Delta t}{2}DM\\) and \\({\\bf B} = {\\bf I} + \\frac{\\Delta t}{2}DM\\)\n\n\nNow try it yourself! üöÄ"
  },
  {
    "objectID": "lectures/lecture15/2_particle_in_a_box.html",
    "href": "lectures/lecture15/2_particle_in_a_box.html",
    "title": "Application: Particle in a Box",
    "section": "",
    "text": "Now let‚Äôs apply our recipe to a concrete problem: finding the eigenfunctions and eigenvalues of the Schr√∂dinger equation with a specific potential \\(V(x)\\)."
  },
  {
    "objectID": "lectures/lecture15/2_particle_in_a_box.html#the-problem-a-rectangular-potential-well",
    "href": "lectures/lecture15/2_particle_in_a_box.html#the-problem-a-rectangular-potential-well",
    "title": "Application: Particle in a Box",
    "section": "The Problem: A Rectangular Potential Well",
    "text": "The Problem: A Rectangular Potential Well\nWe‚Äôll solve for the eigenfunctions of a ‚Äúparticle in a box‚Äù‚Äîa rectangular potential well where \\(V(x) = 0\\) inside the box and \\(V(x) = V_0\\) (a large value) outside.\n\n\n\nParticle in a Box\n\n\nWhat do we expect?\nFrom a mathematical perspective, this is like finding standing waves on a string with fixed endpoints. Just as a guitar string can only vibrate at certain frequencies, this differential equation will only have solutions for certain eigenvalues \\(E\\)."
  },
  {
    "objectID": "lectures/lecture15/2_particle_in_a_box.html#step-1-define-the-parameters",
    "href": "lectures/lecture15/2_particle_in_a_box.html#step-1-define-the-parameters",
    "title": "Application: Particle in a Box",
    "section": "Step 1: Define the Parameters",
    "text": "Step 1: Define the Parameters\nWe need to specify:\n\nThe spatial domain and grid\nPhysical constants (\\(\\hbar\\), mass \\(m\\))\nThe potential function \\(V(x)\\)"
  },
  {
    "objectID": "lectures/lecture15/2_particle_in_a_box.html#step-2-build-the-potential-energy-matrix",
    "href": "lectures/lecture15/2_particle_in_a_box.html#step-2-build-the-potential-energy-matrix",
    "title": "Application: Particle in a Box",
    "section": "Step 2: Build the Potential Energy Matrix",
    "text": "Step 2: Build the Potential Energy Matrix\nThe potential is zero inside the box and \\(V_0\\) outside:\n\\[V(x) = \\begin{cases} 0 & \\text{if } |x| &lt; d/2 \\\\ V_0 & \\text{if } |x| \\geq d/2 \\end{cases}\\]\nIn matrix form, this is a diagonal matrix:\n\n\n\n\n\n\n\n\n\n\n\n\nWhy V‚ÇÄ = 20 eV?\n\n\n\nThe number of bound states depends on the ratio between the well depth \\(V_0\\) and the characteristic energy scale of the box. For an electron in a 1 nm box, the ground state energy is approximately 0.38 eV. The eigenvalues scale as \\(E_n \\propto n^2\\), so:\n\n\\(E_1 \\approx 0.38\\) eV\n\\(E_2 \\approx 1.5\\) eV\n\n\\(E_3 \\approx 3.4\\) eV\n‚Ä¶\n\nOnly states with \\(E_n &lt; V_0\\) are ‚Äúbound‚Äù (confined to the well). With \\(V_0 = 20\\) eV, we get about 7 bound states‚Äîenough to see interesting structure!"
  },
  {
    "objectID": "lectures/lecture15/2_particle_in_a_box.html#step-3-build-the-kinetic-energy-matrix",
    "href": "lectures/lecture15/2_particle_in_a_box.html#step-3-build-the-kinetic-energy-matrix",
    "title": "Application: Particle in a Box",
    "section": "Step 3: Build the Kinetic Energy Matrix",
    "text": "Step 3: Build the Kinetic Energy Matrix\nThe kinetic energy term involves the second derivative. Using finite differences:\n\\[-\\frac{\\hbar^2}{2m}\\frac{d^2}{dx^2} \\rightarrow -\\frac{\\hbar^2}{2m \\cdot dx^2}\\begin{bmatrix} -2 & 1 & 0 & \\cdots \\\\ 1 & -2 & 1 & \\cdots \\\\ \\vdots & \\ddots & \\ddots & \\ddots \\end{bmatrix}\\]"
  },
  {
    "objectID": "lectures/lecture15/2_particle_in_a_box.html#step-4-assemble-the-hamiltonian-matrix",
    "href": "lectures/lecture15/2_particle_in_a_box.html#step-4-assemble-the-hamiltonian-matrix",
    "title": "Application: Particle in a Box",
    "section": "Step 4: Assemble the Hamiltonian Matrix",
    "text": "Step 4: Assemble the Hamiltonian Matrix\nThe full matrix is simply the sum:\n\\[\\mathbf{H} = \\mathbf{T} + \\mathbf{V}\\]"
  },
  {
    "objectID": "lectures/lecture15/2_particle_in_a_box.html#step-5-solve-the-eigenvalue-problem",
    "href": "lectures/lecture15/2_particle_in_a_box.html#step-5-solve-the-eigenvalue-problem",
    "title": "Application: Particle in a Box",
    "section": "Step 5: Solve the Eigenvalue Problem",
    "text": "Step 5: Solve the Eigenvalue Problem\nWe use scipy.sparse.linalg.eigsh to find the eigenvalues and eigenvectors. The k parameter specifies how many eigenvalues we want, and which='SM' means we want the smallest magnitude eigenvalues."
  },
  {
    "objectID": "lectures/lecture15/2_particle_in_a_box.html#visualizing-the-results",
    "href": "lectures/lecture15/2_particle_in_a_box.html#visualizing-the-results",
    "title": "Application: Particle in a Box",
    "section": "Visualizing the Results",
    "text": "Visualizing the Results\nLet‚Äôs plot the eigenfunctions (solutions) along with their corresponding eigenvalues (energies):"
  },
  {
    "objectID": "lectures/lecture15/2_particle_in_a_box.html#interpreting-the-results",
    "href": "lectures/lecture15/2_particle_in_a_box.html#interpreting-the-results",
    "title": "Application: Particle in a Box",
    "section": "Interpreting the Results",
    "text": "Interpreting the Results\nWhat did we find?\n\nDiscrete eigenvalues: Only certain values of \\(E\\) are allowed‚Äîthese are the horizontal dashed lines.\nStructured eigenfunctions: Each solution has a characteristic shape with a specific number of ‚Äúnodes‚Äù (zero crossings).\nExponential decay outside the box: The solutions decay exponentially in the region where \\(V(x) &gt; E\\). This is a consequence of the differential equation!\n\n\n\n\n\n\n\nA Mathematical Observation\n\n\n\nNotice that lower eigenvalues correspond to eigenfunctions with fewer oscillations. This is a general property of such eigenvalue problems‚Äîhigher eigenvalues = more oscillations."
  },
  {
    "objectID": "lectures/lecture15/2_particle_in_a_box.html#eigenvalue-scaling",
    "href": "lectures/lecture15/2_particle_in_a_box.html#eigenvalue-scaling",
    "title": "Application: Particle in a Box",
    "section": "Eigenvalue Scaling",
    "text": "Eigenvalue Scaling\nLet‚Äôs examine how the eigenvalues grow with the index (quantum number):\n\n\n\n\n\n\nThe eigenvalues increase roughly as \\(n^2\\). This is actually the analytical result for an infinite square well!"
  },
  {
    "objectID": "lectures/lecture15/2_particle_in_a_box.html#exercise-double-well-potential",
    "href": "lectures/lecture15/2_particle_in_a_box.html#exercise-double-well-potential",
    "title": "Application: Particle in a Box",
    "section": "Exercise: Double Well Potential",
    "text": "Exercise: Double Well Potential\nTry modifying the code to create two potential wells close together. This is called a ‚Äúdouble well‚Äù potential:\n\n\n\nDouble Well\n\n\nSuggested parameters:\n\nTwo wells of width 1 nm each\nSeparated by a thin barrier (try 0.2 nm and 2 nm)\n\nWhat to observe:\n\nWhen the wells are far apart, you‚Äôll find pairs of nearly identical eigenvalues (one for each well)\nWhen the wells are close, the eigenvalues ‚Äúsplit‚Äù due to the coupling between wells\n\nThis is a nice example of how the structure of \\(V(x)\\) affects the eigenvalue spectrum!"
  },
  {
    "objectID": "lectures/lecture15/2_particle_in_a_box.html#summary",
    "href": "lectures/lecture15/2_particle_in_a_box.html#summary",
    "title": "Application: Particle in a Box",
    "section": "Summary",
    "text": "Summary\nIn this lecture, we:\n\nDiscretized the Schr√∂dinger equation using finite differences\nConstructed the Hamiltonian matrix \\(\\mathbf{H} = \\mathbf{T} + \\mathbf{V}\\)\nSolved the matrix eigenvalue problem \\(\\mathbf{H}\\vec{\\Psi} = E\\vec{\\Psi}\\)\nVisualized the eigenfunctions and eigenvalues\n\nThe key insight is that any second-order differential equation with boundary conditions can be approached this way. The Schr√∂dinger equation is just one example‚Äîthe same techniques apply to vibrating strings, heat conduction, and many other physical systems."
  },
  {
    "objectID": "lectures/lecture15/1_quantum_mechanics.html#introduction-a-differential-equation-problem",
    "href": "lectures/lecture15/1_quantum_mechanics.html#introduction-a-differential-equation-problem",
    "title": "Quantum Mechanics",
    "section": "Introduction: A Differential Equation Problem",
    "text": "Introduction: A Differential Equation Problem\nToday we will solve a famous differential equation from physics: the Schr√∂dinger equation. Don‚Äôt worry‚Äîyou don‚Äôt need to understand quantum mechanics to follow this lecture! We will treat the Schr√∂dinger equation simply as what it is: a second-order differential equation that we can convert into a matrix eigenvalue problem.\nThe approach we‚Äôll use connects several things we‚Äôve learned:\n\nFinite differences to approximate derivatives (from earlier lectures)\nMatrix formulation of differential equations\nEigenvalue problems of the form \\(A\\vec{x} = \\lambda \\vec{x}\\) (from linear algebra)\n\nThe physical interpretation is interesting but optional for our purposes. What matters is that we can apply our numerical tools to solve this equation."
  },
  {
    "objectID": "lectures/lecture15/1_quantum_mechanics.html#the-equation-we-want-to-solve",
    "href": "lectures/lecture15/1_quantum_mechanics.html#the-equation-we-want-to-solve",
    "title": "Quantum Mechanics",
    "section": "The Equation We Want to Solve",
    "text": "The Equation We Want to Solve\nThe stationary Schr√∂dinger equation in one dimension is:\n\\[\n-\\frac{\\hbar^2}{2m}\\frac{d^2 \\Psi(x)}{dx^2} + V(x)\\Psi(x) = E\\Psi(x)\n\\]\nLet‚Äôs identify what each term means mathematically:\n\n\n\n\n\n\n\n\nSymbol\nMathematical Role\nPhysical Meaning (optional)\n\n\n\n\n\\(\\Psi(x)\\)\nUnknown function we want to find\nWave function\n\n\n\\(\\frac{d^2 \\Psi}{dx^2}\\)\nSecond derivative of \\(\\Psi\\)\nRelated to kinetic energy\n\n\n\\(V(x)\\)\nA given function (the ‚Äúpotential‚Äù)\nPotential energy\n\n\n\\(E\\)\nUnknown constant (eigenvalue)\nTotal energy\n\n\n\\(\\hbar, m\\)\nPhysical constants\nPlanck‚Äôs constant, mass\n\n\n\n\n\n\n\n\n\nWhat kind of problem is this?\n\n\n\nThis is an eigenvalue problem! We‚Äôre looking for:\n\nEigenfunctions \\(\\Psi(x)\\): special functions that, when processed through the differential operator, return a constant times themselves\nEigenvalues \\(E\\): the constants associated with each eigenfunction\n\nThis is exactly like \\(A\\vec{x} = \\lambda\\vec{x}\\) from linear algebra, but in function space!"
  },
  {
    "objectID": "lectures/lecture15/1_quantum_mechanics.html#from-differential-equation-to-matrix-problem",
    "href": "lectures/lecture15/1_quantum_mechanics.html#from-differential-equation-to-matrix-problem",
    "title": "Quantum Mechanics",
    "section": "From Differential Equation to Matrix Problem",
    "text": "From Differential Equation to Matrix Problem\nOur strategy is to convert this continuous differential equation into a discrete matrix equation that we can solve with standard linear algebra tools.\n\nStep 1: Discretize the Domain\nInstead of considering \\(\\Psi(x)\\) for all real \\(x\\), we evaluate it only at discrete points:\n\\[x_1, x_2, x_3, \\ldots, x_N\\]\nwith spacing \\(\\delta x\\). Our unknown function \\(\\Psi(x)\\) becomes a vector:\n\\[\\vec{\\Psi} = \\begin{bmatrix} \\Psi(x_1) \\\\ \\Psi(x_2) \\\\ \\vdots \\\\ \\Psi(x_N) \\end{bmatrix}\\]\n\n\nStep 2: Discretize the Second Derivative\nWe‚Äôve seen before that the second derivative can be approximated by finite differences:\n\\[\\frac{d^2\\Psi}{dx^2}\\bigg|_{x_i} \\approx \\frac{\\Psi(x_{i+1}) - 2\\Psi(x_i) + \\Psi(x_{i-1})}{\\delta x^2}\\]\nThis operation can be written as a matrix multiplication:\n\\[\\frac{d^2}{dx^2}\\vec{\\Psi} \\approx \\frac{1}{\\delta x^2}\n\\begin{bmatrix}\n-2 & 1  & 0 & 0 & \\cdots & 0\\\\\n1 & -2 & 1 & 0 & \\cdots & 0\\\\\n0 & 1  & -2 & 1 & \\cdots & 0\\\\\n\\vdots & & \\ddots & \\ddots & \\ddots & \\vdots\\\\\n0 & \\cdots & 0  & 1 & -2 & 1\\\\\n0 & \\cdots & 0  &  0 &  1 & -2\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\Psi(x_{1})\\\\\n\\Psi(x_{2})\\\\\n\\Psi(x_{3})\\\\\n\\vdots\\\\\n\\Psi(x_{N-1})\\\\\n\\Psi(x_{N})\n\\end{bmatrix}\\]\nThis is a tridiagonal matrix with \\(-2\\) on the diagonal and \\(1\\) on the off-diagonals.\n\n\n\n\n\n\nBoundary Conditions\n\n\n\nThe matrix above implicitly assumes \\(\\Psi(x_0) = 0\\) and \\(\\Psi(x_{N+1}) = 0\\) (the function vanishes at the boundaries). These are called Dirichlet boundary conditions.\n\n\n\n\nStep 3: Discretize the Potential Term\nThe potential energy term \\(V(x)\\Psi(x)\\) is simpler‚Äîit‚Äôs just multiplication by a function at each point. In matrix form, this becomes a diagonal matrix:\n\\[V(x)\\vec{\\Psi} =\n\\begin{bmatrix}\nV(x_{1}) & 0  & \\cdots & 0\\\\\n0 & V(x_{2}) & \\cdots & 0\\\\\n\\vdots & & \\ddots & \\vdots\\\\\n0 & \\cdots &  0 & V(x_{N})\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\Psi(x_{1})\\\\\n\\Psi(x_{2})\\\\\n\\vdots\\\\\n\\Psi(x_{N})\n\\end{bmatrix}\\]\n\n\nStep 4: Assemble the Full Matrix\nCombining both terms, our differential equation becomes:\n\\[\\underbrace{\\left(-\\frac{\\hbar^2}{2m}\\mathbf{T} + \\mathbf{V}\\right)}_{\\mathbf{H}}\\vec{\\Psi} = E\\vec{\\Psi}\\]\nwhere:\n\n\\(\\mathbf{T}\\) is the tridiagonal second-derivative matrix\n\\(\\mathbf{V}\\) is the diagonal potential matrix\n\\(\\mathbf{H} = -\\frac{\\hbar^2}{2m}\\mathbf{T} + \\mathbf{V}\\) is called the Hamiltonian matrix\n\nThis is now a standard matrix eigenvalue problem: \\(\\mathbf{H}\\vec{\\Psi} = E\\vec{\\Psi}\\)"
  },
  {
    "objectID": "lectures/lecture15/1_quantum_mechanics.html#summary-the-recipe",
    "href": "lectures/lecture15/1_quantum_mechanics.html#summary-the-recipe",
    "title": "Quantum Mechanics",
    "section": "Summary: The Recipe",
    "text": "Summary: The Recipe\nTo solve the Schr√∂dinger equation numerically:\n\nChoose a spatial grid \\(x_1, \\ldots, x_N\\) with spacing \\(\\delta x\\)\nBuild the kinetic energy matrix \\(\\mathbf{T}\\) (tridiagonal: \\(-2\\) diagonal, \\(1\\) off-diagonal)\nBuild the potential energy matrix \\(\\mathbf{V}\\) (diagonal with values \\(V(x_i)\\))\nCombine into \\(\\mathbf{H} = -\\frac{\\hbar^2}{2m}\\mathbf{T} + \\mathbf{V}\\)\nSolve the eigenvalue problem \\(\\mathbf{H}\\vec{\\Psi} = E\\vec{\\Psi}\\) using scipy.sparse.linalg.eigsh\n\nThe eigenvalues \\(E\\) are the allowed energies, and the eigenvectors \\(\\vec{\\Psi}\\) are the corresponding wave functions.\n\n\n\n\n\n\nKey Insight\n\n\n\nWe transformed a continuous differential equation into a discrete matrix eigenvalue problem. The mathematics is the same as finding eigenvalues of any matrix‚Äîscipy does the heavy lifting!\n\n\nIn the next section, we‚Äôll apply this recipe to a concrete example: a particle trapped in a ‚Äúbox‚Äù (a potential well)."
  },
  {
    "objectID": "lectures/lecture15/1_quantum_mechanics.html#exercise-building-the-second-derivative-matrix",
    "href": "lectures/lecture15/1_quantum_mechanics.html#exercise-building-the-second-derivative-matrix",
    "title": "Quantum Mechanics",
    "section": "Exercise: Building the Second Derivative Matrix",
    "text": "Exercise: Building the Second Derivative Matrix\nBefore we move on, let‚Äôs practice building the matrix representation of the second derivative operator.\nTask: Create a function that builds the tridiagonal matrix for the second derivative and test it on a simple function \\(f(x) = x^2\\), for which we know the analytical second derivative is \\(f''(x) = 2\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse scipy.sparse.diags() to build the tridiagonal matrix efficiently:\nfrom scipy.sparse import diags\nD2 = diags([-2, 1, 1], [0, -1, 1], shape=(N, N)) / dx**2\n\nFirst argument [-2, 1, 1]: the values on each diagonal\nSecond argument [0, -1, 1]: the positions (0 = main, -1 = below, +1 = above)\nshape=(N, N): the matrix dimensions\n\nTo apply a sparse matrix to a vector, use D2 @ f (the result is a dense array).\n\n\n\n\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#part-2-numerical-methods-for-odes",
    "href": "lectures/lecture15/ode-review-slides.html#part-2-numerical-methods-for-odes",
    "title": "Solving Differential Equations",
    "section": "Part 2: Numerical Methods for ODEs",
    "text": "Part 2: Numerical Methods for ODEs\n\nWhy we need numerical methods\nHow the Euler method works (from Taylor series!)\nPractice implementing your own ODE solver\nApply to physics problems\n\n\nüì± Open now: https://fcichos.github.io/EMPP/lectures/lecture15/lecture15a.qmd\n(or find ‚ÄúRepetition: ODE Review‚Äù in Week 11 on the website)"
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#why-differential-equations",
    "href": "lectures/lecture15/ode-review-slides.html#why-differential-equations",
    "title": "Solving Differential Equations",
    "section": "Why Differential Equations?",
    "text": "Why Differential Equations?\nPhysics is written in the language of differential equations:\n\n\n\n\n\n\n\nEquation\nPhysics\n\n\n\n\n\\(F = ma = m\\frac{d^2x}{dt^2}\\)\nNewton‚Äôs mechanics\n\n\n\\(\\frac{dN}{dt} = -\\lambda N\\)\nRadioactive decay\n\n\n\\(\\frac{d^2\\theta}{dt^2} = -\\frac{g}{L}\\sin\\theta\\)\nPendulum\n\n\n\\(i\\hbar\\frac{\\partial\\Psi}{\\partial t} = \\hat{H}\\Psi\\)\nQuantum mechanics\n\n\n\n\nProblem: Most of these cannot be solved analytically!"
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#analytical-vs.-numerical-solutions",
    "href": "lectures/lecture15/ode-review-slides.html#analytical-vs.-numerical-solutions",
    "title": "Solving Differential Equations",
    "section": "Analytical vs.¬†Numerical Solutions",
    "text": "Analytical vs.¬†Numerical Solutions\nAnalytical solution: Exact formula (when possible)\n\\[N(t) = N_0 e^{-\\lambda t}\\]\n\nNumerical solution: Step-by-step approximation\n\\[N(t + \\Delta t) \\approx N(t) + \\Delta t \\cdot \\frac{dN}{dt}\\]\n\n\nKey insight: We trade exactness for the ability to solve any ODE!"
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#the-foundation-taylor-series",
    "href": "lectures/lecture15/ode-review-slides.html#the-foundation-taylor-series",
    "title": "Solving Differential Equations",
    "section": "The Foundation: Taylor Series",
    "text": "The Foundation: Taylor Series\nAny smooth function can be expanded around a point \\(t_0\\):\n\\[f(t) = f(t_0) + (t-t_0)f'(t_0) + \\frac{(t-t_0)^2}{2!}f''(t_0) + \\frac{(t-t_0)^3}{3!}f'''(t_0) + \\ldots\\]\n\nWhat does each term represent?\n\n\\(f(t_0)\\) ‚Äî value at the starting point\n\\((t-t_0)f'(t_0)\\) ‚Äî linear change (slope)\n\\(\\frac{(t-t_0)^2}{2!}f''(t_0)\\) ‚Äî curvature correction\nHigher terms ‚Äî finer corrections"
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#taylor-series-visual-intuition",
    "href": "lectures/lecture15/ode-review-slides.html#taylor-series-visual-intuition",
    "title": "Solving Differential Equations",
    "section": "Taylor Series: Visual Intuition",
    "text": "Taylor Series: Visual Intuition\n\nEuler method: Keep only the first two terms (linear approximation)"
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#the-euler-method-core-idea",
    "href": "lectures/lecture15/ode-review-slides.html#the-euler-method-core-idea",
    "title": "Solving Differential Equations",
    "section": "The Euler Method: Core Idea",
    "text": "The Euler Method: Core Idea\nIf we truncate Taylor series after the linear term:\n\\[f(t_0 + \\Delta t) \\approx f(t_0) + \\Delta t \\cdot f'(t_0)\\]\n\nIn words:\n\n‚ÄúThe new value equals the old value plus the change‚Äù\n\n\n‚ÄúChange = slope √ó time step‚Äù\n\n\n\nThis is the Euler method ‚Äî the simplest ODE solver!"
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#euler-method-single-step-visualized",
    "href": "lectures/lecture15/ode-review-slides.html#euler-method-single-step-visualized",
    "title": "Solving Differential Equations",
    "section": "Euler Method: Single Step Visualized",
    "text": "Euler Method: Single Step Visualized"
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#euler-method-step-by-step",
    "href": "lectures/lecture15/ode-review-slides.html#euler-method-step-by-step",
    "title": "Solving Differential Equations",
    "section": "Euler Method: Step by Step",
    "text": "Euler Method: Step by Step\nGiven: \\(\\frac{dy}{dt} = f(y, t)\\) with initial condition \\(y(0) = y_0\\)\n\nAlgorithm:\n\nStart with \\(y_0\\) at \\(t = 0\\)\nCalculate the slope: \\(\\text{slope} = f(y_0, t_0)\\)\nTake a step: \\(y_1 = y_0 + \\Delta t \\cdot \\text{slope}\\)\nRepeat from the new point\n\n\n\nfor j in range(1, num_steps):\n    slope = derivative(y[j-1], t[j-1])\n    y[j] = y[j-1] + dt * slope"
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#example-radioactive-decay",
    "href": "lectures/lecture15/ode-review-slides.html#example-radioactive-decay",
    "title": "Solving Differential Equations",
    "section": "Example: Radioactive Decay",
    "text": "Example: Radioactive Decay\nThe decay equation:\n\\[\\frac{dN}{dt} = -k N\\]\n\nWhat we know:\n\n\\(N\\) = number of radioactive nuclei\n\\(k\\) = decay constant\nThe derivative depends on the current value of \\(N\\)\n\n\n\nApplying Euler:\n\\[N(t + \\Delta t) = N(t) + \\Delta t \\cdot (-k N(t))\\]"
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#translating-math-to-code",
    "href": "lectures/lecture15/ode-review-slides.html#translating-math-to-code",
    "title": "Solving Differential Equations",
    "section": "Translating Math to Code",
    "text": "Translating Math to Code\nThe math: \\[\\frac{dN}{dt} = -kN\\] \\[N_{new} = N_{old} + \\Delta t \\cdot (-kN_{old})\\]\n\nThe code:\ndef decay(N, k):\n    \"\"\"The derivative: dN/dt = -kN\"\"\"\n    return -k * N\n\ndef euler_step(N, dt, k):\n    \"\"\"One step of Euler method\"\"\"\n    return N + dt * decay(N, k)\n\n\nNotice: The code directly mirrors the math!"
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#the-complete-euler-solver",
    "href": "lectures/lecture15/ode-review-slides.html#the-complete-euler-solver",
    "title": "Solving Differential Equations",
    "section": "The Complete Euler Solver",
    "text": "The Complete Euler Solver\n# Define the derivative\ndef decay(N, k):\n    return -k * N\n\n# Setup\nt = np.arange(0, 5, 0.1)  # time points\ndt = t[1] - t[0]           # time step\nN = np.zeros(len(t))       # storage for results\nN[0] = 100                 # initial condition\n\n# Solve!\nfor j in range(1, len(t)):\n    N[j] = N[j-1] + dt * decay(N[j-1], k=1)\n\nThat‚Äôs it! You‚Äôve just built an ODE solver."
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#your-turn-exercise-1",
    "href": "lectures/lecture15/ode-review-slides.html#your-turn-exercise-1",
    "title": "Solving Differential Equations",
    "section": "üéØ Your Turn: Exercise 1",
    "text": "üéØ Your Turn: Exercise 1\nOn the webpage, implement the Euler method for radioactive decay:\n\nDefine the decay(N, *params) function\nDefine the euler(N, t, dt, derivs, *params) function\nRun the time-stepping loop\nCompare your result to the analytical solution\n\n‚è±Ô∏è Time: 10 minutes\nüí° Hint: The Euler step is just N + derivs(N, *params) * dt"
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#how-accurate-is-eulers-method",
    "href": "lectures/lecture15/ode-review-slides.html#how-accurate-is-eulers-method",
    "title": "Solving Differential Equations",
    "section": "How Accurate is Euler‚Äôs Method?",
    "text": "How Accurate is Euler‚Äôs Method?\n\nError: Proportional to \\(\\Delta t\\) (first-order method) ‚Äî smaller steps = better accuracy!"
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#improving-accuracy-the-key-insight",
    "href": "lectures/lecture15/ode-review-slides.html#improving-accuracy-the-key-insight",
    "title": "Solving Differential Equations",
    "section": "Improving Accuracy: The Key Insight",
    "text": "Improving Accuracy: The Key Insight\nEuler‚Äôs problem: Uses slope only at the start of the interval\n\nBetter idea: Use the average slope across the interval!\n\\[f(t_0 + \\Delta t) \\approx f(t_0) + \\Delta t \\cdot \\frac{f'(t_0) + f'(t_0 + \\Delta t)}{2}\\]\n\n\nBut wait‚Ä¶ We don‚Äôt know \\(f'(t_0 + \\Delta t)\\) yet!\nSolution: First predict using Euler, then correct using the average."
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#improved-euler-method-heuns-method",
    "href": "lectures/lecture15/ode-review-slides.html#improved-euler-method-heuns-method",
    "title": "Solving Differential Equations",
    "section": "Improved Euler Method (Heun‚Äôs Method)",
    "text": "Improved Euler Method (Heun‚Äôs Method)\nStep 1 ‚Äî Predict: Use regular Euler to estimate the endpoint\n\\[y_{pred} = y_n + \\Delta t \\cdot f'(y_n)\\]\n\nStep 2 ‚Äî Correct: Average the slopes at start and predicted endpoint\n\\[y_{n+1} = y_n + \\frac{\\Delta t}{2}\\left(f'(y_n) + f'(y_{pred})\\right)\\]\n\n\nResult: Error is now proportional to \\((\\Delta t)^2\\) ‚Äî much better!"
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#improved-euler-visual-intuition",
    "href": "lectures/lecture15/ode-review-slides.html#improved-euler-visual-intuition",
    "title": "Solving Differential Equations",
    "section": "Improved Euler: Visual Intuition",
    "text": "Improved Euler: Visual Intuition"
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#improved-euler-in-code",
    "href": "lectures/lecture15/ode-review-slides.html#improved-euler-in-code",
    "title": "Solving Differential Equations",
    "section": "Improved Euler in Code",
    "text": "Improved Euler in Code\ndef improved_euler(y, dt, derivs, *params):\n    # Step 1: Predict using basic Euler\n    y_pred = y + derivs(y, *params) * dt\n    \n    # Step 2: Average the slopes\n    slope_start = derivs(y, *params)\n    slope_end = derivs(y_pred, *params)\n    average_slope = 0.5 * (slope_start + slope_end)\n    \n    # Step 3: Take the step with average slope\n    return y + dt * average_slope\n\nKey difference: Two derivative evaluations per step instead of one."
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#your-turn-exercise-2",
    "href": "lectures/lecture15/ode-review-slides.html#your-turn-exercise-2",
    "title": "Solving Differential Equations",
    "section": "üéØ Your Turn: Exercise 2",
    "text": "üéØ Your Turn: Exercise 2\nOn the webpage, implement the improved Euler method:\n\nModify the euler function to use slope averaging\nApply it to radioactive decay\nCompare with basic Euler and the analytical solution\n\n‚è±Ô∏è Time: 8 minutes\nüí° Hint: First calculate N_pred, then average the derivatives"
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#second-order-odes-the-challenge",
    "href": "lectures/lecture15/ode-review-slides.html#second-order-odes-the-challenge",
    "title": "Solving Differential Equations",
    "section": "Second-Order ODEs: The Challenge",
    "text": "Second-Order ODEs: The Challenge\nMany physics problems involve second derivatives:\n\\[\\frac{d^2y}{dt^2} = -\\omega^2 y \\quad \\text{(harmonic oscillator)}\\]\n\nProblem: Our Euler methods work on first-order ODEs!\n\n\nSolution: Convert to a system of first-order ODEs."
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#the-trick-introduce-velocity",
    "href": "lectures/lecture15/ode-review-slides.html#the-trick-introduce-velocity",
    "title": "Solving Differential Equations",
    "section": "The Trick: Introduce Velocity",
    "text": "The Trick: Introduce Velocity\nOriginal equation: \\[\\frac{d^2y}{dt^2} = -\\omega^2 y\\]\n\nDefine: \\(v = \\frac{dy}{dt}\\) (velocity)\n\n\nNow we have two first-order equations:\n\\[\\frac{dy}{dt} = v\\] \\[\\frac{dv}{dt} = -\\omega^2 y\\]\n\n\nState vector: \\(\\vec{s} = \\begin{pmatrix} y \\\\ v \\end{pmatrix}\\)"
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#visualizing-the-state-vector",
    "href": "lectures/lecture15/ode-review-slides.html#visualizing-the-state-vector",
    "title": "Solving Differential Equations",
    "section": "Visualizing the State Vector",
    "text": "Visualizing the State Vector"
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#harmonic-oscillator-the-derivative-function",
    "href": "lectures/lecture15/ode-review-slides.html#harmonic-oscillator-the-derivative-function",
    "title": "Solving Differential Equations",
    "section": "Harmonic Oscillator: The Derivative Function",
    "text": "Harmonic Oscillator: The Derivative Function\ndef harmonic_oscillator(state, omega):\n    \"\"\"\n    state = [y, v] = [position, velocity]\n    Returns [dy/dt, dv/dt] = [v, -omega¬≤¬∑y]\n    \"\"\"\n    y, v = state\n    dydt = v\n    dvdt = -omega**2 * y\n    return np.array([dydt, dvdt])\n\nNotice: The function takes a vector and returns a vector!"
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#solving-the-harmonic-oscillator",
    "href": "lectures/lecture15/ode-review-slides.html#solving-the-harmonic-oscillator",
    "title": "Solving Differential Equations",
    "section": "Solving the Harmonic Oscillator",
    "text": "Solving the Harmonic Oscillator\n# Initial conditions: y=1, v=0 (released from displacement)\nstate = np.zeros((len(t), 2))\nstate[0] = [1, 0]\n\n# Time evolution\nfor j in range(1, len(t)):\n    state[j] = improved_euler(state[j-1], dt, \n                               harmonic_oscillator, omega=1)\n\n# Extract position and velocity\ny = state[:, 0]\nv = state[:, 1]\n\nThe solver doesn‚Äôt change! Only the derivative function changes."
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#harmonic-oscillator-results",
    "href": "lectures/lecture15/ode-review-slides.html#harmonic-oscillator-results",
    "title": "Solving Differential Equations",
    "section": "Harmonic Oscillator: Results",
    "text": "Harmonic Oscillator: Results"
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#your-turn-exercise-3",
    "href": "lectures/lecture15/ode-review-slides.html#your-turn-exercise-3",
    "title": "Solving Differential Equations",
    "section": "üéØ Your Turn: Exercise 3",
    "text": "üéØ Your Turn: Exercise 3\nOn the webpage, solve the harmonic oscillator:\n\nDefine harmonic_oscillator(y, *params) returning [v, -k¬≤¬∑y]\nUse the provided improved Euler solver\nPlot position vs.¬†time\nCompare with \\(\\cos(\\omega t)\\)\n\n‚è±Ô∏è Time: 10 minutes\nüí° Hint: The state vector is [position, velocity]"
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#coupled-equations-predator-prey",
    "href": "lectures/lecture15/ode-review-slides.html#coupled-equations-predator-prey",
    "title": "Solving Differential Equations",
    "section": "Coupled Equations: Predator-Prey",
    "text": "Coupled Equations: Predator-Prey\nSome systems have variables that influence each other:\n\\[\\frac{dx}{dt} = \\alpha x - \\beta xy \\quad \\text{(prey)}\\] \\[\\frac{dy}{dt} = \\delta xy - \\gamma y \\quad \\text{(predators)}\\]\n\nThe Lotka-Volterra equations:\n\nPrey grow exponentially (\\(\\alpha x\\)) but are eaten (\\(-\\beta xy\\))\nPredators die without food (\\(-\\gamma y\\)) but reproduce when fed (\\(\\delta xy\\))"
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#understanding-the-interactions",
    "href": "lectures/lecture15/ode-review-slides.html#understanding-the-interactions",
    "title": "Solving Differential Equations",
    "section": "Understanding the Interactions",
    "text": "Understanding the Interactions\n\nMore prey ‚Üí more food ‚Üí predators increase\nMore predators ‚Üí more hunting ‚Üí prey decrease\nLess prey ‚Üí predators starve ‚Üí predators decrease\nFewer predators ‚Üí less hunting ‚Üí prey recover\n‚Ä¶ cycle repeats!\n\n\nThis creates oscillations in both populations!"
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#lotka-volterra-the-code",
    "href": "lectures/lecture15/ode-review-slides.html#lotka-volterra-the-code",
    "title": "Solving Differential Equations",
    "section": "Lotka-Volterra: The Code",
    "text": "Lotka-Volterra: The Code\ndef lotka_volterra(state, alpha, beta, gamma, delta):\n    \"\"\"\n    state = [x, y] = [prey, predators]\n    \"\"\"\n    x, y = state\n    \n    dxdt = alpha*x - beta*x*y    # prey dynamics\n    dydt = delta*x*y - gamma*y   # predator dynamics\n    \n    return np.array([dxdt, dydt])\n\nSame structure as before! Vector in, vector out."
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#lotka-volterra-results",
    "href": "lectures/lecture15/ode-review-slides.html#lotka-volterra-results",
    "title": "Solving Differential Equations",
    "section": "Lotka-Volterra: Results",
    "text": "Lotka-Volterra: Results\n\nThe closed loop in phase space is called a limit cycle ‚Äî the system oscillates forever!"
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#your-turn-exercise-4",
    "href": "lectures/lecture15/ode-review-slides.html#your-turn-exercise-4",
    "title": "Solving Differential Equations",
    "section": "üéØ Your Turn: Exercise 4",
    "text": "üéØ Your Turn: Exercise 4\nOn the webpage, solve the Lotka-Volterra system:\n\nDefine lotka_volterra(z, *params)\nSet initial populations: prey=10, predators=7\nSet parameters: Œ±=1.0, Œ≤=0.1, Œ≥=0.1, Œ¥=0.075\nPlot both populations vs.¬†time\n\n‚è±Ô∏è Time: 10 minutes\nüí° Bonus: Try plotting predators vs.¬†prey to see the limit cycle!"
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#what-about-scipy.odeint",
    "href": "lectures/lecture15/ode-review-slides.html#what-about-scipy.odeint",
    "title": "Solving Differential Equations",
    "section": "What About scipy.odeint?",
    "text": "What About scipy.odeint?\nYou‚Äôve used scipy.integrate.odeint in previous lectures:\nfrom scipy.integrate import odeint\n\nsolution = odeint(derivative_function, initial_condition, t_array)\n\nWhat does it do internally?\n\nUses Runge-Kutta methods (like improved Euler, but even better)\nAutomatically adjusts step size for accuracy\nHandles stiff equations\n\n\n\nNow you understand the principle! odeint is just a fancier version of what you built today."
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#comparison-of-methods",
    "href": "lectures/lecture15/ode-review-slides.html#comparison-of-methods",
    "title": "Solving Differential Equations",
    "section": "Comparison of Methods",
    "text": "Comparison of Methods\n\n\n\nMethod\nError\nEvaluations/step\nUse case\n\n\n\n\nEuler\n\\(O(\\Delta t)\\)\n1\nLearning, simple problems\n\n\nImproved Euler\n\\(O(\\Delta t^2)\\)\n2\nBetter accuracy\n\n\nRunge-Kutta 4\n\\(O(\\Delta t^4)\\)\n4\nStandard choice\n\n\nscipy.odeint\nAdaptive\nVaries\nProduction code\n\n\n\n\nRule of thumb: Use scipy for real work, but understand Euler to debug!"
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#common-mistakes-to-avoid",
    "href": "lectures/lecture15/ode-review-slides.html#common-mistakes-to-avoid",
    "title": "Solving Differential Equations",
    "section": "Common Mistakes to Avoid",
    "text": "Common Mistakes to Avoid\n\n\n\n\n\n\n\n\nMistake\nSymptom\nFix\n\n\n\n\n\\(\\Delta t\\) too large\nSolution explodes or oscillates\nReduce step size\n\n\nWrong initial conditions\nWrong starting point\nCheck units and values\n\n\nDerivative sign error\nSolution grows instead of decays\nCheck your physics!\n\n\nForgetting to update\nSolution stays constant\ny[j] = ... not y = ..."
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#debugging-strategy-for-odes",
    "href": "lectures/lecture15/ode-review-slides.html#debugging-strategy-for-odes",
    "title": "Solving Differential Equations",
    "section": "Debugging Strategy for ODEs",
    "text": "Debugging Strategy for ODEs\n\nTest with known solution ‚Äî Compare to analytical result\nCheck energy conservation ‚Äî Does total energy drift?\nVary \\(\\Delta t\\) ‚Äî Solution should converge as \\(\\Delta t \\to 0\\)\nPlot intermediate steps ‚Äî See where things go wrong\n\n\n# Energy check for harmonic oscillator\nE = 0.5 * v**2 + 0.5 * omega**2 * y**2\nplt.plot(t, E)  # Should be constant!"
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#summary-the-recipe-for-solving-odes",
    "href": "lectures/lecture15/ode-review-slides.html#summary-the-recipe-for-solving-odes",
    "title": "Solving Differential Equations",
    "section": "Summary: The Recipe for Solving ODEs",
    "text": "Summary: The Recipe for Solving ODEs\n\nWrite the ODE ‚Äî What‚Äôs \\(\\frac{dy}{dt} = f(y, t)\\)?\nDefine the derivative function ‚Äî def f(y, params): return ...\nSet initial conditions ‚Äî What‚Äôs \\(y(0)\\)?\nChoose time array and step size ‚Äî t = np.arange(0, T, dt)\nLoop and update ‚Äî y[j] = y[j-1] + dt * f(y[j-1])\nPlot and verify ‚Äî Does it make physical sense?"
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#what-we-learned-today",
    "href": "lectures/lecture15/ode-review-slides.html#what-we-learned-today",
    "title": "Solving Differential Equations",
    "section": "What We Learned Today",
    "text": "What We Learned Today\n\n\n\n\n\n\n\nConcept\nKey Point\n\n\n\n\nTaylor series\nFoundation of numerical methods\n\n\nEuler method\nSimplest ODE solver: \\(y_{n+1} = y_n + \\Delta t \\cdot f'\\)\n\n\nImproved Euler\nBetter accuracy with slope averaging\n\n\nState vectors\nHandle 2nd order & coupled ODEs\n\n\nscipy.odeint\nProduction-\n\n\nready implementation\n\n\n\n\n\nKey insight: All numerical ODE solvers approximate the future using the present and the derivative!"
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#next-steps",
    "href": "lectures/lecture15/ode-review-slides.html#next-steps",
    "title": "Solving Differential Equations",
    "section": "Next Steps",
    "text": "Next Steps\nNow that you understand ODEs numerically, you‚Äôre ready for:\n\nDiffusion equation ‚Äî PDEs (partial differential equations)\nQuantum mechanics ‚Äî Schr√∂dinger equation\nChaos ‚Äî Sensitive dependence on initial conditions\nAdvanced methods ‚Äî Runge-Kutta, adaptive stepping"
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#practice-resources",
    "href": "lectures/lecture15/ode-review-slides.html#practice-resources",
    "title": "Solving Differential Equations",
    "section": "Practice Resources",
    "text": "Practice Resources\nüì± Webpage exercises: Complete all 4 exercises in lecture15a.qmd\nüìö Additional practice:\n\nModify decay constant \\(k\\) and observe changes\nTry different initial conditions for oscillator\nExplore what happens with very large \\(\\Delta t\\)\n\n\nRemember: The best way to understand numerical methods is to implement them yourself!"
  },
  {
    "objectID": "lectures/lecture15/ode-review-slides.html#questions",
    "href": "lectures/lecture15/ode-review-slides.html#questions",
    "title": "Solving Differential Equations",
    "section": "Questions?",
    "text": "Questions?\nThe exercises are available online for practice at home.\n\nKey equations to remember:\nEuler: \\(y_{n+1} = y_n + \\Delta t \\cdot f(y_n)\\)\nImproved Euler: \\(y_{n+1} = y_n + \\frac{\\Delta t}{2}(f(y_n) + f(y_{pred}))\\)\n\n\nGood luck with the exercises! üöÄ"
  },
  {
    "objectID": "lectures/lecture13/spherical-waves.html",
    "href": "lectures/lecture13/spherical-waves.html",
    "title": "Spherical waves",
    "section": "",
    "text": "After we have had a look at plane waves, we can explore a second solution of the homogeneous wave equation - Spherical Waves. Spherical waves are elementary waves that are for example considered in Huygens principle. So if we develop some code to visualize spherical waves, we may also verify Huygens principle later."
  },
  {
    "objectID": "lectures/lecture13/spherical-waves.html#equations",
    "href": "lectures/lecture13/spherical-waves.html#equations",
    "title": "Spherical waves",
    "section": "Equations",
    "text": "Equations\nA spherical wave is as well described by two exponentials containing the spatial and temporal dependence of the wave. The only difference is, that the wavefronts shall describe spheres instead of planes. We therefore need \\(|\\vec{k}||\\vec{r}|=k r=const\\). The product of the magntitudes of the wavevector and the distance from the source are constant. If we further generalize the position of the source to \\(\\vec{r}_{0}\\) we can write a spherical wave by\n\\[\\begin{equation}\nE=\\frac{E_{0}}{|\\vec{r}-\\vec{r}_{0}|}e^{i k|\\vec{r}-\\vec{r}_{0}|} e^{-i\\omega t}\n\\end{equation}\\]\nNote that we have to introduce an additional scaling of the amplitude with the inverse distance of the source. This is due to energy conservation, as we require that all the energy that flows through all spheres around the source is constant."
  },
  {
    "objectID": "lectures/lecture13/spherical-waves.html#electric-field",
    "href": "lectures/lecture13/spherical-waves.html#electric-field",
    "title": "Spherical waves",
    "section": "Electric field",
    "text": "Electric field\nLets have a look at the electric field of the spherical wave. Below is some code plotting the electric field is space. The source is at the origin and the plot nicely shows, that the amplitude decays with the distance.\n\n\n\n\n\n\nThe line plots below show that the field amplitude rapidly decays and the intensity follows a \\(1/r^2\\) law as expected. The slight deiviation at small distances is an artifact from our discretization. We used the image above to extract the line plot and therefore never exactly hit \\(r=0\\)."
  },
  {
    "objectID": "lectures/lecture13/spherical-waves.html#animation",
    "href": "lectures/lecture13/spherical-waves.html#animation",
    "title": "Spherical waves",
    "section": "Animation",
    "text": "Animation\nWe can also visualize the animation our spherical wave to check for the direction of the wave propagation.\nnorm = mpl.colors.Normalize(vmin=-5e6, vmax=5e6)\ncmap = cm.seismic\nm = cm.ScalarMappable(norm=norm, cmap=cmap)\ncanvas = Canvas(width=300, height=300,sync_image_data=True)\ndisplay(canvas)\ndef animate(k,time):\n    for t in time:\n        field=spherical_wave(k,omega0,r,r0,t)\n        data=np.zeros([300,300,3])\n        tmp=np.real(field.transpose())\n        c=m.to_rgba(tmp)\n        with hold_canvas(canvas):\n            canvas.put_image_data(c[:,:,:3]*255,0,0)\n        sleep(0.02)\ntime= np.linspace(0,1e-14,200)\nanimate(k,time)"
  },
  {
    "objectID": "lectures/lecture13/spherical-waves.html#plot-the-intensity-in-an-image-plane",
    "href": "lectures/lecture13/spherical-waves.html#plot-the-intensity-in-an-image-plane",
    "title": "Spherical waves",
    "section": "Plot the intensity in an image plane",
    "text": "Plot the intensity in an image plane\nAs we have now the electric field in space, wqe may also chose an arbitrary plane in space to record the intensity of that wave in space. Here we want to know the intensity in a plane at 10 ¬µm distance from the source, which is again at the origin. The intensity cross section at the screen is a Lorentzian function."
  },
  {
    "objectID": "lectures/lecture13/spherical-waves.html#interference-between-a-spherical-and-a-plane-wave",
    "href": "lectures/lecture13/spherical-waves.html#interference-between-a-spherical-and-a-plane-wave",
    "title": "Spherical waves",
    "section": "Interference between a spherical and a plane wave",
    "text": "Interference between a spherical and a plane wave\nIn the section on plane waves, we had a look at the interference pattern of plane waves in space. We now have a look at the interference of a plane wave and a spherical wave. The plane wave thereby probes the distortion of the spherical wavefronts and the interference pattern stores this information on the shape of the spherical wavefronts. This is exactly what is done in holography. Taking this interference pattern as a ‚Äúdiffraction grating‚Äù will allow you to restore information on the spherical wavefonts."
  },
  {
    "objectID": "lectures/lecture13/plane-waves.html",
    "href": "lectures/lecture13/plane-waves.html",
    "title": "Electromagnetic Waves",
    "section": "",
    "text": "In the previous parts we have dealt with mechanics essentially. Even if we have described Brownian motion, this has been done by a particular type of Newtons equation of motion, it is much like mechanics. Now we would like to have a look at some examples from electromagnetic waves. We will not solve the wave equation but look at some solution using the complex notion of the electric field. This shall train our use of complex numbers. The special solutions are the plane wave and the spherical wave and we will be able to simulate a number of things especially with the spherical waves as they are part of Huuygens principle."
  },
  {
    "objectID": "lectures/lecture13/plane-waves.html#plane-waves",
    "href": "lectures/lecture13/plane-waves.html#plane-waves",
    "title": "Electromagnetic Waves",
    "section": "Plane waves",
    "text": "Plane waves\nWe will start with plane waves. Plane waves are solutions of the homogeneous wave equation and are the simplest solutions of the wave equation. They are also the basis for the description of more complicated waves. We will have a look at the electric field of a plane wave and its propagation in space and time.\n\nEquations\nA plane wave is a solution of the homogeneous wave equation and is given in its complex form by\n\\[\\begin{equation}\nE=E_{0}e^{i\\vec{k}\\cdot \\vec{r}}e^{-i\\omega t}\n\\end{equation}\\]\nwhere the two exponentials contain a spatial and a temporal phase. \\(E_{0}\\) denotes the amplitude of the plane wave. The plane is defined by the shape of the wavefront which is given by \\(\\vec{k}\\cdot \\vec{r}=const\\), which is just the definition of a plane perpendicular to \\(\\vec{k}\\).\nA wave is a physical quantity which oscillates in space and time. Its energy current density is related to the square magnitude of the amplitude. We will include in the following the spatial and the temporal phase. For plotting just the spatial variation of the electric field, you may just use the spatial part of the equation\n\\[\\begin{equation}\nE=E_{0}e^{i\\vec{k}\\cdot \\vec{r}}\n\\end{equation}\\]\nBut since we also want to see the wave propagate, we will directly include also the temporal dependence in our function. In all of the examples below we set the amplitude of the wave \\(E_{0}=1\\).\nThe propagation of the wave is defined by wavevector \\(\\vec{k}\\). In vacuum, the wavevector is just real valued\n\\[\\begin{equation}\n\\vec{k}_{0}=\n\\begin{pmatrix}\nk_{0x} \\\\\nk_{0y}\\\\\nk_{0z}\\\\\n\\end{pmatrix}\n\\end{equation}\\]\nThe wavevector is providing the direction in which the wavefronts propagate. It is also proportional to the momentum of the wave, which will be important if we consider the refraction process a bit later. The magnitude of the wavevector is related to the wavelength \\(\\lambda\\).\n\\[\\begin{equation}\nk_{0}=\\frac{2\\pi}{\\lambda_{0}}=\\frac{\\omega}{c_{0}}\n\\end{equation}\\]\nAt the same time, its magnitude is also given by the angular frequency divided by the speed of light. The latter is called a dispersion relation.\nIn a medium, the wavevector is by a factor of \\(n\\) longer, where n is the refractive index. Since the refractive index may be a complex number, e.g.¬†\\(n=\\eta+i\\kappa\\), the wavevector can be complex as well. It is then given by\n\\[\\begin{equation}\n\\vec{k}=n\\vec{k}_{0}=\n\\begin{pmatrix}\nk_{x}^{\\prime}+ik_{x}^{\\prime\\prime} \\\\\nk_{y}^{\\prime}+ik_{y}^{\\prime\\prime} \\\\\nk_{z}^{\\prime}+ik_{z}^{\\prime\\prime} \\\\\n\\end{pmatrix}\n\\end{equation}\\]\nThe complex refractive index means that there is some damping of the electromagnetic wave due to absorption, for example.\nThe wavelength is then related to\n\\[\\begin{equation}\n\\Re(k)=\\eta \\frac{2\\pi}{\\lambda_{0}}\n\\end{equation}\\]\nand the imaginary part gives the damping\n\\[\\begin{equation}\n\\Im(k)=\\kappa \\frac{2\\pi}{\\lambda_{0}}\n\\end{equation}\\]\n\n\nElectric field\n\n\n\n\n\n\nLets have a look at waves and wave propagation. We want to create a wave, which has a wavelength of 532 nm in vacuum.\n\n\n\n\n\n\nIt shall propagate along the z-direction and we wull have a look at the x-z plane.\n\n\n\n\n\n\nWe can plot the electric field in the x-z plane by defining a grid of points (x,z). This is done by the meshgrid function of numpy. The meshgrid returns a 2-dimensional array for each coordinate. Have a look at the values in the meshgrid.\n\n\n\n\n\n\nIn the last lines, we defined an array of X,0,Z, where X and Z are already 2-dimensional array. This finally gives an array 3D vectors, which we can use to calculate the electric field at any point in space. If we want to plot the electric field, we have to calculate the real part of the complex values, as the electric field is a physical quantity, which is always real. There is not much to see for a plane wave in the intensity plot, as the intensity of a plane wave is constant in space. Yet, if you want to plot it, you have to calculate the magnitude square of the electric field, e.g.\n\\[\\begin{equation}\nI\\propto |E|^{2}\n\\end{equation}\\]\n\n\n\n\n\n\n\n\nPlane wave propagation\nThe above graph shows a static snapshot of the plane wave at a time \\(t=0\\). We know, however, that a plane wave is propagating in space and time. Since we know how to animate things, we may do that using the ipycanvas module.\n\nx=np.linspace(-2.5e-6,2.5e-6,300)\nz=np.linspace(0,5e-6,300)\n\nX,Z=np.meshgrid(x,z)\nr=np.array([X,0,Z],dtype=object)\ncanvas = Canvas(width=300, height=300,sync_image_data=False)\ndisplay(canvas)\nTo do the animation I use a little trick to get the same color map as in the matplotlib plotting. The function below uses the matplotlib color map seismic and the corresponding mapping of values with a given minimum vmin and maximum vmax value. The mapping is done in the animation function with c=m.to_rgba(tmp).\n\n#normalize the color map to a certain value range\nnorm = mpl.colors.Normalize(vmin=-1, vmax=1)\n\ncmap = cm.seismic\n\n# do the mapping of values to color values.\nm = cm.ScalarMappable(norm=norm, cmap=cmap)\nThis is our animation function, where I provide time and the wavevector as arguments, such that we may change both parameters easily.\ndef animate(k,time):\n    for t in time:\n        field=plane_wave(k,omega0,r,t)\n        tmp=np.real(field.transpose())\n        c=m.to_rgba(tmp)\n        with hold_canvas(canvas):\n            canvas.put_image_data(c[:,:,:3]*255,0,0)\n            #canvas.put_image_data(data*255,0,0)\n        sleep(0.02)\nWith the call below, you may animate the wave now with different refractive indices.\n\neta=1.\nkappa=0.0\nn=eta+kappa*1j\n\nk=n*k0*vec\ntime= np.linspace(0,5e-14,500)\nanimate(k,time)\n\n\nImaginary wave vector\nIf we now create a material, which has an imaginary part of the refractive index, we see that the amplitude decays and the wave fades.\n\n\n\n\n\n\nThe above plots show the electric field amplitude in the x-z plane. We may also have a look the field amplitude and intensity as a function of the z-position by chosing a single x-value. In the plot below, you may notice two things. The first is, that the wave decays exponentially with distance \\(z\\). Intensity and field decay with different decay length. The field decays with \\(\\exp(-\\kappa*k_{0}z)\\) while the intensity of cause decays twice as fast \\(\\exp(-2\\kappa*k_{0}z)\\) due to the fact the the intensity is the square of the electric field.\n\n\n\n\n\n\n\n\nAnimation\nOf course, we should not miss the animation.\ndisplay(canvas)\nk=n*k0*vec\ntime= np.linspace(0,5e-14,500)\nanimate(k,time)\n\n\nInterference of two plane waves\nIt is not very difficult to calculate from the definitions we did above now the interference of two plane waves, which have different directions of the wavevector. The total field in space is then just the sum of the two fields\n\\[\\begin{equation}\n\\vec{E}=\\vec{E}_{1}+\\vec{E}_{2}\n\\end{equation}\\]\nThe interesting thing is now to look at the intensity which\n\\[\\begin{equation}\nI\\propto |\\vec{E}|^2=|\\vec{E}_{1}|^2+|\\vec{E}_{2}|^2 + \\vec{E}_{1}^{*} \\vec{E}_{2}+\\vec{E}_{2}^{*}\\vec{E}_{1}\n\\end{equation}\\]\n\n\n\n\n\n\nWhile the field pattern still looks complicated, the intensity pattern is just a set of bright lines.\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced Topics: Plane wave at a boundary\n\n\n\n\n\nWe want to go a bit further now and have a look at the wave at a boundary between vaccum and glass for example. At this boundary, the electromagnetic wave is reflected and refracted such that two new wavevectors arise. These are easily calculated by the law of reflection and the law of refraction. Besides that, also the amplitude of the waves change. To calculate the field we need the so-called Fresnel equations.\n\nFresnel equations\nWhen electromagnetic waves hit a boundary, they will be reflected and refracted. The amplitude of the reflected and refracted wave is determined by the refractive index of the two materials, the angles and the polarizations. For the latter we differentiate between a polarization in the incident plane (the p-polarization) and perpendicular to the incident plane (s-polarization).\n\n\n\nFresnel\n\n\nFor each of the polarization we in general obtain a coeffcient for the reflection and one for the refraction. To make our calculation a bit simpler, we will assume only s-polarization. Then the two Fresnel coefficients are calculated as\n\\[\\begin{equation}\n\\left( \\frac{E_{0t}}{E_{0e}} \\right)_s = t_s =\\frac{2n_1 \\cos{\\alpha}}{n_1\\cos{\\alpha}+n_2\\cos{\\beta}}\n\\end{equation}\\]\n\\[\\begin{equation}\n\\left( \\frac{E_{0r}}{E_{0e}} \\right)_s = r_s =\\frac{n_1\\cos{\\alpha}-n_2\\cos{\\beta}}{n_1\\cos{\\alpha}+n_2\\cos{\\beta}}\n\\end{equation}\\]\nwhere \\(\\alpha\\) and \\(\\beta\\) are the incident and refraction angles, respectively. Note that the Fresnel coefficients are for the amplitudes and can be negative to account for a phase jump by \\(\\pi\\). To obtain the coefficients for the intensities, one has to square the Fresnel coefficients.\nTo bring everything correctly together, we therefore have to define a number of things. We will need a function calculating the outgoing angle from Snells law. And we need at least two functions calculating the reflection and transmission coefficient for one polarization. We use the s-polarization, where the electric field is always parallel to the interface.\n\n\n\n\n\n\nWith the definition of the Fresnel coefficients, we may now plot the reflection and the transmission coefficients. Note that the sum of reflection and transmission coefficients for the intensities have to add up to one if there is no absorption.\n\n\n\n\n\n\n\n\nIncident wave\nWe want to study the electric fields and the intensities at various angles. The most interesting one, is a case where we have total internal reflection. This happens, if light is propagating from the higher refractive index to a lower refractive index. If we start in glass (\\(n_1=1.5\\)) and transmit to vacuum \\(n_2=1\\), then at all angles above \\(\\theta_{c}=\\sin^{-1}(n_2/n_1)=41.810314895778596\\) are total internally reflected.\n\n\n\n\n\n\nWe may now specify or calculate the corresponding wavevectors for an incident angle of \\(45^{\\circ}\\). In general all waves (reflect, refracted) have to match with their phase at the boundary. If the boundary is along the x-direction, we therefore have\n\\[\\begin{equation}\nk_{x,in}=k_{x,r}=k_{x,t}\n\\end{equation}\\]\nThis fixes one component of all wavevectors in the plane. What is then missing, is the z-component of the wavevectors. The incident wavevector is providing \\(k_{z,in}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReflected wave\nFor the reflected wave the z-component of the wavevector is just flipped in sign, e.g.¬†\\(k_{z,r}=-k_{z,in}\\).\n\n\n\n\n\n\n\n\nRefracted wave\nThe magnitude of the z-component of the transmitted wave can be obtained from the conservation of momentum. The momentum of the wave is proportional to the magnitude of the wavevector on both sides.\n\\[\\begin{equation}\nk_{1}^2=k_{2}^{2}\n\\end{equation}\\]\nwhich is, due to \\(k=nk_{0}\\) the same as\n\\[\\begin{equation}\nn_{1}^2(k_{0x,in}^2+k_{0z,in}^{2})=n_2^2 (k_{0x,t}^{2}+k_{0z,t}^2)\n\\end{equation}\\]\nfrom which we get\n\\[\\begin{equation}\nk_{0z,t}=\\pm \\frac{1}{n_{1}}\\sqrt{n_2^2 k_{0z,in}^2 -(n_{1}^2-n_{2}^2)k_{0x}^{2}}\n\\end{equation}\\]\nIf we go from a medium with high refrective index to a lower one, the second term in the root may surpass the first one and the whole solution will become imaginary. The wave in the lower refractive index medium \\(n_{2}\\) is then evanescent.\n\n\n\n\n\n\nThe total field thus containes three components. In medium 1, the field consists of the incident and the reflected wave. In medium 2, we just have the transmitted wave, with a possible evanescent solution.\n\n\n\n\n\n\nThe plots below show the electric field on the left side and the intensity on the right side. Interestingly, the intensity is that of a standing wave in medium 1, while it is just decaying in medium 2. Note that the electric field is oscillating along the interface in medium 2 but not at all in z-direction. This means that there is no energy transport along the z-direction anymore.\n\n\n\n\n\n\nWe will also have a look ath the propagation of the wave yb defining our animation.\ncanvas = Canvas(width=500, height=500,sync_image_data=True)\ndisplay(canvas)\n\ndef animate(k,time):\n    for t in time:\n        field=np.zeros([500,500],dtype=complex)\n        field1=plane_wave(k1,omega0,r1,t)\n        field2=plane_wave(k2,omega0,r1,t)\n        field3=plane_wave(k3,omega0,r2,t)\n\n        beta=snell(n1,n2,alpha)\n        r=rs(n1,n2,alpha,beta)\n        t=ts(n1,n2,alpha,beta)\n\n        field[0:250,:]=field1+r*field2\n        field[250:,:]=t*field3\n        tmp=np.real(field.transpose())\n\n        c=m.to_rgba(tmp)\n        with hold_canvas(canvas):\n            canvas.put_image_data(c[:,:,:3]*255,0,0)\n        sleep(0.02)\n\ntime= np.linspace(0,1e-14,100)\nanimate(k,time)\nAs it is apparent from our simulation, the wave is longitudinal in medium 2 at this angle. Try to modify the incident angles yourself to see if the wave becomes propagating in medium 2.\nIn the last plot, we will have a look at the intensity in medium 1 and medium 2. What is nicely visible, is that the intensity decays in medium 2 with increasing distance. As compared to the absorbing case, there is not oscillation of the field in the z-direction, hence no energy transfer. Convince yourself that this is indeed an exponential decay by using the appropriate semilog plot."
  },
  {
    "objectID": "lectures/PEDAGOGICAL_NOTES.html",
    "href": "lectures/PEDAGOGICAL_NOTES.html",
    "title": "Pedagogical Notes: Brownian Motion Restructuring",
    "section": "",
    "text": "This document explains the rationale for splitting Brownian motion into two separate modules taught at different points in the course.\n\n\n\n\n\nBy Week 2-3, students have barely mastered: - Basic Python syntax (variables, data types) - Simple functions - Basic plotting\nIntroducing object-oriented programming at this stage adds: - The concept of self - Constructors (__init__) - Instance vs.¬†class variables - Methods vs.¬†functions - The entire OOP paradigm\nThis creates cognitive overload when students should be focusing on: - Understanding the physics of random walks - Getting comfortable with numpy - Building confidence with loops and arrays\n\n\n\nThe original 2_brownian_motion.qmd explicitly states: &gt; ‚ÄúWe will apply our newly acquired knowledge about classes‚Ä¶‚Äù\nThis confirms it was designed as an application of OOP, not an introduction to programming fundamentals.\n\n\n\n\n\n\nFile: lectures/lecture02/4_brownian_motion_simple.qmd\nFocus: - ‚úÖ Physics first: What is Brownian motion? Why does it happen? - ‚úÖ Simple functions with clear inputs/outputs - ‚úÖ Using lists and numpy arrays - ‚úÖ Basic visualization - ‚úÖ Statistical analysis (MSD)\nLearning Outcomes: - Students understand the physics - Students can simulate random walks - Students gain confidence with functions and arrays - Students see immediate, satisfying results\nKey Advantage: Students can focus on one new concept at a time: - The physics of diffusion - Random number generation - Trajectory storage in arrays\n\n\n\nFile: lectures/lecture05/2_brownian_motion.qmd\nFocus: - ‚úÖ Revisiting familiar physics with new tools - ‚úÖ Showing why classes help organize code - ‚úÖ Managing multiple particles with different properties - ‚úÖ Encapsulating state and behavior\nLearning Outcomes: - Students see the value of OOP through direct comparison - Students understand when to use classes - Students can design and implement their own classes - Students appreciate code organization in complex simulations\nKey Advantage: Students see the need for better organization because: - They‚Äôve already written the ‚Äúmessy‚Äù version with functions - They understand the physics, so can focus on the programming structure - They can compare the two approaches directly\n\n\n\n\n\n\nStudents encounter Brownian motion twice: - First pass: Understand the physics, basic implementation - Second pass: Better implementation, deeper understanding\n\n\n\n\nWeek 2-3: Concrete ‚Äúhere‚Äôs how to simulate random steps‚Äù\nWeek 6-7: Abstract ‚Äúhere‚Äôs how to organize complex simulations‚Äù\n\n\n\n\n\nTeach OOP when students see why they need it\nAfter managing messy lists of particles, classes make sense\n\n\n\n\n\nWeek 2-3: ‚ÄúI can simulate physics!‚Äù\nWeek 6-7: ‚ÄúI can write professional code!‚Äù\n\n\n\n\n\n\n\nEmphasize: - ‚ÄúFocus on the physics, not the programming tricks‚Äù - ‚ÄúThis is a ‚Äòquick and dirty‚Äô implementation - we‚Äôll improve it later‚Äù - Celebrate when students get their first random walk working\nCommon Student Questions: - Q: ‚ÄúWhy are we using lists instead of [something fancier]?‚Äù - A: ‚ÄúWe‚Äôre keeping it simple so we can focus on understanding the physics. We‚Äôll optimize later.‚Äù\nExtension Activities: - Compare different diffusion coefficients - Add boundaries (particles bounce off walls) - Try 3D motion\n\n\n\nEmphasize: - ‚ÄúRemember our Brownian motion simulation? Let‚Äôs make it better!‚Äù - Compare the two implementations side-by-side - Show how classes reduce bugs (each particle manages its own state)\nCommon Student Questions: - Q: ‚ÄúIs the OOP version really better?‚Äù - A: ‚ÄúFor this small example, maybe not. But imagine 1000 particles with different sizes, charges, and interactions. Then classes become essential.‚Äù\nExtension Activities: - Add particle-particle interactions - Create different particle types (subclasses) - Implement particle tracking/analysis methods\n\n\n\n\n\n\nTest understanding of: - What causes Brownian motion (physics) - How to generate random steps - How to store and plot trajectories - What MSD tells us\nAvoid asking about: Classes, OOP, complex data structures\n\n\n\nTest understanding of: - When to use classes vs.¬†functions - How to design a class - Instance vs.¬†class variables - How OOP improves code organization\nCan reference: The Week 2-3 Brownian motion as a comparison point\n\n\n\n\nYou‚Äôll know this restructuring works when:\n‚úÖ Week 2-3 students are excited about simulating motion (not confused by self)\n‚úÖ Week 6-7 students say ‚Äúoh, this makes the code cleaner!‚Äù (not ‚Äúwhy are we doing this?‚Äù)\n‚úÖ Fewer questions about syntax, more about physics and design\n‚úÖ Students can extend their simulations independently\n\n\n\nThis two-stage approach could be applied to other topics: - Planetary motion: Simple version (Week 4-5), OOP version with multiple planets (Week 8-9) - Wave simulations: Function-based first, class-based for complex systems - Data analysis: Simple functions, then analysis classes\nThe pattern: Physics first, organization later works well for computational physics courses where students are learning both domains simultaneously.\n\nLast updated: 2024 For questions about this restructuring, see the commit history or course coordinator."
  },
  {
    "objectID": "lectures/PEDAGOGICAL_NOTES.html#overview",
    "href": "lectures/PEDAGOGICAL_NOTES.html#overview",
    "title": "Pedagogical Notes: Brownian Motion Restructuring",
    "section": "",
    "text": "This document explains the rationale for splitting Brownian motion into two separate modules taught at different points in the course."
  },
  {
    "objectID": "lectures/PEDAGOGICAL_NOTES.html#the-problem-with-teaching-classes-in-week-2-3",
    "href": "lectures/PEDAGOGICAL_NOTES.html#the-problem-with-teaching-classes-in-week-2-3",
    "title": "Pedagogical Notes: Brownian Motion Restructuring",
    "section": "",
    "text": "By Week 2-3, students have barely mastered: - Basic Python syntax (variables, data types) - Simple functions - Basic plotting\nIntroducing object-oriented programming at this stage adds: - The concept of self - Constructors (__init__) - Instance vs.¬†class variables - Methods vs.¬†functions - The entire OOP paradigm\nThis creates cognitive overload when students should be focusing on: - Understanding the physics of random walks - Getting comfortable with numpy - Building confidence with loops and arrays\n\n\n\nThe original 2_brownian_motion.qmd explicitly states: &gt; ‚ÄúWe will apply our newly acquired knowledge about classes‚Ä¶‚Äù\nThis confirms it was designed as an application of OOP, not an introduction to programming fundamentals."
  },
  {
    "objectID": "lectures/PEDAGOGICAL_NOTES.html#the-two-stage-solution",
    "href": "lectures/PEDAGOGICAL_NOTES.html#the-two-stage-solution",
    "title": "Pedagogical Notes: Brownian Motion Restructuring",
    "section": "",
    "text": "File: lectures/lecture02/4_brownian_motion_simple.qmd\nFocus: - ‚úÖ Physics first: What is Brownian motion? Why does it happen? - ‚úÖ Simple functions with clear inputs/outputs - ‚úÖ Using lists and numpy arrays - ‚úÖ Basic visualization - ‚úÖ Statistical analysis (MSD)\nLearning Outcomes: - Students understand the physics - Students can simulate random walks - Students gain confidence with functions and arrays - Students see immediate, satisfying results\nKey Advantage: Students can focus on one new concept at a time: - The physics of diffusion - Random number generation - Trajectory storage in arrays\n\n\n\nFile: lectures/lecture05/2_brownian_motion.qmd\nFocus: - ‚úÖ Revisiting familiar physics with new tools - ‚úÖ Showing why classes help organize code - ‚úÖ Managing multiple particles with different properties - ‚úÖ Encapsulating state and behavior\nLearning Outcomes: - Students see the value of OOP through direct comparison - Students understand when to use classes - Students can design and implement their own classes - Students appreciate code organization in complex simulations\nKey Advantage: Students see the need for better organization because: - They‚Äôve already written the ‚Äúmessy‚Äù version with functions - They understand the physics, so can focus on the programming structure - They can compare the two approaches directly"
  },
  {
    "objectID": "lectures/PEDAGOGICAL_NOTES.html#pedagogical-principles-applied",
    "href": "lectures/PEDAGOGICAL_NOTES.html#pedagogical-principles-applied",
    "title": "Pedagogical Notes: Brownian Motion Restructuring",
    "section": "",
    "text": "Students encounter Brownian motion twice: - First pass: Understand the physics, basic implementation - Second pass: Better implementation, deeper understanding\n\n\n\n\nWeek 2-3: Concrete ‚Äúhere‚Äôs how to simulate random steps‚Äù\nWeek 6-7: Abstract ‚Äúhere‚Äôs how to organize complex simulations‚Äù\n\n\n\n\n\nTeach OOP when students see why they need it\nAfter managing messy lists of particles, classes make sense\n\n\n\n\n\nWeek 2-3: ‚ÄúI can simulate physics!‚Äù\nWeek 6-7: ‚ÄúI can write professional code!‚Äù"
  },
  {
    "objectID": "lectures/PEDAGOGICAL_NOTES.html#teaching-tips",
    "href": "lectures/PEDAGOGICAL_NOTES.html#teaching-tips",
    "title": "Pedagogical Notes: Brownian Motion Restructuring",
    "section": "",
    "text": "Emphasize: - ‚ÄúFocus on the physics, not the programming tricks‚Äù - ‚ÄúThis is a ‚Äòquick and dirty‚Äô implementation - we‚Äôll improve it later‚Äù - Celebrate when students get their first random walk working\nCommon Student Questions: - Q: ‚ÄúWhy are we using lists instead of [something fancier]?‚Äù - A: ‚ÄúWe‚Äôre keeping it simple so we can focus on understanding the physics. We‚Äôll optimize later.‚Äù\nExtension Activities: - Compare different diffusion coefficients - Add boundaries (particles bounce off walls) - Try 3D motion\n\n\n\nEmphasize: - ‚ÄúRemember our Brownian motion simulation? Let‚Äôs make it better!‚Äù - Compare the two implementations side-by-side - Show how classes reduce bugs (each particle manages its own state)\nCommon Student Questions: - Q: ‚ÄúIs the OOP version really better?‚Äù - A: ‚ÄúFor this small example, maybe not. But imagine 1000 particles with different sizes, charges, and interactions. Then classes become essential.‚Äù\nExtension Activities: - Add particle-particle interactions - Create different particle types (subclasses) - Implement particle tracking/analysis methods"
  },
  {
    "objectID": "lectures/PEDAGOGICAL_NOTES.html#assessment-implications",
    "href": "lectures/PEDAGOGICAL_NOTES.html#assessment-implications",
    "title": "Pedagogical Notes: Brownian Motion Restructuring",
    "section": "",
    "text": "Test understanding of: - What causes Brownian motion (physics) - How to generate random steps - How to store and plot trajectories - What MSD tells us\nAvoid asking about: Classes, OOP, complex data structures\n\n\n\nTest understanding of: - When to use classes vs.¬†functions - How to design a class - Instance vs.¬†class variables - How OOP improves code organization\nCan reference: The Week 2-3 Brownian motion as a comparison point"
  },
  {
    "objectID": "lectures/PEDAGOGICAL_NOTES.html#success-metrics",
    "href": "lectures/PEDAGOGICAL_NOTES.html#success-metrics",
    "title": "Pedagogical Notes: Brownian Motion Restructuring",
    "section": "",
    "text": "You‚Äôll know this restructuring works when:\n‚úÖ Week 2-3 students are excited about simulating motion (not confused by self)\n‚úÖ Week 6-7 students say ‚Äúoh, this makes the code cleaner!‚Äù (not ‚Äúwhy are we doing this?‚Äù)\n‚úÖ Fewer questions about syntax, more about physics and design\n‚úÖ Students can extend their simulations independently"
  },
  {
    "objectID": "lectures/PEDAGOGICAL_NOTES.html#future-considerations",
    "href": "lectures/PEDAGOGICAL_NOTES.html#future-considerations",
    "title": "Pedagogical Notes: Brownian Motion Restructuring",
    "section": "",
    "text": "This two-stage approach could be applied to other topics: - Planetary motion: Simple version (Week 4-5), OOP version with multiple planets (Week 8-9) - Wave simulations: Function-based first, class-based for complex systems - Data analysis: Simple functions, then analysis classes\nThe pattern: Physics first, organization later works well for computational physics courses where students are learning both domains simultaneously.\n\nLast updated: 2024 For questions about this restructuring, see the commit history or course coordinator."
  },
  {
    "objectID": "LECTURE_ORDER_RATIONALE.html",
    "href": "LECTURE_ORDER_RATIONALE.html",
    "title": "Week 6 Lecture Order Rationale",
    "section": "",
    "text": "After careful pedagogical consideration, Week 6 lectures are ordered as:\n\nLecture 1: Planetary Motion (easier)\nLecture 2: Spring Pendulum & Chaos (more complex)\n\n\n\n\n\n\nPlanetary Motion First: - ‚úÖ Intuitive physics (everyone knows planets orbit the Sun) - ‚úÖ Single dominant force (gravity) - ‚úÖ Stable, predictable behavior - ‚úÖ Clear analytical solutions to verify against - ‚úÖ Historical context students know (Newton, Kepler)\nSpring Pendulum Second: - üîÑ More abstract (when do you see a spring pendulum?) - üîÑ Multiple competing forces (spring + gravity + centrifugal) - üîÑ Complex energy exchange between modes - üîÑ Can become chaotic (harder to predict) - üîÑ Less familiar historical context\n\n\n\nStudents build skills progressively:\nPlanetary Motion (Week 6.1)\n‚îú‚îÄ Learn polar coordinate ODE framework\n‚îú‚îÄ Master odeint with 4-variable system\n‚îú‚îÄ Practice phase space visualization\n‚îú‚îÄ Verify against analytical solutions\n‚îî‚îÄ Build confidence with stable system\n\nSpring Pendulum (Week 6.2)\n‚îú‚îÄ Apply same framework (now familiar!)\n‚îú‚îÄ See how different force changes behavior\n‚îú‚îÄ Understand mode coupling\n‚îú‚îÄ Explore chaotic regimes\n‚îî‚îÄ Appreciate complexity from same tools\n\n\n\nStarting with Planetary Motion: - Students get ‚Äúquick win‚Äù with beautiful orbits - High success rate builds confidence - Analytical verification provides satisfaction - Creates enthusiasm for next challenge\nFollowing with Spring Pendulum: - Students already comfortable with the framework - Can focus on new physics (chaos, coupling) - Challenge feels achievable, not overwhelming - Complexity is motivating, not discouraging\n\n\n\nThe progression tells a story:\n\n‚ÄúSame Framework, Different Physics‚Äù\n\nWeek 5: Learn the tools\nLecture 1: Apply to elegant, stable system (planets)\nLecture 2: Apply to complex, chaotic system (pendulum)\nInsight: Same math, vastly different outcomes!\n\n‚ÄúFrom Order to Chaos‚Äù\n\nLecture 1: Stable elliptical orbits (order)\nLecture 2: Chaotic trajectories (disorder)\nInsight: Even deterministic systems can be unpredictable!\n\n\n\n\n\nIf we did Spring Pendulum First: - ‚ùå Students face chaos before understanding stability - ‚ùå Complex coupling before simple central force - ‚ùå Less intuitive starting point - ‚ùå Higher risk of confusion/frustration - ‚ùå Planetary motion would feel like ‚Äúgoing backwards‚Äù in difficulty\nOur Chosen Order (Planetary First): - ‚úÖ Stable before chaotic - ‚úÖ Simple before complex - ‚úÖ Intuitive before abstract - ‚úÖ Natural difficulty progression - ‚úÖ Each lecture builds on previous\n\n\n\n\n\n\nBegin with material most likely to succeed, building student confidence and engagement.\n\n\n\nIntroduce mathematical framework in simpler context before applying to complex scenarios.\n\n\n\nUse success with simpler system to motivate tackling harder problems.\n\n\n\nMove from familiar/intuitive ‚Üí unfamiliar/abstract ‚Üí complex/chaotic.\n\n\n\nCreate narrative arc: master tools ‚Üí apply elegantly ‚Üí push to limits.\n\n\n\n\n\n\n                   Confidence\n                        ‚Üë\n                        |\n                   HIGH |    ‚ï±‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n                        |   ‚ï± Spring\n                        |  ‚ï±  Pendulum\n                   MED  | ‚ï±   mastered\n                        |‚ï±\n                        |‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n                   LOW  |  Planetary\n                        |  Motion\n                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí\n                         L1    L2      Time\n\n\n\nAfter Lecture 1 (Planetary Motion): &gt; ‚ÄúWow! I just simulated planet orbits and verified Kepler‚Äôs laws! This is amazing!‚Äù\nAfter Lecture 2 (Spring Pendulum): &gt; ‚ÄúI can‚Äôt believe the same math that gave us stable orbits can create chaos! The phase space plots are wild!‚Äù\nWeek 7 Transition: &gt; ‚ÄúThese simulations are getting complex. I need better ways to organize my code‚Ä¶‚Äù\n\n\n\n\n\n\n‚ÄúThe spring pendulum is more directly connected to week 4-5 oscillation examples.‚Äù\nCounter: True, but planetary motion better demonstrates Week 5‚Äôs ODE tools. The spring pendulum‚Äôs complexity could obscure the method.\n\n\n\n‚ÄúGroup all oscillator problems together.‚Äù\nCounter: Week 6 is about applying ODE methods, not just oscillators. The contrast (stable vs.¬†chaotic) is more pedagogically valuable than thematic similarity.\n\n\n\n\n\n\n\nWorked Example Effect: Students learn better when given clear, successful examples before complex variations (Sweller, 1988)\nCognitive Load Theory: Introduce one new difficulty at a time (Chandler & Sweller, 1991)\nScaffolding: Temporary support structures enable learners to accomplish tasks beyond their current ability (Wood et al., 1976)\nFlow Theory: Optimal learning occurs when challenge slightly exceeds but doesn‚Äôt overwhelm skill level (Csikszentmihalyi, 1990)\n\n\n\n\nMost computational physics textbooks introduce: - Central force problems (gravity) before - Coupled systems and chaos\nExamples: - Giordano & Nakanishi: Planetary motion (Ch 4) ‚Üí Chaos (Ch 6) - Newman: Orbits (Ch 8) ‚Üí Nonlinear systems (Ch 10) - Landau: Two-body problem ‚Üí Many-body ‚Üí Chaos\n\n\n\n\n\n\nIn Lecture 1: - Emphasize: ‚ÄúThis sets the pattern for solving central force problems‚Äù - Preview: ‚ÄúNext time we‚Äôll see what happens with a different force‚Äù - Build confidence: ‚ÄúYou‚Äôre using the same tools that predicted Neptune!‚Äù\nIn Lecture 2: - Connect: ‚ÄúRemember how we solved planetary motion? Same approach!‚Äù - Contrast: ‚ÄúBut watch what happens when we change the force law‚Ä¶‚Äù - Celebrate complexity: ‚ÄúYou‚Äôre now exploring cutting-edge physics (chaos theory)!‚Äù\n\n\n\nSuggested Study Path: 1. Master planetary motion (Lecture 1) completely 2. Ensure you understand phase space plots 3. Then tackle spring pendulum (Lecture 2) 4. Compare and contrast the two systems 5. Reflect on how force laws affect behavior\n\n\n\nCommon Issues by Lecture:\nLecture 1 (Planetary Motion): - Confusion about units (explain AU-year system carefully) - Eccentricity calculations (provide formula sheet) - Numerical accuracy (explain tolerance settings)\nLecture 2 (Spring Pendulum): - Identifying chaos (teach visual signatures) - Parameter exploration (guide systematic variation) - Energy calculation (spring vs.¬†gravity terms)\n\n\n\n\nThe lecture order (Planetary Motion ‚Üí Spring Pendulum) is pedagogically optimal because it:\n\n‚úÖ Builds from simple to complex\n‚úÖ Progresses from intuitive to abstract\n‚úÖ Moves from stable to chaotic\n‚úÖ Scaffolds mathematical framework\n‚úÖ Maximizes student success probability\n‚úÖ Creates natural narrative arc\n‚úÖ Motivates Week 7 code organization\n\nThis order respects how students actually learn: confidence through early success, skills through practice, deep understanding through progressive challenge.\n\nLast Updated: Week 6 Restructuring Approved By: Course Design Team Review Date: Before Week 6 delivery\n\n\n\n\nChandler, P., & Sweller, J. (1991). Cognitive load theory and the format of instruction. Cognition and Instruction, 8(4), 293-332.\nCsikszentmihalyi, M. (1990). Flow: The psychology of optimal experience. Harper & Row.\nSweller, J. (1988). Cognitive load during problem solving. Cognitive Science, 12(2), 257-285.\nWood, D., Bruner, J. S., & Ross, G. (1976). The role of tutoring in problem solving. Journal of Child Psychology and Psychiatry, 17(2), 89-100."
  },
  {
    "objectID": "LECTURE_ORDER_RATIONALE.html#the-decision-planetary-motion-spring-pendulum",
    "href": "LECTURE_ORDER_RATIONALE.html#the-decision-planetary-motion-spring-pendulum",
    "title": "Week 6 Lecture Order Rationale",
    "section": "",
    "text": "After careful pedagogical consideration, Week 6 lectures are ordered as:\n\nLecture 1: Planetary Motion (easier)\nLecture 2: Spring Pendulum & Chaos (more complex)"
  },
  {
    "objectID": "LECTURE_ORDER_RATIONALE.html#why-this-order",
    "href": "LECTURE_ORDER_RATIONALE.html#why-this-order",
    "title": "Week 6 Lecture Order Rationale",
    "section": "",
    "text": "Planetary Motion First: - ‚úÖ Intuitive physics (everyone knows planets orbit the Sun) - ‚úÖ Single dominant force (gravity) - ‚úÖ Stable, predictable behavior - ‚úÖ Clear analytical solutions to verify against - ‚úÖ Historical context students know (Newton, Kepler)\nSpring Pendulum Second: - üîÑ More abstract (when do you see a spring pendulum?) - üîÑ Multiple competing forces (spring + gravity + centrifugal) - üîÑ Complex energy exchange between modes - üîÑ Can become chaotic (harder to predict) - üîÑ Less familiar historical context\n\n\n\nStudents build skills progressively:\nPlanetary Motion (Week 6.1)\n‚îú‚îÄ Learn polar coordinate ODE framework\n‚îú‚îÄ Master odeint with 4-variable system\n‚îú‚îÄ Practice phase space visualization\n‚îú‚îÄ Verify against analytical solutions\n‚îî‚îÄ Build confidence with stable system\n\nSpring Pendulum (Week 6.2)\n‚îú‚îÄ Apply same framework (now familiar!)\n‚îú‚îÄ See how different force changes behavior\n‚îú‚îÄ Understand mode coupling\n‚îú‚îÄ Explore chaotic regimes\n‚îî‚îÄ Appreciate complexity from same tools\n\n\n\nStarting with Planetary Motion: - Students get ‚Äúquick win‚Äù with beautiful orbits - High success rate builds confidence - Analytical verification provides satisfaction - Creates enthusiasm for next challenge\nFollowing with Spring Pendulum: - Students already comfortable with the framework - Can focus on new physics (chaos, coupling) - Challenge feels achievable, not overwhelming - Complexity is motivating, not discouraging\n\n\n\nThe progression tells a story:\n\n‚ÄúSame Framework, Different Physics‚Äù\n\nWeek 5: Learn the tools\nLecture 1: Apply to elegant, stable system (planets)\nLecture 2: Apply to complex, chaotic system (pendulum)\nInsight: Same math, vastly different outcomes!\n\n‚ÄúFrom Order to Chaos‚Äù\n\nLecture 1: Stable elliptical orbits (order)\nLecture 2: Chaotic trajectories (disorder)\nInsight: Even deterministic systems can be unpredictable!\n\n\n\n\n\nIf we did Spring Pendulum First: - ‚ùå Students face chaos before understanding stability - ‚ùå Complex coupling before simple central force - ‚ùå Less intuitive starting point - ‚ùå Higher risk of confusion/frustration - ‚ùå Planetary motion would feel like ‚Äúgoing backwards‚Äù in difficulty\nOur Chosen Order (Planetary First): - ‚úÖ Stable before chaotic - ‚úÖ Simple before complex - ‚úÖ Intuitive before abstract - ‚úÖ Natural difficulty progression - ‚úÖ Each lecture builds on previous"
  },
  {
    "objectID": "LECTURE_ORDER_RATIONALE.html#pedagogical-principles-applied",
    "href": "LECTURE_ORDER_RATIONALE.html#pedagogical-principles-applied",
    "title": "Week 6 Lecture Order Rationale",
    "section": "",
    "text": "Begin with material most likely to succeed, building student confidence and engagement.\n\n\n\nIntroduce mathematical framework in simpler context before applying to complex scenarios.\n\n\n\nUse success with simpler system to motivate tackling harder problems.\n\n\n\nMove from familiar/intuitive ‚Üí unfamiliar/abstract ‚Üí complex/chaotic.\n\n\n\nCreate narrative arc: master tools ‚Üí apply elegantly ‚Üí push to limits."
  },
  {
    "objectID": "LECTURE_ORDER_RATIONALE.html#student-experience",
    "href": "LECTURE_ORDER_RATIONALE.html#student-experience",
    "title": "Week 6 Lecture Order Rationale",
    "section": "",
    "text": "Confidence\n                        ‚Üë\n                        |\n                   HIGH |    ‚ï±‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n                        |   ‚ï± Spring\n                        |  ‚ï±  Pendulum\n                   MED  | ‚ï±   mastered\n                        |‚ï±\n                        |‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n                   LOW  |  Planetary\n                        |  Motion\n                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí\n                         L1    L2      Time\n\n\n\nAfter Lecture 1 (Planetary Motion): &gt; ‚ÄúWow! I just simulated planet orbits and verified Kepler‚Äôs laws! This is amazing!‚Äù\nAfter Lecture 2 (Spring Pendulum): &gt; ‚ÄúI can‚Äôt believe the same math that gave us stable orbits can create chaos! The phase space plots are wild!‚Äù\nWeek 7 Transition: &gt; ‚ÄúThese simulations are getting complex. I need better ways to organize my code‚Ä¶‚Äù"
  },
  {
    "objectID": "LECTURE_ORDER_RATIONALE.html#alternative-perspectives-considered",
    "href": "LECTURE_ORDER_RATIONALE.html#alternative-perspectives-considered",
    "title": "Week 6 Lecture Order Rationale",
    "section": "",
    "text": "‚ÄúThe spring pendulum is more directly connected to week 4-5 oscillation examples.‚Äù\nCounter: True, but planetary motion better demonstrates Week 5‚Äôs ODE tools. The spring pendulum‚Äôs complexity could obscure the method.\n\n\n\n‚ÄúGroup all oscillator problems together.‚Äù\nCounter: Week 6 is about applying ODE methods, not just oscillators. The contrast (stable vs.¬†chaotic) is more pedagogically valuable than thematic similarity."
  },
  {
    "objectID": "LECTURE_ORDER_RATIONALE.html#supporting-evidence",
    "href": "LECTURE_ORDER_RATIONALE.html#supporting-evidence",
    "title": "Week 6 Lecture Order Rationale",
    "section": "",
    "text": "Worked Example Effect: Students learn better when given clear, successful examples before complex variations (Sweller, 1988)\nCognitive Load Theory: Introduce one new difficulty at a time (Chandler & Sweller, 1991)\nScaffolding: Temporary support structures enable learners to accomplish tasks beyond their current ability (Wood et al., 1976)\nFlow Theory: Optimal learning occurs when challenge slightly exceeds but doesn‚Äôt overwhelm skill level (Csikszentmihalyi, 1990)\n\n\n\n\nMost computational physics textbooks introduce: - Central force problems (gravity) before - Coupled systems and chaos\nExamples: - Giordano & Nakanishi: Planetary motion (Ch 4) ‚Üí Chaos (Ch 6) - Newman: Orbits (Ch 8) ‚Üí Nonlinear systems (Ch 10) - Landau: Two-body problem ‚Üí Many-body ‚Üí Chaos"
  },
  {
    "objectID": "LECTURE_ORDER_RATIONALE.html#implementation-notes",
    "href": "LECTURE_ORDER_RATIONALE.html#implementation-notes",
    "title": "Week 6 Lecture Order Rationale",
    "section": "",
    "text": "In Lecture 1: - Emphasize: ‚ÄúThis sets the pattern for solving central force problems‚Äù - Preview: ‚ÄúNext time we‚Äôll see what happens with a different force‚Äù - Build confidence: ‚ÄúYou‚Äôre using the same tools that predicted Neptune!‚Äù\nIn Lecture 2: - Connect: ‚ÄúRemember how we solved planetary motion? Same approach!‚Äù - Contrast: ‚ÄúBut watch what happens when we change the force law‚Ä¶‚Äù - Celebrate complexity: ‚ÄúYou‚Äôre now exploring cutting-edge physics (chaos theory)!‚Äù\n\n\n\nSuggested Study Path: 1. Master planetary motion (Lecture 1) completely 2. Ensure you understand phase space plots 3. Then tackle spring pendulum (Lecture 2) 4. Compare and contrast the two systems 5. Reflect on how force laws affect behavior\n\n\n\nCommon Issues by Lecture:\nLecture 1 (Planetary Motion): - Confusion about units (explain AU-year system carefully) - Eccentricity calculations (provide formula sheet) - Numerical accuracy (explain tolerance settings)\nLecture 2 (Spring Pendulum): - Identifying chaos (teach visual signatures) - Parameter exploration (guide systematic variation) - Energy calculation (spring vs.¬†gravity terms)"
  },
  {
    "objectID": "LECTURE_ORDER_RATIONALE.html#conclusion",
    "href": "LECTURE_ORDER_RATIONALE.html#conclusion",
    "title": "Week 6 Lecture Order Rationale",
    "section": "",
    "text": "The lecture order (Planetary Motion ‚Üí Spring Pendulum) is pedagogically optimal because it:\n\n‚úÖ Builds from simple to complex\n‚úÖ Progresses from intuitive to abstract\n‚úÖ Moves from stable to chaotic\n‚úÖ Scaffolds mathematical framework\n‚úÖ Maximizes student success probability\n‚úÖ Creates natural narrative arc\n‚úÖ Motivates Week 7 code organization\n\nThis order respects how students actually learn: confidence through early success, skills through practice, deep understanding through progressive challenge.\n\nLast Updated: Week 6 Restructuring Approved By: Course Design Team Review Date: Before Week 6 delivery"
  },
  {
    "objectID": "LECTURE_ORDER_RATIONALE.html#references",
    "href": "LECTURE_ORDER_RATIONALE.html#references",
    "title": "Week 6 Lecture Order Rationale",
    "section": "",
    "text": "Chandler, P., & Sweller, J. (1991). Cognitive load theory and the format of instruction. Cognition and Instruction, 8(4), 293-332.\nCsikszentmihalyi, M. (1990). Flow: The psychology of optimal experience. Harper & Row.\nSweller, J. (1988). Cognitive load during problem solving. Cognitive Science, 12(2), 257-285.\nWood, D., Bruner, J. S., & Ross, G. (1976). The role of tutoring in problem solving. Journal of Child Psychology and Psychiatry, 17(2), 89-100."
  },
  {
    "objectID": "Figures.html",
    "href": "Figures.html",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "import matplotlib as mpl\nimport matplotlib.font_manager as font_manager\nfrom IPython.core.display import HTML\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nplt.rcParams.update({'font.size': 10,\n                     'lines.linewidth': 1,\n                     'lines.markersize': 10,\n                     'axes.labelsize': 9,\n                     'axes.labelpad': 4,\n                     'xtick.labelsize' : 8,\n                     'ytick.labelsize' : 8,\n                     'xtick.top' : True,\n                     'xtick.direction' : 'in',\n                     'ytick.right' : True,\n                     'ytick.direction' : 'in',\n                     'figure.dpi': 150})\n\n\ndef get_size(w,h):\n    return((w/2.54,h/2.54))\n\n\nplt.figure(figsize=get_size(8,8))\nx=np.linspace(0,np.pi*4,200)\nplt.plot(x,np.sin(x),color='k')\nplt.xlabel(r\"angle $\\theta$ in [rad]\")\nplt.ylabel(r\"$\\sin(\\theta)$\")\nplt.tight_layout()\nplt.savefig(\"figure_example4.png\", dpi=600, transparent=True)\nplt.show()\n\n\n\n\n\n\n\n\n\ndef set_size(w,h, ax=None):\n    \"\"\" w, h: width, height in inches \"\"\"\n    if not ax: ax=plt.gca()\n    l = ax.figure.subplotpars.left\n    r = ax.figure.subplotpars.right\n    t = ax.figure.subplotpars.top\n    b = ax.figure.subplotpars.bottom\n    figw = float(w)/(r-l)\n    figh = float(h)/(t-b)\n    ax.figure.set_size_inches(figw, figh)\n\n\n\nfig=plt.figure(dpi=150)\nax=plt.axes()\nax.plot(x,np.sin(x),color='k')\nax.set_xlabel(r\"angle $\\theta$ in [rad]\")\nax.set_ylabel(r\"$\\sin(\\theta)$\")\nset_size(3,2)\nplt.savefig(\"figure_example5.pdf\", bbox_inches='tight', transparent=True)\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom IPython.display import HTML, display\n\ndef make_html(fontname):\n    return \"&lt;p&gt;{font}: &lt;span style='font-family:{font}; font-size: 24px;'&gt;{font}&lt;/p&gt;\".format(font=fontname)\n\ncode = \"\\n\".join([make_html(font) for font in sorted(set([f.name for f in font_manager.fontManager.ttflist]))])\n\ndisplay(HTML(\"&lt;div style='column-count: 2;'&gt;{}&lt;/div&gt;\".format(code)))\n\n.Aqua Kana: .Aqua Kana\n.CJK Symbols Fallback HK: .CJK Symbols Fallback HK\n.Keyboard: .Keyboard\n.New York: .New York\n.SF Arabic: .SF Arabic\n.SF Arabic Rounded: .SF Arabic Rounded\n.SF Armenian: .SF Armenian\n.SF Armenian Rounded: .SF Armenian Rounded\n.SF Camera: .SF Camera\n.SF Compact Rounded: .SF Compact Rounded\n.SF Georgian: .SF Georgian\n.SF Georgian Rounded: .SF Georgian Rounded\n.SF Hebrew: .SF Hebrew\n.SF Hebrew Rounded: .SF Hebrew Rounded\n.SF NS Mono: .SF NS Mono\n.SF NS Rounded: .SF NS Rounded\n.SF Soft Numeric: .SF Soft Numeric\n.ThonburiUI: .ThonburiUI\nAcademy Engraved LET: Academy Engraved LET\nAdelle Sans Devanagari: Adelle Sans Devanagari\nAkayaKanadaka: AkayaKanadaka\nAkayaTelivigala: AkayaTelivigala\nAl Bayan: Al Bayan\nAl Nile: Al Nile\nAl Tarikh: Al Tarikh\nAmerican Typewriter: American Typewriter\nAndale Mono: Andale Mono\nAnnai MN: Annai MN\nApple Braille: Apple Braille\nApple Chancery: Apple Chancery\nApple LiGothic: Apple LiGothic\nApple LiSung: Apple LiSung\nApple SD Gothic Neo: Apple SD Gothic Neo\nApple Symbols: Apple Symbols\nAppleGothic: AppleGothic\nAppleMyungjo: AppleMyungjo\nArial: Arial\nArial Black: Arial Black\nArial Hebrew: Arial Hebrew\nArial Narrow: Arial Narrow\nArial Rounded MT Bold: Arial Rounded MT Bold\nArial Unicode MS: Arial Unicode MS\nArima Koshi: Arima Koshi\nArima Madurai: Arima Madurai\nAthelas: Athelas\nAvenir: Avenir\nAvenir Next: Avenir Next\nAvenir Next Condensed: Avenir Next Condensed\nAyuthaya: Ayuthaya\nBM Dohyeon: BM Dohyeon\nBM Hanna 11yrs Old: BM Hanna 11yrs Old\nBM Hanna Air: BM Hanna Air\nBM Hanna Pro: BM Hanna Pro\nBM Jua: BM Jua\nBM Kirang Haerang: BM Kirang Haerang\nBM Yeonsung: BM Yeonsung\nBaghdad: Baghdad\nBai Jamjuree: Bai Jamjuree\nBaloo 2: Baloo 2\nBaloo Bhai 2: Baloo Bhai 2\nBaloo Bhaijaan: Baloo Bhaijaan\nBaloo Bhaina 2: Baloo Bhaina 2\nBaloo Chettan 2: Baloo Chettan 2\nBaloo Da 2: Baloo Da 2\nBaloo Paaji 2: Baloo Paaji 2\nBaloo Tamma 2: Baloo Tamma 2\nBaloo Tammudu 2: Baloo Tammudu 2\nBaloo Thambi 2: Baloo Thambi 2\nBangla MN: Bangla MN\nBangla Sangam MN: Bangla Sangam MN\nBaoli SC: Baoli SC\nBaskerville: Baskerville\nBeirut: Beirut\nBiauKaiHK: BiauKaiHK\nBig Caslon: Big Caslon\nBodoni 72: Bodoni 72\nBodoni 72 Oldstyle: Bodoni 72 Oldstyle\nBodoni 72 Smallcaps: Bodoni 72 Smallcaps\nBodoni Ornaments: Bodoni Ornaments\nBradley Hand: Bradley Hand\nBrush Script MT: Brush Script MT\nCambay Devanagari: Cambay Devanagari\nChakra Petch: Chakra Petch\nChalkboard: Chalkboard\nChalkboard SE: Chalkboard SE\nChalkduster: Chalkduster\nCharm: Charm\nCharmonman: Charmonman\nCharter: Charter\nCochin: Cochin\nComic Sans MS: Comic Sans MS\nCopperplate: Copperplate\nCorsiva Hebrew: Corsiva Hebrew\nCourier: Courier\nCourier New: Courier New\nDIN Alternate: DIN Alternate\nDIN Condensed: DIN Condensed\nDamascus: Damascus\nDecoType Naskh: DecoType Naskh\nDejaVu Sans: DejaVu Sans\nDejaVu Sans Display: DejaVu Sans Display\nDejaVu Sans Mono: DejaVu Sans Mono\nDejaVu Serif: DejaVu Serif\nDejaVu Serif Display: DejaVu Serif Display\nDevanagari MT: Devanagari MT\nDevanagari Sangam MN: Devanagari Sangam MN\nDidot: Didot\nDiwan Kufi: Diwan Kufi\nDiwan Thuluth: Diwan Thuluth\nEuphemia UCAS: Euphemia UCAS\nFahkwang: Fahkwang\nFarah: Farah\nFarisi: Farisi\nFutura: Futura\nGalvji: Galvji\nGeeza Pro: Geeza Pro\nGeneva: Geneva\nGeorgia: Georgia\nGill Sans: Gill Sans\nGotu: Gotu\nGujarati MT: Gujarati MT\nGujarati Sangam MN: Gujarati Sangam MN\nGungSeo: GungSeo\nGurmukhi MN: Gurmukhi MN\nGurmukhi MT: Gurmukhi MT\nGurmukhi Sangam MN: Gurmukhi Sangam MN\nHannotate SC: Hannotate SC\nHanziPen SC: HanziPen SC\nHeadLineA: HeadLineA\nHei: Hei\nHeiti TC: Heiti TC\nHelvetica: Helvetica\nHelvetica Neue: Helvetica Neue\nHerculanum: Herculanum\nHiragino Maru Gothic Pro: Hiragino Maru Gothic Pro\nHiragino Mincho ProN: Hiragino Mincho ProN\nHiragino Sans: Hiragino Sans\nHiragino Sans GB: Hiragino Sans GB\nHiragino Sans TC: Hiragino Sans TC\nHoefler Text: Hoefler Text\nHubballi: Hubballi\nITF Devanagari: ITF Devanagari\nImpact: Impact\nInaiMathi: InaiMathi\nIowan Old Style: Iowan Old Style\nJaini: Jaini\nJaini Purva: Jaini Purva\nK2D: K2D\nKai: Kai\nKailasa: Kailasa\nKaiti SC: Kaiti SC\nKannada MN: Kannada MN\nKannada Sangam MN: Kannada Sangam MN\nKatari: Katari\nKavivanar: Kavivanar\nKefa: Kefa\nKhmer MN: Khmer MN\nKhmer Sangam MN: Khmer Sangam MN\nKlee: Klee\nKoHo: KoHo\nKodchasan: Kodchasan\nKohinoor Bangla: Kohinoor Bangla\nKohinoor Devanagari: Kohinoor Devanagari\nKohinoor Gujarati: Kohinoor Gujarati\nKohinoor Telugu: Kohinoor Telugu\nKokonor: Kokonor\nKrub: Krub\nKrungthep: Krungthep\nKufiStandardGK: KufiStandardGK\nLahore Gurmukhi: Lahore Gurmukhi\nLantinghei SC: Lantinghei SC\nLao MN: Lao MN\nLao Sangam MN: Lao Sangam MN\nLava Devanagari: Lava Devanagari\nLava Kannada: Lava Kannada\nLava Telugu: Lava Telugu\nLiHei Pro: LiHei Pro\nLiSong Pro: LiSong Pro\nLibian SC: Libian SC\nLingWai SC: LingWai SC\nLingWai TC: LingWai TC\nLucida Grande: Lucida Grande\nLuminari: Luminari\nMaku: Maku\nMalayalam MN: Malayalam MN\nMalayalam Sangam MN: Malayalam Sangam MN\nMali: Mali\nMarion: Marion\nMarker Felt: Marker Felt\nMenlo: Menlo\nMicrosoft Sans Serif: Microsoft Sans Serif\nMishafi: Mishafi\nMishafi Gold: Mishafi Gold\nModak: Modak\nMonaco: Monaco\nMshtakan: Mshtakan\nMukta: Mukta\nMukta Mahee: Mukta Mahee\nMukta Malar: Mukta Malar\nMukta Vaani: Mukta Vaani\nMuna: Muna\nMyanmar MN: Myanmar MN\nMyanmar Sangam MN: Myanmar Sangam MN\nNadeem: Nadeem\nNanum Brush Script: Nanum Brush Script\nNanum Gothic: Nanum Gothic\nNanum Myeongjo: Nanum Myeongjo\nNew Peninim MT: New Peninim MT\nNiramit: Niramit\nNoteworthy: Noteworthy\nNoto Nastaliq Urdu: Noto Nastaliq Urdu\nNoto Sans Adlam: Noto Sans Adlam\nNoto Sans Armenian: Noto Sans Armenian\nNoto Sans Avestan: Noto Sans Avestan\nNoto Sans Bamum: Noto Sans Bamum\nNoto Sans Bassa Vah: Noto Sans Bassa Vah\nNoto Sans Batak: Noto Sans Batak\nNoto Sans Bhaiksuki: Noto Sans Bhaiksuki\nNoto Sans Brahmi: Noto Sans Brahmi\nNoto Sans Buginese: Noto Sans Buginese\nNoto Sans Buhid: Noto Sans Buhid\nNoto Sans Canadian Aboriginal: Noto Sans Canadian Aboriginal\nNoto Sans Carian: Noto Sans Carian\nNoto Sans Caucasian Albanian: Noto Sans Caucasian Albanian\nNoto Sans Chakma: Noto Sans Chakma\nNoto Sans Cham: Noto Sans Cham\nNoto Sans Coptic: Noto Sans Coptic\nNoto Sans Cuneiform: Noto Sans Cuneiform\nNoto Sans Cypriot: Noto Sans Cypriot\nNoto Sans Duployan: Noto Sans Duployan\nNoto Sans Egyptian Hieroglyphs: Noto Sans Egyptian Hieroglyphs\nNoto Sans Elbasan: Noto Sans Elbasan\nNoto Sans Glagolitic: Noto Sans Glagolitic\nNoto Sans Gothic: Noto Sans Gothic\nNoto Sans Gunjala Gondi: Noto Sans Gunjala Gondi\nNoto Sans Hanifi Rohingya: Noto Sans Hanifi Rohingya\nNoto Sans Hanunoo: Noto Sans Hanunoo\nNoto Sans Hatran: Noto Sans Hatran\nNoto Sans Imperial Aramaic: Noto Sans Imperial Aramaic\nNoto Sans Inscriptional Pahlavi: Noto Sans Inscriptional Pahlavi\nNoto Sans Inscriptional Parthian: Noto Sans Inscriptional Parthian\nNoto Sans Javanese: Noto Sans Javanese\nNoto Sans Kaithi: Noto Sans Kaithi\nNoto Sans Kannada: Noto Sans Kannada\nNoto Sans Kayah Li: Noto Sans Kayah Li\nNoto Sans Kharoshthi: Noto Sans Kharoshthi\nNoto Sans Khojki: Noto Sans Khojki\nNoto Sans Khudawadi: Noto Sans Khudawadi\nNoto Sans Lepcha: Noto Sans Lepcha\nNoto Sans Limbu: Noto Sans Limbu\nNoto Sans Linear A: Noto Sans Linear A\nNoto Sans Linear B: Noto Sans Linear B\nNoto Sans Lisu: Noto Sans Lisu\nNoto Sans Lycian: Noto Sans Lycian\nNoto Sans Lydian: Noto Sans Lydian\nNoto Sans Mahajani: Noto Sans Mahajani\nNoto Sans Mandaic: Noto Sans Mandaic\nNoto Sans Manichaean: Noto Sans Manichaean\nNoto Sans Marchen: Noto Sans Marchen\nNoto Sans Masaram Gondi: Noto Sans Masaram Gondi\nNoto Sans Meetei Mayek: Noto Sans Meetei Mayek\nNoto Sans Mende Kikakui: Noto Sans Mende Kikakui\nNoto Sans Meroitic: Noto Sans Meroitic\nNoto Sans Miao: Noto Sans Miao\nNoto Sans Modi: Noto Sans Modi\nNoto Sans Mongolian: Noto Sans Mongolian\nNoto Sans Mro: Noto Sans Mro\nNoto Sans Multani: Noto Sans Multani\nNoto Sans Myanmar: Noto Sans Myanmar\nNoto Sans NKo: Noto Sans NKo\nNoto Sans Nabataean: Noto Sans Nabataean\nNoto Sans New Tai Lue: Noto Sans New Tai Lue\nNoto Sans Newa: Noto Sans Newa\nNoto Sans Ol Chiki: Noto Sans Ol Chiki\nNoto Sans Old Hungarian: Noto Sans Old Hungarian\nNoto Sans Old Italic: Noto Sans Old Italic\nNoto Sans Old North Arabian: Noto Sans Old North Arabian\nNoto Sans Old Permic: Noto Sans Old Permic\nNoto Sans Old Persian: Noto Sans Old Persian\nNoto Sans Old South Arabian: Noto Sans Old South Arabian\nNoto Sans Old Turkic: Noto Sans Old Turkic\nNoto Sans Oriya: Noto Sans Oriya\nNoto Sans Osage: Noto Sans Osage\nNoto Sans Osmanya: Noto Sans Osmanya\nNoto Sans Pahawh Hmong: Noto Sans Pahawh Hmong\nNoto Sans Palmyrene: Noto Sans Palmyrene\nNoto Sans Pau Cin Hau: Noto Sans Pau Cin Hau\nNoto Sans PhagsPa: Noto Sans PhagsPa\nNoto Sans Phoenician: Noto Sans Phoenician\nNoto Sans Psalter Pahlavi: Noto Sans Psalter Pahlavi\nNoto Sans Rejang: Noto Sans Rejang\nNoto Sans Samaritan: Noto Sans Samaritan\nNoto Sans Saurashtra: Noto Sans Saurashtra\nNoto Sans Sharada: Noto Sans Sharada\nNoto Sans Siddham: Noto Sans Siddham\nNoto Sans Sora Sompeng: Noto Sans Sora Sompeng\nNoto Sans Sundanese: Noto Sans Sundanese\nNoto Sans Syloti Nagri: Noto Sans Syloti Nagri\nNoto Sans Syriac: Noto Sans Syriac\nNoto Sans Tagalog: Noto Sans Tagalog\nNoto Sans Tagbanwa: Noto Sans Tagbanwa\nNoto Sans Tai Le: Noto Sans Tai Le\nNoto Sans Tai Tham: Noto Sans Tai Tham\nNoto Sans Tai Viet: Noto Sans Tai Viet\nNoto Sans Takri: Noto Sans Takri\nNoto Sans Thaana: Noto Sans Thaana\nNoto Sans Tifinagh: Noto Sans Tifinagh\nNoto Sans Tirhuta: Noto Sans Tirhuta\nNoto Sans Ugaritic: Noto Sans Ugaritic\nNoto Sans Vai: Noto Sans Vai\nNoto Sans Wancho: Noto Sans Wancho\nNoto Sans Warang Citi: Noto Sans Warang Citi\nNoto Sans Yi: Noto Sans Yi\nNoto Serif Ahom: Noto Serif Ahom\nNoto Serif Balinese: Noto Serif Balinese\nNoto Serif Hmong Nyiakeng: Noto Serif Hmong Nyiakeng\nNoto Serif Kannada: Noto Serif Kannada\nNoto Serif Myanmar: Noto Serif Myanmar\nNoto Serif Yezidi: Noto Serif Yezidi\nOctober Compressed Devanagari: October Compressed Devanagari\nOctober Compressed Tamil: October Compressed Tamil\nOctober Condensed Devanagari: October Condensed Devanagari\nOctober Condensed Tamil: October Condensed Tamil\nOctober Devanagari: October Devanagari\nOctober Tamil: October Tamil\nOptima: Optima\nOriya MN: Oriya MN\nOriya Sangam MN: Oriya Sangam MN\nOsaka: Osaka\nPCMyungjo: PCMyungjo\nPSL Ornanong Pro: PSL Ornanong Pro\nPT Mono: PT Mono\nPT Sans: PT Sans\nPT Serif: PT Serif\nPT Serif Caption: PT Serif Caption\nPadyakke Expanded One: Padyakke Expanded One\nPalatino: Palatino\nPapyrus: Papyrus\nParty LET: Party LET\nPhosphate: Phosphate\nPilGi: PilGi\nPlantagenet Cherokee: Plantagenet Cherokee\nRaanana: Raanana\nRockwell: Rockwell\nSTFangsong: STFangsong\nSTHeiti: STHeiti\nSTIX Two Math: STIX Two Math\nSTIX Two Text: STIX Two Text\nSTIXGeneral: STIXGeneral\nSTIXIntegralsD: STIXIntegralsD\nSTIXIntegralsSm: STIXIntegralsSm\nSTIXIntegralsUp: STIXIntegralsUp\nSTIXIntegralsUpD: STIXIntegralsUpD\nSTIXIntegralsUpSm: STIXIntegralsUpSm\nSTIXNonUnicode: STIXNonUnicode\nSTIXSizeFiveSym: STIXSizeFiveSym\nSTIXSizeFourSym: STIXSizeFourSym\nSTIXSizeOneSym: STIXSizeOneSym\nSTIXSizeThreeSym: STIXSizeThreeSym\nSTIXSizeTwoSym: STIXSizeTwoSym\nSTIXVariants: STIXVariants\nSama Devanagari: Sama Devanagari\nSama Gujarati: Sama Gujarati\nSama Gurmukhi: Sama Gurmukhi\nSama Kannada: Sama Kannada\nSama Malayalam: Sama Malayalam\nSama Tamil: Sama Tamil\nSana: Sana\nSarabun: Sarabun\nSathu: Sathu\nSavoye LET: Savoye LET\nSeravek: Seravek\nShobhika: Shobhika\nShree Devanagari 714: Shree Devanagari 714\nSignPainter: SignPainter\nSilom: Silom\nSimSong: SimSong\nSinhala MN: Sinhala MN\nSinhala Sangam MN: Sinhala Sangam MN\nSkia: Skia\nSnell Roundhand: Snell Roundhand\nSongti SC: Songti SC\nSrisakdi: Srisakdi\nSukhumvit Set: Sukhumvit Set\nSuperclarendon: Superclarendon\nSymbol: Symbol\nSystem Font: System Font\nTahoma: Tahoma\nTamil MN: Tamil MN\nTamil Sangam MN: Tamil Sangam MN\nTelugu MN: Telugu MN\nTelugu Sangam MN: Telugu Sangam MN\nThonburi: Thonburi\nTimes: Times\nTimes New Roman: Times New Roman\nTiro Bangla: Tiro Bangla\nTiro Devanagari Hindi: Tiro Devanagari Hindi\nTiro Devanagari Marathi: Tiro Devanagari Marathi\nTiro Devanagari Sanskrit: Tiro Devanagari Sanskrit\nTiro Gurmukhi: Tiro Gurmukhi\nTiro Kannada: Tiro Kannada\nTiro Tamil: Tiro Tamil\nTiro Telugu: Tiro Telugu\nToppan Bunkyu Gothic: Toppan Bunkyu Gothic\nToppan Bunkyu Midashi Gothic: Toppan Bunkyu Midashi Gothic\nToppan Bunkyu Midashi Mincho: Toppan Bunkyu Midashi Mincho\nToppan Bunkyu Mincho: Toppan Bunkyu Mincho\nTrattatello: Trattatello\nTrebuchet MS: Trebuchet MS\nTsukushi A Round Gothic: Tsukushi A Round Gothic\nTsukushi B Round Gothic: Tsukushi B Round Gothic\nVerdana: Verdana\nWaseem: Waseem\nWawati SC: Wawati SC\nWawati TC: Wawati TC\nWebdings: Webdings\nWingdings: Wingdings\nWingdings 2: Wingdings 2\nWingdings 3: Wingdings 3\nXingkai SC: Xingkai SC\nYuGothic: YuGothic\nYuKyokasho Yoko: YuKyokasho Yoko\nYuMincho: YuMincho\nYuanti SC: Yuanti SC\nYuppy SC: Yuppy SC\nYuppy TC: Yuppy TC\nZapf Dingbats: Zapf Dingbats\nZapfino: Zapfino\ncmb10: cmb10\ncmex10: cmex10\ncmmi10: cmmi10\ncmr10: cmr10\ncmss10: cmss10\ncmsy10: cmsy10\ncmtt10: cmtt10\n\n\n\nplt.rcParams.update({'font.size': 10,\n                     'lines.linewidth': 1,\n                     'lines.markersize': 10,\n                     'axes.labelsize': 9,\n                     'axes.labelpad': 4,\n                     'xtick.labelsize' : 8,\n                     'ytick.labelsize' : 8,\n                     'xtick.top' : True,\n                     'xtick.direction' : 'in',\n                     'ytick.right' : True,\n                     'ytick.direction' : 'in',\n                     'figure.dpi': 150})\n\n\ncmfont = font_manager.FontProperties(fname=mpl.get_data_path() + '/fonts/ttf/cmr10.ttf')\n\nplt.rcParams.update({'font.size': 10,\n                     'axes.titlesize': 12,\n                     'axes.labelsize': 9,\n                     'axes.labelpad': 4,\n                     'lines.linewidth': 1,\n                     'lines.markersize': 10,\n                     'xtick.labelsize' : 8,\n                     'ytick.labelsize' : 8,\n                     'xtick.top' : True,\n                     'xtick.direction' : 'in',\n                     'ytick.right' : True,\n                     'ytick.direction' : 'in',\n                     'font.family' : 'serif',\n                     'font.serif' : cmfont.get_name(),\n                     \"axes.formatter.use_mathtext\": True,\n                     'text.usetex': True,\n                     'mathtext.fontset' : 'cm'\n                    })\n\n\nx=np.linspace(0,np.pi,100)\n\n\nplt.figure(figsize=get_size(6,5),dpi=150)\nplt.plot(x,np.sin(x))\nplt.xlabel(r\"velocity $v$\")\nplt.ylabel(r\"position $r$\")\nplt.savef\nplt.show()"
  }
]