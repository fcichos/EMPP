[
  {
    "objectID": "Assignment 4.html",
    "href": "Assignment 4.html",
    "title": "Assignment 3",
    "section": "",
    "text": "Problem 1\nThere are two files with a list of integers a.txt and b.txt. Calculate the greatest common divisors of each pair of values from these files and store them in the file result.txt. Check out the numpy function np.gcd() for this purpose.¬†\nAufgabenstellung klarer!\n\nimport numpy as np\n\nwith open('a.txt', 'r') as file_1, open('b.txt', 'r') as file_2, open('result.txt', 'w') as result:\n    for a, b in zip(file_1, file_2):\n        result.write(f\"{np.gcd(int(a),int(b))}\\n\")\n\n\nfile=open('a.txt','r')\nfile1=open('b.txt','r')\n\n\na=file.readlines()\nb=file1.readlines()\n\n\nlist(zip(a,b))\n\n[('12\\n', '5\\n'),\n ('2\\n', '8\\n'),\n ('4\\n', '4\\n'),\n ('3\\n', '6\\n'),\n ('4', '87\\n')]\n\n\nProblem 2\nAssign the 100 numbers between -10 and 10 including -10 and 10 to the variable x using the linspace function of numpy. Assign the function values ( x¬≤-20 ) to the variable y. Store the paired values in the file named ‚Äúdata.txt‚Äù in the form ‚Äúx, y‚Äù (e.g.¬†-10, 80) for each line of the file.\nLeerzeichen war die Falle!\n\nimport numpy as np\n\nx=np.linspace(-10,10,100)\ny=x**2-20\n\nwith open('data.txt', 'w') as file:\n    for x, y in zip(x, y):\n        file.write(f\"{x}, {y}\\n\")\n\nProblem 3\nWrite a Python function projectile_motion(v0, theta, dt)¬†that simulates the projectile motion of an object launched at an initial velocity v0 (in m/s) and angle theta (in degrees). The function should return a tuple containing:\nThe maximum height reached by the object (in meters). The total time of flight (in seconds). The horizontal range (in meters).\nUse a time step dt (in seconds) for the simulation. Advance the time inside the function with a loop. Note that calling the function is not required for the task! Can be done only with the math module.¬†\nWhat if one of the arguments is 0?\nAufgabenstellung klarer!\n\nimport math\n\ndef projectile_motion(v0, theta, dt):\n    if(v0 == 0 or theta == 0 or dt == 0): return (0,0,0)\n    \n    theta_rad = math.radians(theta)\n    \n    v0x = v0 * math.cos(theta_rad)\n    v0y = v0 * math.sin(theta_rad)\n    \n    x_points = []\n    y_points = []\n    \n    x = 0\n    y = 0\n    t = 0\n    \n    while y &gt;= 0:\n        x = v0x * t\n        y = v0y * t - 0.5 * 9.81 * t**2\n        x_points.append(x)\n        y_points.append(y)\n        t += dt\n    \n    max_height = max(y_points)\n    total_time = t - dt\n    horizontal_range = x_points[-1]\n    \n    return (max_height, total_time, horizontal_range)\n\n\nx=np.linspace(0,10,100)-5\ny=np.linspace(0,10,100)-5\n\n\nX,Y=np.meshgrid(x,y)\n\n\nr=np.sqrt(X**2+Y**2)\n\n\n\nimport matplotlib.pyplot as plt\n\n\nplt.contour(r)"
  },
  {
    "objectID": "lectures/PEDAGOGICAL_NOTES.html",
    "href": "lectures/PEDAGOGICAL_NOTES.html",
    "title": "Pedagogical Notes: Brownian Motion Restructuring",
    "section": "",
    "text": "This document explains the rationale for splitting Brownian motion into two separate modules taught at different points in the course.\n\n\n\n\n\nBy Week 2-3, students have barely mastered: - Basic Python syntax (variables, data types) - Simple functions - Basic plotting\nIntroducing object-oriented programming at this stage adds: - The concept of self - Constructors (__init__) - Instance vs.¬†class variables - Methods vs.¬†functions - The entire OOP paradigm\nThis creates cognitive overload when students should be focusing on: - Understanding the physics of random walks - Getting comfortable with numpy - Building confidence with loops and arrays\n\n\n\nThe original 2_brownian_motion.qmd explicitly states: &gt; ‚ÄúWe will apply our newly acquired knowledge about classes‚Ä¶‚Äù\nThis confirms it was designed as an application of OOP, not an introduction to programming fundamentals.\n\n\n\n\n\n\nFile: lectures/lecture02/4_brownian_motion_simple.qmd\nFocus: - ‚úÖ Physics first: What is Brownian motion? Why does it happen? - ‚úÖ Simple functions with clear inputs/outputs - ‚úÖ Using lists and numpy arrays - ‚úÖ Basic visualization - ‚úÖ Statistical analysis (MSD)\nLearning Outcomes: - Students understand the physics - Students can simulate random walks - Students gain confidence with functions and arrays - Students see immediate, satisfying results\nKey Advantage: Students can focus on one new concept at a time: - The physics of diffusion - Random number generation - Trajectory storage in arrays\n\n\n\nFile: lectures/lecture05/2_brownian_motion.qmd\nFocus: - ‚úÖ Revisiting familiar physics with new tools - ‚úÖ Showing why classes help organize code - ‚úÖ Managing multiple particles with different properties - ‚úÖ Encapsulating state and behavior\nLearning Outcomes: - Students see the value of OOP through direct comparison - Students understand when to use classes - Students can design and implement their own classes - Students appreciate code organization in complex simulations\nKey Advantage: Students see the need for better organization because: - They‚Äôve already written the ‚Äúmessy‚Äù version with functions - They understand the physics, so can focus on the programming structure - They can compare the two approaches directly\n\n\n\n\n\n\nStudents encounter Brownian motion twice: - First pass: Understand the physics, basic implementation - Second pass: Better implementation, deeper understanding\n\n\n\n\nWeek 2-3: Concrete ‚Äúhere‚Äôs how to simulate random steps‚Äù\nWeek 6-7: Abstract ‚Äúhere‚Äôs how to organize complex simulations‚Äù\n\n\n\n\n\nTeach OOP when students see why they need it\nAfter managing messy lists of particles, classes make sense\n\n\n\n\n\nWeek 2-3: ‚ÄúI can simulate physics!‚Äù\nWeek 6-7: ‚ÄúI can write professional code!‚Äù\n\n\n\n\n\n\n\nEmphasize: - ‚ÄúFocus on the physics, not the programming tricks‚Äù - ‚ÄúThis is a ‚Äòquick and dirty‚Äô implementation - we‚Äôll improve it later‚Äù - Celebrate when students get their first random walk working\nCommon Student Questions: - Q: ‚ÄúWhy are we using lists instead of [something fancier]?‚Äù - A: ‚ÄúWe‚Äôre keeping it simple so we can focus on understanding the physics. We‚Äôll optimize later.‚Äù\nExtension Activities: - Compare different diffusion coefficients - Add boundaries (particles bounce off walls) - Try 3D motion\n\n\n\nEmphasize: - ‚ÄúRemember our Brownian motion simulation? Let‚Äôs make it better!‚Äù - Compare the two implementations side-by-side - Show how classes reduce bugs (each particle manages its own state)\nCommon Student Questions: - Q: ‚ÄúIs the OOP version really better?‚Äù - A: ‚ÄúFor this small example, maybe not. But imagine 1000 particles with different sizes, charges, and interactions. Then classes become essential.‚Äù\nExtension Activities: - Add particle-particle interactions - Create different particle types (subclasses) - Implement particle tracking/analysis methods\n\n\n\n\n\n\nTest understanding of: - What causes Brownian motion (physics) - How to generate random steps - How to store and plot trajectories - What MSD tells us\nAvoid asking about: Classes, OOP, complex data structures\n\n\n\nTest understanding of: - When to use classes vs.¬†functions - How to design a class - Instance vs.¬†class variables - How OOP improves code organization\nCan reference: The Week 2-3 Brownian motion as a comparison point\n\n\n\n\nYou‚Äôll know this restructuring works when:\n‚úÖ Week 2-3 students are excited about simulating motion (not confused by self)\n‚úÖ Week 6-7 students say ‚Äúoh, this makes the code cleaner!‚Äù (not ‚Äúwhy are we doing this?‚Äù)\n‚úÖ Fewer questions about syntax, more about physics and design\n‚úÖ Students can extend their simulations independently\n\n\n\nThis two-stage approach could be applied to other topics: - Planetary motion: Simple version (Week 4-5), OOP version with multiple planets (Week 8-9) - Wave simulations: Function-based first, class-based for complex systems - Data analysis: Simple functions, then analysis classes\nThe pattern: Physics first, organization later works well for computational physics courses where students are learning both domains simultaneously.\n\nLast updated: 2024 For questions about this restructuring, see the commit history or course coordinator."
  },
  {
    "objectID": "lectures/PEDAGOGICAL_NOTES.html#overview",
    "href": "lectures/PEDAGOGICAL_NOTES.html#overview",
    "title": "Pedagogical Notes: Brownian Motion Restructuring",
    "section": "",
    "text": "This document explains the rationale for splitting Brownian motion into two separate modules taught at different points in the course."
  },
  {
    "objectID": "lectures/PEDAGOGICAL_NOTES.html#the-problem-with-teaching-classes-in-week-2-3",
    "href": "lectures/PEDAGOGICAL_NOTES.html#the-problem-with-teaching-classes-in-week-2-3",
    "title": "Pedagogical Notes: Brownian Motion Restructuring",
    "section": "",
    "text": "By Week 2-3, students have barely mastered: - Basic Python syntax (variables, data types) - Simple functions - Basic plotting\nIntroducing object-oriented programming at this stage adds: - The concept of self - Constructors (__init__) - Instance vs.¬†class variables - Methods vs.¬†functions - The entire OOP paradigm\nThis creates cognitive overload when students should be focusing on: - Understanding the physics of random walks - Getting comfortable with numpy - Building confidence with loops and arrays\n\n\n\nThe original 2_brownian_motion.qmd explicitly states: &gt; ‚ÄúWe will apply our newly acquired knowledge about classes‚Ä¶‚Äù\nThis confirms it was designed as an application of OOP, not an introduction to programming fundamentals."
  },
  {
    "objectID": "lectures/PEDAGOGICAL_NOTES.html#the-two-stage-solution",
    "href": "lectures/PEDAGOGICAL_NOTES.html#the-two-stage-solution",
    "title": "Pedagogical Notes: Brownian Motion Restructuring",
    "section": "",
    "text": "File: lectures/lecture02/4_brownian_motion_simple.qmd\nFocus: - ‚úÖ Physics first: What is Brownian motion? Why does it happen? - ‚úÖ Simple functions with clear inputs/outputs - ‚úÖ Using lists and numpy arrays - ‚úÖ Basic visualization - ‚úÖ Statistical analysis (MSD)\nLearning Outcomes: - Students understand the physics - Students can simulate random walks - Students gain confidence with functions and arrays - Students see immediate, satisfying results\nKey Advantage: Students can focus on one new concept at a time: - The physics of diffusion - Random number generation - Trajectory storage in arrays\n\n\n\nFile: lectures/lecture05/2_brownian_motion.qmd\nFocus: - ‚úÖ Revisiting familiar physics with new tools - ‚úÖ Showing why classes help organize code - ‚úÖ Managing multiple particles with different properties - ‚úÖ Encapsulating state and behavior\nLearning Outcomes: - Students see the value of OOP through direct comparison - Students understand when to use classes - Students can design and implement their own classes - Students appreciate code organization in complex simulations\nKey Advantage: Students see the need for better organization because: - They‚Äôve already written the ‚Äúmessy‚Äù version with functions - They understand the physics, so can focus on the programming structure - They can compare the two approaches directly"
  },
  {
    "objectID": "lectures/PEDAGOGICAL_NOTES.html#pedagogical-principles-applied",
    "href": "lectures/PEDAGOGICAL_NOTES.html#pedagogical-principles-applied",
    "title": "Pedagogical Notes: Brownian Motion Restructuring",
    "section": "",
    "text": "Students encounter Brownian motion twice: - First pass: Understand the physics, basic implementation - Second pass: Better implementation, deeper understanding\n\n\n\n\nWeek 2-3: Concrete ‚Äúhere‚Äôs how to simulate random steps‚Äù\nWeek 6-7: Abstract ‚Äúhere‚Äôs how to organize complex simulations‚Äù\n\n\n\n\n\nTeach OOP when students see why they need it\nAfter managing messy lists of particles, classes make sense\n\n\n\n\n\nWeek 2-3: ‚ÄúI can simulate physics!‚Äù\nWeek 6-7: ‚ÄúI can write professional code!‚Äù"
  },
  {
    "objectID": "lectures/PEDAGOGICAL_NOTES.html#teaching-tips",
    "href": "lectures/PEDAGOGICAL_NOTES.html#teaching-tips",
    "title": "Pedagogical Notes: Brownian Motion Restructuring",
    "section": "",
    "text": "Emphasize: - ‚ÄúFocus on the physics, not the programming tricks‚Äù - ‚ÄúThis is a ‚Äòquick and dirty‚Äô implementation - we‚Äôll improve it later‚Äù - Celebrate when students get their first random walk working\nCommon Student Questions: - Q: ‚ÄúWhy are we using lists instead of [something fancier]?‚Äù - A: ‚ÄúWe‚Äôre keeping it simple so we can focus on understanding the physics. We‚Äôll optimize later.‚Äù\nExtension Activities: - Compare different diffusion coefficients - Add boundaries (particles bounce off walls) - Try 3D motion\n\n\n\nEmphasize: - ‚ÄúRemember our Brownian motion simulation? Let‚Äôs make it better!‚Äù - Compare the two implementations side-by-side - Show how classes reduce bugs (each particle manages its own state)\nCommon Student Questions: - Q: ‚ÄúIs the OOP version really better?‚Äù - A: ‚ÄúFor this small example, maybe not. But imagine 1000 particles with different sizes, charges, and interactions. Then classes become essential.‚Äù\nExtension Activities: - Add particle-particle interactions - Create different particle types (subclasses) - Implement particle tracking/analysis methods"
  },
  {
    "objectID": "lectures/PEDAGOGICAL_NOTES.html#assessment-implications",
    "href": "lectures/PEDAGOGICAL_NOTES.html#assessment-implications",
    "title": "Pedagogical Notes: Brownian Motion Restructuring",
    "section": "",
    "text": "Test understanding of: - What causes Brownian motion (physics) - How to generate random steps - How to store and plot trajectories - What MSD tells us\nAvoid asking about: Classes, OOP, complex data structures\n\n\n\nTest understanding of: - When to use classes vs.¬†functions - How to design a class - Instance vs.¬†class variables - How OOP improves code organization\nCan reference: The Week 2-3 Brownian motion as a comparison point"
  },
  {
    "objectID": "lectures/PEDAGOGICAL_NOTES.html#success-metrics",
    "href": "lectures/PEDAGOGICAL_NOTES.html#success-metrics",
    "title": "Pedagogical Notes: Brownian Motion Restructuring",
    "section": "",
    "text": "You‚Äôll know this restructuring works when:\n‚úÖ Week 2-3 students are excited about simulating motion (not confused by self)\n‚úÖ Week 6-7 students say ‚Äúoh, this makes the code cleaner!‚Äù (not ‚Äúwhy are we doing this?‚Äù)\n‚úÖ Fewer questions about syntax, more about physics and design\n‚úÖ Students can extend their simulations independently"
  },
  {
    "objectID": "lectures/PEDAGOGICAL_NOTES.html#future-considerations",
    "href": "lectures/PEDAGOGICAL_NOTES.html#future-considerations",
    "title": "Pedagogical Notes: Brownian Motion Restructuring",
    "section": "",
    "text": "This two-stage approach could be applied to other topics: - Planetary motion: Simple version (Week 4-5), OOP version with multiple planets (Week 8-9) - Wave simulations: Function-based first, class-based for complex systems - Data analysis: Simple functions, then analysis classes\nThe pattern: Physics first, organization later works well for computational physics courses where students are learning both domains simultaneously.\n\nLast updated: 2024 For questions about this restructuring, see the commit history or course coordinator."
  },
  {
    "objectID": "lectures/lecture13/4_repetition.html",
    "href": "lectures/lecture13/4_repetition.html",
    "title": "Repetition Meshgrid and Vector Fields",
    "section": "",
    "text": "This time, the exercises are a bit more complicated and perhaps only for the more advanced. You may, however, do some training with the easier repetitions before and come back and test yourself. We will focus on creating visualizations of vector fields using meshgrids and quiver plots. Vector fields are used to represent the spatial distribution of physical quantities like electric fields, fluid velocities, or magnetic fields. By visualizing vector fields, we can gain insights into the behavior of these quantities in different regions of space.\nNote that matplotlib and numpy are already imported and may be used with plt and np respectively.\n\n\n\n\n\n\nSelf-Exercise 1: Electric Field of a Point Charge\n\n\n\nCreate a 2D vector field plot of the electric field from a point charge located at the origin. The electric field vectors should point radially outward from the charge, with magnitude proportional to 1/r¬≤. This visualization helps understand the spatial distribution of electric fields.\nThe electric field is given by: \\(\\vec{E}(\\vec{r}) = \\frac{q}{4\\pi\\epsilon_0} \\frac{\\vec{r}}{r^3}\\)\nwhere:\n\n\\(\\vec{E}\\) is the electric field vector\n\\(q\\) is the charge\n\\(\\epsilon_0\\) is the permittivity of free space\n\\(\\vec{r}\\) is the position vector\n\\(r\\) is the distance from the charge\n\nLook up the plt.quiver() function in the Matplotlib documentation for plotting vector fields or check out the hint.\n\n\n\n\n\n\n\n\nUse np.meshgrid(x, y) to create X, Y grids\nCalculate R = sqrt(X¬≤ + Y¬≤) for distance\nCalculate Ex = X/R¬≥ and Ey = Y/R¬≥\nUse plt.quiver(X, Y, Ex, Ey) for the vector field\nRemember to avoid division by zero at the origin\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Exercise 2: Standing Wave Pattern\n\n\n\nVisualize a 2D standing wave pattern that might occur in a square membrane (like a drum head). This type of visualization is useful in understanding wave modes in musical instruments or quantum mechanical systems.\nThe wave function is given by: \\(\\psi(x,y,t) = \\sin(mx)\\cos(ny)\\)\nwhere: - \\(m, n\\) are mode numbers (integers) - \\(x, y\\) are positions on the membrane - \\(\\psi\\) is the displacement amplitude\n\n\n\n\n\n\n\n\nUse np.meshgrid(x, y) to create X, Y grids\nTry different mode numbers (m, n) for different patterns\nUse plt.contourf() for filled contours\nAdd a colorbar to show amplitude scale\nLook up the matplotlib plotting of surfaces with ax = plt.subplot(122, projection='3d') surf = ax.plot_surface()\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Exercise 3: Quantum Wave Packet\n\n\n\nCreate a 3D visualization of a Gaussian wave packet, which represents a localized quantum particle. This type of visualization is fundamental in understanding quantum mechanical states and probability distributions.\nThe wave function is given by: \\(\\psi(x,y) = A\\exp\\left(-\\frac{(x-x_0)^2 + (y-y_0)^2}{2\\sigma^2}\\right)\\)\nwhere: - \\(A\\) is the amplitude - \\((x_0, y_0)\\) is the center position - \\(\\sigma\\) is the width of the packet\n\n\n\n\n\n\n\n\nUse np.meshgrid(x, y) to create X, Y grids\nCalculate œà using the Gaussian formula\nCreate both a 3D surface plot and a 2D probability density plot\nUse as the probability density is |œà|¬≤\nLook up the matplotlib plotting of surfaces with ax = plt.subplot(133, projection='3d') surf = ax.plot_surface(X, Y, prob_density, cmap='viridis')\n\n\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "lectures/lecture14/3_huygens_principle.html",
    "href": "lectures/lecture14/3_huygens_principle.html",
    "title": "Huygens principle",
    "section": "",
    "text": "Huygens‚Äô principle is a theorem from wave optics that states that each point in space experiencing an electromagnetic wave acts as a source of spherical waves. This means that any wave can be expanded into a superposition of spherical waves, which is fundamental to phenomena like Mie scattering. While the classical understanding is that accelerated charges are the source of electromagnetic waves, Huygens‚Äô principle provides a powerful mathematical framework for describing wave propagation and diffraction effects."
  },
  {
    "objectID": "lectures/lecture14/3_huygens_principle.html#diffraction-pattern-of-a-single-slit",
    "href": "lectures/lecture14/3_huygens_principle.html#diffraction-pattern-of-a-single-slit",
    "title": "Huygens principle",
    "section": "Diffraction pattern of a single slit",
    "text": "Diffraction pattern of a single slit\nLet‚Äôs use Huygens principle to understand how waves behave when they pass through a small opening like a slit. We can do this by treating many points along the slit as sources of waves, and seeing how these waves combine. This will help us understand diffraction - what happens when waves bend around edges. Remember the function we wrote down the last lecture, i.e.\n\\[\\begin{equation}\nE=\\frac{E_{0}}{|\\vec{r}-\\vec{r}_{0}|}e^{i k|\\vec{r}-\\vec{r}_{0}|} e^{-i\\omega t}\n\\end{equation}\\]\nthat describes the spherical wave. We can use this as a python function\n\n\n\n\n\n\nto calculate the electric field at a point in space. Let‚Äôs show first of all that we can reconstruct a plane wave from many spherical waves. We can do this by summing up many spherical waves with the same wavevector \\(\\vec{k}\\) but different positions \\(\\vec{r}_{0}\\). The code below does this for a plane wave propagating in the z-direction.\n\n\n\n\n\n\nThe result nicely shows the emergence of a plane wave from a sum of spherical waves. The intensity pattern is almost constant in the x-z plane, which is what we expect from a plane wave. The samll deviations are the result of the limited number of sources we used to sum up the spherical waves.\nWe now want to do the same but only over a limited range of x. This is what we would do for a single slit. The code below does this for a single slit of width \\(d=2\\) ¬µm. The next cell defines the space for our calculation again. The value of \\(d\\) denotes the slit width, which we want to vary to see the effect of changing slit width vs.¬†wavelength, which we chose to be \\(\\lambda=532\\) nm.\n\n\n\n\n\n\nThe next cell sums up the electric field of 200 spherical waves in the x-z plane, similar to the plane_wave_sum() function above but limited to sources within the slit width, such that we can plot the intensity or the field in space. Like the plane wave example, this demonstrates how multiple spherical waves combine, but now with sources constrained to a finite region.\n\n\n\n\n\n\nLet us plot the wavefronts and the intensity pattern in space. As the intensity decays strongly with distance from the slit, we do that by taking the log of the intensity. The left plot shows the real part of the field, revealing the wave fronts as they emerge from the slit and propagate outward. The right plot shows the logarithm of the intensity pattern, which helps visualize how the light spreads out as it diffracts through the slit opening. The wave fronts start planar at the slit but gradually curve outward, while the intensity pattern shows characteristic diffraction fringes."
  },
  {
    "objectID": "lectures/lecture14/3_huygens_principle.html#farfield-vs.-nearfield",
    "href": "lectures/lecture14/3_huygens_principle.html#farfield-vs.-nearfield",
    "title": "Huygens principle",
    "section": "Farfield vs.¬†nearfield",
    "text": "Farfield vs.¬†nearfield\nWhen light passes through a slit, the pattern of light we observe depends on how far away we measure it from the slit. We can divide this into two regions: the ‚Äúnear field‚Äù (at distance \\(r \\sim \\lambda\\)) and the ‚Äúfar field‚Äù (at distance \\(r \\gg \\lambda\\)). In the near field, which is typically within a few wavelengths \\(\\lambda\\) from the slit, the light pattern closely resembles the shape of the slit itself. However, in the far field, which is at distances \\(r \\gg \\lambda\\), the light spreads out significantly and creates distinctive patterns of bright and dark bands. This spreading out of light is called diffraction. To demonstrate this difference, let‚Äôs look at two distances: a near field measurement at \\(r = 1\\) ¬µm from the slit, and a far field measurement at \\(r = 100\\) ¬µm away. These measurements will show how dramatically different the light patterns can be in these two regions.\n\n\n\n\n\n\nThe two plots below show the drastic difference between the diffraction pattern in the near field and the far field. The near field resembles indeed the shadow picture, while the far field intensity pattern is considerable wider than the slit. This even becomes worse, if we make the slit narrower."
  },
  {
    "objectID": "lectures/lecture14/3_huygens_principle.html#comparison-to-the-analytical-solution",
    "href": "lectures/lecture14/3_huygens_principle.html#comparison-to-the-analytical-solution",
    "title": "Huygens principle",
    "section": "Comparison to the analytical solution",
    "text": "Comparison to the analytical solution\nNow that we understand how to calculate the diffraction pattern by adding up many Huygens sources (those spherical waves we created above), we can compare this to what physicists have worked out mathematically. When physicists solve this problem on paper, they do essentially the same thing we did in our code - they add up the contributions from many points along the slit, each acting as a source of waves. After doing all the math (which involves some complex calculus we won‚Äôt worry about now), they get a relatively simple formula for the intensity pattern far from the slit:\n\\[\\begin{equation}\nI=I_{0}\\left (\\frac{\\sin(\\delta)}{\\delta}\\right )^2\n\\end{equation}\\]\nwhere \\[\\begin{equation}\n\\delta=\\frac{\\pi d}{\\lambda}\\sin(\\theta)\n\\end{equation}\\]\nHere, \\(I_0\\) is just the maximum intensity, \\(d\\) is the width of our slit, \\(\\lambda\\) is the wavelength of light we‚Äôre using, and \\(\\theta\\) is the angle away from the center of the pattern (imagine drawing a line from the slit to any point on our screen - \\(\\theta\\) is the angle between this line and the straight-ahead direction). Let‚Äôs see how well this mathematical formula matches up with our numerical calculation where we added up all those Huygens sources one by one.\n\n\n\n\n\n\n\n\n\n\n\n\nThe plot below compares two ways of calculating the same thing: the black dotted line shows our numerical simulation using 200 point sources of waves, while the solid black line shows the mathematical formula that physicists derived. As you can see, they match quite well! This tells us that our computer simulation using Huygens‚Äô principle (adding up lots of wave sources) gives nearly the same result as the mathematical solution.\nAn interesting question to consider is: how many wave sources do we need to get an accurate result? Right now we‚Äôre using 200 sources spread across the width of the slit, but would 100 be enough? What about 1000? You can modify the code above to try different numbers of sources and see how it affects the accuracy of the simulation compared to the mathematical solution. This helps us understand how many ‚Äúpoints‚Äù of light we need to consider to get a good approximation of reality.\n\n\n\n\n\n\nLet‚Äôs explore how changing different parameters affects our diffraction pattern! Two key things we can adjust are:\n\nThe wavelength of light (Œª) - try using red light (650nm) versus blue light (450nm)\nThe width of the slit (d) - what happens when you make it wider or narrower?\n\nYou can modify these values in the code above and observe how the pattern changes. What patterns do you notice? Does the diffraction spread out more or less when you use longer wavelengths? What about with wider slits?\nAn even more interesting case is when we have multiple slits side by side - this is called a diffraction grating. Diffraction gratings are used in many real-world applications, from spectrometers that analyze starlight to the rainbow patterns you see on DVDs and CDs.\nLet‚Äôs look at what happens with 10 slits in a row, each separated by a distance D. The mathematical formula for this gets a bit more complicated, but it‚Äôs just combining two effects:\n\nThe single-slit diffraction pattern we saw before (the sin(Œ¥)/Œ¥ term)\nA new term that accounts for how the waves from multiple slits interfere (the sin(NŒ≥)/sin(Œ≥) term)\n\nHere‚Äôs the full formula for the intensity pattern:\n\\[\\begin{equation}\nI=I_{0}\\left (\\frac{\\sin(\\delta)}{\\delta}\\right )^2\\left (\\frac{\\sin(N\\gamma)}{\\sin(\\gamma)}\\right )^2\n\\end{equation}\\]\nwhere\n\\[\\begin{equation}\n  \\gamma=\\frac{\\pi D}{\\lambda}\\sin(\\theta)\n\\end{equation}\\]\nN is the number of slits (10 in our case), D is the distance between slits, and the other terms are the same as before.\nTry modifying the code to simulate this multiple-slit case! Some questions to consider:\n\nWhat happens when you change the spacing D between slits?\nHow does the pattern change if you use more or fewer slits?\nCan you explain why a CD or DVD creates rainbow patterns in sunlight based on what you‚Äôve learned about diffraction gratings?"
  },
  {
    "objectID": "lectures/lecture14/3_huygens_principle.html#focus-pattern-of-a-spherical-mirror",
    "href": "lectures/lecture14/3_huygens_principle.html#focus-pattern-of-a-spherical-mirror",
    "title": "Huygens principle",
    "section": "Focus pattern of a spherical mirror",
    "text": "Focus pattern of a spherical mirror\nHuygens‚Äô principle can also be used to understand how light reflects off a spherical mirror. When light hits a mirror, it reflects off at an angle equal to the angle of incidence. This means that the light rays all converge at a single point called the focal point \\(f\\). We can use Huygens‚Äô principle to understand how this happens by treating each point on the mirror as a source of waves. The code below calculates the electric field at a point in space due to a spherical mirror with radius of curvature \\(R = 10\\) ¬µm.\n\n\n\n\n\n\nThe two images show the field and the intensity pattern of the spherical mirror. The field plot shows the wavefronts converging at the focal point, while the intensity plot shows the bright spot at the focal point where the light rays all converge. The spot in the center is the focal point. The intensity pattern is called the point-spread function of the mirror, and it tells us how light from a point source will spread out after reflecting off the mirror."
  },
  {
    "objectID": "lectures/lecture15/lecture15a.html",
    "href": "lectures/lecture15/lecture15a.html",
    "title": "Repetition Differential Equations",
    "section": "",
    "text": "In today‚Äôs lecture, we will review and expand upon the numerical methods for solving differential equations that you‚Äôve encountered in your mathematics courses. This is particularly important for physics students, as differential equations are the language in which many physical laws are written. Whether we‚Äôre dealing with \\(F = ma\\) in classical mechanics, Maxwell‚Äôs equations in electromagnetism, or Schr√∂dinger‚Äôs equation in quantum mechanics, the ability to solve differential equations is essential.\nWhile analytical solutions are elegant and preferred when available, many real-world physical problems require numerical approaches. We will focus on numerical methods that you can implement yourself, starting with the simplest approaches and building up to more sophisticated techniques.\nTo begin this review, we will first have a look back at the Taylor expansion of a function \\(f(t)\\) around a point \\(t_0\\):\n\\[\nf(t) = f(t_{0}) + (t - t_0) f^{\\prime}(t_0) + \\frac{(t - t_0)^2}{2!} f^{\\prime\\prime}(t_0) + \\frac{(t - t_0)^3}{3!} f^{(3)}(t_0) + \\ldots\n\\]\nwhich tells us that the function \\(f(t)\\) can be approximated by a polynomial of increasing order around the point \\(t_0\\). The first term is the value of the function at \\(t_0\\), the second term is the slope of the function at \\(t_0\\), the third term is the curvature of the function at \\(t_0\\), and so on.\nThe Taylor expansion is the basis for many numerical methods for solving differential equations. The simplest of these methods is the Euler method, which approximates the function by a straight line. We will start with the Euler method and then move on to more advanced techniques like the improved Euler method."
  },
  {
    "objectID": "lectures/lecture15/lecture15a.html#euler-method",
    "href": "lectures/lecture15/lecture15a.html#euler-method",
    "title": "Repetition Differential Equations",
    "section": "Euler Method",
    "text": "Euler Method\nIf we truncate the series at the first term, we get the linear approximation of the function:\n\\[\nf(t) \\approx f(t_{0}) + (t - t_0) f^{\\prime}(t)= f(t_{0}) + \\Delta t f^{\\prime}(t)\n\\]\nThis is the equation of a straight line with slope \\(f^{\\prime}(t)\\) and intercept \\(f(t_0)\\) and we abbreviated \\(\\Delta t = t - t_0\\).\n\nEuler Method for Radioactive Decay\nWe would like to try the method with some simple first order problem, which is the radioactive decay. The decay of a radioactive nucleus is described by the first-order differential equation:\n\\[\n\\frac{dN(t)}{dt} = -k N(t)\n\\]\nwhere \\(k\\) is a decay constant. If \\(N(t)\\) is the number of radioactive nuclei at time \\(t\\), the equation tells us that the rate of decay is proportional to the number of nuclei present and some rate constant \\(k\\). The solution to this differential equation is:\n\\[\nN(t) = N(0) e^{-kt}\n\\]\nwhere \\(N(0)\\) is the number of nuclei at time \\(t=0\\). We therefore need to know some initial condition, which we define as \\(N(0) = 100\\). Comparing to our Taylor expansion, we can see that \\(N=f(t)\\) and the slope of the function is \\(f^{\\prime}(t) = -kN(t)\\). Further we have the value of the function at \\(t_0 = 0\\). We can now use the linear approximation to solve the ODE numerically. Inserting the values into the linear approximation, we get:\n\\[\nN(t + \\Delta t) = N(t) + \\Delta t f^{\\prime}(t)\n\\]\nWe can now implement the Euler method to solve the ODE numerically.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution Radioactive Decay\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImproved Euler Method\nWhile the above code is what we did fo the simples numerical integration, you can now modify the code inside the euler function to implement any improved Euler method. An improved Euler method (also called Heun‚Äôs method) is based on averaging the slopes at two points. To understand why this works better than the simple Euler method, let‚Äôs look at the Taylor expansion again around \\(t_0\\):\n\\[\nf(t_0 + \\Delta t) \\approx f(t_0) + \\Delta t f'(t_0)\n\\]\nWe can improve this simple formula by taking the avarage of the derivative at \\(t_0\\) and the derivative at the predicted point \\(t_0 + \\Delta t\\):\n\\[\nf(t_0 + \\Delta t) \\approx f(t_0) + \\frac{\\Delta t}{2} (f'(t_0) + f'(t_0 + \\Delta t))\n\\]\nThis effectively now introduces the curvature of the function at \\(t_0\\) into the approximation. We can see this by expanding the improved Euler method in a Taylor series. The first derivative at \\(t_0\\) is \\(f'(t_0)\\), and the derivative at the predicted point \\(f'(t_0 + \\Delta t)\\) can be expanded as \\(f'(t_0) + \\Delta t f''(t_0)\\). When we average these two derivatives and multiply by \\(\\Delta t\\), we get \\(\\Delta t f'(t_0) + \\frac{\\Delta t^2}{2} f''(t_0)\\), which matches the first two terms in the Taylor expansion. This is why the improved Euler method has an error of order \\(O(\\Delta t^2)\\) compared to \\(O(\\Delta t)\\) for the simple Euler method.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution Radioactive Decay Improved Euler\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis already provides a very good approximation to the analytical solution.\n\n\nHigher order differential equations\nThe above methods can be easily extended to higher order differential equations. For example, the second order differential equation\n\\[\n\\frac{d^2 y}{dt^2} = -k^2 y\n\\]\ncan be converted into two first order differential equations by introducing a new variable \\(v = \\frac{dy}{dt}\\):\n\\[\n\\begin{align}\n\\frac{dy}{dt} &= v \\\\\n\\frac{dv}{dt} &= -k^2 y\n\\end{align}\n\\]\nWe can now use the improved Euler method to solve these two equations simultaneously. The code below shows how to solve the above second order differential equation.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution Harmonic Oscillator"
  },
  {
    "objectID": "lectures/lecture15/lecture15a.html#solving-coupled-differential-equations",
    "href": "lectures/lecture15/lecture15a.html#solving-coupled-differential-equations",
    "title": "Repetition Differential Equations",
    "section": "Solving Coupled Differential Equations",
    "text": "Solving Coupled Differential Equations\nWe may also solve systems of coupled differential equations such as the Lotka-Volterra equations which describe predator-prey dynamics. In this system, we have two populations \\(x(t)\\) (prey) and \\(y(t)\\) (predators) that interact according to:\n\\[\n\\begin{align}\n\\frac{dx}{dt} &= \\alpha x - \\beta xy \\\\\n\\frac{dy}{dt} &= \\delta xy - \\gamma y\n\\end{align}\n\\]\nwhere \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\), and \\(\\delta\\) are positive constants representing the interaction between the species. Here \\(\\alpha\\) is the growth rate of prey in absence of predators, \\(\\beta\\) is the rate at which predators eat prey, \\(\\delta\\) is the efficiency of turning eaten prey into new predators, and \\(\\gamma\\) is the death rate of predators in absence of prey.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution Lotka-Volterra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Lotka-Volterra equations show the cyclic behavior of predator-prey populations. The prey population grows until it reaches a maximum, at which point the predator population increases due to the abundance of prey. The predators then reduce the prey population, leading to a decrease in the predator population. This cycle continues indefinitely, demonstrating the complex dynamics of ecological systems.\n\n\n\n\n\n\nThe population values below 1 mean that we have less than one individual in our population, which is not physically meaningful in a biological context. Our model treats populations as continuous variables, but real populations consist of discrete individuals. When the numerical solution shows population values less than 1, this indicates that the model has entered a regime where its continuous approximation breaks down and we should instead use a discrete model that can properly handle individual organisms. In practice, once a population drops below 1, we should consider it extinct.\nThis limitation of continuous models is particularly important in conservation biology and population management, where understanding the threshold for population viability is crucial. When modeling endangered species or small populations, we often need to switch to discrete population models or include additional factors like demographic stochasticity that become important at low population numbers."
  },
  {
    "objectID": "lectures/lecture12/wave_equation.html",
    "href": "lectures/lecture12/wave_equation.html",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import sparse\n\n# Parameters\nL = 1.0        # length of string\nT = 2.0        # total time\nc = 1.0        # wave speed\nnx = 100       # spatial points\nnt = 1000      # time points\ndx = L/nx\ndt = T/nt\n\n# Check stability (CFL condition)\nif c*dt/dx &gt; 1:\n    print(\"Warning: Solution may be unstable!\")\n\n# Initialize arrays\nx = np.linspace(0, L, nx)\nt = np.linspace(0, T, nt)\n\n# Initial conditions (for example, a Gaussian pulse)\nu = np.exp(-(x-L/2)**2/0.01)  # initial displacement\nv = np.zeros_like(x)           # initial velocity\n\n# Create matrices for spatial derivatives\ndiagonals = [[1], [-2], [1]]\noffsets = [-1, 0, 1]\nD2 = sparse.diags(diagonals, offsets, shape=(nx-2, nx-2))/(dx**2)\n\n# Time stepping matrices\nI = sparse.eye(nx-2)\nA = I - (c*dt/2)**2 * D2\nB = 2*I + (c*dt)**2 * D2\n\n# Arrays to store solution\nu_history = np.zeros((nt, nx))\nu_history[0, 1:-1] = u[1:-1]\n\n# First time step (using forward difference)\nu_history[1, 1:-1] = u[1:-1] + dt*v[1:-1] + (c*dt)**2/2 * D2.dot(u[1:-1])\n\n# Main time-stepping loop\nfor n in range(1, nt-1):\n    # Solve for next time step\n    rhs = B.dot(u_history[n, 1:-1]) - A.dot(u_history[n-1, 1:-1])\n    u_history[n+1, 1:-1] = sparse.linalg.spsolve(A, rhs)\n\n# Plot results\nplt.figure(figsize=(10, 6))\nplt.imshow(u_history, aspect='auto', extent=[0, L, T, 0])\nplt.colorbar(label='Displacement')\nplt.xlabel('Position')\nplt.ylabel('Time')\nplt.title('Wave Equation Solution')\nplt.show()\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.sparse import diags\nfrom scipy.sparse.linalg import solve\n\ndef crank_nicolson_wave(L=1.0, T=1.0, nx=100, nt=1000, c=1.0):\n    \"\"\"\n    Solve the wave equation using Crank-Nicolson scheme\n    ‚àÇ¬≤u/‚àÇt¬≤ = c¬≤(‚àÇ¬≤u/‚àÇx¬≤)\n\n    Parameters:\n    L: length of spatial domain\n    T: total time\n    nx: number of spatial points\n    nt: number of time points\n    c: wave speed\n    \"\"\"\n\n    # Set up grid\n    dx = L/nx\n    dt = T/nt\n    x = np.linspace(0, L, nx+1)\n    t = np.linspace(0, T, nt+1)\n\n    # Compute stability parameter\n    r = (c*dt/dx)**2\n\n    # Initialize solution array\n    u = np.zeros((nt+1, nx+1))\n\n    # Initial condition (plane wave)\n    k = 2*np.pi/L  # wavenumber\n    u[0,:] = np.sin(k*x)  # Initial displacement\n\n    # Initial velocity\n    u[1,:] = u[0,:] + c*k*np.cos(k*x)*dt\n\n    # Set up matrices for implicit scheme\n    main_diag = 2*(1-r)*np.ones(nx-1)\n    off_diag = r*np.ones(nx-2)\n\n    A = diags([off_diag, main_diag, off_diag], [-1, 0, 1], format='csc')\n\n    # Time stepping\n    for n in range(1, nt):\n        # Right hand side\n        b = 2*u[n,1:-1] - u[n-1,1:-1]\n\n        # Solve system\n        u[n+1,1:-1] = solve(A, b)\n\n        # Apply boundary conditions (periodic)\n        u[n+1,0] = u[n+1,-2]\n        u[n+1,-1] = u[n+1,1]\n\n    return x, t, u\n\n# Run simulation\nL = 1.0  # Length of domain\nT = 1.0  # Total time\nnx = 100  # Number of spatial points\nnt = 1000  # Number of time points\nc = 1.0   # Wave speed\n\nx, t, u = crank_nicolson_wave(L, T, nx, nt, c)\n\n# Plot results at specific times\nplt.figure(figsize=(12, 8))\ntimes = [0, 0.25, 0.5, 0.75, 1.0]  # Times at which to show solution\nfor time in times:\n    time_index = int(time * nt/T)\n    plt.plot(x, u[time_index,:], label=f't = {time:.2f}')\n\nplt.xlabel('Position (z)')\nplt.ylabel('Electric field amplitude (E)')\nplt.title('Electromagnetic Wave Propagation')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Optional: Print maximum amplitude at each time for stability check\nprint(\"\\nMaximum amplitudes at selected times:\")\nfor time in times:\n    time_index = int(time * nt/T)\n    print(f\"t = {time:.2f}: {np.max(np.abs(u[time_index,:])):.6f}\")"
  },
  {
    "objectID": "lectures/lecture12/3_diffusion_equation copy.html",
    "href": "lectures/lecture12/3_diffusion_equation copy.html",
    "title": "Diffusion equation",
    "section": "",
    "text": "diffusionViz = {\n  const width = 800, height = 400;\n  const margin = {top: 30, right: 30, bottom: 30, left: 40};\n\n  // Grid parameters\n  const nx = 20;  // number of spatial points\n  const nt = 10;  // number of time points\n  const dx = 1.0/(nx-1);\n  const dt = 0.1;\n  const D = 0.1;\n\n  // Create SVG\n  const svg = d3.create(\"svg\")\n    .attr(\"width\", width)\n    .attr(\"height\", height);\n\n  // Setup scales\n  const xScale = d3.scaleLinear()\n    .domain([0, 1])\n    .range([margin.left, width - margin.right]);\n\n  const yScale = d3.scaleLinear()\n    .domain([0, (nt-1)*dt])\n    .range([margin.top, height - margin.bottom]);\n\n  // Draw grid\n  for(let i = 0; i &lt; nx; i++) {\n    svg.append(\"line\")\n      .attr(\"x1\", xScale(i*dx))\n      .attr(\"y1\", margin.top)\n      .attr(\"x2\", xScale(i*dx))\n      .attr(\"y2\", height - margin.bottom)\n      .attr(\"stroke\", \"#ddd\")\n      .attr(\"stroke-width\", 1);\n  }\n\n  for(let j = 0; j &lt; nt; j++) {\n    svg.append(\"line\")\n      .attr(\"x1\", margin.left)\n      .attr(\"y1\", yScale(j*dt))\n      .attr(\"x2\", width - margin.right)\n      .attr(\"y2\", yScale(j*dt))\n      .attr(\"stroke\", \"#ddd\")\n      .attr(\"stroke-width\", 1);\n  }\n\n  // Create solution array\n  let u = Array(nt).fill().map(() =&gt; Array(nx).fill(0));\n\n  // Initial condition (Gaussian)\n  u[0] = Array.from({length: nx}, (_, i) =&gt; {\n    const x = i*dx;\n    const sigma = 0.1, mu = 0.5;\n    const exponent = -Math.pow(x-mu, 2)/(2*Math.pow(sigma, 2));\n    return Math.exp(exponent);\n  });\n\n  // Compute solution\n  for(let j = 0; j &lt; nt-1; j++) {\n    for(let i = 1; i &lt; nx-1; i++) {\n      u[j+1][i] = u[j][i] + D*dt/(dx*dx)*(u[j][i+1] - 2*u[j][i] + u[j][i-1]);\n    }\n  }\n\n  // Draw solution points\n  const points = svg.append(\"g\");\n  let currentTime = 0;\n\n  function update() {\n    // Clear previous points\n    points.selectAll(\"*\").remove();\n\n    // Draw all points up to current time\n    for(let j = 0; j &lt;= currentTime; j++) {\n      for(let i = 0; i &lt; nx; i++) {\n        points.append(\"circle\")\n          .attr(\"cx\", xScale(i*dx))\n          .attr(\"cy\", yScale(j*dt))\n          .attr(\"r\", 3)\n          .attr(\"fill\", u[j][i] &gt; 0.1 ? \"steelblue\" : \"#ddd\");\n      }\n    }\n\n    // Highlight current computation points\n    if(currentTime &lt; nt-1) {\n      for(let i = 1; i &lt; nx-1; i++) {\n        // Previous time points (inputs)\n        points.append(\"circle\")\n          .attr(\"cx\", xScale((i-1)*dx))\n          .attr(\"cy\", yScale(currentTime*dt))\n          .attr(\"r\", 5)\n          .attr(\"fill\", \"orange\");\n\n        points.append(\"circle\")\n          .attr(\"cx\", xScale(i*dx))\n          .attr(\"cy\", yScale(currentTime*dt))\n          .attr(\"r\", 5)\n          .attr(\"fill\", \"orange\");\n\n        points.append(\"circle\")\n          .attr(\"cx\", xScale((i+1)*dx))\n          .attr(\"cy\", yScale(currentTime*dt))\n          .attr(\"r\", 5)\n          .attr(\"fill\", \"orange\");\n\n        // Current point being computed\n        points.append(\"circle\")\n          .attr(\"cx\", xScale(i*dx))\n          .attr(\"cy\", yScale((currentTime+1)*dt))\n          .attr(\"r\", 5)\n          .attr(\"fill\", \"red\");\n      }\n    }\n\n    currentTime = (currentTime + 1) % nt;\n  }\n\n  // Add axes\n  svg.append(\"g\")\n    .attr(\"transform\", `translate(0,${height - margin.bottom})`)\n    .call(d3.axisBottom(xScale).ticks(nx-1));\n\n  svg.append(\"g\")\n    .attr(\"transform\", `translate(${margin.left},0)`)\n    .call(d3.axisLeft(yScale).ticks(nt-1));\n\n  // Add labels\n  svg.append(\"text\")\n    .attr(\"x\", width/2)\n    .attr(\"y\", height - 5)\n    .attr(\"text-anchor\", \"middle\")\n    .text(\"Space (x)\");\n\n  svg.append(\"text\")\n    .attr(\"transform\", \"rotate(-90)\")\n    .attr(\"x\", -height/2)\n    .attr(\"y\", 15)\n    .attr(\"text-anchor\", \"middle\")\n    .text(\"Time (t)\");\n\n  d3.interval(update, 1000);\n\n  return svg.node();\n}"
  },
  {
    "objectID": "lectures/lecture01/02-lecture01.html",
    "href": "lectures/lecture01/02-lecture01.html",
    "title": "Variables & Numbers",
    "section": "",
    "text": "In the previous lessons, you created plots and calculations using code like:\nm = 2.0    # mass\ng = 9.81   # gravity\nh = 10.0   # height\nE_pot = m * g * h\nBut what exactly are m, g, and h? These are variables - containers that store values you can use in calculations.\n\n\n\n\n\n\nWhy Variables Matter in Physics\n\n\n\nIn physics, we work with:\n\nPhysical quantities: mass, velocity, energy, time\nConstants: gravitational acceleration, speed of light, Planck‚Äôs constant\nCalculated results: kinetic energy, momentum, force\n\nVariables let us store these values and use them in equations, just like in mathematical physics notation!",
    "crumbs": [
      "üéØ Week 2: Python Building Blocks",
      "Variables & Numbers (What You Just Used)"
    ]
  },
  {
    "objectID": "lectures/lecture01/02-lecture01.html#what-you-just-used",
    "href": "lectures/lecture01/02-lecture01.html#what-you-just-used",
    "title": "Variables & Numbers",
    "section": "",
    "text": "In the previous lessons, you created plots and calculations using code like:\nm = 2.0    # mass\ng = 9.81   # gravity\nh = 10.0   # height\nE_pot = m * g * h\nBut what exactly are m, g, and h? These are variables - containers that store values you can use in calculations.\n\n\n\n\n\n\nWhy Variables Matter in Physics\n\n\n\nIn physics, we work with:\n\nPhysical quantities: mass, velocity, energy, time\nConstants: gravitational acceleration, speed of light, Planck‚Äôs constant\nCalculated results: kinetic energy, momentum, force\n\nVariables let us store these values and use them in equations, just like in mathematical physics notation!",
    "crumbs": [
      "üéØ Week 2: Python Building Blocks",
      "Variables & Numbers (What You Just Used)"
    ]
  },
  {
    "objectID": "lectures/lecture01/02-lecture01.html#variables-in-python",
    "href": "lectures/lecture01/02-lecture01.html#variables-in-python",
    "title": "Variables & Numbers",
    "section": "Variables in Python",
    "text": "Variables in Python\n\nSymbol Names\nIn physics, we use symbols like \\(m\\) for mass, \\(v\\) for velocity, and \\(F\\) for force. Python variable names work similarly, but with some rules.\nVariable names in Python can include alphanumerical characters a-z, A-Z, 0-9, and the special character _. Normal variable names must start with a letter or an underscore. By convention, variable names typically start with a lower-case letter, while Class names start with a capital letter and internal variables start with an underscore.\n\n\n\n\n\n\nPhysics Variable Naming Tips\n\n\n\nGood physics variable names:\n\nm, mass for mass\nv, velocity for velocity\nE_kin, E_pot for kinetic and potential energy\ntheta, phi for angles (Greek letters spelled out)\ng for gravitational acceleration\nc for speed of light\n\nDescriptive names for clarity:\n\nelectron_mass instead of just m when you have multiple masses\ninitial_velocity, final_velocity instead of v1, v2\n\n\n\n\n\n\n\n\n\nReserved Keywords\n\n\n\nThere are a number of Python keywords that cannot be used as variable names because Python uses them for other things. These keywords are:\nand, as, assert, break, class, continue, def, del, elif, else, except, exec, finally, for, from, global, if, import, in, is, lambda, not, or, pass, print, raise, return, try, while, with, yield\nImportant for physics: The keyword lambda cannot be used as a variable name (common for wavelength). Use lambda_ or wavelength instead!\n\n\n\n\nVariable Assignment\nThe assignment operator in Python is =. Python is a dynamically typed language, so we do not need to specify the type of a variable when we create one.\nLet‚Äôs assign some physics values:\n\n\n\n\n\n\nImportant: In Python, = means assignment, not equality!\n\nMath: \\(F = ma\\) (force equals mass times acceleration)\nPython: F = m * a (calculate m*a and store in F)\n\nAlthough not explicitly specified, a variable does have a type associated with it (e.g., integer, float, string). The type is derived from the value that was assigned to it. To determine the type of a variable, we can use the type function.\n\n\n\n\n\n\nMost physics calculations use floats (floating-point numbers) because they have decimal points.\nIf we assign a new value to a variable, its type can change.\n\n\n\n\n\n\nIf we try to use a variable that has not yet been defined, we get a NameError error.\n\n\n\n\n\n\nCommon Mistake\n\n\n\nTrying to use a variable before defining it:\nE_kin = 0.5 * m * v**2  # ERROR if m and v aren't defined yet!\nAlways define variables before using them in calculations.",
    "crumbs": [
      "üéØ Week 2: Python Building Blocks",
      "Variables & Numbers (What You Just Used)"
    ]
  },
  {
    "objectID": "lectures/lecture01/02-lecture01.html#number-types",
    "href": "lectures/lecture01/02-lecture01.html#number-types",
    "title": "Variables & Numbers",
    "section": "Number Types",
    "text": "Number Types\nPython supports various number types, including integers, floating-point numbers, and complex numbers. In physics, you‚Äôll primarily use floats for continuous quantities and integers for counting (like number of particles, timesteps, etc.).\n\n\n\n\n\n\nWhich Type for Physics?\n\n\n\n\nFloats: Mass, energy, position, velocity, time ‚Üí anything with units and decimals\nIntegers: Particle counts, loop iterations, array indices\nComplex: Quantum mechanics, wave functions, AC circuits\n\n\n\n\nComparison of Number Types\n\n\n\n\n\n\n\n\n\nType\nExample\nDescription\nPhysics Use Cases\n\n\n\n\nint\n42\nWhole numbers\nParticle count, quantum number \\(n\\), array index\n\n\nfloat\n3.14159\nDecimal numbers (15-17 digit precision)\nMass, energy, position, velocity, time\n\n\ncomplex\n2 + 3j\nNumbers with real and imaginary parts\nWave functions \\(\\psi\\), AC impedance\n\n\nbool\nTrue / False\nLogical values\nCollision detection, boundary conditions\n\n\n\n\n\n\n\n\n\nExamples for Number Types\n\n\n\n\n\n\nIntegers\nInteger Representation: Integers are whole numbers without a decimal point.\n\n\n\n\n\n\nPhysics Example:\n\n\n\n\n\n\nBinary, Octal, and Hexadecimal: Integers can be represented in different bases:\n\n\n\n\n\n\n\n\nFloating Point Numbers\nFloating Point Representation: Numbers with a decimal point are treated as floating-point values. This is what you‚Äôll use most in physics!\n\n\n\n\n\n\nScientific Notation:\n\n\n\n\n\n\nMaximum Float Value: Python handles large floats, converting them to infinity if they exceed the maximum representable value (~10¬≥‚Å∞‚Å∏).\n\n\n\n\n\n\n\n\n\n\n\n\nFloat Precision in Physics\n\n\n\nFloats have ~15-17 significant digits. This is usually more than enough for physics calculations, but be aware:\ng = 9.81                    # Good enough for most problems\ng_precise = 9.80665         # Standard gravity (more digits)\nFor most undergraduate physics, 3-4 significant figures are sufficient!\n\n\n\n\nComplex Numbers\nComplex Number Representation: Complex numbers have a real and an imaginary part. Essential in quantum mechanics and AC circuit analysis!\n\n\n\n\n\n\n\n\n\n\n\n\nPhysics Applications\n\n\n\nQuantum Mechanics: Wave functions are complex: \\(\\psi = a + bi\\)\nAC Circuits: Impedance \\(Z = R + iX\\) where \\(R\\) is resistance, \\(X\\) is reactance\nWave Propagation: \\(E = E_0 e^{i(kx - \\omega t)}\\)\n\n\n\nAccessors for Complex Numbers:\n\nc.real: Real part of the complex number.\nc.imag: Imaginary part of the complex number.\n\n\n\n\n\n\n\n\nComplex Conjugate: Use the .conjugate() method to get the complex conjugate (important for calculating probabilities in QM: \\(|\\psi|^2 = \\psi \\cdot \\psi^*\\)).\n\n\n\n\n\n\nAbsolute Value (Magnitude):",
    "crumbs": [
      "üéØ Week 2: Python Building Blocks",
      "Variables & Numbers (What You Just Used)"
    ]
  },
  {
    "objectID": "lectures/lecture01/02-summary01.html",
    "href": "lectures/lecture01/02-summary01.html",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "Click to expand Python Basics Cheat Sheet\n\n\n\n\n\n\n\n\n\nx = 1.0  # Assigns 1.0 to variable x\nmy_variable = \"Hello\"  # Assigns string \"Hello\"\n\n\n\nUse a-z, A-Z, 0-9, and _\nStart with letter or underscore\nCase-sensitive\nAvoid reserved keywords\n\n\n\n\n\n\n\n\nType\nExample\nDescription\n\n\n\n\nint\n5\nWhole numbers\n\n\nfloat\n3.14\nDecimal numbers\n\n\ncomplex\n2 + 3j\nReal + imaginary\n\n\nbool\nTrue\nLogical values\n\n\n\n\n\n\nint_num = int(3.14)    # 3\nfloat_num = float(5)   # 5.0\nstr_num = str(42)      # \"42\"\n\n\n\n\na, b = 10, 3\nsum_result = a + b   # Addition\ndiff_result = a - b  # Subtraction\nprod_result = a * b  # Multiplication\ndiv_result = a / b   # Division (float)\nint_div_result = a // b  # Integer division\nmod_result = a % b   # Modulus\npower_result = a ** b  # Exponentiation\n\n\n\nc = 2 + 4j\nreal_part = c.real     # 2.0\nimag_part = c.imag     # 4.0\nconjugate = c.conjugate()  # 2 - 4j\n\n\n\ntype(variable)  # Returns type\nisinstance(variable, type)  # Checks type\n\n\n\nimport math\n\nsqrt_result = math.sqrt(16)\nlog_result = math.log(100, 10)\nsin_result = math.sin(math.pi/2)"
  },
  {
    "objectID": "lectures/lecture01/02-summary01.html#python-basics-cheat-sheet",
    "href": "lectures/lecture01/02-summary01.html#python-basics-cheat-sheet",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "Click to expand Python Basics Cheat Sheet\n\n\n\n\n\n\n\n\n\nx = 1.0  # Assigns 1.0 to variable x\nmy_variable = \"Hello\"  # Assigns string \"Hello\"\n\n\n\nUse a-z, A-Z, 0-9, and _\nStart with letter or underscore\nCase-sensitive\nAvoid reserved keywords\n\n\n\n\n\n\n\n\nType\nExample\nDescription\n\n\n\n\nint\n5\nWhole numbers\n\n\nfloat\n3.14\nDecimal numbers\n\n\ncomplex\n2 + 3j\nReal + imaginary\n\n\nbool\nTrue\nLogical values\n\n\n\n\n\n\nint_num = int(3.14)    # 3\nfloat_num = float(5)   # 5.0\nstr_num = str(42)      # \"42\"\n\n\n\n\na, b = 10, 3\nsum_result = a + b   # Addition\ndiff_result = a - b  # Subtraction\nprod_result = a * b  # Multiplication\ndiv_result = a / b   # Division (float)\nint_div_result = a // b  # Integer division\nmod_result = a % b   # Modulus\npower_result = a ** b  # Exponentiation\n\n\n\nc = 2 + 4j\nreal_part = c.real     # 2.0\nimag_part = c.imag     # 4.0\nconjugate = c.conjugate()  # 2 - 4j\n\n\n\ntype(variable)  # Returns type\nisinstance(variable, type)  # Checks type\n\n\n\nimport math\n\nsqrt_result = math.sqrt(16)\nlog_result = math.log(100, 10)\nsin_result = math.sin(math.pi/2)"
  },
  {
    "objectID": "lectures/lecture01/01-plotting-basics.html",
    "href": "lectures/lecture01/01-plotting-basics.html",
    "title": "Plotting Basics",
    "section": "",
    "text": "As physicists, we need to visualize our data and calculations. A good plot can:\n\nReveal patterns and relationships\nMake predictions visible\nCommunicate results effectively\nHelp us understand complex phenomena\n\nPython‚Äôs Matplotlib library is the standard tool for creating scientific plots.",
    "crumbs": [
      "üöÄ Week 1: Your First Physics Code",
      "Quick Win: Plotting Your First Graph"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-plotting-basics.html#why-plotting-matters-in-physics",
    "href": "lectures/lecture01/01-plotting-basics.html#why-plotting-matters-in-physics",
    "title": "Plotting Basics",
    "section": "",
    "text": "As physicists, we need to visualize our data and calculations. A good plot can:\n\nReveal patterns and relationships\nMake predictions visible\nCommunicate results effectively\nHelp us understand complex phenomena\n\nPython‚Äôs Matplotlib library is the standard tool for creating scientific plots.",
    "crumbs": [
      "üöÄ Week 1: Your First Physics Code",
      "Quick Win: Plotting Your First Graph"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-plotting-basics.html#getting-started-with-matplotlib",
    "href": "lectures/lecture01/01-plotting-basics.html#getting-started-with-matplotlib",
    "title": "Plotting Basics",
    "section": "Getting Started with Matplotlib",
    "text": "Getting Started with Matplotlib\nMatplotlib is not built into Python - we need to import it:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n\n\n\n\nConvention\n\n\n\nWe import matplotlib.pyplot as plt - this is the standard shorthand used everywhere in Python!",
    "crumbs": [
      "üöÄ Week 1: Your First Physics Code",
      "Quick Win: Plotting Your First Graph"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-plotting-basics.html#your-first-plot",
    "href": "lectures/lecture01/01-plotting-basics.html#your-first-plot",
    "title": "Plotting Basics",
    "section": "Your First Plot",
    "text": "Your First Plot\nThe most basic plotting command is:\nplt.plot(x, y)\nplt.show()\nLet‚Äôs plot a sine wave:\n\n\n\n\n\n\n\n\n\n\n\n\nWhat This Does\n\n\n\n\nnp.linspace(0, 4*np.pi, 100): Creates 100 evenly-spaced points from 0 to 4œÄ\nplt.figure(figsize=(6, 4)): Creates a new figure that‚Äôs 6√ó4 inches\nplt.plot(x, y): Plots y versus x\nplt.show(): Displays the plot",
    "crumbs": [
      "üöÄ Week 1: Your First Physics Code",
      "Quick Win: Plotting Your First Graph"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-plotting-basics.html#adding-labels",
    "href": "lectures/lecture01/01-plotting-basics.html#adding-labels",
    "title": "Plotting Basics",
    "section": "Adding Labels",
    "text": "Adding Labels\nA plot without labels is useless! Always add: - x-axis label: What‚Äôs on the horizontal axis? - y-axis label: What‚Äôs on the vertical axis? - Title: What does this plot show?\n\n\n\n\n\n\n\n\n\n\n\n\nUsing LaTeX in Labels\n\n\n\nYou can use mathematical notation in labels with LaTeX syntax:\nplt.xlabel(r'$\\theta$ (radians)')  # Greek letters\nplt.ylabel(r'$\\sin(\\theta)$')      # Math expressions\nThe r before the string means ‚Äúraw string‚Äù - it treats backslashes literally.",
    "crumbs": [
      "üöÄ Week 1: Your First Physics Code",
      "Quick Win: Plotting Your First Graph"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-plotting-basics.html#physics-example-projectile-motion",
    "href": "lectures/lecture01/01-plotting-basics.html#physics-example-projectile-motion",
    "title": "Plotting Basics",
    "section": "Physics Example: Projectile Motion",
    "text": "Physics Example: Projectile Motion\nLet‚Äôs plot something more physics-y! A ball launched at 45¬∞:\n\n\n\n\n\n\n\n\n\n\n\n\nWhat‚Äôs New?\n\n\n\n\nplt.grid(True, alpha=0.3): Adds a grid (alpha=0.3 makes it semi-transparent)\nPhysics equations turned into beautiful visualization!",
    "crumbs": [
      "üöÄ Week 1: Your First Physics Code",
      "Quick Win: Plotting Your First Graph"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-plotting-basics.html#customizing-your-plot",
    "href": "lectures/lecture01/01-plotting-basics.html#customizing-your-plot",
    "title": "Plotting Basics",
    "section": "Customizing Your Plot",
    "text": "Customizing Your Plot\nYou can control colors, line styles, and markers:\n\nColor and Line Style Options\nplt.plot(x, y, 'r-')   # red solid line\nplt.plot(x, y, 'b--')  # blue dashed line\nplt.plot(x, y, 'go')   # green circles\nplt.plot(x, y, 'k:')   # black dotted line\nColor codes: r (red), g (green), b (blue), k (black), c (cyan), m (magenta), y (yellow)\nLine styles: - (solid), -- (dashed), : (dotted), -. (dash-dot)\nMarkers: o (circle), s (square), ^ (triangle), * (star), + (plus)\n\n\nExample",
    "crumbs": [
      "üöÄ Week 1: Your First Physics Code",
      "Quick Win: Plotting Your First Graph"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-plotting-basics.html#adding-a-legend",
    "href": "lectures/lecture01/01-plotting-basics.html#adding-a-legend",
    "title": "Plotting Basics",
    "section": "Adding a Legend",
    "text": "Adding a Legend\nWhen you have multiple lines, use a legend to identify them:\n\n\n\n\n\n\n\n\n\n\n\n\nLegend Locations\n\n\n\nCommon locations: 'upper right', 'upper left', 'lower right', 'lower left', 'center', 'best' (automatic)",
    "crumbs": [
      "üöÄ Week 1: Your First Physics Code",
      "Quick Win: Plotting Your First Graph"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-plotting-basics.html#physics-example-damped-oscillator",
    "href": "lectures/lecture01/01-plotting-basics.html#physics-example-damped-oscillator",
    "title": "Plotting Basics",
    "section": "Physics Example: Damped Oscillator",
    "text": "Physics Example: Damped Oscillator\nLet‚Äôs visualize different damping regimes:",
    "crumbs": [
      "üöÄ Week 1: Your First Physics Code",
      "Quick Win: Plotting Your First Graph"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-plotting-basics.html#plotting-data-points-not-just-lines",
    "href": "lectures/lecture01/01-plotting-basics.html#plotting-data-points-not-just-lines",
    "title": "Plotting Basics",
    "section": "Plotting Data Points (Not Just Lines)",
    "text": "Plotting Data Points (Not Just Lines)\nSometimes you want to show discrete data points:\n\n\n\n\n\n\n\n\n\n\n\n\nMarkers Without Lines\n\n\n\nUse 'ro' (red circles) to plot only markers without connecting lines. Use 'r-o' to plot both line and markers.",
    "crumbs": [
      "üöÄ Week 1: Your First Physics Code",
      "Quick Win: Plotting Your First Graph"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-plotting-basics.html#saving-your-plots",
    "href": "lectures/lecture01/01-plotting-basics.html#saving-your-plots",
    "title": "Plotting Basics",
    "section": "Saving Your Plots",
    "text": "Saving Your Plots\nTo save a plot to a file:\nplt.savefig('my_plot.png')        # PNG format\nplt.savefig('my_plot.pdf')        # PDF format (best for papers!)\nplt.savefig('my_plot.png', dpi=300)  # High resolution PNG\nAlways save before plt.show()!\n\n\n\n\n\n\n\n\n\n\n\n\nFile Formats\n\n\n\n\nPDF: Best for inclusion in LaTeX documents and papers\nPNG: Good for presentations and web\nSVG: Vector format, good for editing in Inkscape/Illustrator",
    "crumbs": [
      "üöÄ Week 1: Your First Physics Code",
      "Quick Win: Plotting Your First Graph"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-plotting-basics.html#quick-reference-essential-commands",
    "href": "lectures/lecture01/01-plotting-basics.html#quick-reference-essential-commands",
    "title": "Plotting Basics",
    "section": "Quick Reference: Essential Commands",
    "text": "Quick Reference: Essential Commands\n# Setup\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Create plot\nplt.figure(figsize=(6, 4))\nplt.plot(x, y, 'b-', linewidth=2, label='My Data')\n\n# Labels and title\nplt.xlabel('X Label')\nplt.ylabel('Y Label')\nplt.title('My Plot')\n\n# Extras\nplt.legend(loc='best')\nplt.grid(True, alpha=0.3)\n\n# Save and show\nplt.savefig('filename.pdf')\nplt.show()",
    "crumbs": [
      "üöÄ Week 1: Your First Physics Code",
      "Quick Win: Plotting Your First Graph"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-plotting-basics.html#practice-exercises",
    "href": "lectures/lecture01/01-plotting-basics.html#practice-exercises",
    "title": "Plotting Basics",
    "section": "Practice Exercises üéØ",
    "text": "Practice Exercises üéØ\n\nExercise 1: Free Fall Velocity\nPlot the velocity of a falling object: \\(v = gt\\)\n\nUse time from 0 to 5 seconds\nSet \\(g = 9.81\\) m/s¬≤\nAdd proper labels and title\nUse a red line with linewidth=2\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ng = 9.81\nt = np.linspace(0, 5, 100)\nv = g * t\n\nplt.figure(figsize=(6, 4))\nplt.plot(t, v, 'r-', linewidth=2)\nplt.xlabel('Time (s)')\nplt.ylabel('Velocity (m/s)')\nplt.title('Free Fall Velocity')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\nExercise 2: Kinetic Energy vs Velocity\nPlot kinetic energy as a function of velocity for a 1000 kg car.\n\n\\(E_k = \\frac{1}{2}mv^2\\)\nVelocity from 0 to 40 m/s\nAdd labels with proper units\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nm = 1000  # kg\nv = np.linspace(0, 40, 100)\nE_k = 0.5 * m * v**2\n\nplt.figure(figsize=(6, 4))\nplt.plot(v, E_k/1000, 'b-', linewidth=2)  # Convert to kJ\nplt.xlabel('Velocity (m/s)')\nplt.ylabel('Kinetic Energy (kJ)')\nplt.title('Kinetic Energy of a 1000 kg Car')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\nExercise 3: Compare Trajectories\nPlot projectile trajectories for launch angles of 30¬∞, 45¬∞, and 60¬∞ on the same plot.\n\nUse \\(v_0 = 20\\) m/s for all\nAdd a legend\nUse different colors for each angle\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nv0 = 20.0\ng = 9.81\nangles = [30, 45, 60]\ncolors = ['r', 'b', 'g']\n\nplt.figure(figsize=(8, 5))\n\nfor angle, color in zip(angles, colors):\n    theta = np.radians(angle)\n    t_max = 2 * v0 * np.sin(theta) / g\n    t = np.linspace(0, t_max, 100)\n    x = v0 * np.cos(theta) * t\n    y = v0 * np.sin(theta) * t - 0.5 * g * t**2\n    plt.plot(x, y, color=color, linewidth=2, label=f'{angle}¬∞')\n\nplt.xlabel('Horizontal Distance (m)')\nplt.ylabel('Height (m)')\nplt.title('Projectile Trajectories at Different Angles')\nplt.legend(loc='upper right')\nplt.grid(True, alpha=0.3)\nplt.show()",
    "crumbs": [
      "üöÄ Week 1: Your First Physics Code",
      "Quick Win: Plotting Your First Graph"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-plotting-basics.html#common-mistakes-to-avoid",
    "href": "lectures/lecture01/01-plotting-basics.html#common-mistakes-to-avoid",
    "title": "Plotting Basics",
    "section": "Common Mistakes to Avoid ‚ö†Ô∏è",
    "text": "Common Mistakes to Avoid ‚ö†Ô∏è\n\nForgetting plt.show(): Your plot won‚Äôt display!\nNo labels: Always label axes and add a title\nSaving after showing: Use plt.savefig() BEFORE plt.show()\nToo many points: 100-500 points is usually enough for smooth curves\nUgly default sizes: Always set figsize for better proportions",
    "crumbs": [
      "üöÄ Week 1: Your First Physics Code",
      "Quick Win: Plotting Your First Graph"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-plotting-basics.html#whats-next",
    "href": "lectures/lecture01/01-plotting-basics.html#whats-next",
    "title": "Plotting Basics",
    "section": "What‚Äôs Next?",
    "text": "What‚Äôs Next?\nYou now know the basics of plotting! In later lectures, you‚Äôll learn:\n\nScatter plots and histograms (Lecture 4)\nLogarithmic plots for exponential data\nMultiple subplots in one figure\nContour and 3D plots for multivariable functions\nAnimations for time-dependent phenomena\n\nBut these basics will get you through 90% of your physics plotting needs! üéâ",
    "crumbs": [
      "üöÄ Week 1: Your First Physics Code",
      "Quick Win: Plotting Your First Graph"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-plotting-basics.html#summary",
    "href": "lectures/lecture01/01-plotting-basics.html#summary",
    "title": "Plotting Basics",
    "section": "Summary ‚úÖ",
    "text": "Summary ‚úÖ\nEssential Matplotlib Commands:\nplt.figure(figsize=(6,4))     # Create new figure\nplt.plot(x, y, 'b-')          # Plot data\nplt.xlabel('label')           # X-axis label\nplt.ylabel('label')           # Y-axis label\nplt.title('title')            # Title\nplt.legend()                  # Add legend\nplt.grid(True)                # Add grid\nplt.savefig('file.pdf')       # Save figure\nplt.show()                    # Display plot\nRemember: Good plots communicate clearly. Always include labels, units, and legends!",
    "crumbs": [
      "üöÄ Week 1: Your First Physics Code",
      "Quick Win: Plotting Your First Graph"
    ]
  },
  {
    "objectID": "lectures/lecture06/1_input_output.html",
    "href": "lectures/lecture06/1_input_output.html",
    "title": "Input and output",
    "section": "",
    "text": "To try out all of the functions of todays notebook, we will need to use the embedded JupyterLite notebook. To use the notebook, click ob File -&gt; Open from URL and paste the following link into the input field:\nhttps://raw.githubusercontent.com/fcichos/EMPP24/refs/heads/main/seminars/1_input_output.ipynb\nTo download the data files, click ob File -&gt; Open from URL and paste the following links into the input field:\nhttps://raw.githubusercontent.com/fcichos/EMPP24/refs/heads/main/seminars/MyData.txt\nhttps://raw.githubusercontent.com/fcichos/EMPP24/refs/heads/main/seminars/2018-04-11_sds011_sensor_12253.csv\nhttps://raw.githubusercontent.com/fcichos/EMPP24/refs/heads/main/seminars/2018-04-12_sds011_sensor_12253.csv\nYou should then have 3 data files and one notebook. You can then go into fullscreen mode.\nFull Screen"
  },
  {
    "objectID": "lectures/lecture06/1_curve_fitting.html",
    "href": "lectures/lecture06/1_curve_fitting.html",
    "title": "Curve fitting",
    "section": "",
    "text": "Let‚Äôs take a break from physics-related topics and explore another crucial area: curve fitting. We‚Äôll focus on demonstrating how to apply the least-squares method to fit a quadratic function with three parameters to experimental data. It‚Äôs worth noting that this approach can be applied to more complex functions or even simpler linear models.\nBefore diving into the fitting process, it‚Äôs essential to consider how to best estimate your model parameters. In some cases, you may be able to derive explicit estimators for the parameters, which can simplify the fitting procedure. Therefore, it‚Äôs advisable to carefully consider your approach before beginning the actual fitting process.\nFor those who want to delve deeper into this subject, you might find it interesting to explore concepts like maximum likelihood estimation. This method offers an alternative approach to parameter estimation and can provide valuable insights in certain scenarios."
  },
  {
    "objectID": "lectures/lecture06/1_curve_fitting.html#idea",
    "href": "lectures/lecture06/1_curve_fitting.html#idea",
    "title": "Curve fitting",
    "section": "Idea",
    "text": "Idea\nIn experimental physics, we often collect data points to understand the underlying physical phenomena. This process involves fitting a mathematical model to the experimental data.\nThe data typically comes as a series of paired points:\n\n\n\nx-data\ny-data\n\n\n\n\n\\(x_{1}\\)\n\\(y_{1}\\)\n\n\n\\(x_{2}\\)\n\\(y_{2}\\)\n\n\n‚Ä¶\n‚Ä¶\n\n\n\\(x_{N}\\)\n\\(y_{N}\\)\n\n\n\nEach point \\(\\{x_i, y_i\\}\\) may represent the result of multiple independent measurements. For instance, \\(y_1\\) could be the mean of several measurements \\(y_{1,j}\\):\n\\[y_1 = \\frac{1}{N}\\sum_{j=1}^N y_{1,j}\\]\nWhen these measurements have an uncertainty \\(\\sigma\\) for individual readings, the sum of all measurements has a variance of \\(N\\sigma^2\\) and a standard deviation of \\(\\sqrt{N}\\sigma\\). Consequently, the mean value has an associated error (standard deviation) known as the Standard Error of the Mean (SEOM):\n\\[\\sigma_{SEOM} = \\frac{\\sigma}{\\sqrt{N}}\\]\nThis SEOM is crucial in physics measurements.\nIt‚Äôs also important to note the definition of variance:\n\\[\\sigma_1^2 = \\frac{1}{N} \\sum_{j=1}^N (y_{1,j} - y_1)^2\\]\nThis statistical framework forms the basis for analyzing experimental data and fitting mathematical models to understand the underlying physics."
  },
  {
    "objectID": "lectures/lecture06/1_curve_fitting.html#least-squares",
    "href": "lectures/lecture06/1_curve_fitting.html#least-squares",
    "title": "Curve fitting",
    "section": "Least squares",
    "text": "Least squares\nIn experimental physics, we often collect data points to understand the underlying physical phenomena. To make sense of this data, we fit a mathematical model to it. One common method for fitting data is the least squares method.\n\nWhy use least squares fitting?\nThe goal of least squares fitting is to find the set of parameters for our model that best describes the data. This is done by minimizing the differences (or residuals) between the observed data points and the model‚Äôs predictions.\n\n\nGaussian uncertainty and probability\nWhen we take measurements, there is always some uncertainty. Often, this uncertainty can be modeled using a Gaussian (normal) distribution. This distribution is characterized by its mean (average value) and standard deviation (a measure of the spread of the data).\nIf we describe our data with a model function, which delivers a function value \\(f(x_{i},a)\\) for a set of parameters \\(a\\) at the position \\(x_{i}\\), the Gaussian uncertainty dictates a probability of finding a data value \\(y_{i}\\):\n\\[\\begin{equation}\np_{y_{i}}=\\frac{1}{\\sqrt{2\\pi}\\sigma_{i}}\\exp\\left(-\\frac{(y_{i}-f(x_{i},a))^2}{2\\sigma_{i}^2}\\right)\n\\end{equation}\\]\nHere, \\(\\sigma_{i}\\) represents the uncertainty in the measurement \\(y_{i}\\).\n\n\nCombining probabilities for multiple data points\nTo understand how well our model fits all the data points, we need to consider the combined probability of observing all the data points. This is done by multiplying the individual probabilities:\n\\[\\begin{equation}\np(y_{1},\\ldots,y_{N})=\\prod_{i=1}^{N}\\frac{1}{\\sqrt{2\\pi}\\sigma_{i}}\\exp\\left(-\\frac{(y_{i}-f(x_{i},a))^2}{2\\sigma_{i}^2}\\right)\n\\end{equation}\\]\n\n\nMaximizing the joint probability\nThe best fit of the model to the data is achieved when this joint probability is maximized. To simplify the calculations, we take the logarithm of the joint probability:\n\\[\\begin{equation}\n\\ln(p(y_{1},\\ldots,y_{N}))=-\\frac{1}{2}\\sum_{i=1}^{N}\\left( \\frac{y_{i}-f(x_{i},a)}{\\sigma_{i}}\\right)^2 - \\sum_{i=1}^{N}\\ln\\left( \\sigma_{i}\\sqrt{2\\pi}\\right)\n\\end{equation}\\]\nThe first term on the right side (except the factor 1/2) is the least squared deviation, also known as \\(\\chi^{2}\\):\n\\[\\begin{equation}\n\\chi^{2} =\\sum_{i=1}^{N}\\left( \\frac{y_{i}-f(x_{i},a)}{\\sigma_{i}}\\right)^2\n\\end{equation}\\]\nThe second term is just a constant value given by the uncertainties of our experimental data."
  },
  {
    "objectID": "lectures/lecture06/1_curve_fitting.html#data",
    "href": "lectures/lecture06/1_curve_fitting.html#data",
    "title": "Curve fitting",
    "section": "Data",
    "text": "Data\nLet‚Äôs have a look at the meaning of this equation. Let‚Äôs assume we measure the trajectory of a ball that has been thrown at an angle \\(\\alpha\\) with an initial velocity \\(v_{0}\\). We have collected data points by measuring the height of the ball above the ground at equally spaced distances from the throwing person.\n\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.5.2 from the internet...\n    (need help?)\n    \n\n\n\n\nThe table above shows the measured data points \\(y_{i}\\) at the position \\(x_{i}\\) with the associated uncertainties \\(\\sigma_{i}\\).\nWe can plot the data and expect, of course, a parabola. Therefore, we model our experimental data with a parabola like\n\\[\\begin{equation}\ny = ax^2 + bx + c\n\\end{equation}\\]\nwhere the parameter \\(a\\) must be negative since the parabola is inverted.\nI have created an interactive plot with an interact widget, as this allows you to play around with the parameters. The value of \\(\\chi^2\\) is also included in the legend, so you can get an impression of how good your fit of the data is.\n\nviewof aSlider = Inputs.range([-4, 0], { label: \"a\", step: 0.01, value: -1.7 });\nviewof bSlider = Inputs.range([-2, 2], { label: \"b\", step: 0.01, value: 1.3 });\nviewof cSlider = Inputs.range([-2, 2], { label: \"c\", step: 0.01, value: 1.0 });\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfiltered = transpose(data);\n// Create the plot\n\nxValues = Array.from({ length: 100 }, (_, i) =&gt; i / 100);\nparabolaData = xValues.map(x =&gt; ({ x, y: parabola(x, aSlider, bSlider, cSlider) }));\n\n\nparabola = (x, a, b, c) =&gt; a * x**2 + b * x + c\n\ncalculateChiSquared = (data, a, b, c) =&gt; {\n  let chisq = 0\n  let x= data.map(d =&gt; d.x)\n  let y= data.map(d =&gt; d.y)\n  let err= data.map(d =&gt; d.error)\n  for (let i = 0; i &lt; x.length; i++) {\n    let y_model = parabola(x[i], a, b, c)\n    chisq += ((y[i] - y_model) / err[i])**2\n  }\n  return chisq\n}\n\nchisq = calculateChiSquared(filtered, aSlider, bSlider, cSlider)\n\nPlot.plot({\n  marks: [\n    Plot.dot(filtered, { x: \"x\", y: \"y\" }),\n    Plot.ruleY(filtered, { x: \"x\", y1: d =&gt; d.y - d.error, y2: d =&gt; d.y + d.error }),\n    Plot.line(parabolaData, { x: \"x\", y: \"y\" }),\n    Plot.text([{ x: 0.8, y: 1.5, label: `œá¬≤: ${chisq.toFixed(2)}` }], {\n          x: \"x\",\n          y: \"y\",\n          text: \"label\",\n          dy: -10, // Adjust vertical position if needed\n          fill: \"black\", // Set text color\n          fontSize: 16\n        }),\n    Plot.frame()\n  ],\n  x: {\n    label: \"X Axis\",\n    labelAnchor: \"center\",\n    labelOffset: 35,\n    grid: true,\n    tickFormat: \".2f\", // Format ticks to 2 decimal places\n    domain: [0, 1]\n  },\n  y: {\n    label: \"Y Axis\",\n    grid: true,\n    tickFormat: \".2f\", // Format ticks to 2 decimal places\n    labelAnchor: \"center\",  // Center the label on its axis\n    labelAngle: -90,\n    labelOffset: 60,\n    domain: [0, 2],\n  },\n  width: 400,\n  height: 400,\n  marginLeft: 100,\n  marginBottom: 40,\n  style: {\n    fontSize: \"14px\",          // This sets the base font size\n    \"axis.label\": {\n      fontSize: \"18px\",        // This sets the font size for axis labels\n      fontWeight: \"bold\"       // Optionally make it bold\n    },\n    \"axis.tick\": {\n      fontSize: \"14px\"         // This sets the font size for tick labels\n    }\n  },\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe have that troubling point at the right edge with a large uncertainty. However, since the value of \\(\\chi^2\\) divides the deviation by the uncertainty \\(\\sigma_{i}\\), the weight for this point overall in the \\(\\chi^2\\) is smaller than for the other points.\n\\[\\begin{equation}\n\\chi^{2}=\\sum_{i=1}^{N}\\left( \\frac{y_{i}-f(x_{i},a)}{\\sigma_{i}}\\right)^2\n\\end{equation}\\]\nYou may simply check the effect by changing the uncertainty of the last data points in the error array."
  },
  {
    "objectID": "lectures/lecture06/1_curve_fitting.html#least-square-fitting",
    "href": "lectures/lecture06/1_curve_fitting.html#least-square-fitting",
    "title": "Curve fitting",
    "section": "Least square fitting",
    "text": "Least square fitting\nTo find the best fit of the model to the experimental data, we use the least squares method. This method minimizes the sum of the squared differences between the observed data points and the model‚Äôs predictions.\nMathematically, we achieve this by minimizing the least squares, i.e., finding the parameters \\(a\\) that minimize the following expression:\n\\[\\begin{equation}\n\\frac{d\\chi^{2}}{da}=\\sum_{i=1}^{N}\\frac{1}{\\sigma_{i}^2}\\frac{df(x_{i},a)}{da}[y_{i}-f(x_{i},a)]=0\n\\end{equation}\\]\nThis kind of least squares minimization is done by fitting software using different types of algorithms.\n\nFitting with SciPy\nLet‚Äôs do some fitting using the SciPy library, which is a powerful tool for scientific computing in Python. We will use the curve_fit method from the optimize sub-module of SciPy.\nFirst, we need to define the model function we would like to fit to the data. In this case, we will use our parabola function:\n\n\n\n\n\n\nNext, we need to provide initial guesses for the parameters. These initial guesses help the fitting algorithm start the search for the optimal parameters:\n\n\n\n\n\n\nWe then call the curve_fit function to perform the fitting:\n\n\n\n\n\n\n\n\n\n\n\n\ncurve_fit Function\n\n\n\n\n\nThe curve_fit function is used to fit a model function to data. It finds the optimal parameters for the model function that minimize the sum of the squared residuals between the observed data and the model‚Äôs predictions.\n\nParameters\n\nparabola:\n\nThis is the model function that you want to fit to the data. In this case, parabola is a function that represents a quadratic equation of the form ( y = ax^2 + bx + c ).\n\nx_data:\n\nThis is the array of independent variable data points (the x-values).\n\ny_data:\n\nThis is the array of dependent variable data points (the y-values).\n\nsigma=err:\n\nThis parameter specifies the uncertainties (standard deviations) of the y-data points. The err array contains the uncertainties for each y-data point. These uncertainties are used to weight the residuals in the least squares optimization.\n\np0=init_guess:\n\nThis parameter provides the initial guesses for the parameters of the model function. The init_guess array contains the initial guesses for the parameters ( a ), ( b ), and ( c ). Providing good initial guesses can help the optimization algorithm converge more quickly and accurately.\n\nabsolute_sigma=True:\n\nThis parameter indicates whether the provided sigma values are absolute uncertainties. If absolute_sigma is set to True, the sigma values are treated as absolute uncertainties. If absolute_sigma is set to False, the sigma values are treated as relative uncertainties, and the covariance matrix of the parameters will be scaled accordingly.\n\n\n\n\nReturn Value\nThe curve_fit function returns two values:\n\npopt:\n\nAn array containing the optimal values for the parameters of the model function that minimize the sum of the squared residuals.\n\npcov:\n\nThe covariance matrix of the optimal parameters. The diagonal elements of this matrix provide the variances of the parameter estimates, and the off-diagonal elements provide the covariances between the parameter estimates.\n\n\n\n\n\n\nThe fit variable contains the results of the fitting process. It is composed of various results, which we can split into the fitted parameters and the covariance matrix:\n\n\n\n\n\n\nThe ans variable contains the fitted parameters fit_a, fit_b, and fit_c, while the cov variable contains the covariance matrix. Let‚Äôs have a look at the fit and the \\(\\chi^{2}\\) value first:\n\n\n\n\n\n\nWe can then plot the fitted curve along with the original data points and the \\(\\chi^{2}\\) value:\n\n\n\n\n\n\n\n\n\\(\\chi\\)-squared value\nThe value of \\(\\chi^2\\) gives you a measure of the quality of the fit. We can judge the quality by calculating the expectation value of \\(\\chi^2\\):\n\\[\\begin{equation}\n\\langle \\chi^{2}\\rangle =\\sum_{i=1}^{N} \\frac{\\langle (y_{i}-f(x_{i},a) )^2\\rangle }{\\sigma_{i}^2}=\\sum_{i=1}^{N} \\frac{\\sigma_{i}^2}{\\sigma_{i}^2}=N\n\\end{equation}\\]\nSo, the mean of the least squared deviation increases with the number of data points. Therefore:\n\n\\(\\chi^{2} \\gg N\\) means that the fit is bad.\n\\(\\chi^{2} &lt; N\\) means that the uncertainties are wrong.\n\nThe first case may occur if you don‚Äôt have a good fit to your data, for example, if you are using the wrong model. The second case typically occurs if you don‚Äôt have accurate estimates of the uncertainties and you assume all uncertainties to be constant.\nIt is really important to have a good estimate of the uncertainties and to include them in your fit. If you include the uncertainties in your fit, it is called a weighted fit. If you don‚Äôt include the uncertainties (meaning you keep them constant), it is called an unweighted fit.\nFor our fit above, we obtain a \\(\\chi^{2}\\) which is on the order of \\(N=10\\), which tells you that I have created the data with reasonable accuracy.\n\n\nResiduals\nAnother way to assess the quality of the fit is by looking at the residuals. Residuals are defined as the deviation of the data from the model for the best fit:\n\\[\\begin{equation}\nr_i = y_i - f(x_{i},a)\n\\end{equation}\\]\nThe residuals can also be expressed as the percentage of the deviation of the data from the fit:\n\\[\\begin{equation}\nr_i = 100 \\left( \\frac{y_i - f(x_{i},a)}{y_i} \\right)\n\\end{equation}\\]\n\n\nImportance of Residuals\nResiduals are important because they provide insight into how well the model fits the data. If the residuals show only statistical fluctuations around zero, then the fit and likely also the model are good. However, if there are systematic patterns in the residuals, it may indicate that the model is not adequately capturing the underlying relationship in the data.\n\n\nVisualizing Residuals\nLet‚Äôs visualize the residuals to better understand their distribution. We will plot the residuals as a function of the independent variable \\(x\\).\n\n\n\n\n\n\n\n\n\n\n\n\nCommon Patterns in Residuals\n\n\n\n\n\nRandom Fluctuations Around Zero:\n\nIf the residuals are randomly scattered around zero, it suggests that the model is a good fit for the data.\n\nSystematic Patterns:\n\nIf the residuals show a systematic pattern (e.g., a trend or periodicity), it may indicate that the model is not capturing some aspect of the data. This could suggest the need for a more complex model.\n\nIncreasing or Decreasing Trends:\n\nIf the residuals increase or decrease with \\(x\\), it may indicate heteroscedasticity (non-constant variance) or that a different functional form is needed."
  },
  {
    "objectID": "lectures/lecture06/1_curve_fitting.html#covariance-matrix",
    "href": "lectures/lecture06/1_curve_fitting.html#covariance-matrix",
    "title": "Curve fitting",
    "section": "Covariance Matrix",
    "text": "Covariance Matrix\nIn the previous sections, we discussed how to fit a model to experimental data and assess the quality of the fit using residuals. Now, let‚Äôs take a closer look at the uncertainties in the fit parameters and how they are related to each other. This is where the covariance matrix comes into play.\n\nPurpose of the Covariance Matrix\nThe covariance matrix provides important information about the uncertainties in the fit parameters and how these uncertainties are related to each other. It helps us understand the precision of the parameter estimates and whether the parameters are independent or correlated.\n\n\nUnderstanding Covariance\nCovariance is a measure of how much two random variables change together. If the covariance between two variables is positive, it means that they tend to increase or decrease together. If the covariance is negative, it means that one variable tends to increase when the other decreases. If the covariance is zero, it means that the variables are independent.\n\n\nCovariance Matrix in Curve Fitting\nWhen we fit a model to data, we obtain estimates for the parameters of the model. These estimates have uncertainties due to the measurement errors in the data. The covariance matrix quantifies these uncertainties and the relationships between them.\nFor a model with three parameters \\((a, b, c)\\), the covariance matrix is a \\(3 \\times 3\\) matrix that looks like this:\n\\[\\begin{equation}\n{\\rm cov}(p_{i}, p_{j}) =\n\\begin{bmatrix}\n\\sigma_{aa}^{2} & \\sigma_{ab}^{2} & \\sigma_{ac}^{2} \\\\\n\\sigma_{ba}^{2} & \\sigma_{bb}^{2} & \\sigma_{bc}^{2} \\\\\n\\sigma_{ca}^{2} & \\sigma_{cb}^{2} & \\sigma_{cc}^{2}\n\\end{bmatrix}\n\\end{equation}\\]\nThe diagonal elements provide the variances (squared uncertainties) of the fit parameters, while the off-diagonal elements describe the covariances between the parameters.\n\n\nExample\nLet‚Äôs calculate the covariance matrix for our fitted model and interpret the results.\n\n\n\n\n\n\n\n\nInterpreting the Covariance Matrix\nThe covariance matrix provides valuable information about the uncertainties in the fit parameters:\n\nDiagonal Elements: The diagonal elements represent the variances of the parameters. The square root of these values gives the standard deviations (uncertainties) of the parameters.\nOff-Diagonal Elements: The off-diagonal elements represent the covariances between the parameters. If these values are large, it indicates that the parameters are correlated.\n\n\n\nGenerating Synthetic Data\nTo better understand the covariance matrix, let‚Äôs generate synthetic data and fit the model to each dataset. This will help us visualize the uncertainties in the parameters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation Matrix\nTo better understand the relationships between the parameters, we can normalize the covariance matrix to obtain the correlation matrix. The correlation matrix has values between -1 and 1, where 1 indicates perfect positive correlation, -1 indicates perfect negative correlation, and 0 indicates no correlation.\n\n\n\n\n\n\n\n\nVisualizing the Covariance and Correlation\nLet‚Äôs visualize the covariance and correlation between the parameters using scatter plots.\n\n\n\n\n\n\nBy examining the covariance and correlation matrices, we can gain a deeper understanding of the uncertainties in the fit parameters and how they are related to each other.\n\n\nImproving the Model\nIf we find that the parameters are highly correlated, we might want to find a better model containing more independent parameters. For example, we can write down a different model:\n\\[\\begin{equation}\ny = a(x - b)^2 + c\n\\end{equation}\\]\nThis model also contains three parameters, but the parameter \\(b\\) directly refers to the maximum of our parabola, while the parameter \\(a\\) denotes its curvature.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe see from the covariance matrix that the new model has a smaller correlation of the parameters with each other.\n\n\n\n\n\n\nThis is also expressed by our correlation matrix.\n\n\n\n\n\n\nBy examining the covariance and correlation matrices, we can gain valuable insights into the uncertainties in the fit parameters and how to improve our model."
  },
  {
    "objectID": "lectures/lecture08/2_integration.html",
    "href": "lectures/lecture08/2_integration.html",
    "title": "Numerical Integration",
    "section": "",
    "text": "This lecture covers numerical integration methods, which are essential for computing definite integrals of functions. We‚Äôll explore three different methods with increasing accuracy: the Box method, Trapezoid method, and Simpson‚Äôs method.",
    "crumbs": [
      "‚ö° Week 5: Classical Mechanics 2",
      "Tool: Numerical Integration"
    ]
  },
  {
    "objectID": "lectures/lecture08/2_integration.html#box-method-rectangle-method",
    "href": "lectures/lecture08/2_integration.html#box-method-rectangle-method",
    "title": "Numerical Integration",
    "section": "Box Method (Rectangle Method)",
    "text": "Box Method (Rectangle Method)\n\n\n\nBox Method Illustration\n\n\nThe Box method is the simplest approach for numerical integration. It approximates the function in each interval \\(\\Delta x\\) with a constant value taken at the left endpoint of the interval.\n\\[\\begin{equation}\n\\int_{a}^{b} f(x) dx \\approx \\sum_{i=1}^{N} f(x_{i}) \\Delta x\n\\end{equation}\\]",
    "crumbs": [
      "‚ö° Week 5: Classical Mechanics 2",
      "Tool: Numerical Integration"
    ]
  },
  {
    "objectID": "lectures/lecture08/2_integration.html#trapezoid-method",
    "href": "lectures/lecture08/2_integration.html#trapezoid-method",
    "title": "Numerical Integration",
    "section": "Trapezoid Method",
    "text": "Trapezoid Method\n\n\n\nTrapezoid Method Illustration\n\n\nThe Trapezoid method improves upon the Box method by approximating the function with linear segments between points.\n\\[\\begin{equation}\n\\int_{a}^{b} f(x) dx \\approx \\sum_{i=1}^{N} \\frac{f(x_i) + f(x_{i-1})}{2} \\Delta x\n\\end{equation}\\]",
    "crumbs": [
      "‚ö° Week 5: Classical Mechanics 2",
      "Tool: Numerical Integration"
    ]
  },
  {
    "objectID": "lectures/lecture08/2_integration.html#simpsons-method",
    "href": "lectures/lecture08/2_integration.html#simpsons-method",
    "title": "Numerical Integration",
    "section": "Simpson‚Äôs Method",
    "text": "Simpson‚Äôs Method\n\n\n\nSimpson‚Äôs Method Illustration\n\n\nSimpson‚Äôs method provides higher accuracy by approximating the function with parabolic segments.\n\\[\\begin{equation}\n\\int_{a}^{b} f(x) dx \\approx \\frac{\\Delta x}{3} \\sum_{i=1}^{(N-1)/2} \\left(f(x_{i-1}) + 4f(x_i) + f(x_{i+1})\\right)\n\\end{equation}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nSimpson‚Äôs Rule for Numerical Integration\n\n\n\n\n\nSimpson‚Äôs Rule is a method for numerical integration that approximates the definite integral of a function by using quadratic polynomials.\n\nFor an integral \\(\\int_a^b f(x)dx\\), Simpson‚Äôs Rule fits a quadratic function through three points:\n\n\\(f(a)\\)\n\\(f(\\frac{a+b}{2})\\)\n\\(f(b)\\)\n\nLet‚Äôs define:\n\n\\(h = \\frac{b-a}{2}\\)\n\\(x_0 = a\\)\n\\(x_1 = \\frac{a+b}{2}\\)\n\\(x_2 = b\\)\n\nThe quadratic approximation has the form: \\[P(x) = Ax^2 + Bx + C\\]\nThis polynomial must satisfy: \\[f(x_0) = Ax_0^2 + Bx_0 + C\\] \\[f(x_1) = Ax_1^2 + Bx_1 + C\\] \\[f(x_2) = Ax_2^2 + Bx_2 + C\\]\nUsing Lagrange interpolation: \\[P(x) = f(x_0)L_0(x) + f(x_1)L_1(x) + f(x_2)L_2(x)\\]\nwhere \\(L_0\\), \\(L_1\\), \\(L_2\\) are the Lagrange basis functions.\n\n\nFinal Formula\nThe integration of this polynomial leads to Simpson‚Äôs Rule:\n\\[\\int_a^b f(x)dx \\approx \\frac{h}{3}[f(a) + 4f(\\frac{a+b}{2}) + f(b)]\\]\n\n\nError Term\nThe error in Simpson‚Äôs Rule is proportional to:\n\\[-\\frac{h^5}{90}f^{(4)}(\\xi)\\]\nfor some \\(\\xi \\in [a,b]\\)\n\n\nComposite Simpson‚Äôs Rule\nFor better accuracy, we can divide the interval into \\(n\\) subintervals (where \\(n\\) is even):\n\\[\\int_a^b f(x)dx \\approx \\frac{h}{3}[f(x_0) + 4\\sum_{i=1}^{n/2}f(x_{2i-1}) + 2\\sum_{i=1}^{n/2-1}f(x_{2i}) + f(x_n)]\\]\nwhere \\(h = \\frac{b-a}{n}\\)\nThe method is particularly effective for integrating functions that can be well-approximated by quadratic polynomials over small intervals.",
    "crumbs": [
      "‚ö° Week 5: Classical Mechanics 2",
      "Tool: Numerical Integration"
    ]
  },
  {
    "objectID": "lectures/lecture08/2_integration.html#comparison-of-methods",
    "href": "lectures/lecture08/2_integration.html#comparison-of-methods",
    "title": "Numerical Integration",
    "section": "Comparison of Methods",
    "text": "Comparison of Methods\nLet‚Äôs compare the accuracy of all three methods:",
    "crumbs": [
      "‚ö° Week 5: Classical Mechanics 2",
      "Tool: Numerical Integration"
    ]
  },
  {
    "objectID": "lectures/lecture08/2_integration.html#error-analysis",
    "href": "lectures/lecture08/2_integration.html#error-analysis",
    "title": "Numerical Integration",
    "section": "Error Analysis",
    "text": "Error Analysis\nThe three methods have different convergence rates:\n\nBox Method: Error ‚àù \\(\\Delta x\\) (linear convergence)\nTrapezoid Method: Error ‚àù \\(\\Delta x^2\\) (quadratic convergence)\nSimpson‚Äôs Method: Error ‚àù \\(\\Delta x^4\\) (fourth-order convergence)\n\nThis explains why Simpson‚Äôs method typically achieves higher accuracy with fewer points. For example, doubling the number of points in Simpson‚Äôs method reduces the error by a factor of 16",
    "crumbs": [
      "‚ö° Week 5: Classical Mechanics 2",
      "Tool: Numerical Integration"
    ]
  },
  {
    "objectID": "lectures/lecture09/2_coupled_pendula.html",
    "href": "lectures/lecture09/2_coupled_pendula.html",
    "title": "Coupled Pendula",
    "section": "",
    "text": "We will continue our course with some physical problems, we are going to tackle. One of the more extensive solutions will consider two coupled pendula. This belongs to the class of coupled oscillators, which are extremely important. They will later yield propagating waves. They are important for phonons, i.e.¬†coupled vibration of atoms in solids, but there are also many other axamples. One can realize the coupled oscillation on different ways. He we will do that not with spring oscillators, by with pendula."
  },
  {
    "objectID": "lectures/lecture09/2_coupled_pendula.html#description-of-the-problem",
    "href": "lectures/lecture09/2_coupled_pendula.html#description-of-the-problem",
    "title": "Coupled Pendula",
    "section": "Description of the problem",
    "text": "Description of the problem\n\nSketch\nThe image below depicts the sitution we would like to cover in our first project. These are two pendula, which have the length \\(L_{1}\\) and \\(L_{2}\\). Both are coupled with a spring of spring constant \\(k\\), which is relaxed, when both pendula are at rest. You may want to include a generalized version where the spring is mounted at a distance \\(c\\) from the turning points of the pendula.\nIf you develop the equation of motion, write down as a sum of torques. Use one equation of motion for each pendulum. The result will be two coupled differential equations for the angular coordinates. They are solved by the scipy odeint function without any friction.\n\n\n\n\n\n\nFigure¬†1: Sketch of the two coupled pendula.\n\n\n\n\n\nEquations of motion\nThe equations of motion of the two coupled pendula have the following form:\n\\[\\begin{eqnarray}\nI_{1}\\ddot{\\theta_{1}}&=&-m_{1}gL_{1}\\sin(\\theta_{1})-kc^2[\\sin(\\theta_{1})-\\sin(\\theta_{2})]\\\\\nI_{2}\\ddot{\\theta_{2}}&=&-m_{2}gL_{2}\\sin(\\theta_{2})+kc^2[\\sin(\\theta_{1})-\\sin(\\theta_{2})]\n\\end{eqnarray}\\]\nHere, \\(\\theta_{1}, \\theta_{2}\\) measure the angle of the two pendula with the length \\(L_{1},L_{2}\\). \\(k\\) is the spring constant of the spring coupling both pendula. If you use a variable coupling position of the spring name the length of the coupling from the turning point \\(c\\)."
  },
  {
    "objectID": "lectures/lecture09/2_coupled_pendula.html#solving-the-problem",
    "href": "lectures/lecture09/2_coupled_pendula.html#solving-the-problem",
    "title": "Coupled Pendula",
    "section": "Solving the problem",
    "text": "Solving the problem\n\nSetting up the function\nIn our previous lecture, we used the odeint function of scipy to solve the driven damped harmonic oscillator. Remeber that we used the array\nstate[0] -&gt; position\nstate[1] -&gt; velocity\nto exchange position and velocity with the solver via the function that defines the physical problem\ndef SHO(state, time):\n    g0 = state[1]               -&gt;velocity\n    g1 = -k/m * state [0]       -&gt;acceleration\n    return np.array([g0, g1])\nfor a coupled system of different equations, we can now extend the state array. In the case of the coupled system of equations it has the following structure\ndef SHO(state, time):\n    g0 = how the velocity of object 1 depends on the velocities of all objects\n    g1 = how the acceleration of object 1 depends on the acceleration of all objects\n    g2 = how the velocity of object 2 depends on the velocities of all objects\n    g3 = how the acceleration of object 2 depends on the acceleration of all objects\n    return np.array([g0, g1])\nSo the state vector just gets longer and the coupling is in the definition of the velocities and accelerations. The results are then the positions and velocities of the objects. Use this type of scheme to define the problem and write a function, which returns the state of the objects as before.\n\n\n\n\n\n\n\n\nDefine initial parameters\nWe want to define some parameters of the pendula\n\nlength of the pendulum 1, \\(L_1\\)=3\nlength of the pendulum 2, \\(L_2\\)=3\ngravitational acceleration, \\(g=9.81\\)\nmass at the end of the pendula, \\(m=1\\)\ndistance where the coupling spring is mounted, \\(c=2\\)\nspring constant of the coupling spring, \\(k=5\\)\n\n\n\n\n\n\n\nAs compared to our previous problem of a damped driven pendulum, where we had two initial conditions for the second order differential equation, we have now two second order differential equation. We therefore need 4 initial parameters, which are the initial elongations of both pendula and their corresponding initial angular velocities We will notice, that the solution,i.e.¬†the motion of the pendula, will strongly depend on the initial conditions.\n\n\n\n\n\n\n\n\nSolve the equation of motion\nWe have to define a timeperiod over which we would like to obtain the solution. We use here a period of 400s where we calculate the solution at 10000 points along the 400s.\n\n\n\n\n\n\nWe are now ready to calculate the solution. Finally, we extract also the angles of the individual pendula, their angular velocities and the position of the point masses at the end of the pendulum. This can be then readily used to create some animation.\n\n\n\n\n\n\n\n\nPlotting\nFirst, get some impression of how the angles change over time.\n\n\n\n\n\n\n\n\nAnimation\nThe plot of the angles over time is not always giving a good insight. With our knowledge about animations, we may easily animate the motion of the two pendula as well. Yet, the animation code will not work in the website version. But you may copy and paste the code to your own Jupyter Notebook environment.\n\n\n\n\n\n\nFor the physics, it has neven been interesting to define at which distance the pendula are mounted at a ceiling for example. For drawing them we have to do that together with some additional parameters, which define for example the position of where to draw in the canvas and the conversion of meters to pixels.\n\n\n\n\n\n\nThe function below will do the drawing for us. We define a function such that we can create a background animation with a thread. Note that we have inserted sleep(t[1]-t[0]) at the end. The drawing of the 4 objects will be pretty fast so that we can wait a certain amount of time until we display the next frame. That means at the end, that our simulation will run in real time.\n\n\n\n\n\n\n\n\n\n\n\n\nLooks pretty slow, but remember the pendula are 3 meters long."
  },
  {
    "objectID": "lectures/lecture09/2_coupled_pendula.html#normal-modes",
    "href": "lectures/lecture09/2_coupled_pendula.html#normal-modes",
    "title": "Coupled Pendula",
    "section": "Normal Modes",
    "text": "Normal Modes\nWe will not cover all the physical details here, but you might remember your mechanis lectures, that two coupled oscillators show distinct modes of motion, which we would call the normal modes. Four two coupled pendula there are two normal modes, where both pendula move with the same frequency. We may force the system into one of its normal modes, by specifying its initial conditions properly.\n\nIn-phase motion\nThe first one, will create an in-phase motion of the two pendula by setting their initial elongation equal, i.e.¬†\\(\\theta_{1}(t=0)=\\theta_{2}(t=0)\\). Both pendula then oscillate with their natural frequency and the coupling spring is never elongated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOut-of-phase motion\nThe second one, will create a motion in which the two pendula are out-of-phase by a phase angle of \\(\\pi\\) , i.e.¬†\\(\\theta_{1}(t=0)=-\\theta_{2}(t=0)\\). Both pendula then oscillate with a frequency higher than their natural frequency. This is due to the fact that there is a higher restoring force due to the action of the spring.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBeat case\nThe last case is not a normal mode but represents a more general case. We start with two different initial angles, i.e.¬†\\(\\theta_{1}(t=0)=\\pi/12\\) and \\(\\theta_{2}(t=0)=0\\). This is the so-called beat case, where the pendula exchange energy. The oscillation, which is at the beginning in only in the first pendulum is then transfer to the second one. This transfer of energy is continuously occurring from one pendulum to the other since there‚Äôs nowhere for the energy to go. From this point it‚Äôs easily to recognize how the wife is generated. In a set of many coupled pendula one pendulum is starting to oscillate and is transferring it‚Äôs energy to the next one and then to the next one and then to the next one and this way the energy is propagating along all oscillators.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComputation of energy (here for the beat case)\nAfter we have had a look at the motion of the individual pendula, we may also check, the energies in the system. We have to calculate the potential and kinetic energies of the pendula and we should not forget the potential energy stored in the spring.\n\nPotential energy of the pendula\nThe potential energy plot below nicely shows the exchange of energy between the two pendula in the beat case.\n\n\n\n\n\n\n\n\nPotential energy of the spring\n\n\n\n\n\n\n\n\nKinetic energies\n\n\n\n\n\n\n\n\nTotal energy\nAs the total energy in the system shall nbe conserved, the sum of all energy contributions should yield a flat line.\n\n\n\n\n\n\n\n\nTotal energy exchange of the pendula\nWhile the plot above Shows the total energy of both pendula we may now have a look at the total energy in each pendulum. The plots clearly show that the energy is exchanged between the two pendula. The residual ripples on the curve results from the fact that we here exclude the potential energy stored in the spring."
  },
  {
    "objectID": "lectures/lecture07/1_differentiation.html",
    "href": "lectures/lecture07/1_differentiation.html",
    "title": "Numerical Differentiation",
    "section": "",
    "text": "Differentiation is one of the most fundamental operations in physics‚Äîit allows us to describe rates of change, from the motion of particles to the evolution of fields. While we briefly encountered derivatives when exploring array slicing, we‚Äôll now develop a deeper understanding of numerical differentiation and the mathematical machinery that makes it work on a computer.",
    "crumbs": [
      "‚ö° Week 5: Classical Mechanics 2",
      "Numerical Differentiation"
    ]
  },
  {
    "objectID": "lectures/lecture07/1_differentiation.html#introduction-why-derivatives-matter-in-physics",
    "href": "lectures/lecture07/1_differentiation.html#introduction-why-derivatives-matter-in-physics",
    "title": "Numerical Differentiation",
    "section": "Introduction: Why Derivatives Matter in Physics",
    "text": "Introduction: Why Derivatives Matter in Physics\nDerivatives describe how quantities change, making them indispensable throughout physics. Consider velocity, which is the derivative of position with respect to time: \\[v = \\frac{dx}{dt}\\]. This tells us how rapidly an object‚Äôs position changes. Taking another derivative gives acceleration: \\[a = \\frac{dv}{dt}\\], describing how velocity itself changes. These concepts combine in Newton‚Äôs second law, \\[F = ma = m\\frac{d^2x}{dt^2}\\], connecting force to the second derivative of position.\nIn calculus courses, you learn to differentiate functions analytically‚Äîapplying rules to obtain exact symbolic expressions. But in computational physics, we often work with discrete data points from experiments or numerical simulations, not continuous analytical functions. We might measure a particle‚Äôs position at specific time intervals, or simulate a system on a finite grid. In these cases, we need numerical differentiation‚Äîmethods to approximate derivatives from discrete data.\nThe challenge is that the mathematical definition of a derivative involves a limit as the interval approaches zero, but on a computer, we work with finite intervals. How accurately can we approximate derivatives from discrete points? What methods work best? These are the questions we‚Äôll explore in this lecture.",
    "crumbs": [
      "‚ö° Week 5: Classical Mechanics 2",
      "Numerical Differentiation"
    ]
  },
  {
    "objectID": "lectures/lecture07/1_differentiation.html#first-order-derivative",
    "href": "lectures/lecture07/1_differentiation.html#first-order-derivative",
    "title": "Numerical Differentiation",
    "section": "First Order Derivative",
    "text": "First Order Derivative\n\nThe Basic Idea: From Calculus to Computation\nRecall from calculus that the derivative of a function \\(f(x)\\) at a point \\(x\\) is defined as the limit of the difference quotient as the interval \\(\\Delta x\\) shrinks to zero:\n\\[\nf^{\\prime}(x) = \\lim_{\\Delta x \\rightarrow 0} \\frac{f(x + \\Delta x) - f(x)}{\\Delta x}\n\\]\nThis limit captures the instantaneous rate of change‚Äîthe slope of the tangent line at a single point. But computers can‚Äôt handle infinitesimals or continuous limits. Instead, we work with finite differences. The simplest approximation drops the limit and uses a small but finite \\(\\Delta x\\):\n\\[\nf^{\\prime}_{i} \\approx \\frac{f_{i+1} - f_{i}}{\\Delta x}\n\\]\nIn this forward difference method, we approximate the derivative at position \\(i\\) by comparing the function value at \\(i\\) with the value one step ahead at \\(i+1\\). The name ‚Äúforward‚Äù comes from the fact that we look ahead (to the right) from our current position. Geometrically, this computes the slope of a secant line connecting two nearby points rather than the true tangent.\nThe forward difference is intuitive and easy to implement, but it‚Äôs not the most accurate approach. Let‚Äôs see how we can do better using Taylor series‚Äîa powerful tool that connects the discrete and continuous worlds.\n\n\nImproving Accuracy: The Central Difference Formula\nThe forward difference works, but we can achieve significantly better accuracy using a clever trick from calculus: the Taylor series expansion. Taylor series express a function‚Äôs values near a point in terms of its derivatives at that point‚Äîexactly what we need!\nConsider the Taylor expansion of our function around position \\(x_0\\):\n\\[\nf(x) = f(x_{0}) + (x - x_0) f^{\\prime}(x) + \\frac{(x - x_0)^2}{2!} f^{\\prime\\prime}(x) + \\frac{(x - x_0)^3}{3!} f^{(3)}(x) + \\ldots\n\\]\nFor discrete points separated by spacing \\(\\Delta x\\), we can write the function value at position \\(i+1\\) as:\n\\[\nf_{i+1} = f_{i} + \\Delta x f_{i}^{\\prime} + \\frac{\\Delta x^2}{2!} f_{i}^{\\prime\\prime} + \\frac{\\Delta x^3}{3!} f_{i}^{(3)} + \\ldots\n\\]\nSimilarly, expanding backward to position \\(i-1\\) gives:\n\\[\nf_{i-1} = f_{i} - \\Delta x f_{i}^{\\prime} + \\frac{\\Delta x^2}{2!} f_{i}^{\\prime\\prime} - \\frac{\\Delta x^3}{3!} f_{i}^{(3)} + \\ldots\n\\]\nNow comes the elegant part: if we subtract the backward expansion from the forward expansion, something remarkable happens:\n\\[\nf_{i+1} - f_{i-1} = 2 \\Delta x f_{i}^{\\prime} + O(\\Delta x^3)\n\\]\nNotice what happened‚Äîthe constant term \\(f_i\\) cancels, and more importantly, the second-order term \\(\\frac{\\Delta x^2}{2!} f_{i}^{\\prime\\prime}\\) also cancels (it appeared with opposite signs). The first-order derivative term doubles to \\(2\\Delta x f_{i}^{\\prime}\\), while the third-order and higher terms combine with odd powers of \\(\\Delta x\\). Solving for the derivative and neglecting terms of order \\(\\Delta x^3\\) and higher, we obtain the central difference formula:\n\\[\nf^{\\prime}_{i} \\approx \\frac{f_{i+1} - f_{i-1}}{2 \\Delta x}\n\\]\nThis is significantly more accurate than the forward difference because the leading error term is proportional to \\(\\Delta x^3\\) instead of \\(\\Delta x^2\\). In physics terms, if you halve your grid spacing, the forward difference error decreases by a factor of 4, but the central difference error decreases by a factor of 8. This makes central differences the preferred choice for most applications.\nThe key insight is that by using information from both sides of point \\(i\\) (symmetrically sampling at \\(i-1\\) and \\(i+1\\)), we achieve cancellation of the second-order error. This is analogous to how symmetric configurations often lead to simplifications in physics‚Äîthink of symmetric charge distributions simplifying electric field calculations, or how symmetric potentials lead to parity conservation in quantum mechanics.\n\n\nExample: Derivative of sin(x) using Arrays\nLet‚Äôs implement the central difference formula using arrays. We‚Äôll calculate the derivative of \\(\\sin(x)\\), which we know should give us \\(\\cos(x)\\).\n\n\n\n\n\n\nLet‚Äôs plot the results:\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Array Slicing for Derivatives\n\n\n\nWhen we write y[2:] we get all elements from index 2 to the end: \\([f_2, f_3, f_4, ..., f_n]\\)\nWhen we write y[:-2] we get all elements from the start up to (but not including) the last 2: \\([f_0, f_1, f_2, ..., f_{n-2}]\\)\nSo y[2:] - y[:-2] gives us: \\([f_2 - f_0, f_3 - f_1, f_4 - f_2, ..., f_n - f_{n-2}]\\)\nThis is exactly \\(f_{i+1} - f_{i-1}\\) for each point \\(i\\)!\n\n\n\n\nHandling Boundaries: Forward and Backward Differences\nThe central difference formula requires points on both sides of our evaluation point, which creates a problem at the boundaries. At the leftmost point (index 0), there‚Äôs no point to the left; at the rightmost point, there‚Äôs no point to the right. We need special treatment for these edge cases.\nAt the left boundary, we use the forward difference: \\[\nf^{\\prime}_{0} \\approx \\frac{f_{1} - f_{0}}{\\Delta x}\n\\]\nAt the right boundary, we use the backward difference: \\[\nf^{\\prime}_{n} \\approx \\frac{f_{n} - f_{n-1}}{\\Delta x}\n\\]\nThese are less accurate than central differences (with \\(O(\\Delta x^2)\\) error instead of \\(O(\\Delta x^3)\\)), but they‚Äôre the best we can do with the available points at the boundaries. In some applications, more sophisticated boundary treatments using multiple points or extrapolation techniques can improve accuracy, but for most purposes, these simple formulas suffice.\nLet‚Äôs implement a complete derivative function that combines central differences in the interior with forward/backward differences at the boundaries:\n\n\n\n\n\n\nNow let‚Äôs test it:\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced: Matrix Representation of Derivatives\n\n\n\n\n\nHere‚Äôs a cool alternative perspective! We can represent the derivative operation as a matrix multiplication. This might seem fancy, but it‚Äôs actually quite elegant and shows that differentiation is just a linear operation.\n\nThe Linear Operator Perspective\nHere‚Äôs a profound insight: differentiation is a linear operator. This means that the derivative of a sum equals the sum of derivatives, and constants can be pulled out: \\(\\frac{d}{dx}[af(x) + bg(x)] = a\\frac{df}{dx} + b\\frac{dg}{dx}\\). All linear operators on finite-dimensional spaces can be represented as matrices!\nFor discrete points \\(x_i\\), we can express the forward difference formula as a matrix-vector multiplication:\n\\[\nf^{\\prime} = \\frac{1}{\\Delta x}\n\\begin{bmatrix}\n-1 & 1  & 0 & 0 & 0 & 0\\\\\n0 & -1 & 1 & 0 & 0 & 0\\\\\n0 & 0  & -1 & 1 & 0 & 0\\\\\n0 & 0  & 0  & -1 & 1 & 0\\\\\n0 & 0  & 0  &  0 & -1 & 1\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nf_{1}\\\\\nf_{2}\\\\\nf_{3}\\\\\nf_{4}\\\\\nf_{5}\\\\\nf_{6}\\\\\n\\end{bmatrix}\n\\]\nEach row of this matrix encodes the finite difference formula for one point. The first row computes \\(-f_1 + f_2\\) (which is \\(f_2 - f_1\\)), the second row computes \\(-f_2 + f_3\\), and so on. When we multiply this matrix by the vector of function values and divide by \\(\\Delta x\\), we get all the derivatives at once!\nThis matrix perspective might seem like mathematical overkill for a simple derivative calculation, but it reveals deep structure. The differentiation matrix is sparse (mostly zeros) and has a specific banded structure‚Äînonzero elements only appear near the diagonal. This sparsity makes operations computationally efficient even for large grids.\n\n\nImplementation with SciPy\n\n\n\n\n\n\n\n\nFor Central Differences\nFor central differences \\(\\frac{f_{i+1} - f_{i-1}}{2\\Delta x}\\), we use:\n\n\n\n\n\n\n\n\nWhy This Matrix Approach Matters\nThe matrix representation offers several advantages beyond conceptual elegance. First, it provides clarity‚Äîseeing differentiation as matrix multiplication makes explicit that we‚Äôre applying a linear operator. This connects to the broader mathematical framework of linear algebra that pervades physics, from quantum mechanics (where operators act on state vectors) to continuum mechanics (where differential operators describe field evolution).\nSecond, it‚Äôs computationally efficient. Modern numerical libraries like NumPy are highly optimized for matrix operations, often using hardware-accelerated BLAS (Basic Linear Algebra Subprograms) implementations. By formulating differentiation as a matrix operation, we leverage decades of optimization work.\nThird, this perspective becomes essential when solving partial differential equations (PDEs), which we‚Äôll encounter in more advanced computational physics. Many PDE solution methods involve constructing large differentiation matrices and solving linear systems. Understanding the matrix representation now prepares you for these more complex problems.\nFinally, it demonstrates that seemingly different mathematical operations can be equivalent‚Äîfinite differences and matrix multiplication are just two views of the same underlying computation. This kind of mathematical flexibility is valuable throughout physics.\nNote that the @ operator in Python denotes matrix multiplication (following proper linear algebra rules), distinct from element-wise multiplication using *.",
    "crumbs": [
      "‚ö° Week 5: Classical Mechanics 2",
      "Numerical Differentiation"
    ]
  },
  {
    "objectID": "lectures/lecture07/1_differentiation.html#second-order-derivative-measuring-curvature",
    "href": "lectures/lecture07/1_differentiation.html#second-order-derivative-measuring-curvature",
    "title": "Numerical Differentiation",
    "section": "Second Order Derivative: Measuring Curvature",
    "text": "Second Order Derivative: Measuring Curvature\nThe second derivative measures curvature‚Äîhow rapidly the rate of change itself is changing. In physics, second derivatives are ubiquitous. They appear in Newton‚Äôs second law \\[F = m\\ddot{x}\\] in the wave equation \\[\\frac{\\partial^2 u}{\\partial t^2} = c^2\\frac{\\partial^2 u}{\\partial x^2}\\] and in the Schr√∂dinger equation \\[-\\frac{\\hbar^2}{2m}\\frac{d^2\\psi}{dx^2} + V\\psi = E\\psi\\]. Understanding how to compute second derivatives numerically is therefore essential.\nWe can derive the second derivative formula using the same Taylor series approach. This time, instead of subtracting the forward and backward expansions to eliminate the second derivative term, we add them to isolate it:\n\\[\nf_{i+1} + f_{i-1} = 2f_{i} + \\Delta x^2 f_{i}^{\\prime\\prime} + O(\\Delta x^4)\n\\]\nNotice that the first derivative terms \\(\\pm\\Delta x f_{i}^{\\prime}\\) cancel when we add, while the second derivative terms double. The third-order terms also cancel due to their opposite signs. Solving for the second derivative and neglecting fourth-order and higher terms:\n\\[\nf_{i}^{\\prime\\prime}\\approx \\frac{f_{i-1}-2f_{i}+f_{i+1}}{\\Delta x^2}\n\\]\nThis formula has the same order of accuracy as our central difference formula for the first derivative‚Äîthe error is \\(O(\\Delta x^2)\\). The symmetric combination of three points (\\(i-1\\), \\(i\\), and \\(i+1\\)) captures the local curvature at point \\(i\\).\nPhysically, you can interpret this formula as measuring how much the function deviates from linear behavior. If \\(f\\) is perfectly linear, the three points lie on a straight line, and \\(f_{i-1} - 2f_i + f_{i+1} = 0\\), giving zero second derivative. Any curvature‚Äîeither concave up (positive second derivative) or concave down (negative)‚Äîproduces a nonzero result.\n\nExample: Second Derivative of sin(x)\nThe second derivative of \\(\\sin(x)\\) is \\(-\\sin(x)\\)‚Äîa beautiful result showing that sine is an eigenfunction of the second derivative operator with eigenvalue \\(-1\\). This property underlies simple harmonic motion and wave phenomena throughout physics. Let‚Äôs verify that our numerical formula reproduces this result:",
    "crumbs": [
      "‚ö° Week 5: Classical Mechanics 2",
      "Numerical Differentiation"
    ]
  },
  {
    "objectID": "lectures/lecture07/1_differentiation.html#using-scipy-standing-on-the-shoulders-of-giants",
    "href": "lectures/lecture07/1_differentiation.html#using-scipy-standing-on-the-shoulders-of-giants",
    "title": "Numerical Differentiation",
    "section": "Using SciPy: Standing on the Shoulders of Giants",
    "text": "Using SciPy: Standing on the Shoulders of Giants\nWhile implementing differentiation algorithms yourself builds understanding, production code should use well-tested libraries. The SciPy module provides robust, optimized differentiation functions that handle edge cases and numerical precision issues more carefully than our simple implementations.\nThe scipy.misc.derivative function calculates numerical derivatives of any function you provide. It uses adaptive algorithms that automatically choose appropriate step sizes and combines multiple Richardson extrapolation steps to improve accuracy:\n\n\n\n\n\n\nYou can also calculate higher derivatives by specifying the n parameter:\n\n\n\n\n\n\nThe order parameter controls the number of points used in the finite difference formula. Higher-order formulas use more neighboring points to achieve better accuracy by canceling more error terms in the Taylor expansion. This is analogous to using higher-order approximations in perturbation theory or more terms in a multipole expansion‚Äîmore computational effort yields more accurate results:",
    "crumbs": [
      "‚ö° Week 5: Classical Mechanics 2",
      "Numerical Differentiation"
    ]
  },
  {
    "objectID": "lectures/lecture07/1_differentiation.html#summary-from-continuous-to-discrete-calculus",
    "href": "lectures/lecture07/1_differentiation.html#summary-from-continuous-to-discrete-calculus",
    "title": "Numerical Differentiation",
    "section": "Summary: From Continuous to Discrete Calculus",
    "text": "Summary: From Continuous to Discrete Calculus\nThis lecture bridged the gap between the continuous calculus you learn in mathematics courses and the discrete computations required for numerical physics. We developed methods to approximate derivatives from finite data points, understanding both how these methods work and why they achieve different levels of accuracy.\nNumerical differentiation approximates derivatives using finite differences‚Äîcomputing slopes from discrete function values rather than taking continuous limits. The simplest approach is the forward difference, but we showed that central differences achieve higher accuracy by symmetrically sampling points on both sides of our evaluation point. The cancellation of even-order error terms in the Taylor expansion makes central differences significantly more accurate than one-sided methods.\nThe key formulas for uniformly spaced grids are:\n\nFirst derivative: \\(f^{\\prime}_{i} \\approx \\frac{f_{i+1} - f_{i-1}}{2 \\Delta x}\\) with error \\(O(\\Delta x^2)\\)\nSecond derivative: \\(f^{\\prime\\prime}_{i} \\approx \\frac{f_{i-1}-2f_{i}+f_{i+1}}{\\Delta x^2}\\) with error \\(O(\\Delta x^2)\\)\n\nWe implemented these formulas using NumPy array slicing, leveraging vectorization for efficiency. We also explored the matrix representation of differentiation, revealing that finite difference operations are linear transformations that can be expressed as sparse matrix multiplication. This perspective becomes crucial when solving partial differential equations.\nFinally, we examined SciPy‚Äôs production-ready differentiation functions, which employ sophisticated adaptive algorithms and Richardson extrapolation to achieve high accuracy.\nThese numerical differentiation techniques are fundamental tools for computational physics. They allow us to work with experimental data, approximate solutions to differential equations, and simulate physical systems on computers. In the next lectures, we‚Äôll apply these tools to solve ordinary and partial differential equations, bringing us closer to simulating real physical phenomena.\n\n\n\n\n\n\nThe Bigger Picture: Why Numerical Differentiation Matters\n\n\n\nPhysics is fundamentally a science of change, and derivatives are the mathematical language we use to describe that change. Most of the laws governing the physical universe are expressed as differential equations‚Äîrelationships involving derivatives of physical quantities. Newton‚Äôs second law relates force to the second time derivative of position: \\(F = m\\frac{d^2x}{dt^2}\\). Maxwell‚Äôs equations describe how electric and magnetic fields evolve through space and time using partial derivatives. The Schr√∂dinger equation governs quantum mechanics through a second spatial derivative. The heat equation, wave equation, diffusion equation‚Äîall involve derivatives.\nWhen we solve these equations on a computer, we must discretize them, replacing continuous derivatives with finite difference approximations. Every numerical simulation of physical phenomena‚Äîfrom weather forecasting to quantum chemistry to computational fluid dynamics‚Äîrelies on the principles of numerical differentiation we‚Äôve explored here.\nIn upcoming lectures, we‚Äôll apply these tools to solve ordinary differential equations (ODEs), allowing us to predict the time evolution of dynamical systems. Later, we‚Äôll extend to partial differential equations (PDEs), simulating fields and waves. The numerical differentiation techniques you‚Äôve learned today form the foundation for all of these more advanced topics in computational physics.",
    "crumbs": [
      "‚ö° Week 5: Classical Mechanics 2",
      "Numerical Differentiation"
    ]
  },
  {
    "objectID": "lectures/lecture07/1_differentiation copy.html",
    "href": "lectures/lecture07/1_differentiation copy.html",
    "title": "Numerical Differentiation",
    "section": "",
    "text": "While we did introduce derivatives shortly already when exploring the slicing of arrays, we will now look at the numerical differentiation in more detail. This will require again a little bit of math."
  },
  {
    "objectID": "lectures/lecture07/1_differentiation copy.html#first-order-derivative",
    "href": "lectures/lecture07/1_differentiation copy.html#first-order-derivative",
    "title": "Numerical Differentiation",
    "section": "First Order Derivative",
    "text": "First Order Derivative\nOur previous method of finding the derivative was based on the definition of the derivative itself. The derivative of a function \\(f(x)\\) at a point \\(x\\) is defined as the limit of the difference quotient as the interval \\(\\Delta x\\) goes to zero:\n\\[\nf^{\\prime}(x) = \\lim_{\\Delta x \\rightarrow 0} \\frac{f(x + \\Delta x) - f(x)}{\\Delta x}\n\\]\nIf we do not take the limit, we can approximate the derivative by:\n\\[\nf^{\\prime}_{i} \\approx \\frac{f_{i+1} - f_{i}}{\\Delta x}\n\\]\nHere, we look to the right of the current position \\(i\\) and divide by the interval \\(\\Delta x\\). It is not difficult to see that the resulting local error \\(\\delta\\) at each step is given by:\n\\[\n\\delta = f_{i+1} - f_{i} - \\Delta x f^{\\prime}(x_i) = \\frac{1}{2} \\Delta x^2 f^{\\prime \\prime}(x_i) + O(\\Delta x^3)\n\\]\nIt can be seen that the error is proportional to the square of the interval \\(\\Delta x\\). This is the reason why the method is called first order accurate. The error is of the order of \\(\\Delta x^{2}\\).\nA better expression can be found using the Taylor expansion around the position \\(x_0\\):\n\\[\nf(x) = f(x_{0}) + (x - x_0) f^{\\prime}(x) + \\frac{(x - x_0)^2}{2!} f^{\\prime\\prime}(x) + \\frac{(x - x_0)^3}{3!} f^{(3)}(x) + \\ldots\n\\]\nIn discrete notation, this gives:\n\\[\nf_{i+1} = f_{i} + \\Delta x f_{i}^{\\prime} + \\frac{\\Delta x^2}{2!} f_{i}^{\\prime\\prime} + \\frac{\\Delta x^3}{3!} f_{i}^{(3)} + \\ldots\n\\]\nThe same can be done to obtain the function value at \\(i-1\\):\n\\[\nf_{i-1} = f_{i} - \\Delta x f_{i}^{\\prime} + \\frac{\\Delta x^2}{2!} f_{i}^{\\prime\\prime} - \\frac{\\Delta x^3}{3!} f_{i}^{(3)} + \\ldots\n\\]\nSubtracting these two equations, we get:\n\\[\nf_{i+1} - f_{i-1} = 2 \\Delta x f_{i}^{\\prime} + O(\\Delta x^3)\n\\]\nsuch that the second order term in \\(\\Delta x\\) disappears. Neglecting the higher-order terms, we have\n\\[\nf^{\\prime}_{i} \\approx \\frac{f_{i+1} - f_{i-1}}{2 \\Delta x}\n\\]\nan thus have a first order derivative which is even more accurate than the one obtained from the definition of the derivative.\nWe can continue that type of derivation now to obtain higher order approximation of the first derivative with better accuracy. For that purpose you may calculate now \\(f_{i\\pm 2}\\) and combining that with \\(f_{i+1}-f_{i-1}\\) will lead to\n\\[\\begin{equation}\nf_{i}^{\\prime}=\\frac{1}{12 \\Delta x}(f_{i-2}-8f_{i-1}+8f_{i+1}-f_{i+2})\n\\end{equation}\\]\nThis can be used to give even better values for the first derivative.\nLet`s try out one of the formulas in the following code cell. We will write a function that calculates the derivative of a given function at a given position \\(x\\). The function will take the function \\(f\\) as and argument, which is new to us. We will also introduce a small interval \\(h=\\Delta x\\) which will be used to calculate the derivative. The function will return the derivative of the function at the given position \\(x\\).\n\n\n\n\n\n\nNote that the definition contains additional parameters *params which are passed to the function f. This is a general way to pass additional parameters to the function f which is used in the definition of the derivative.\nWe will try to calculate the derivative of the \\(\\sin(x)\\) function:\n\n\n\n\n\n\nWe can plot this and nicely obtain our cosine function\n\n\n\n\n\n\n\nMatrix Version of the First Derivative\nIf we supply the above function with an array of positions \\(x_{i}\\) at which we would like to calculate the derivative, we obtain an array of derivative values. We can also write this procedure in a different way, which will be helpful for solving differential equations later.\nIf we consider the above finite difference formulas for a set of positions \\(x_{i}\\), we can represent the first derivative at these positions by a matrix operation as well:\n\\[\nf^{\\prime} = \\frac{1}{\\Delta x}\n\\begin{bmatrix}\n-1 & 1  & 0 & 0 & 0 & 0\\\\\n0 & -1 & 1 & 0 & 0 & 0\\\\\n0 & 0  & -1 & 1 & 0 & 0\\\\\n0 & 0  & 0  & -1 & 1 & 0\\\\\n0 & 0  & 0  &  0 & -1 & 1\\\\\n0 & 0  & 0  &  0 &  0 & -1\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nf_{1}\\\\\nf_{2}\\\\\nf_{3}\\\\\nf_{4}\\\\\nf_{5}\\\\\nf_{6}\\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\frac{f_{2} - f_{1}}{\\Delta x}\\\\\n\\frac{f_{3} - f_{2}}{\\Delta x}\\\\\n\\frac{f_{4} - f_{3}}{\\Delta x}\\\\\n\\frac{f_{5} - f_{4}}{\\Delta x}\\\\\n\\frac{f_{6} - f_{5}}{\\Delta x}\\\\\n\\frac{0 - f_{6}}{\\Delta x}\\\\\n\\end{bmatrix}\n\\]\nNote that here we took the derivative only to the right side! Each row of the matrix, when multiplied by the vector containing the function values, gives the derivative of the function \\(f\\) at the corresponding position \\(x_{i}\\). The resulting vector represents the derivative in a certain position region.\nWe will demonstrate how to generate such a matrix with the SciPy module below."
  },
  {
    "objectID": "lectures/lecture07/1_differentiation copy.html#second-order-derivative",
    "href": "lectures/lecture07/1_differentiation copy.html#second-order-derivative",
    "title": "Numerical Differentiation",
    "section": "Second order derivative",
    "text": "Second order derivative\nWhile we did before calculate the first derivative, we can also calculate the second derivative of a function. In the previous calculations we evaluated \\(f_{i+1} - f_{i-1}\\). We can now also use the sum of both to arrive at\n\\[\\begin{equation}\nf_{i}^{\\prime\\prime}\\approx \\frac{f_{i-1}-2f_{i}+f_{i+1}}{\\Delta x^2}\n\\end{equation}\\]\nwhich gives the basic equation for calculating the second order derivative and the next order may be obtained from\n\\[\\begin{equation}\nf_{i}^{\\prime\\prime}\\approx \\frac{1}{12 \\Delta x^{2}}(-f_{i-2}+16f_{i-1}-30 f_{i}+16f_{i+1}-f_{i+2})\n\\end{equation}\\]\nwhich is again better than our previous formula, yet needs more function values to be calculated."
  },
  {
    "objectID": "lectures/lecture07/1_differentiation copy.html#scipy-module",
    "href": "lectures/lecture07/1_differentiation copy.html#scipy-module",
    "title": "Numerical Differentiation",
    "section": "SciPy Module",
    "text": "SciPy Module\nOf course, we are not the first to define some functions for calculating the derivative of functions numerically. This is already implemented in different modules. One module is the above mentioned SciPy module.\nThe SciPy module provides the method derivative, which we can call with\nderivative(f,x,dx=1.0,n=1):\nThis will calculate the n\\(th\\) derivative of the function \\(f\\) at the position \\(x\\) with a intervall \\(dx=1.0\\) (default value).\n\n\n\n\n\n\nWe also have the option to define the order parameter, which is not the order of the derivative but rather the number of points used to calculate the derivative according to our scheme earlier.\n\n\n\n\n\n\n\nMatrix Version\nThe SciPy module allows us to construct matrices as mentioned above. We will need the diags method from the SciPy module for that purpose.\n\n\n\n\n\n\nLet‚Äôs assume we want to calculate the derivative of the sin function at certain positions.\n\n\n\n\n\n\nThe diags function uses a set of numbers that should be distributed along the diagonals of the matrix. If you supply a list like in the example below, the numbers are distributed using the offsets as defined in the second list. The shape keyword defines the shape of the matrix. Try the example in the next cell with the .todense() suffix. This converts the otherwise unreadable sparse output to a readable matrix form.\n\n\n\n\n\n\nTo comply with our previous definition of \\(N=100\\) data points and the interval \\(\\Delta x\\), we define:\n\n\n\n\n\n\nThe derivative is then simply a matrix-vector multiplication, which is done either by np.dot(m,y) or just by the @ operator.\n\n\n\n\n\n\nLet‚Äôs plot the original function and its numerical derivative.\n\n\n\n\n\n\nCheck for yourself that the following line of code will calculate the second derivative.\n\n\n\n\n\n\nLet‚Äôs plot the original function and its second numerical derivative.\n\n\n\n\n\n\nThis demonstrates how to use the SciPy module to construct matrices for numerical differentiation and how to apply these matrices to compute first and second derivatives.\n\n\n\n\n\n\nApplications of the Matrix Method\n\n\n\n\n\nThe matrix method for computing derivatives is particularly useful in several contexts, especially in numerical analysis and computational mathematics. Here are some key applications:\n\nSolving Differential Equations:\n\nOrdinary Differential Equations (ODEs): The matrix method can be used to discretize ODEs, transforming them into a system of linear equations that can be solved using linear algebra techniques.\nPartial Differential Equations (PDEs): Similarly, PDEs can be discretized using finite difference methods, where derivatives are approximated by matrix operations. This is essential in fields like fluid dynamics, heat transfer, and electromagnetics.\n\nNumerical Differentiation:\n\nThe matrix method provides a systematic way to approximate derivatives of functions given discrete data points. This is useful in data analysis, signal processing, and any application where you need to estimate the rate of change from sampled data.\n\nStability and Accuracy Analysis:\n\nBy representing derivative operations as matrices, it becomes easier to analyze the stability and accuracy of numerical schemes. This is crucial for ensuring that numerical solutions to differential equations are reliable.\n\nOptimization Problems:\n\nIn optimization, especially in gradient-based methods, the matrix method can be used to compute gradients and Hessians efficiently. This is important in machine learning, operations research, and various engineering disciplines.\n\nFinite Element Analysis (FEA):\n\nIn FEA, the matrix method is used to approximate derivatives and integrals over complex geometries. This is widely used in structural engineering, biomechanics, and materials science.\n\nControl Theory:\n\nIn control theory, especially in the design and analysis of control systems, the matrix method can be used to model and simulate the behavior of dynamic systems."
  },
  {
    "objectID": "lectures/lecture10/2_planetary_motion.html#physical-model",
    "href": "lectures/lecture10/2_planetary_motion.html#physical-model",
    "title": "Planetary Motion",
    "section": "Physical Model",
    "text": "Physical Model\nThe motion of planets around the Sun is a classic problem in physics that beautifully demonstrates Newton‚Äôs laws of motion and universal gravitation. While this motion might seem very different from a spring pendulum, the mathematical description is surprisingly similar. The main difference is that instead of a spring force, we now have gravity as our central force.\nIn both cases, we describe the motion using two coordinates: the distance from the center (\\(r\\)) and an angle (\\(\\theta\\)). For planetary motion, \\(r\\) is the distance between the planet and the Sun, and \\(\\theta\\) describes the planet‚Äôs angular position in its orbit.\nThe equations of motion contain two parts: The first equation describes the radial acceleration (\\(\\ddot{r}\\)), and the second describes the angular acceleration (\\(\\ddot{\\theta}\\)):\n\\[\\begin{eqnarray}\n\\ddot{r}&=&r\\dot{\\theta}^2-\\frac{G\\, M}{r^2}\\\\\n\\ddot{\\theta}&=&-\\frac{1}{r}2\\dot{r}\\dot{\\theta}\n\\end{eqnarray}\\]\nIn the first equation, the term \\(r\\dot{\\theta}^2\\) represents the centrifugal effect - the tendency of the planet to move away from the Sun due to its orbital motion. The term \\(-\\frac{G\\, M}{r^2}\\) is Newton‚Äôs gravitational force (divided by mass), pulling the planet toward the Sun. \\(G\\) is the gravitational constant, and \\(M\\) is the mass of the Sun.\nThe second equation describes how the angular motion changes. The term \\(2\\dot{r}\\dot{\\theta}\\) represents the coupling between radial and angular motion - as the planet moves closer to or farther from the Sun, its angular velocity must change to conserve angular momentum, similar to how an ice skater spins faster when pulling their arms in.\nThe solution to these equations gives us the famous orbital equation:\n\\[\nr(\\theta)=\\frac{p}{1+\\epsilon \\cos(\\theta)}\n\\]\nThis is the equation of a conic section, where \\(p\\) and \\(\\epsilon\\) determine the orbit‚Äôs shape. The parameter \\(p\\) is related to the angular momentum \\(L\\):\n\\[\\begin{equation}\np=\\frac{L^2}{G M m^2}\n\\end{equation}\\]\nThe eccentricity \\(\\epsilon\\) depends on both the energy \\(E\\) and angular momentum \\(L\\):\n\\[\\begin{equation}\n\\epsilon=\\sqrt{1+\\frac{2\\frac{E}{m}\\frac{L^2}{m^2}}{G^2M^2}}\n\\end{equation}\\]\nWhen \\(0 &lt; \\epsilon &lt; 1\\), we get an elliptical orbit (the case for all planets in our solar system). A perfect circle occurs when \\(\\epsilon = 0\\). These equations, derived by Newton and studied by Kepler, explain not only planetary orbits but also the paths of comets, artificial satellites, and many other celestial objects."
  },
  {
    "objectID": "lectures/lecture10/2_planetary_motion.html#numerical-solution",
    "href": "lectures/lecture10/2_planetary_motion.html#numerical-solution",
    "title": "Planetary Motion",
    "section": "Numerical Solution",
    "text": "Numerical Solution\nFor the numerical solution, we will use the odeint function from the scipy.integrate module. This function solves ordinary differential equations (ODEs) given an initial state and a time array. We will integrate the equations of motion for a planet with an initial radius \\(r_0\\), radial velocity \\(v_0\\), angle \\(\\theta_0\\), and angular velocity \\(\\omega_0\\).\n\nInitial Parameters: Planets\nWe first define the initial parameters for the planet‚Äôs motion. We set the mass of the Sun \\(M=1\\) and the mass of the planet \\(m=1\\) for simplicity. The gravitational constant \\(G\\) is set to \\(4\\pi^2\\) to simplify the equations. We also define the initial radius \\(r_0\\), radial velocity \\(v_0\\), angle \\(\\theta_0\\), and angular velocity \\(\\omega_0\\).\n\n\n\n\n\n\n\n\nSolution: Planets\nT\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting: Planets\nWe can now plot the numerical solution of the planetary motion. The black dashed line represents the analytical solution for the orbit of the planet. The black solid line shows the numerical solution obtained by integrating the equations of motion.\n\nTrajectory\n\n\n\n\n\n\n\n\nEnergy\nFinally, we can plot the total energy of the planet as a function of time. The total energy is the sum of the kinetic and potential energy of the planet. We can see that the energy is conserved over time, as expected for a system with no external forces."
  },
  {
    "objectID": "lectures/lecture10/DampedOscillation.html",
    "href": "lectures/lecture10/DampedOscillation.html",
    "title": "Appendix: Fourier Analysis of a damped oscillation",
    "section": "",
    "text": "Consider a generalised description of the oscillation by\n\\[\\begin{equation}\n    f(t)=a(t)\\cos(\\omega_S t)\n\\end{equation}\\]\nWhere \\(a(t)\\) is the time varying amplitude. This product of two functions can be treated with the help of the so-called convolution theorem. A convolution is represented by\n\\[\\begin{equation}\\label{eq:convint}\n    f(t)*g(t)=\\int_{-\\infty}^{\\infty} f(\\tau)g(t-\\tau)d\\tau\n\\end{equation}\\]\nwhich means, that one sums up all contributions of a function \\(g(t)\\) centered at a time value of \\(\\tau\\) with amplitude \\(f(\\tau)\\). Convolutions play an important role for example in optics, where the microscope resolution function convolutes the structural images of all objects.\nThis convolution integral can be transformed into a product of the Fourier transforms \\((\\mathscr{F}\\)) of both functions\n\\[\\begin{equation}\n    H(\\omega)G(\\omega)=\\mathscr{F}(f*g)\n\\end{equation}\\]\nSo the product of the Fourier transform of the individual functions in the product is the same as the Fourier transform of the convolution integral Eq. \\(\\ref{eq:convint}\\).\nWhat is relevant in the discussed case of the oscillating guitar string is the inverse relation of the convolution theorem\n\\[\\begin{equation}\n    H(\\omega)*G(\\omega)=\\mathscr{F}(f(t) g(t))\n\\end{equation}\\]\nThis means that the Fourier transform of a product of two functions is equivalent to a convolution of the Fourier transforms of the individual functions.\nThe convolution integral of the Fourier transformed function then corresponds to\n\\[\\begin{equation}\\label{eq:inverse_conv}\n    H(\\omega)*G(\\omega)=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty} F(\\Omega)G(\\omega-\\Omega)d\\Omega\n\\end{equation}\\]\nUsing this relation and\n\\[\\begin{equation}\n    H(\\omega)=\\mathscr{F}(\\cos(\\omega_S t))\n\\end{equation}\\]\nand\n\\[\\begin{equation}\n    G(\\omega)=\\mathscr{F}(\\Theta(t) e^{-t/\\tau})\n\\end{equation}\\]\nI can compute all types of amplitude modulated harmonic oscillations in Fourier space. In particular, the Fourier transform \\(\\mathscr{F}\\) of a harmonic function \\(\\cos(\\omega_S t)\\) results in a so-called delta function (\\(\\delta(t)\\))\n\\[\\begin{equation}\\label{eq:cosineft}\n    H(\\omega)=\\mathscr{F}(\\cos(\\omega_S t))=\\sqrt{\\frac{\\pi}{2}} \\delta(\\omega+\\omega_S)-\\sqrt{\\frac{\\pi}{2}} \\delta(\\omega+\\omega_S)\n\\end{equation}\\]\nThe \\(\\delta\\)-function has the properties as described in section \\(\\ref{sec:delta}\\).\n\\[\\begin{equation}\\label{eq:lorentz}\n    G(\\omega)=\\mathscr{F}(\\Theta(t)e^{-t/\\tau})=\\frac{1}{\\sqrt{2\\pi}}\\frac{i \\tau}{(i-\\tau\\omega)}\n\\end{equation}\\]\nThe squared magnitude of Eq. \\(\\ref{eq:lorentz}\\) yields a Lorentzian lineshape\n\\[\\begin{equation}\n    |G(\\omega)|^{2}=\\frac{1}{2 \\pi} \\frac{\\tau^2}{1+\\tau^{2}\\omega^{2}}\n\\end{equation}\\]\nfor the frequency spectrum, which is very common in physics. This Lorentzian function has a maximum at \\(\\omega=0\\) with \\(|G(\\omega)|^2=\\tau^2/2\\pi\\).\nWith the help of Eq. \\(\\ref{eq:cosineft}\\) and Eq. \\(\\ref{eq:lorentz}\\) I can write the convolution integral \\(\\ref{eq:inverse_conv}\\) as\n\\[\\begin{equation}\\label{Delta Functions}\n    H(\\omega)*G(\\omega)=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}  \\sqrt{\\frac{\\pi}{2}} [\\delta(\\Omega-\\omega_S) + \\delta(\\Omega+\\omega_S)] \\frac{1}{\\sqrt{2\\pi}}\\frac{i \\tau}{(i-\\tau (\\omega-\\Omega))} d\\Omega\n\\end{equation}\\]\nThe integration can be carried out using the integration rule in Eq. \\(\\ref{delta}\\) for the delta function which leaves me with the integral of the function displayed in Eq. \\(\\ref{Delta Functions}\\) as equal to\n\\[\\begin{equation}\\label{Hallo}\n    F(\\omega)= H(\\omega)*G(\\omega)=\\frac{1}{\\sqrt{2\\pi}}\\cdot\\frac{\\sqrt\\pi}{\\sqrt2 \\sqrt{2\\pi}}\\left [\\frac{i\\tau}{(i-\\tau(\\omega-\\omega_S))} + \\frac{i\\tau}{(i-\\tau(\\omega+\\omega_S))}\\right ]\n\\end{equation}\\]\nThis yields\n\\[\\begin{equation}\nF(\\omega)=\\frac{1}{\\sqrt{2\\pi}}\\frac{1/\\tau+i\\omega}{(1/\\tau+i\\omega)^2+\\omega_S^2}\n\\end{equation}\\]"
  },
  {
    "objectID": "lectures/lecture05/01-lecture05.html",
    "href": "lectures/lecture05/01-lecture05.html",
    "title": "Lecture 5",
    "section": "",
    "text": "Introduction to classes in Python (optional, for organizing code).\nRotational kinematics and dynamics (moment of inertia, angular momentum).\n\n\n\n\n\nSimulating the motion of a rotating object (e.g., a spinning disk) and calculating its angular momentum.\nVisualizing the effect of torque on the object‚Äôs rotation.\nHomework: Extend the simulation to include the effect of external forces, such as friction."
  },
  {
    "objectID": "lectures/lecture05/01-lecture05.html#angular-momentum-and-rotational-motion",
    "href": "lectures/lecture05/01-lecture05.html#angular-momentum-and-rotational-motion",
    "title": "Lecture 5",
    "section": "",
    "text": "Introduction to classes in Python (optional, for organizing code).\nRotational kinematics and dynamics (moment of inertia, angular momentum).\n\n\n\n\n\nSimulating the motion of a rotating object (e.g., a spinning disk) and calculating its angular momentum.\nVisualizing the effect of torque on the object‚Äôs rotation.\nHomework: Extend the simulation to include the effect of external forces, such as friction."
  },
  {
    "objectID": "lectures/lecture05/brownian_motion_comparison.html#introduction",
    "href": "lectures/lecture05/brownian_motion_comparison.html#introduction",
    "title": "Brownian Motion: Comparing Two Approaches",
    "section": "Introduction",
    "text": "Introduction\nThis document compares two ways to simulate Brownian motion: the simple function-based approach (Week 2-3) and the object-oriented approach (Week 6-7). Both accomplish the same physics, but they organize the code differently.\n\n\n\n\n\n\nKey Question\n\n\n\nWhen should you use classes vs.¬†simple functions? This comparison will help you decide!"
  },
  {
    "objectID": "lectures/lecture05/brownian_motion_comparison.html#the-same-physics-two-implementations",
    "href": "lectures/lecture05/brownian_motion_comparison.html#the-same-physics-two-implementations",
    "title": "Brownian Motion: Comparing Two Approaches",
    "section": "The Same Physics, Two Implementations",
    "text": "The Same Physics, Two Implementations\nLet‚Äôs simulate 3 particles and compare how each approach handles this task.\n\nApproach 1: Simple Functions (Week 2-3)\nPros: - ‚úÖ Easy to understand - ‚úÖ Quick to write - ‚úÖ Good for small problems - ‚úÖ Direct and explicit\nCons: - ‚ùå Manual management of multiple particles - ‚ùå Data and behavior are separated - ‚ùå Hard to extend (what if particles have different properties?) - ‚ùå Easy to make mistakes (which array goes with which particle?)\n\n\n\n\n\n\n\n\n\n\n\n\nWhat if we had 100 particles?\n\n\n\nWith the function approach, we‚Äôd need either: - 200 separate variables (x1, y1, x2, y2, ‚Ä¶, x100, y100) ‚Üê Impossible! - Complex nested lists that are hard to manage - Careful indexing that‚Äôs error-prone\n\n\n\n\n\nApproach 2: Object-Oriented (Week 6-7)\nPros: - ‚úÖ Each particle manages its own data - ‚úÖ Easy to handle many particles - ‚úÖ Each particle can have different properties - ‚úÖ Behavior is bundled with data - ‚úÖ Less error-prone - ‚úÖ Easy to extend\nCons: - ‚ùå More complex syntax (need to learn self, __init__, etc.) - ‚ùå More setup code initially - ‚ùå Overkill for very simple problems"
  },
  {
    "objectID": "lectures/lecture05/brownian_motion_comparison.html#scaling-up-100-particles",
    "href": "lectures/lecture05/brownian_motion_comparison.html#scaling-up-100-particles",
    "title": "Brownian Motion: Comparing Two Approaches",
    "section": "Scaling Up: 100 Particles",
    "text": "Scaling Up: 100 Particles\nNow let‚Äôs see which approach scales better:\n\nApproach 1: Requires Lists of Lists\n\n\n\n\n\n\n\n\nApproach 2: Just a List of Objects"
  },
  {
    "objectID": "lectures/lecture05/brownian_motion_comparison.html#adding-features-a-real-world-scenario",
    "href": "lectures/lecture05/brownian_motion_comparison.html#adding-features-a-real-world-scenario",
    "title": "Brownian Motion: Comparing Two Approaches",
    "section": "Adding Features: A Real-World Scenario",
    "text": "Adding Features: A Real-World Scenario\nLet‚Äôs say you need to add: 1. Particle radius (R) 2. Calculate D from R using Stokes-Einstein: \\(D = k_BT/(6\\pi\\eta R)\\) 3. Track particle ID\n\nApproach 1: Gets Messy Fast\n\n\n\n\n\n\n\n\nApproach 2: Natural and Clean"
  },
  {
    "objectID": "lectures/lecture05/brownian_motion_comparison.html#decision-guide-when-to-use-each-approach",
    "href": "lectures/lecture05/brownian_motion_comparison.html#decision-guide-when-to-use-each-approach",
    "title": "Brownian Motion: Comparing Two Approaches",
    "section": "Decision Guide: When to Use Each Approach",
    "text": "Decision Guide: When to Use Each Approach\n\nUse Simple Functions When:\n\n‚úÖ You‚Äôre just learning Python\n‚úÖ Quick one-off calculations\n‚úÖ Very simple problems (1-2 particles)\n‚úÖ You won‚Äôt need to extend the code\n‚úÖ Focus is on understanding the physics first\n\n\n\nUse Classes When:\n\n‚úÖ Many objects with their own properties\n‚úÖ Objects need to maintain state over time\n‚úÖ You‚Äôll extend or modify the code later\n‚úÖ Multiple people will use your code\n‚úÖ Objects have both data AND behavior\n‚úÖ You want professional, maintainable code"
  },
  {
    "objectID": "lectures/lecture05/brownian_motion_comparison.html#summary-table",
    "href": "lectures/lecture05/brownian_motion_comparison.html#summary-table",
    "title": "Brownian Motion: Comparing Two Approaches",
    "section": "Summary Table",
    "text": "Summary Table\n\n\n\nAspect\nSimple Functions\nObject-Oriented\n\n\n\n\nLearning curve\nEasy\nModerate\n\n\nInitial code\nLess\nMore\n\n\nScalability\nPoor (many lists)\nExcellent\n\n\nMaintainability\nDifficult beyond 3 particles\nEasy\n\n\nExtensibility\nHard to add features\nEasy to add features\n\n\nError-prone?\nYes (index mismatches)\nNo (data stays together)\n\n\nBest for\nLearning, simple problems\nReal simulations"
  },
  {
    "objectID": "lectures/lecture05/brownian_motion_comparison.html#your-turn-practice-both",
    "href": "lectures/lecture05/brownian_motion_comparison.html#your-turn-practice-both",
    "title": "Brownian Motion: Comparing Two Approaches",
    "section": "Your Turn: Practice Both!",
    "text": "Your Turn: Practice Both!\nTry extending each approach to: 1. Add particle color as a property 2. Implement a method to calculate the MSD 3. Add particle-particle interactions (check if they‚Äôre too close)\nYou‚Äôll quickly see why classes become valuable for complex simulations!"
  },
  {
    "objectID": "lectures/lecture05/brownian_motion_comparison.html#key-takeaway",
    "href": "lectures/lecture05/brownian_motion_comparison.html#key-takeaway",
    "title": "Brownian Motion: Comparing Two Approaches",
    "section": "Key Takeaway",
    "text": "Key Takeaway\n\nBoth approaches work for the physics! The difference is code organization, maintainability, and scalability. Start simple (functions), then use classes when your code gets complex. This is the natural progression of a computational physicist! üéì"
  },
  {
    "objectID": "lectures/lecture02/02-lecture02.html",
    "href": "lectures/lecture02/02-lecture02.html",
    "title": "Modules",
    "section": "",
    "text": "Most of the functionality in Python is provided by modules. The Python Standard Library is a large collection of modules that provides cross-platform implementations of common facilities such as access to the operating system, file I/O, string management, network communication, math, web-scraping, text manipulation, machine learning and much more.\nTo use a module in a Python module it first has to be imported. A module can be imported using the import statement. For example, to import the module math, which contains many standard mathematical functions, we can do:\n\n\n\n\n\n\nThis includes the whole module and makes it available for use later in the program. Alternatively, we can chose to import all symbols (functions and variables) in a module so that we don‚Äôt need to use the prefix ‚Äúmath.‚Äù every time we use something from the math module:\n\n\n\n\n\n\nThis pattern can be very convenient, but in large programs that include many modules it is often a good idea to keep the symbols from each module in their own namespaces, by using the import math pattern. This would eliminate potentially confusing problems.\n\nNamespaces\n\n\n\n\n\n\nNamespaces\n\n\n\nA namespace is an identifier used to organize objects, e.g.¬†the methods and variables of a module. The prefix math. we have used in the previous section is such a namespace. You may also create your own namespace for a module. This is done by using the import math as mymath pattern.\n\n\n\n\n\n\n\n\nYou may also only import specific functions of a module.\n\n\n\n\n\n\n\n\nDirectory of a module\nOnce a module is imported, we can list the symbols it provides using the dir function:\n\n\n\n\n\n\nAnd using the function help we can get a description of each function (almost .. not all functions have docstrings, as they are technically called, but the vast majority of functions are documented this way).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also use the help function directly on modules: Try\nhelp(math)\nSome very useful modules form the Python standard library are os, sys, math, shutil, re, subprocess, multiprocessing, threading.\nA complete lists of standard modules for Python 3 is available at http://docs.python.org/3/library/ .\n\n\nAdvanced topics\n\n\n\n\n\n\nCreate Your Own Modules\n\n\n\n\n\nCreating your own modules in Python is a great way to organize your code and make it reusable. A module is simply a file containing Python definitions and statements. Here‚Äôs how you can create and use your own module:\n\nCreating a Module\nTo create a module, you just need to save your Python code in a file with a .py extension. For example, let‚Äôs create a module named mymodule.py with the following content:\n# mymodule.py\n\ndef greet(name: str) -&gt; str:\n    return f\"Hello, {name}!\"\n\ndef add(a: int, b: int) -&gt; int:\n    return a + b\n\n\nUsing Your Module\nOnce you have created your module, you can import it into other Python scripts using the import statement. Here‚Äôs an example of how to use the mymodule we just created:\n# main.py\n\nimport mymodule\n\n# Use the functions from mymodule\nprint(mymodule.greet(\"Alice\"))\nprint(mymodule.add(5, 3))\n\n\nImporting Specific Functions\nYou can also import specific functions from a module using the from ... import ... syntax:\n# main.py\n\nfrom mymodule import greet, add\n\n# Use the imported functions directly\nprint(greet(\"Bob\"))\nprint(add(10, 7))\n\n\nModule Search Path\nWhen you import a module, Python searches for the module in the following locations: 1. The directory containing the input script (or the current directory if no script is specified). 2. The directories listed in the PYTHONPATH environment variable. 3. The default directory where Python is installed.\nYou can view the module search path by printing the sys.path variable:\nimport sys\nprint(sys.path)\n\n\nCreating Packages\nA package is a way of organizing related modules into a directory hierarchy. A package is simply a directory that contains a special file named __init__.py, which can be empty. Here‚Äôs an example of how to create a package:\nmypackage/\n    __init__.py\n    module1.py\n    module2.py\nYou can then import modules from the package using the dot notation:\n# main.py\n\nfrom mypackage import module1, module2\n\n# Use the functions from the modules\nprint(module1.some_function())\nprint(module2.another_function())\nCreating and using modules and packages in Python helps you organize your code better and makes it easier to maintain and reuse.\n\n\nNamespaces in Packages\nYou can also create sub-packages by adding more directories with __init__.py files. This allows you to create a hierarchical structure for your modules:\nmypackage/\n    __init__.py\n    subpackage/\n        __init__.py\n        submodule.py\nYou can then import submodules using the full package name:\n# main.py\n\nfrom mypackage.subpackage import submodule\n\n# Use the functions from the submodule\nprint(submodule.some_sub_function())"
  },
  {
    "objectID": "lectures/lecture02/01-lecture02.html",
    "href": "lectures/lecture02/01-lecture02.html",
    "title": "Python Code Structures",
    "section": "",
    "text": "Functions are reusable blocks of code that can be executed multiple times from different parts of your program. They help in organizing code, making it more readable, and reducing redundancy. Functions can take input arguments and return output values.\n\nDefining a FunctionCalling a Function\n\n\nA function in Python is defined using the def keyword followed by the name of the function, which is usually descriptive and indicates what the function does. The parameters inside the parentheses indicate what data the function expects to receive. The -&gt; symbol is used to specify the return type of the function.\nHere‚Äôs an example:\n\n\n\n\n\n\n\n\nFunctions can be called by specifying the name of the function followed by parentheses containing the arguments. The arguments passed to the function should match the number and type of parameters defined in the function. Here‚Äôs an example:",
    "crumbs": [
      "üåü Week 3: Modeling Motion",
      "Functions: Reusable Physics Equations"
    ]
  },
  {
    "objectID": "lectures/lecture02/01-lecture02.html#functions",
    "href": "lectures/lecture02/01-lecture02.html#functions",
    "title": "Python Code Structures",
    "section": "",
    "text": "Functions are reusable blocks of code that can be executed multiple times from different parts of your program. They help in organizing code, making it more readable, and reducing redundancy. Functions can take input arguments and return output values.\n\nDefining a FunctionCalling a Function\n\n\nA function in Python is defined using the def keyword followed by the name of the function, which is usually descriptive and indicates what the function does. The parameters inside the parentheses indicate what data the function expects to receive. The -&gt; symbol is used to specify the return type of the function.\nHere‚Äôs an example:\n\n\n\n\n\n\n\n\nFunctions can be called by specifying the name of the function followed by parentheses containing the arguments. The arguments passed to the function should match the number and type of parameters defined in the function. Here‚Äôs an example:",
    "crumbs": [
      "üåü Week 3: Modeling Motion",
      "Functions: Reusable Physics Equations"
    ]
  },
  {
    "objectID": "lectures/lecture02/01-lecture02.html#loops",
    "href": "lectures/lecture02/01-lecture02.html#loops",
    "title": "Python Code Structures",
    "section": "Loops",
    "text": "Loops\nLoops are used to execute a block of code repeatedly. There are two main types of loops in Python: for loops and while loops.\n\nFor LoopWhile Loop\n\n\nA for loop in Python is used to iterate over a sequence (such as a list or string) and execute a block of code for each item in the sequence. Here‚Äôs an example:\n\n\n\n\n\n\n\n\nA while loop in Python is used to execute a block of code while a certain condition is met. The loop continues as long as the condition is true. Here‚Äôs an example:",
    "crumbs": [
      "üåü Week 3: Modeling Motion",
      "Functions: Reusable Physics Equations"
    ]
  },
  {
    "objectID": "lectures/lecture02/01-lecture02.html#conditional-statements",
    "href": "lectures/lecture02/01-lecture02.html#conditional-statements",
    "title": "Python Code Structures",
    "section": "Conditional Statements",
    "text": "Conditional Statements\nConditional statements are used to control the flow of your program based on conditions. The main conditional statements in Python are if, else, and elif.\n\nIf StatementElse StatementElif Statement\n\n\nAn if statement in Python is used to execute a block of code if a certain condition is met. Here‚Äôs an example:\n\n\n\n\n\n\n\n\nAn else statement in Python is used to execute a block of code if the condition in an if statement is not met. Here‚Äôs an example:\n\n\n\n\n\n\n\n\nAn elif statement in Python is used to execute a block of code if the condition in an if statement is not met but under an extra condition. Here‚Äôs an example:",
    "crumbs": [
      "üåü Week 3: Modeling Motion",
      "Functions: Reusable Physics Equations"
    ]
  },
  {
    "objectID": "lectures/lecture02/01-lecture02.html#practice-exercises",
    "href": "lectures/lecture02/01-lecture02.html#practice-exercises",
    "title": "Python Code Structures",
    "section": "Practice Exercises üéØ",
    "text": "Practice Exercises üéØ\n\nExercise 1: Distance Calculator Function\nWrite a function called calculate_distance that takes velocity (m/s) and time (s) as parameters and returns the distance traveled using the formula: \\(d = v \\cdot t\\)\n\nFunction should take two parameters: velocity and time\nReturn the calculated distance\nTest it with velocity = 15 m/s and time = 4 s\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndef calculate_distance(velocity: float, time: float) -&gt; float:\n    return velocity * time\n\n# Test the function\ndistance = calculate_distance(15, 4)\nprint(f\"Distance traveled: {distance} meters\")\n\n\n\n\n\nExercise 2: Even Numbers\nWrite a function called print_even_numbers that takes a number n as input and prints all even numbers from 2 to n (inclusive) using a for loop.\n\nUse a for loop with range()\nOnly print even numbers\nTest it with n = 20\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndef print_even_numbers(n: int):\n    for i in range(2, n + 1, 2):\n        print(i)\n\n# Alternative solution using conditional\ndef print_even_numbers_alt(n: int):\n    for i in range(1, n + 1):\n        if i % 2 == 0:\n            print(i)\n\n# Test the function\nprint_even_numbers(20)\n\n\n\n\n\nExercise 3: Temperature State\nWrite a function called water_state that takes a temperature in Celsius and returns the physical state of water:\n\nReturn ‚Äúsolid‚Äù if temperature &lt; 0\nReturn ‚Äúliquid‚Äù if 0 ‚â§ temperature &lt; 100\nReturn ‚Äúgas‚Äù if temperature ‚â• 100\nTest it with temperatures: -5, 25, 100, 150\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndef water_state(temperature: float) -&gt; str:\n    if temperature &lt; 0:\n        return \"solid\"\n    elif temperature &lt; 100:\n        return \"liquid\"\n    else:\n        return \"gas\"\n\n# Test the function\ntest_temps = [-5, 25, 100, 150]\nfor temp in test_temps:\n    state = water_state(temp)\n    print(f\"At {temp}¬∞C, water is {state}\")\n\n\n\n\n\nExercise 4: Kinetic Energy Calculator\nWrite a function called kinetic_energy that calculates the kinetic energy of an object using \\(E_k = \\frac{1}{2}mv^2\\). The function should:\n\nTake mass (kg) and velocity (m/s) as parameters\nReturn the kinetic energy in Joules\nPrint a warning message if velocity exceeds 100 m/s\nTest it with mass = 1000 kg and velocities from 0 to 120 m/s in steps of 30\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndef kinetic_energy(mass: float, velocity: float) -&gt; float:\n    if velocity &gt; 100:\n        print(f\"Warning: High velocity detected ({velocity} m/s)!\")\n\n    energy = 0.5 * mass * velocity**2\n    return energy\n\n# Test the function\nmass = 1000  # kg\nvelocities = [0, 30, 60, 90, 120]\n\nfor v in velocities:\n    energy = kinetic_energy(mass, v)\n    print(f\"v = {v} m/s: E_k = {energy/1000:.2f} kJ\")\n\n\n\n\n\nExercise 5: Factorial Calculator\nWrite a function called factorial that calculates the factorial of a number n (n!) using a while loop.\n\nUse a while loop (not a for loop)\nReturn the factorial value\nHint: 5! = 5 √ó 4 √ó 3 √ó 2 √ó 1 = 120\nTest it with n = 5 and n = 10\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndef factorial(n: int) -&gt; int:\n    result = 1\n    i = 1\n    while i &lt;= n:\n        result = result * i\n        i += 1\n    return result\n\n# Test the function\nprint(f\"5! = {factorial(5)}\")\nprint(f\"10! = {factorial(10)}\")\n\n\n\n\n\nExercise 6: FizzBuzz (Challenge!)\nWrite a function called fizzbuzz that prints numbers from 1 to n, but:\n\nFor multiples of 3, print ‚ÄúFizz‚Äù instead of the number\nFor multiples of 5, print ‚ÄúBuzz‚Äù instead of the number\nFor multiples of both 3 and 5, print ‚ÄúFizzBuzz‚Äù\nTest it with n = 20\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndef fizzbuzz(n: int):\n    for i in range(1, n + 1):\n        if i % 3 == 0 and i % 5 == 0:\n            print(\"FizzBuzz\")\n        elif i % 3 == 0:\n            print(\"Fizz\")\n        elif i % 5 == 0:\n            print(\"Buzz\")\n        else:\n            print(i)\n\n# Test the function\nfizzbuzz(20)",
    "crumbs": [
      "üåü Week 3: Modeling Motion",
      "Functions: Reusable Physics Equations"
    ]
  },
  {
    "objectID": "lectures/lecture02/01-lecture02.html#summary",
    "href": "lectures/lecture02/01-lecture02.html#summary",
    "title": "Python Code Structures",
    "section": "Summary ‚úÖ",
    "text": "Summary ‚úÖ\nEssential Python Structures:\n# Functions\ndef function_name(parameter: type) -&gt; return_type:\n    # code here\n    return value\n\n# For Loop\nfor i in range(start, stop, step):\n    # code here\n\n# While Loop\nwhile condition:\n    # code here\n\n# Conditionals\nif condition:\n    # code here\nelif another_condition:\n    # code here\nelse:\n    # code here\nRemember: Practice these structures - they are the building blocks of all your Python programs! üöÄ",
    "crumbs": [
      "üåü Week 3: Modeling Motion",
      "Functions: Reusable Physics Equations"
    ]
  },
  {
    "objectID": "lectures/lecture02/4_brownian_motion_simple.html#introduction-what-is-brownian-motion",
    "href": "lectures/lecture02/4_brownian_motion_simple.html#introduction-what-is-brownian-motion",
    "title": "Brownian Motion: Random Walks in Physics",
    "section": "Introduction: What is Brownian Motion?",
    "text": "Introduction: What is Brownian Motion?\nImagine a tiny dust particle floating in water. If you look at it under a microscope, you‚Äôll see something surprising: the particle doesn‚Äôt sit still! Instead, it moves in a random, zigzag pattern. This phenomenon is called Brownian motion, named after the botanist Robert Brown who first described it in 1827.\n\n\n\n\n\n\nAnimation of Brownian motion (c) Wikipedia.\n\n\nWhy Does This Happen?\nWater isn‚Äôt just a smooth, continuous fluid‚Äîit‚Äôs made up of countless tiny molecules in constant motion. These water molecules continuously collide with our particle from all directions:\n\nEach individual collision is tiny and barely noticeable\nMillions of collisions happen every second\nThe collisions come from random directions\nThe combined effect creates the zigzag motion we observe\n\nThis is a beautiful example of how microscopic randomness creates observable macroscopic behavior!\n\n\n\n\n\n\nWhy This Discovery Was Revolutionary\n\n\n\nWhen Robert Brown first observed this motion in 1827, nobody could explain it. For decades, scientists debated: Are atoms and molecules real, or just a useful fiction?\nIn 1905, Albert Einstein provided the crucial breakthrough. He showed mathematically that if matter were made of atoms, Brownian motion would occur exactly as observed. A few years later, Jean Perrin performed careful experiments measuring Brownian motion and used Einstein‚Äôs theory to:\n\nCalculate Avogadro‚Äôs number (the number of molecules in a sample)\nProve that atoms exist! This convinced the remaining skeptics that matter truly is made of discrete particles\n\nThis wasn‚Äôt just about tiny particles jiggling around‚Äîit was definitive proof of the atomic nature of matter.\n\n\n\n\n\n\n\n\nDeep Connections in Physics\n\n\n\nBrownian motion reveals several fundamental principles that appear throughout physics:\nTemperature has a microscopic meaning: The energy of molecular collisions is directly related to temperature. Warmer water means faster-moving molecules, which create more vigorous Brownian motion. This connects the abstract concept of temperature to the kinetic energy of individual atoms!\nFluctuation-dissipation relation: The same molecular collisions that cause random fluctuations (the zigzag motion) also create friction that slows the particle down (dissipation). These aren‚Äôt separate phenomena‚Äîthey‚Äôre two sides of the same coin! This deep connection appears everywhere in physics, from electrical circuits to quantum mechanics.\nStatistical mechanics: Individual molecular collisions are impossibly complex to track, yet their collective behavior follows simple, predictable laws. This is the power of statistical thinking in physics.\nYou‚Äôll encounter these ideas again and again as you progress in physics‚Äîin thermal physics, quantum mechanics, and even in understanding phenomena like noise in electronic circuits or the behavior of polymers. Brownian motion is where many of these big ideas first became concrete and measurable!",
    "crumbs": [
      "üåü Week 3: Modeling Motion",
      "Application: Brownian Motion (Simple)"
    ]
  },
  {
    "objectID": "lectures/lecture02/4_brownian_motion_simple.html#the-physics-behind-random-walks",
    "href": "lectures/lecture02/4_brownian_motion_simple.html#the-physics-behind-random-walks",
    "title": "Brownian Motion: Random Walks in Physics",
    "section": "The Physics Behind Random Walks",
    "text": "The Physics Behind Random Walks\n\nKey Ideas\nWhen a particle undergoes Brownian motion:\n\nEach step is random in direction\nThe step size depends on:\n\nTemperature (warmer ‚Üí more energetic collisions ‚Üí bigger steps)\nTime between observations (longer time ‚Üí more collisions ‚Üí bigger displacement)\nParticle size (smaller particles move more easily)\nAll captured in the diffusion coefficient \\(D\\)\n\n\n\n\nThe Simple Math\nThe typical distance a particle moves grows with time, but not linearly! Instead:\n\\[\\text{typical distance} \\propto \\sqrt{t}\\]\nMore precisely, the mean squared displacement is:\n\\[\\langle r^2 \\rangle = 4Dt\\]\nwhere:\n\n\\(D\\) is the diffusion coefficient (units: m¬≤/s)\n\\(t\\) is the elapsed time\nThe factor of 4 comes from motion in 2D (it would be 6 in 3D)\n\n\n\n\n\n\n\nRandom Steps from a Normal Distribution\n\n\n\nTo simulate random motion, we use np.random.normal() which generates random numbers from a normal (Gaussian) distribution.\nFor Brownian motion, each step has:\n\nMean = 0 (no preferred direction)\nStandard deviation = \\(\\sqrt{2D\\Delta t}\\) (typical step size)\n\nTry it:\nstep = np.random.normal(0, 1.0)  # mean=0, std=1",
    "crumbs": [
      "üåü Week 3: Modeling Motion",
      "Application: Brownian Motion (Simple)"
    ]
  },
  {
    "objectID": "lectures/lecture02/4_brownian_motion_simple.html#lets-simulate-one-particle",
    "href": "lectures/lecture02/4_brownian_motion_simple.html#lets-simulate-one-particle",
    "title": "Brownian Motion: Random Walks in Physics",
    "section": "Let‚Äôs Simulate One Particle!",
    "text": "Let‚Äôs Simulate One Particle!\nWe‚Äôll build up our simulation step by step, starting with the simplest case: one particle moving in 2D.\n\nStep 1: Setting Up Parameters\n\n\n\n\n\n\n\n\nStep 2: Creating a Function for One Random Step\nLet‚Äôs write a function that takes a current position and returns the new position after one random step:\n\n\n\n\n\n\n\n\nStep 3: Simulating a Complete Trajectory\nNow let‚Äôs create a function that simulates many steps:\n\n\n\n\n\n\n\n\nStep 4: Plotting the Trajectory\nLet‚Äôs visualize the random walk:\n\n\n\n\n\n\n\n\n\n\n\n\nExperiment!\n\n\n\nRun the simulation multiple times. Notice how:\n\nEach trajectory is different\nThe particle can end up far from the origin or close to it\nThe path is jagged and random\nThere‚Äôs no preferred direction\n\nTry changing N (number of steps) or dt (time step) to see how it affects the motion!",
    "crumbs": [
      "üåü Week 3: Modeling Motion",
      "Application: Brownian Motion (Simple)"
    ]
  },
  {
    "objectID": "lectures/lecture02/4_brownian_motion_simple.html#multiple-particles",
    "href": "lectures/lecture02/4_brownian_motion_simple.html#multiple-particles",
    "title": "Brownian Motion: Random Walks in Physics",
    "section": "Multiple Particles",
    "text": "Multiple Particles\nNow let‚Äôs simulate many particles at once to see the statistical behavior:\n\n\n\n\n\n\n\nVisualizing Multiple Particles\n\n\n\n\n\n\n\n\n\n\n\n\nWhat Do You See?\n\n\n\nEven though each trajectory is random and unpredictable, the ensemble (all particles together) shows clear statistical patterns:\n\nMost particles stay within a certain radius\nThe density decreases with distance from origin\nThe distribution is roughly circular (no preferred direction)",
    "crumbs": [
      "üåü Week 3: Modeling Motion",
      "Application: Brownian Motion (Simple)"
    ]
  },
  {
    "objectID": "lectures/lecture02/4_brownian_motion_simple.html#analyzing-the-motion-mean-squared-displacement",
    "href": "lectures/lecture02/4_brownian_motion_simple.html#analyzing-the-motion-mean-squared-displacement",
    "title": "Brownian Motion: Random Walks in Physics",
    "section": "Analyzing the Motion: Mean Squared Displacement",
    "text": "Analyzing the Motion: Mean Squared Displacement\nInstead of looking at individual trajectories, let‚Äôs measure the mean squared displacement (MSD), which tells us how far particles have moved on average.\n\nComputing MSD for One Particle\n\n\n\n\n\n\n\n\nMSD for Many Particles (Better Statistics!)\n\n\n\n\n\n\n\n\n\n\n\n\nKey Observations\n\n\n\n\nIndividual trajectories fluctuate a lot around the theoretical prediction\nOn average, the MSD follows \\(\\langle r^2 \\rangle = 4Dt\\)\nFluctuations are larger at long times (fewer data points to average)\nThis is why experimentalists use many particles or long trajectories to measure \\(D\\) accurately!",
    "crumbs": [
      "üåü Week 3: Modeling Motion",
      "Application: Brownian Motion (Simple)"
    ]
  },
  {
    "objectID": "lectures/lecture02/4_brownian_motion_simple.html#challenge-measuring-the-diffusion-coefficient",
    "href": "lectures/lecture02/4_brownian_motion_simple.html#challenge-measuring-the-diffusion-coefficient",
    "title": "Brownian Motion: Random Walks in Physics",
    "section": "Challenge: Measuring the Diffusion Coefficient",
    "text": "Challenge: Measuring the Diffusion Coefficient\nCan you extract the diffusion coefficient from your simulation data?\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe MSD is linear in time: \\(\\text{MSD} = 4Dt\\)\nSo the slope of MSD vs.¬†time gives you \\(4D\\).\nYou can use linear regression or just compute: \\(D = \\text{slope} / 4\\)\nTry fitting a line to your MSD data!",
    "crumbs": [
      "üåü Week 3: Modeling Motion",
      "Application: Brownian Motion (Simple)"
    ]
  },
  {
    "objectID": "lectures/lecture02/4_brownian_motion_simple.html#summary",
    "href": "lectures/lecture02/4_brownian_motion_simple.html#summary",
    "title": "Brownian Motion: Random Walks in Physics",
    "section": "Summary",
    "text": "Summary\nIn this notebook, you learned:\n‚úÖ What Brownian motion is: Random motion caused by molecular collisions\n‚úÖ How to simulate it: Using random steps from a normal distribution\n‚úÖ Key equation: Mean squared displacement \\(\\langle r^2 \\rangle = 4Dt\\)\n‚úÖ Programming skills:\n\nWriting functions with multiple return values\nUsing numpy arrays for trajectories\nCreating loops to simulate multiple particles\nPlotting and analyzing data\n\n‚úÖ Physics insights:\n\nIndividual trajectories are random and unpredictable\nStatistical averages follow predictable patterns\nThe diffusion coefficient can be measured from MSD",
    "crumbs": [
      "üåü Week 3: Modeling Motion",
      "Application: Brownian Motion (Simple)"
    ]
  },
  {
    "objectID": "lectures/lecture02/4_brownian_motion_simple.html#next-steps",
    "href": "lectures/lecture02/4_brownian_motion_simple.html#next-steps",
    "title": "Brownian Motion: Random Walks in Physics",
    "section": "Next Steps",
    "text": "Next Steps\nLater in the course, we‚Äôll revisit Brownian motion using object-oriented programming (classes), which will make it easier to:\n\nManage many particles with different properties\nOrganize complex simulations\nAdd features like particle interactions\n\nBut for now, you have all the tools to simulate and analyze Brownian motion! üéâ",
    "crumbs": [
      "üåü Week 3: Modeling Motion",
      "Application: Brownian Motion (Simple)"
    ]
  },
  {
    "objectID": "lectures/lecture02/4_brownian_motion_simple.html#optional-further-explorations",
    "href": "lectures/lecture02/4_brownian_motion_simple.html#optional-further-explorations",
    "title": "Brownian Motion: Random Walks in Physics",
    "section": "Optional: Further Explorations",
    "text": "Optional: Further Explorations\nTry modifying the code to explore:\n\nEffect of particle size: Change \\(D\\) (smaller \\(D\\) = larger particle)\n3D Brownian motion: Add a \\(z\\) coordinate (hint: MSD becomes \\(6Dt\\))\nDifferent time steps: How does dt affect the simulation?\nTemperature effects: \\(D\\) is proportional to temperature!\nConfined motion: Add boundaries (particles bounce off walls)",
    "crumbs": [
      "üåü Week 3: Modeling Motion",
      "Application: Brownian Motion (Simple)"
    ]
  },
  {
    "objectID": "lectures/lecture02/datatypes_quiz.html",
    "href": "lectures/lecture02/datatypes_quiz.html",
    "title": "Datatypes Quiz",
    "section": "",
    "text": "Test your knowledge of Python data types with this interactive quiz!\n\n\nWhat is the output of len(\"Hello, World!\")?\n\n\n\n\n\n\n\n10\n11\n13\n14\n\nWhat is the result of [1, 2, 3] + [4, 5]?\n\n\n\n\n\n\n\n(1, 2, 3, 4, 5)\n[1, 2, 3, 4, 5]\n[5, 7, 8]\nError\n\nWhich of the following creates an empty dictionary?\n\n{}\n[]\n()\nBoth {} and dict()\n\nWhat is the output of set([1, 2, 2, 3, 3, 3])?\n\n\n\n\n\n\n\n[1, 2, 2, 3, 3, 3]\n{1, 2, 3}\n{1, 2, 2, 3, 3, 3}\nError\n\nWhat is the result of \"Hello\" * 3?\n\n\n\n\n\n\n\nHello Hello Hello\nHelloHelloHello\nHello3\nError\n\n\n\n\n\n\n\n\n\nClick to reveal answers\n\n\n\n\n\n\n13\n[1, 2, 3, 4, 5]\nBoth {} and dict()\n{1, 2, 3}\nHelloHelloHello"
  },
  {
    "objectID": "lectures/lecture02/datatypes_quiz.html#python-data-types-quiz",
    "href": "lectures/lecture02/datatypes_quiz.html#python-data-types-quiz",
    "title": "Datatypes Quiz",
    "section": "",
    "text": "Test your knowledge of Python data types with this interactive quiz!\n\n\nWhat is the output of len(\"Hello, World!\")?\n\n\n\n\n\n\n\n10\n11\n13\n14\n\nWhat is the result of [1, 2, 3] + [4, 5]?\n\n\n\n\n\n\n\n(1, 2, 3, 4, 5)\n[1, 2, 3, 4, 5]\n[5, 7, 8]\nError\n\nWhich of the following creates an empty dictionary?\n\n{}\n[]\n()\nBoth {} and dict()\n\nWhat is the output of set([1, 2, 2, 3, 3, 3])?\n\n\n\n\n\n\n\n[1, 2, 2, 3, 3, 3]\n{1, 2, 3}\n{1, 2, 2, 3, 3, 3}\nError\n\nWhat is the result of \"Hello\" * 3?\n\n\n\n\n\n\n\nHello Hello Hello\nHelloHelloHello\nHello3\nError\n\n\n\n\n\n\n\n\n\nClick to reveal answers\n\n\n\n\n\n\n13\n[1, 2, 3, 4, 5]\nBoth {} and dict()\n{1, 2, 3}\nHelloHelloHello"
  },
  {
    "objectID": "lectures/lecture03/01-lecture03.html",
    "href": "lectures/lecture03/01-lecture03.html",
    "title": "Lecture 3",
    "section": "",
    "text": "Lists and arrays (introduction to numpy for numerical operations).\nBasic vector operations using numpy.\n\n\n\n\n\n\n\nNumpy Array\n\n\n\nThe NumPy array, formally called ndarray in NumPy documentation, is the real workhorse of data structures for scientific and engineering applications. The NumPy array is similar to a list but where all the elements of the list are of the same type. The elements of a NumPy array are usually numbers, but can also be booleans, strings, or other objects. When the elements are numbers, they must all be of the same type. For example, they might be all integers or all floating point numbers. NumPy arrays are more efficient than Python lists for storing and manipulating data.\n\n\n\n\n\n\n\n\n\n\nThere are a number of ways to initialize new numpy arrays, for example from\n\na Python list or tuples using np.array\nusing functions that are dedicated to generating numpy arrays, such as arange, linspace, etc.\nreading data from files which will be covered in the files section of this course.\n\n\n\nFor example, to create new vector and matrix arrays from Python lists we can use the numpy.array function. This is demonstrated in the following cells:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor larger arrays it is inpractical to initialize the data manually, using explicit python lists. Instead we can use one of the many functions in numpy that generate arrays of different forms and shapes. Some of the more common are:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlinspace\n\n\n\nThe linspace function creates an array of N evenly spaced points between a starting point and an ending point. The form of the function is linspace(start, stop, N).If the third argument N is omitted,then N=50. The function linspace always includes the end points.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlogspace\n\n\n\nlogspace is doing equivelent things with logaritmic spacing. The function logspace generates an array of N points between decades 10^start and 10^stop. The form of the function is logspace(start, stop, N). If the third argument N is omitted, then N=50. The function logspace always includes the end points.\n\n\n\n\n\n\n\n\nOther types of array creation techniques are listed below. Try around with these commands to get a feeling what they do.\n\n\n\n\n\n\nmgrid\n\n\n\nmgrid generates a multi-dimensional matrix with increasing value entries, for example in columns and rows. The arguments are similar to arange and linspace.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndiag\n\n\n\ndiag generates a diagonal matrix with the list supplied to it as the diagonal values. The values can be also offset from the main diagonal by using the optional argument k. If k is positive, the diagonal is above the main diagonal, if negative, below the main diagonal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nzeros and ones\n\n\n\nzeros and ones creates a matrix with the dimensions given in the argument and filled with 0 or 1. The argument is a tuple with the dimensions of the matrix. For example, np.zeros((3,3)) creates a 3x3 matrix filled with zeros.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlicing is the name for extracting part of an array by the syntax M[lower:upper:step]. When any of these are unspecified, they default to the values lower=0, upper=size of dimension, step=1. We can also use negative indices to count from the end of the array. Here are some examples:\n\n\n\n\n\n\n\n\n\n\n\n\nAny of the three parameters in M[lower:upper:step] can be ommited.\n\n\n\n\n\n\n\n\n\n\n\n\nNegative indices counts from the end of the array (positive index from the begining) and can be used in any of the three slicing parameters. Here are some examples:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndex slicing works exactly the same way for multidimensional arrays. We can slice along each axis independently. Here are some examples:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDifferences\n\n\n\nSlicing can be effectively used to calculate differences for example for the calculation of derivatives. Here the position \\(y_i\\) of an object has been measured at times \\(t_i\\) and stored in an array each. We wish to calculate the average velocity at the times \\(t_{i}\\) from the arrays by the formula\n\\[\\begin{equation}\n  v_{i}=\\frac{y_i-y_{i-1}}{t_{i}-t_{i-1}}\n  \\end{equation}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArrays can be reshaped into any form, which contains the same number of elements. For example, a 4-element array can be reshaped into a 2x2 array, or a 2x2 array can be reshaped into a 4-element array. Here are some examples:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith newaxis, we can insert new dimensions in an array, for example converting a vector to a column or row matrix. Here are some examples:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing function repeat, tile, vstack, hstack, and concatenate we can create larger vectors and matrices from smaller ones by repeating or stacking. Please try the individual functions yourself in your notebook. We wont discuss them in detail here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConcatenate joins arrays along an existing axis. Here are some examples:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhstack and vstack stack arrays horizontally and vertically. Here are some examples:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAll kinds of mathematical operations can be carried out on arrays. Typically these operation act element wise as seen from the examples below where a is an array of numbers from 0 to 9.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperation between multiple vectors allow in particular very quick operations. The operations address then elements of the same index. These operations are called vector operations since the concern the whole array at the same time. The product between two vectors results therefore not in a dot product, which gives one number but in an array of multiplied elements.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nxxw\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimulating and plotting the trajectory of a projectile under the influence of gravity (2D motion).\nIntroduction to vector addition and resolving vectors into components.\nVisualization: Plotting the path of the projectile and velocity vectors.\nHomework: Simulate projectile motion with air resistance (optional for advanced students)."
  },
  {
    "objectID": "lectures/lecture03/01-lecture03.html#creating-numpy-arrays",
    "href": "lectures/lecture03/01-lecture03.html#creating-numpy-arrays",
    "title": "Lecture 3",
    "section": "",
    "text": "There are a number of ways to initialize new numpy arrays, for example from\n\na Python list or tuples using np.array\nusing functions that are dedicated to generating numpy arrays, such as arange, linspace, etc.\nreading data from files which will be covered in the files section of this course.\n\n\n\nFor example, to create new vector and matrix arrays from Python lists we can use the numpy.array function. This is demonstrated in the following cells:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor larger arrays it is inpractical to initialize the data manually, using explicit python lists. Instead we can use one of the many functions in numpy that generate arrays of different forms and shapes. Some of the more common are:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlinspace\n\n\n\nThe linspace function creates an array of N evenly spaced points between a starting point and an ending point. The form of the function is linspace(start, stop, N).If the third argument N is omitted,then N=50. The function linspace always includes the end points.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlogspace\n\n\n\nlogspace is doing equivelent things with logaritmic spacing. The function logspace generates an array of N points between decades 10^start and 10^stop. The form of the function is logspace(start, stop, N). If the third argument N is omitted, then N=50. The function logspace always includes the end points.\n\n\n\n\n\n\n\n\nOther types of array creation techniques are listed below. Try around with these commands to get a feeling what they do.\n\n\n\n\n\n\nmgrid\n\n\n\nmgrid generates a multi-dimensional matrix with increasing value entries, for example in columns and rows. The arguments are similar to arange and linspace.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndiag\n\n\n\ndiag generates a diagonal matrix with the list supplied to it as the diagonal values. The values can be also offset from the main diagonal by using the optional argument k. If k is positive, the diagonal is above the main diagonal, if negative, below the main diagonal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nzeros and ones\n\n\n\nzeros and ones creates a matrix with the dimensions given in the argument and filled with 0 or 1. The argument is a tuple with the dimensions of the matrix. For example, np.zeros((3,3)) creates a 3x3 matrix filled with zeros."
  },
  {
    "objectID": "lectures/lecture03/01-lecture03.html#manipulating-numpy-arrays",
    "href": "lectures/lecture03/01-lecture03.html#manipulating-numpy-arrays",
    "title": "Lecture 3",
    "section": "",
    "text": "Slicing is the name for extracting part of an array by the syntax M[lower:upper:step]. When any of these are unspecified, they default to the values lower=0, upper=size of dimension, step=1. We can also use negative indices to count from the end of the array. Here are some examples:\n\n\n\n\n\n\n\n\n\n\n\n\nAny of the three parameters in M[lower:upper:step] can be ommited.\n\n\n\n\n\n\n\n\n\n\n\n\nNegative indices counts from the end of the array (positive index from the begining) and can be used in any of the three slicing parameters. Here are some examples:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndex slicing works exactly the same way for multidimensional arrays. We can slice along each axis independently. Here are some examples:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDifferences\n\n\n\nSlicing can be effectively used to calculate differences for example for the calculation of derivatives. Here the position \\(y_i\\) of an object has been measured at times \\(t_i\\) and stored in an array each. We wish to calculate the average velocity at the times \\(t_{i}\\) from the arrays by the formula\n\\[\\begin{equation}\n  v_{i}=\\frac{y_i-y_{i-1}}{t_{i}-t_{i-1}}\n  \\end{equation}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArrays can be reshaped into any form, which contains the same number of elements. For example, a 4-element array can be reshaped into a 2x2 array, or a 2x2 array can be reshaped into a 4-element array. Here are some examples:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith newaxis, we can insert new dimensions in an array, for example converting a vector to a column or row matrix. Here are some examples:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing function repeat, tile, vstack, hstack, and concatenate we can create larger vectors and matrices from smaller ones by repeating or stacking. Please try the individual functions yourself in your notebook. We wont discuss them in detail here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConcatenate joins arrays along an existing axis. Here are some examples:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhstack and vstack stack arrays horizontally and vertically. Here are some examples:"
  },
  {
    "objectID": "lectures/lecture03/01-lecture03.html#applying-mathematical-functions",
    "href": "lectures/lecture03/01-lecture03.html#applying-mathematical-functions",
    "title": "Lecture 3",
    "section": "",
    "text": "All kinds of mathematical operations can be carried out on arrays. Typically these operation act element wise as seen from the examples below where a is an array of numbers from 0 to 9.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperation between multiple vectors allow in particular very quick operations. The operations address then elements of the same index. These operations are called vector operations since the concern the whole array at the same time. The product between two vectors results therefore not in a dot product, which gives one number but in an array of multiplied elements.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nxxw"
  },
  {
    "objectID": "lectures/lecture03/01-lecture03.html#application",
    "href": "lectures/lecture03/01-lecture03.html#application",
    "title": "Lecture 3",
    "section": "",
    "text": "Simulating and plotting the trajectory of a projectile under the influence of gravity (2D motion).\nIntroduction to vector addition and resolving vectors into components.\nVisualization: Plotting the path of the projectile and velocity vectors.\nHomework: Simulate projectile motion with air resistance (optional for advanced students)."
  },
  {
    "objectID": "lectures/lecture04/01-lecture04.html",
    "href": "lectures/lecture04/01-lecture04.html",
    "title": "Lecture 4",
    "section": "",
    "text": "Introduction to loops for numerical integration (e.g., trapezoidal rule).\nFunctions for calculating work, power, and energy.\n\n\n\n\n\nWriting Python code to calculate the work done by a variable force (e.g., spring force) using numerical integration.\nSimulating energy conservation in a closed system (e.g., pendulum).\nVisualization: Plotting energy vs.¬†time for the system.\nHomework: Modify the code to simulate a different system, such as a mass-spring system.\n\n\n\n\n    \n    \n    D3.js Shaded Spheres"
  },
  {
    "objectID": "lectures/lecture04/01-lecture04.html#work-power-energy",
    "href": "lectures/lecture04/01-lecture04.html#work-power-energy",
    "title": "Lecture 4",
    "section": "",
    "text": "Introduction to loops for numerical integration (e.g., trapezoidal rule).\nFunctions for calculating work, power, and energy.\n\n\n\n\n\nWriting Python code to calculate the work done by a variable force (e.g., spring force) using numerical integration.\nSimulating energy conservation in a closed system (e.g., pendulum).\nVisualization: Plotting energy vs.¬†time for the system.\nHomework: Modify the code to simulate a different system, such as a mass-spring system.\n\n\n\n\n    \n    \n    D3.js Shaded Spheres"
  },
  {
    "objectID": "lectures/lecture04/04-plotting copy.html",
    "href": "lectures/lecture04/04-plotting copy.html",
    "title": "Plotting",
    "section": "",
    "text": "Data visualization through plotting is a crucial tool for analyzing and interpreting scientific data and theoretical predictions. While plotting capabilities are not built into Python‚Äôs core, they are available through various external library modules. Matplotlib is widely recognized as the de facto standard for plotting in Python. However, several other powerful plotting libraries exist, including PlotLy, Seaborn, and Bokeh, each offering unique features and capabilities for data visualization.\nAs Matplotlib is an external library (actually a collection of libraries), it must be imported into any script that uses it. While Matplotlib relies heavily on NumPy, importing NumPy separately is not always necessary for basic plotting. However, for most scientific applications, you‚Äôll likely use both. To create 2D plots, you typically start by importing Matplotlib‚Äôs pyplot module:\nThis import introduces the implicit interface of pyplot for creating figures and plots. Matplotlib offers two main interfaces:\nWe will use most of the the the pyplot interface as in the examples below. The section Additional Plotting will refer to the explicit programming of figures.\nWe can set some of the parameters for the appearance of graphs globally. In case you still want to modify a part of it, you can set individual parameters later during plotting. The command used here is the\nfunction, which takes a dictionary with the specific parameters as key."
  },
  {
    "objectID": "lectures/lecture04/04-plotting copy.html#simple-plotting",
    "href": "lectures/lecture04/04-plotting copy.html#simple-plotting",
    "title": "Plotting",
    "section": "Simple Plotting",
    "text": "Simple Plotting\nMatplotlib offers multiple levels of functionality for creating plots. Throughout this section, we‚Äôll primarily focus on using commands that leverage default settings. This approach simplifies the process, as Matplotlib automatically handles much of the graph layout. These high-level commands are ideal for quickly creating effective visualizations without delving into intricate details. At the end of this section, we‚Äôll briefly touch upon more advanced techniques that provide greater control over plot elements and layout.\n\nAnatomy of a Line Plot\nTo create a basic line plot, use the following command:\nplt.plot(x, y)\nBy default, this generates a line plot. However, you can customize the appearance by adjusting various parameters within the plot() function. For instance, you can modify it to resemble a scatter plot by changing certain arguments. The versatility of this command allows for a range of visual representations beyond simple line plots.\nLet‚Äôs create a simple line plot of the sine function over the interval [0, 4œÄ]. We‚Äôll use NumPy to generate the x-values and calculate the corresponding y-values. The following code snippet demonstrates this process:\n1x = np.linspace(0, 4.*np.pi, 100)\n2y = np.sin(x)\n\n3plt.figure(figsize=(4,3))\n4plt.plot(x, y)\n5plt.tight_layout()\n6plt.show()\n\n1\n\nCreate an array of 100 values between 0 and 4œÄ.\n\n2\n\nCalculate the sine of each value in the array.\n\n3\n\ncreate a new figure\n\n4\n\nplot the data\n\n5\n\nautomatically adjust the layout\n\n6\n\nshow the figure\n\n\nHere is the code in a Python cell:\n\n\n\n\n\n\nTry to change the values of the x and y arrays and see how the plot changes.\n\n\n\n\n\n\nWhy use plt.tight_layout()\n\n\n\n\n\nplt.tight_layout() is a very useful function in Matplotlib that automatically adjusts the spacing between plot elements to prevent overlapping and ensure that all elements fit within the figure area. Here‚Äôs what it does:\n\nPadding Adjustment: It adjusts the padding between and around subplots to prevent overlapping of axis labels, titles, and other elements.\nSubplot Spacing: It optimizes the space between multiple subplots in a figure.\nText Accommodation: It ensures that all text elements (like titles, labels, and legends) fit within the figure without being cut off.\nMargin Adjustment: It adjusts the margins around the entire figure to make sure everything fits neatly.\nAutomatic Resizing: If necessary, it can slightly resize subplot areas to accommodate all elements.\nLegend Positioning: It takes into account the presence and position of legends when adjusting layouts.\n\nKey benefits of using plt.tight_layout():\n\nIt saves time in manual adjustment of plot elements.\nIt helps create more professional-looking and readable plots.\nIt‚Äôs particularly useful when creating figures with multiple subplots or when saving figures to files.\n\nYou typically call plt.tight_layout() just before plt.show() or plt.savefig(). For example:\nplt.figure()\n# ... (your plotting code here)\nplt.tight_layout()\nplt.show()\n\n\n\n\nAxis Labels\nTo enhance the clarity and interpretability of our plots, it‚Äôs crucial to provide context through proper labeling. Let‚Äôs add descriptive axis labels to our diagram, a practice that significantly improves the readability and comprehension of the data being presented.\nplt.xlabel('x-label')\nplt.ylabel('y-label')\n\n\n\n\n\n\n\n\nLegends\nplt.plot(..., label=r'$\\sin(x)$')\nplt.legend(loc='lower left')\n\n\n\n\n\n\n\n\nPlots with error bars\nWhen plotting experimental data it is customary to include error bars that indicate graphically the degree of uncertainty that exists in the measurement of each data point. The MatPlotLib function errorbar plots data with error bars attached. It can be used in a way that either replaces or augments the plot function. Both vertical and horizontal error bars can be displayed. The figure below illustrates the use of error bars.\n\n\n\n\n\n\n\n\nSaving figures\nTo save a figure to a file we can use the savefig method in the Figure class. Matplotlib can generate high-quality output in a number formats, including PNG, JPG, EPS, SVG, PGF and PDF. For scientific papers, I recommend using PDF whenever possible. (LaTeX documents compiled with pdflatex can include PDFs using the includegraphics command). In some cases, PGF can also be good alternative."
  },
  {
    "objectID": "lectures/lecture04/04-plotting copy.html#other-plot-types",
    "href": "lectures/lecture04/04-plotting copy.html#other-plot-types",
    "title": "Plotting",
    "section": "Other Plot Types",
    "text": "Other Plot Types\n\nScatter plot\nIf you prefer to use symbols for plotting just use the\nplt.scatter(x,y)\ncommand of pylab. Note that the scatter command requires a x and y values and you can set the marker symbol (see an overview of the marker symbols).\n\n\n\n\n\n\n\n\nHistograms\nA very useful plotting command is also the hist command. It generates a histogram of the data provided. A histogram is a graphical representation of the distribution of numerical data. It is an estimate of the probability distribution of a continuous variable. To construct a histogram, the first step is to ‚Äúbin‚Äù the range of values‚Äîthat is, divide the entire range of values into a series of intervals‚Äîand then count how many values fall into each interval. The bins are usually specified as consecutive, non-overlapping intervals of a variable. The bins must be adjacent, and are often (but not required to be) of equal size.\nWhen using the histogram function, you have flexibility in how the data is grouped. If you only provide the dataset, the function will automatically determine appropriate bins. However, you can also specify custom bins by passing an array of intervals using the syntax hist(data, bins=b), where b is your custom array of bin edges. To normalize the histogram so that the total area under it equals 1, you can set the density parameter to True. It‚Äôs worth noting that the histogram function doesn‚Äôt just create a visual representation; it also returns useful information such as the count of data points in each bin and the bin edges themselves.\n\n\n\n\n\n\nPhysics Interlude- Probability density for finding an oscillating particle\n\n\n\nLet‚Äôs integrate histogram plotting with a fundamental physics concept: the simple harmonic oscillator in one dimension. This system is described by a specific equation of motion:\n\\[\\begin{equation}\n\\ddot{x}(t) = -\\omega^2 x(t)\n\\end{equation}\\]\nFor an initial elongation \\(\\Delta x\\) at \\(t=0\\), the solution is:\n\\[\\begin{equation}\nx(t) = \\Delta x \\cos(\\omega t)\n\\end{equation}\\]\nTo calculate the probability of finding the spring at a certain elongation, we need to consider the time spent at different positions. The time \\(dt\\) spent in the interval [\\(x(t)\\), \\(x(t)+dx\\)] depends on the speed:\n\\[\\begin{equation}\nv(t) = \\frac{dx}{dt} = -\\omega \\Delta x \\sin(\\omega t)\n\\end{equation}\\]\nThe probability of finding the oscillator in a certain interval is the fraction of time spent in this interval, normalized by half the oscillation period \\(T/2\\):\n\\[\\begin{equation}\n\\frac{dt}{T/2} = \\frac{1}{T/2}\\frac{dx}{v(t)} = \\frac{1}{T/2}\\frac{-dx}{\\omega \\Delta x \\sin(\\omega t)}\n\\end{equation}\\]\nGiven that \\(\\omega = 2\\pi/T\\), we can derive the probability density:\n\\[\\begin{equation}\np(x)dx = \\frac{1}{\\pi \\Delta x}\\frac{dx}{\\sqrt{1-\\left(\\frac{x(t)}{\\Delta x}\\right)^2}}\n\\end{equation}\\]\nThis probability density reveals that the spring is more likely to be found at elongations where its speed is low. This principle extends to non-equilibrium physics, where entities moving with variable speed are more likely to be found in locations where they move slowly.\nWe can visualize this using the histogram function. By evaluating the position at equidistant times using the equation of motion and creating a histogram of these positions, we can represent the probability of finding the oscillator at certain positions. When properly normalized, this histogram will reflect the theoretical probability density we derived.\n\n\n\n\n\n\n\n\n\n\nSetting plotting limits and excluding data\nIf you want to zoom in to s specific region of a plot you can set the limits of the individual axes.\n\n\n\n\n\n\n\n\nMasked arrays\nSometimes you encounter situations, when you wish to mask some of the data of your plot, because they are not showing real data as the vertical lines in the plot above. For this purpose, you can mask the data arrays in various ways to not show up. The example below uses the\nnp.ma.masked_where()\nfunction of NumPy, which takes a condition as the first argument and what should be returned if that condition is fulfilled.\n\n\n\n\n\n\nIf you look at the resulting array, you will find, that the entries have not been removed but replaced by --, so the values are not existent and thefore not plotted.\n\n\n\n\n\n\n\nLogarithmic plots\n\n\n\n\n\nData sets can span many orders of magnitude from fractional quantities much smaller than unity to values much larger than unity. In such cases it is often useful to plot the data on logarithmic axes.\n\nSemi-log plots\nFor data sets that vary exponentially in the independent variable, it is often useful to use one or more logarithmic axes. Radioactive decay of unstable nuclei, for example, exhibits an exponential decrease in the number of particles emitted from the nuclei as a function of time.\nMatPlotLib provides two functions for making semi-logarithmic plots, semilogx and semilogy, for creating plots with logarithmic x and y axes, with linear y and x axes, respectively. We illustrate their use in the program below, which made the above plots.\n\n\n\n\n\n\n\n\nLog-log plots\nMatPlotLib can also make log-log or double-logarithmic plots using the function loglog. It is useful when both the \\(x\\) and \\(y\\) data span many orders of magnitude. Data that are described by a power law \\(y=Ax^b\\), where \\(A\\) and \\(b\\) are constants, appear as straight lines when plotted on a log-log plot. Again, the loglog function works just like the plot function but with logarithmic axes.\n\n\n\n\n\n\n\n\n\n\n\n\nCombined plots\nYou can combine multiple data with the same axes by stacking multiple plots.\n\n\n\n\n\n\n\n\nArranging multiple plots\nOften you want to create two or more graphs and place them next to one another, generally because they are related to each other in some way.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnimations\n\n\n\n\n\nMatplotlib can also be used to create animations. The FuncAnimation class makes it easy to create animations by repeatedly calling a function to update the plot. The following example shows a simple pendulum animation.\n\n\n\n\n\n\n\n\n\n\n\nSimple contour plot\n\n\n\n\n\n\nPhysics Interlude\n\n\n\n\n\n\nContour and Density Plots\nA contour plots are useful tools to study two dimensional data, meaning \\(Z(X,Y)\\). A contour plots the lines of constant value of the function \\(Z\\).\n\n\nUnderstanding Wave Interference\nImagine throwing two stones into a pond. Each stone creates circular waves that spread out. When these waves meet, they create interesting patterns - this is called interference. Let‚Äôs explore this using physics and Python!\n\nWhat is a Wave?\nA wave can be described mathematically. For our example, we‚Äôll look at spherical waves (like those in the pond). Each wave has: - An amplitude (how tall the wave is) - A wavelength (distance between wave peaks) - A frequency (how fast it oscillates)\n\n\nMathematical Description\nFor a single wave source, we can write: \\[\\begin{equation}\nU(r)=e^{-i\\,k r}\n\\end{equation}\\]\nWhere: - \\(k\\) is related to the wavelength (\\(k = 2\\pi/\\lambda\\)) - \\(r\\) is the distance from the source - We‚Äôve simplified by ignoring how the wave gets smaller as it travels (\\(1/r\\) term)\n\n\nTwo Wave Sources\nWhen we have two wave sources (like two stones dropped in the pond): 1. Each source creates its own wave 2. The waves combine where they meet 3. The total wave is the sum of both waves\n\n\n\ninterference\n\n\nMathematically: \\[\\begin{equation}\nU_{total} = e^{-i\\,k r_1} + e^{-i\\,k r_2}\n\\end{equation}\\]\nWhere \\(r_1\\) and \\(r_2\\) are the distances from each source.\n\n\nWhat We See (Intensity)\nWhat we actually see is the intensity of the combined waves:\n\\[\\begin{equation}\n\\text{Intensity} \\propto |U_{total}|^2\n\\end{equation}\\]\nThis will show us where the waves:\n\nAdd up (bright regions - constructive interference)\nCancel out (dark regions - destructive interference)\n\nLet‚Äôs create a Python program to visualize this!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColor contour plot\n\n\n\n\n\n\n\n\nImage plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced Plotting - Explicit Version\n\n\n\n\n\nAdvanced Plotting - Explicit Version\nWhile we have so far largely relied on the default setting and the automatic arrangement of plots, there is also a way to precisely design your plot. Python provides the tools of object oriented programming and thus modules provide classes which can be instanced into objects. This explicit interfaces allows you to control all details without the automatisms of pyplot.\nThe figure below, which is taken from the matplotlib documentation website shows the sets of commands and the objects in the figure, the commands refer to. It is a nice reference, when creating a figure.\n\n\n\nanatomy of a figure\n\n\n\nPlots with Multiple Spines\nSometimes it is very useful to plot different quantities in the same plot with the same x-axis but with different y-axes. Here is some example, where each line plot has its own y-axis.\n\n\n\n\n\n\n\n\nInsets\nInsets are plots within plots using their own axes. We therefore need to create two axes systems, if we want to have a main plot and and inset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpine axis\n\n\n\n\n\n\n\n\nPolar plot\n\n\n\n\n\n\n\n\nText annotation\nAnnotating text in matplotlib figures can be done using the text function. It supports LaTeX formatting just like axis label texts and titles:\n\n\n\n\n\n\n\n\n3D Plotting\nMatplotlib was initially designed with only two-dimensional plotting in mind. Around the time of the 1.0 release, some three-dimensional plotting utilities were built on top of Matplotlib‚Äôs two-dimensional display, and the result is a convenient (if somewhat limited) set of tools for three-dimensional data visualization. Three-dimensional plots are enabled by importing the mplot3d toolkit, included with the main Matplotlib installation:\n\n\n\n\n\n\nOnce this submodule is imported, a three-dimensional axes can be created by passing the keyword projection=‚Äò3d‚Äô to any of the normal axes creation routines:\n\nProjection Scence\n\n\n\n\n\n\nWith this three-dimensional axes enabled, we can now plot a variety of three-dimensional plot types. Three-dimensional plotting is one of the functionalities that benefits immensely from viewing figures interactively rather than statically in the notebook; recall that to use interactive figures, you can use %matplotlib notebook rather than %matplotlib inline when running this code.\n\n\nLine Plotting in 3D\nfrom sets of (x, y, z) triples. In analogy with the more common two-dimensional plots discussed earlier, these can be created using the ax.plot3D and ax.scatter3D functions. The call signature for these is nearly identical to that of their two-dimensional counterparts, so you can refer to Simple Line Plots and Simple Scatter Plots for more information on controlling the output. Here we‚Äôll plot a trigonometric spiral, along with some points drawn randomly near the line:\n\n\n\n\n\n\nNotice that by default, the scatter points have their transparency adjusted to give a sense of depth on the page. While the three-dimensional effect is sometimes difficult to see within a static image, an interactive view can lead to some nice intuition about the layout of the points. Use the scatter3D or the plot3D method to plot a random walk in 3-dimensions in your exercise.\n\n\nSurface Plotting\nA surface plot is like a wireframe plot, but each face of the wireframe is a filled polygon. Adding a colormap to the filled polygons can aid perception of the topology of the surface being visualized:"
  },
  {
    "objectID": "lectures/lecture04/04-plotting copy.html#contour-and-density-plots",
    "href": "lectures/lecture04/04-plotting copy.html#contour-and-density-plots",
    "title": "Plotting",
    "section": "Contour and Density Plots",
    "text": "Contour and Density Plots\nA contour plots are useful tools to study two dimensional data, meaning \\(Z(X,Y)\\). A contour plots the lines of constant value of the function \\(Z\\)."
  },
  {
    "objectID": "lectures/lecture04/04-plotting copy.html#understanding-wave-interference",
    "href": "lectures/lecture04/04-plotting copy.html#understanding-wave-interference",
    "title": "Plotting",
    "section": "Understanding Wave Interference",
    "text": "Understanding Wave Interference\nImagine throwing two stones into a pond. Each stone creates circular waves that spread out. When these waves meet, they create interesting patterns - this is called interference. Let‚Äôs explore this using physics and Python!\n\nWhat is a Wave?\nA wave can be described mathematically. For our example, we‚Äôll look at spherical waves (like those in the pond). Each wave has: - An amplitude (how tall the wave is) - A wavelength (distance between wave peaks) - A frequency (how fast it oscillates)\n\n\nMathematical Description\nFor a single wave source, we can write: \\[\\begin{equation}\nU(r)=e^{-i\\,k r}\n\\end{equation}\\]\nWhere: - \\(k\\) is related to the wavelength (\\(k = 2\\pi/\\lambda\\)) - \\(r\\) is the distance from the source - We‚Äôve simplified by ignoring how the wave gets smaller as it travels (\\(1/r\\) term)\n\n\nTwo Wave Sources\nWhen we have two wave sources (like two stones dropped in the pond): 1. Each source creates its own wave 2. The waves combine where they meet 3. The total wave is the sum of both waves\n\n\n\ninterference\n\n\nMathematically: \\[\\begin{equation}\nU_{total} = e^{-i\\,k r_1} + e^{-i\\,k r_2}\n\\end{equation}\\]\nWhere \\(r_1\\) and \\(r_2\\) are the distances from each source.\n\n\nWhat We See (Intensity)\nWhat we actually see is the intensity of the combined waves:\n\\[\\begin{equation}\n\\text{Intensity} \\propto |U_{total}|^2\n\\end{equation}\\]\nThis will show us where the waves:\n\nAdd up (bright regions - constructive interference)\nCancel out (dark regions - destructive interference)\n\nLet‚Äôs create a Python program to visualize this!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColor contour plot\n\n\n\n\n\n\n\n\nImage plot"
  },
  {
    "objectID": "lectures/lecture04/04-plotting copy.html#advanced-plotting---explicit-version",
    "href": "lectures/lecture04/04-plotting copy.html#advanced-plotting---explicit-version",
    "title": "Plotting",
    "section": "Advanced Plotting - Explicit Version",
    "text": "Advanced Plotting - Explicit Version\nWhile we have so far largely relied on the default setting and the automatic arrangement of plots, there is also a way to precisely design your plot. Python provides the tools of object oriented programming and thus modules provide classes which can be instanced into objects. This explicit interfaces allows you to control all details without the automatisms of pyplot.\nThe figure below, which is taken from the matplotlib documentation website shows the sets of commands and the objects in the figure, the commands refer to. It is a nice reference, when creating a figure.\n\n\n\nanatomy of a figure\n\n\n\nPlots with Multiple Spines\nSometimes it is very useful to plot different quantities in the same plot with the same x-axis but with different y-axes. Here is some example, where each line plot has its own y-axis.\n\n\n\n\n\n\n\n\nInsets\nInsets are plots within plots using their own axes. We therefore need to create two axes systems, if we want to have a main plot and and inset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpine axis\n\n\n\n\n\n\n\n\nPolar plot\n\n\n\n\n\n\n\n\nText annotation\nAnnotating text in matplotlib figures can be done using the text function. It supports LaTeX formatting just like axis label texts and titles:\n\n\n\n\n\n\n\n\n3D Plotting\nMatplotlib was initially designed with only two-dimensional plotting in mind. Around the time of the 1.0 release, some three-dimensional plotting utilities were built on top of Matplotlib‚Äôs two-dimensional display, and the result is a convenient (if somewhat limited) set of tools for three-dimensional data visualization. Three-dimensional plots are enabled by importing the mplot3d toolkit, included with the main Matplotlib installation:\n\n\n\n\n\n\nOnce this submodule is imported, a three-dimensional axes can be created by passing the keyword projection=‚Äò3d‚Äô to any of the normal axes creation routines:\n\nProjection Scence\n\n\n\n\n\n\nWith this three-dimensional axes enabled, we can now plot a variety of three-dimensional plot types. Three-dimensional plotting is one of the functionalities that benefits immensely from viewing figures interactively rather than statically in the notebook; recall that to use interactive figures, you can use %matplotlib notebook rather than %matplotlib inline when running this code.\n\n\nLine Plotting in 3D\nfrom sets of (x, y, z) triples. In analogy with the more common two-dimensional plots discussed earlier, these can be created using the ax.plot3D and ax.scatter3D functions. The call signature for these is nearly identical to that of their two-dimensional counterparts, so you can refer to Simple Line Plots and Simple Scatter Plots for more information on controlling the output. Here we‚Äôll plot a trigonometric spiral, along with some points drawn randomly near the line:\n\n\n\n\n\n\nNotice that by default, the scatter points have their transparency adjusted to give a sense of depth on the page. While the three-dimensional effect is sometimes difficult to see within a static image, an interactive view can lead to some nice intuition about the layout of the points. Use the scatter3D or the plot3D method to plot a random walk in 3-dimensions in your exercise.\n\n\nSurface Plotting\nA surface plot is like a wireframe plot, but each face of the wireframe is a filled polygon. Adding a colormap to the filled polygons can aid perception of the topology of the surface being visualized:"
  },
  {
    "objectID": "lectures/lecture04/2_particle_in_a_box.html",
    "href": "lectures/lecture04/2_particle_in_a_box.html",
    "title": "Particle in a box",
    "section": "",
    "text": "Let‚Äôs apply the whole thing to the problem of a particle in a box. This means, we look at a quantum mechanical object in a potential well.\nThe problem is sketched below.\nWe need to define this rectangular box with zero potential energy inside the box and finite potential energy outside. Since the quantum mechanical object is a wave, we expect that only certain standing waves of particular wavelength can exist inside the box. These waves are connected to certain probability densities of finding the particle at certain positions and specific energy values. These are the energy levels, which are often characteristic of the quantum realm."
  },
  {
    "objectID": "lectures/lecture04/2_particle_in_a_box.html#definition-of-the-problem",
    "href": "lectures/lecture04/2_particle_in_a_box.html#definition-of-the-problem",
    "title": "Particle in a box",
    "section": "Definition of the problem",
    "text": "Definition of the problem\nBefore we start, we need to define some quantities:\n\nwe will study a box of d=1 nm in width in a domain of L=5 nm\nwe will use N=1001 points for our \\(x_{i}\\)\nour potential energy shall have a barrier height of 1 eV\nthe potential energy inside the box will be zero\n\n\n\n\n\n\n\n\nPotential energy\nWe first define the diagonal potential energy matrix.\n\n\n\n\n\n\n\n\nKinetic energy\nNext are the derivatives of the kinetic energy matrix.\n\n\n\n\n\n\nAn finally the total Hamilton operator matrix.\n\n\n\n\n\n\n\n\nSolution\nThe last step is to solve the system of coupled equations usering the eigsh method of the scipy module. We can already anticipate that we get multiple solution, e.g.¬†multiple wavelength that fit inside the box. So there must be a certain number of eigenvalues. The method eigsh allows to specify the number of eigenvalues and eigenfunctions \\(n\\) we would like to calculate.\n\n\n\n\n\n\n\n\nPlotting\n\n\n\n\n\n\nThe diagram shows the corresponding energy states (the eigenvalues of the solution) and the value \\(|\\Psi|^2\\), which gives the probability to find the particle inside the box. The latter shows, that in contrast to what we expect from classical theory, where we would expect the particle to be with equal probability found at all positions inside the box, we get in quantum mechanics only certain positions at which we would find the particle. Also, the higher the energy state, the more equally is the particle distributed over the box. For a finite box depth, however, we get only a finite number of energy states in which the particle is bound. A second interesting observation here is that the particle has a finite probability to enter the potential barrier. Especially for the higher energy states, the wavefunction decays exponentially into the barrier. This is similar to the evanescent wave we studied during the last lecture.\n\n\nEnergies of bound states\nIn the case of the particle in a box only certain energies are allowed. The energies which correspond to these distributions are increasing nonlinearly with its index. Below we plot the energy as a function of the index of the energy value. This index is called quantum number as we can enumerate the solutions in quantum mechanics. The graph shows that the energy of the bound states increases with the square of the quantum number, i.e.¬†\\(E_{n}\\propto n^2\\)."
  },
  {
    "objectID": "lectures/lecture04/2_particle_in_a_box.html#where-to-go-from-here",
    "href": "lectures/lecture04/2_particle_in_a_box.html#where-to-go-from-here",
    "title": "Particle in a box",
    "section": "Where to go from here?",
    "text": "Where to go from here?\nYou may try at this point to create two closely spaced potential wells, e.g.¬†two of 1 nm width with a distance of 0.1 nm or with a distance of 2 nm. You should see that for large distances of the wells the energy values in the individual wells are the same, while for the smaller distance they split up into two due to the interaction.\n\n\n\nDouble Well"
  },
  {
    "objectID": "seminars/seminar07/seminar7.html",
    "href": "seminars/seminar07/seminar7.html",
    "title": "Seminar Coding Session 1",
    "section": "",
    "text": "To practice the concepts covered in the lectures, we have prepared a few coding exercises. These exercises are designed to help you to get further used into programming. Each question is related to a physics problem and all of them have a solution that you can check after you have tried to solve them.\nEach exercise is also equipped with a time estimate to help you plan your work. The time estimates are approximate and rather valid for students without prior knowledge of programming and python before this course.\nYou are going to solve at least two of the four exercises below in the seminar. You can choose the exercises you want to solve. One of the students will present the solution to the exercise and we will discuss the solution together.\n\n\n\n\n\n\nExample 1: Distance Calculation\n\n\n\nCalculate the total distance traveled by a car moving at constant velocity for different time intervals. This problem demonstrates the linear relationship between distance and time when velocity remains constant.\nUsing the formula for uniform motion \\(d = v \\cdot t\\), where \\(d\\) is displacement, \\(v\\) is velocity, and \\(t\\) is time, we‚Äôll calculate distances for several time points and visualize the results in a plot. This will help illustrate how distance increases linearly with time when velocity is constant.\nTime estimate: 20-25 minutes\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nCreate an array of time points from 0 to 10 seconds and calculate the distance traveled at each time point using the formula \\(d = v \\cdot t\\). Use a for loop to print the time and distance values. Finally, create a simple plot of distance vs time using plt.plot().\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 2: Weight on Different Planets\n\n\n\nCalculate weight (\\(F = mg\\)) of an object on different planets using a list of gravitational accelerations. The weight force depends on both the mass of the object and the local gravitational acceleration. By comparing the weight of the same object across different planets, we can understand how gravitational forces vary throughout our solar system. We‚Äôll use Newton‚Äôs second law in the form \\(F = mg\\) where \\(m\\) is the object‚Äôs mass and \\(g\\) is the local gravitational acceleration.\nTime estimate: 10-15 minutes\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nCalculate and print the weights in a for loop. Try to use thezip() function to iterate over two lists simultaneously. Think about formatted printing using f-strings. f\"{variable:.2f}\" inside the print function will print the variable with two decimal places.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 3: Simple Kinetic Energy Calculator\n\n\n\nWrite a function to calculate kinetic energy (\\(KE = \\frac{1}{2}mv^2\\)) and test it with different values. This example demonstrates how to create and use a function to calculate the kinetic energy of objects with different masses moving at a given velocity. Kinetic energy is a scalar quantity that depends on both mass and velocity, with velocity having a squared relationship. By testing the function with multiple masses, we can observe how kinetic energy scales with mass while keeping velocity constant.\nTime estimate: 15-20 minutes\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nDefine a function calculate_ke(mass, velocity) that returns the kinetic energy. Use a for loop to calculate and print the kinetic energy for different masses. Use formatted printing to print the results.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 4: Average Speed Calculator\n\n\n\nCalculate average speed from a list of distances and times (segments of a journey). First, sum up the total distances traveled and times taken for the complete journey. Then, use the formula \\(v_{avg} = \\frac{\\text{total distance}}{\\text{total time}}\\) to find the overall average speed. We‚Äôll also calculate individual segment speeds for comparison.\nTime estimate: 20-25 minutes\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nCalculate the total distance and total time using the sum() function. Calculate the average speed using the formula \\(v_{avg} = \\frac{\\text{total distance}}{\\text{total time}}\\). Use a for loop to calculate and print the speed for each segment.\n\n\n\n\n\n\n\n\n\n\n\nTip"
  },
  {
    "objectID": "seminars/seminar09/add ons.html",
    "href": "seminars/seminar09/add ons.html",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "Example 6: Simple Particle Class\n\n\n\nCreate a Particle class to represent a point mass with basic properties (mass, position, velocity) and methods to calculate its kinetic and potential energy. This introduces basic class concepts without getting too complex.\nTime estimate: 20 minutes\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nYou‚Äôll need to:\n\nStore mass, position, and velocity as class attributes (self.mass etc.)\nUse numpy arrays for position and velocity vectors\nDefine gravitational constant g = 9.81\nUse np.linalg.norm() to calculate speed from velocity vector\nRemember formulas: K = 1/2mv¬≤, U = mgh, E = K + U\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 8: Two-Body System\n\n\n\nCreate a simple class to calculate the gravitational force and potential energy between two masses. This introduces basic physics calculations in an object-oriented way.\nTime estimate: 20 minutes\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nYou‚Äôll need to:\n\nUse the gravitational force equation F = Gm1m2/r¬≤\nUse the gravitational potential energy equation U = -Gm1m2/r\nCreate a method to calculate and print both values\nUse scientific notation (:.2e) for readable output\nInclude units (N for force, J for energy) in the output\n\n\n\n\n\n\n\n\n\n\n\n\nNote"
  },
  {
    "objectID": "seminars/seminar08/seminar8.html",
    "href": "seminars/seminar08/seminar8.html",
    "title": "Seminar Coding Exercises 2",
    "section": "",
    "text": "Building on our previous coding exercise, we have prepared another set of problems to further develop your programming skills. Like before, these exercises apply physics concepts and include solutions you can check after attempting them yourself.\nEach exercise includes a time estimate to help you plan your work. These estimates are approximate and particularly relevant for students who are still new to programming and Python.\nAs in the previous seminar, you will choose and solve two of the five exercises below. One student will present their solution to each chosen exercise, followed by a group discussion of the approach and implementation.\n\n\n\n\n\n\nExample 1: Free Fall Motion\n\n\n\nWrite a program that calculates and visualizes the position and velocity of a freely falling object at different times. This exercise will help you understand how to implement basic physics equations in Python and create meaningful visualizations of the results.\nUse the equations of motion for an object under constant acceleration (gravity):\n\n\\(y(t) = y_0 + v_0t - \\frac{1}{2}gt^2\\)\n\\(v(t) = v_0 - gt\\)\n\nWhere \\(g = 9.81\\) m/s¬≤ (acceleration due to gravity), initial height \\(y_0 = 100\\)m, and initial velocity \\(v_0 = 0\\) m/s. Calculate values for the first 5 seconds in 0.5s intervals and create plots showing how both position and velocity change over time.\nTime estimate: 15-20 minutes\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nYou‚Äôll need to:\n\nCreate a time array using np.arange() from 0 to 5s with 0.5s steps\nUse the equations to calculate position and velocity arrays\nCreate a figure with two subplots:\n\nLeft subplot: Position vs Time\nRight subplot: Velocity vs Time\n\nLabel your axes and add titles\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 2: Projectile Range Calculator\n\n\n\nWrite a function that calculates the range of a projectile given its initial velocity and launch angle. This exercise will help you understand how to implement trigonometric functions and explore how launch angle affects projectile motion.\nUse:\n\nRange = \\((v_0^2 \\sin(2\\theta)) / g\\)\n\nwhere \\(v_0\\) is the initial velocity and \\(\\theta\\) is the launch angle.\nTest the function for angles between 0¬∞ and 90¬∞ in steps of 15¬∞ and determine which angle gives the maximum range. Consider how this relates to the theoretical maximum at 45¬∞.\nTime estimate: 15-20 minutes\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nYou‚Äôll need to:\n\nDefine a function that takes initial velocity (\\(v_0\\)) and angle (\\(\\theta\\)) as inputs\nConvert the angle from degrees to radians using \\(\\text{np.deg2rad}()\\)\nUse the range equation: \\(\\frac{v_0^2 \\sin(2\\theta)}{g}\\)\nTest the function with angles from \\(0^\\circ\\) to \\(90^\\circ\\) in steps of \\(15^\\circ\\)\nPrint the range for each angle\nCalculate and print the maximum range at \\(45^\\circ\\)\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 3: Simple Harmonic Motion\n\n\n\nCalculate and plot the position of a mass on a spring over time using:\n\n\\(x(t) = A\\cos(\\omega t)\\)\n\nwhere \\(A\\) is amplitude and \\(\\omega\\) is angular frequency (\\(\\omega = \\sqrt{k/m}\\)). Plot the position over several oscillation periods to visualize the periodic motion. Use appropriate axis labels and title to clearly show what is being plotted.\nTime estimate: 15-20 minutes\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nYou‚Äôll need to:\n\nCalculate omega using \\(\\omega = \\sqrt{k/m}\\)\nCreate a time array using np.linspace() from 0 to 10s with 1000 points\nCalculate position using \\(x(t) = A\\cos(\\omega t)\\)\nCreate a plot with:\n\nPosition vs Time\nAppropriate axis labels\nTitle\nGrid\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 4: Work and Energy\n\n\n\nCalculate the work done by a variable force \\(F(x) = kx^2\\) over a distance. Use numerical integration (simple Riemann sum) to find the work:\n\n\\(W = \\int_{x_1}^{x_2} F(x)dx\\)\n\nCreate a function that takes the force constant k, start position x1, end position x2, and number of integration steps as inputs. Calculate the work done by breaking the interval into small steps and summing the force times the distance for each step.\nTime estimate: 15-20 minutes\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nYou‚Äôll need to:\n\nCreate a function that takes x and k as inputs to calculate force\nCreate a function that:\n\nCreates an array of x values using np.linspace()\nCalculates step size dx\nCalculates force at each x value\nMultiplies force by dx and sums to get total work\n\nTest the function with sample values and display result\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 5: Elastic Collision Analysis\n\n\n\nWrite a program that analyzes an elastic collision between two objects. Given initial velocities and masses, calculate final velocities using conservation of momentum and kinetic energy:\n\nConservation of Momentum: \\(m_1v_1 + m_2v_2 = m_1v_1' + m_2v_2'\\)\nConservation of Energy: \\(\\frac{1}{2}m_1v_1^2 + \\frac{1}{2}m_2v_2^2 = \\frac{1}{2}m_1v_1'^2 + \\frac{1}{2}m_2v_2'^2\\)\n\nWrite a function that takes masses and initial velocities as inputs and returns the final velocities. Then test the function with different combinations of masses and velocities to verify that both momentum and energy are conserved in each case.\nTime estimate: 20-25 minutes\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nYou‚Äôll need to:\n\nUse the elastic collision equations:\n\n\\(v_1' = \\frac{m_1 - m_2}{m_1 + m_2}v_1 + \\frac{2m_2}{m_1 + m_2}v_2\\)\n\\(v_2' = \\frac{2m_1}{m_1 + m_2}v_1 + \\frac{m_2 - m_1}{m_1 + m_2}v_2\\)\n\nCreate a function that:\n\nTakes masses and initial velocities as input\nReturns final velocities using the equations\n\nFor each test case:\n\nCalculate final velocities\nVerify momentum conservation\nVerify energy conservation\n\nPrint results showing:\n\nInitial conditions\nFinal velocities\nConservation of momentum and energy\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote"
  },
  {
    "objectID": "seminars/seminar06/Report/Figures/Figure1/Figure1.html",
    "href": "seminars/seminar06/Report/Figures/Figure1/Figure1.html",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "import matplotlib as mpl\nimport matplotlib.font_manager as font_manager\nfrom IPython.core.display import HTML\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom directory_tree import display_tree\n\n\nplt.rcParams.update({'font.size': 12,\n                     'lines.linewidth': 1,\n                     'lines.markersize': 10,\n                     'axes.labelsize': 11,\n                     'xtick.labelsize' : 10,\n                     'ytick.labelsize' : 10,\n                     'xtick.top' : True,\n                     'xtick.direction' : 'in',\n                     'ytick.right' : True,\n                     'ytick.direction' : 'in',}) \n\n%config InlineBackend.figure_format = 'retina'\n\n\ndef get_size(w,h):\n    return((w/2.54,h/2.54))\n\n\nplt.figure(figsize=get_size(7,5),dpi=150)\nx=np.linspace(0,np.pi*4,200)\nplt.plot(x,np.sin(x)*np.cos(2*x),color='r')\nplt.xlabel(r\"angle $\\theta$ in [rad]\")\nplt.ylabel(r\"$\\sin(\\theta)$\")\nplt.savefig(\"../figure1.pdf\",bbox_inches = 'tight')\nplt.show()"
  },
  {
    "objectID": "seminars/seminar06/25_publication_ready_figures.html",
    "href": "seminars/seminar06/25_publication_ready_figures.html",
    "title": "Erstellung ver√∂ffentlichungsreifer Diagramme",
    "section": "",
    "text": "Optimale Plotgr√∂√üen f√ºr wissenschaftliche Arbeiten\nIn diesem Leitfaden zeige ich Ihnen, wie Sie Plots in einer standardisierten, publikationstauglichen Gr√∂√üe erstellen k√∂nnen. Die Hinweise sind sowohl f√ºr Jupyter-Notebooks als auch f√ºr wissenschaftliche Arbeiten wie Semesterarbeiten geeignet.\nBeim Export von Plots als PDF-Dateien werden Vektorgrafiken erzeugt, die sich nachtr√§glich verlustfrei skalieren lassen. Dies erm√∂glicht zwar eine flexible Anpassung an ein- oder zweispaltige Layoutformate, kann aber zu Inkonsistenzen f√ºhren: Unterschiedliche Skalierungsfaktoren resultieren oft in verschiedenen Gr√∂√üen von Achsenbeschriftungen und Markierungen, was das Gesamtbild der Arbeit beeintr√§chtigt.\nDie bessere Strategie ist es, von Anfang an einheitliche Plotgr√∂√üen zu verwenden: - Definieren Sie Standards f√ºr ein- und zweispaltige Abbildungen - Legen Sie einheitliche Gr√∂√üen f√ºr Achsenbeschriftungen und Markierungen fest - Erstellen Sie alle Plots direkt in der finalen Gr√∂√üe\nIm Folgenden stelle ich praktische Techniken vor, mit denen Sie direkt aus Jupyter-Notebooks publikationsreife Plots erstellen und speichern k√∂nnen."
  },
  {
    "objectID": "seminars/seminar06/25_publication_ready_figures.html#erstellen-eines-diagramms-mit-einer-bestimmten-gr√∂√üe-des-begrenzungsrahmens",
    "href": "seminars/seminar06/25_publication_ready_figures.html#erstellen-eines-diagramms-mit-einer-bestimmten-gr√∂√üe-des-begrenzungsrahmens",
    "title": "Erstellung ver√∂ffentlichungsreifer Diagramme",
    "section": "Erstellen eines Diagramms mit einer bestimmten Gr√∂√üe des Begrenzungsrahmens",
    "text": "Erstellen eines Diagramms mit einer bestimmten Gr√∂√üe des Begrenzungsrahmens\nWenn Sie einen Plot in matplotlib erstellen, k√∂nnen Sie eine Gr√∂√üe mit dem Parameter figsize festlegen, z.B.\nplt.figure(figsize=(3,2))\nf√ºr eine Abbildung mit einer Breite von 3 inches bzw. 7,62 cm und einer H√∂he von 2 inches (5,08 cm). Wenn Sie diesen Parameter nicht verwenden oder sogar den Befehl plt.figure() nicht nutzen, verwendet matplotlib die Standardgr√∂√üe, die h√§ufig 8 inches mal 6 inches betr√§gt. Diese Standardgr√∂√üe ist viel zu gro√ü, da die Abbildung dann fast eine ganze A4-Seite breit w√§re. Eine angemessene Gr√∂√üe f√ºr einen Plot in einer einzelnen Spalte eines zweispaltigen Dokuments w√§ren die oben genannten 3 inches mal 2 inches, da die gesamte Seitenbreite 21 cm minus einem Rand von etwa 3 cm auf jeder Seite eine Spaltenbreite von ungef√§hr (21-6)/2=7,5 cm ergibt.\nDer in Figure¬†1 gezeigte Plot wurde mit den folgenden Befehlen erstellt\nplt.figure(figsize=(3,2), dpi=150)\nx=np.linspace(0,np.pi*4,200)\nplt.plot(x,np.sin(x),color='k')\nplt.xlabel(r\"angle $\\theta$ in [rad]\")\nplt.ylabel(r\"$\\sin(\\theta)$\")\nplt.savefig(\"figure_example.pdf\",\n    bbox_inches = 'tight')\nplt.show()\nDie daraus resultierende PDF-Datei enth√§lt eine Grafik mit einem Begrenzungsrahmen, der genau 3 Zoll mal 2 Zoll gro√ü ist. Wenn Sie das Diagramm in ein beliebiges Zeichenprogramm wie Adobe Illustrator, Affinity Designer oder sogar in eine Textverarbeitungssoftware wie Word oder Pages einf√ºgen, hat der Begrenzungsrahmen dieses Diagramms genau diese Gr√∂√üe, und Sie k√∂nnen weitere Diagramme anordnen, um eine ganze Abbildung zu erstellen, ohne die Skalierung √§ndern zu m√ºssen. Wenn Sie das Diagramm in einem zweispaltigen LaTeX-Manuskript verwenden, kann es ohne Skalierung verwendet werden, d.h. durch includegraphics{Figure 1.pdf} wird es in der entsprechenden Gr√∂√üe √ºber eine Spalte angezeigt.\nEs gibt noch ein paar weitere Dinge zu beachten.\n\nW√§hrend der Begrenzungsrahmen dieser Abbildung diese Gr√∂√üe hat, ist der Achsenrahmen kleiner, und oft bleibt auf der linken/unteren Seite ein gewisser Leerraum zwischen den Achsenbeschriftungen und dem Rand des Begrenzungsrahmens. Das h√§ngt sehr stark von Ihrem spezifischen Diagramm ab. Wie Sie eine Abbildung mit einer festen Achsenrahmengr√∂√üe erstellen, wird im zweiten Abschnitt behandelt.\nDie Schriftgr√∂√üe auf der Achse betr√§gt jetzt 10 oder 11 Punkte, was der Schriftgr√∂√üe der meisten Dokumente entspricht, die Sie mit dieser Abbildung erstellen. Ich habe die folgenden plt.rcParams verwendet: ‚Äòaxes.labelsize‚Äô: 11, ‚Äòxtick.labelsize‚Äô : 10, ‚Äòytick.labelsize‚Äô : 10 f√ºr die gezeigte Darstellung.\nSie werden auch feststellen, dass die Arbeit mit dieser Abbildungsgr√∂√üe in einem Jupyter-Notebook nicht gut ist. Das hat damit zu tun, wie Jupyter die Ausgabe in eine PNG-Datei √ºbersetzt, die inline angezeigt wird. Eine M√∂glichkeit, den Plot im Jupyter-Notebook zu vergr√∂√üern, aber die PDF-Gr√∂√üe beizubehalten, besteht darin, den Parameter dpi im Befehl plt.figure(figsize=(3,2), dpi=150) zu erh√∂hen. Normalerweise ist er auf dpi=75 eingestellt, was jetzt viel zu klein ist. Eine Einstellung von dpi=150 scheint ein vern√ºnftiger Kompromiss zwischen Bildschirm- und Druckgr√∂√üe zu sein. Wenn Sie v√∂llig unabh√§ngig sein wollen\nDer Befehl plt.savefig verwendet einen zus√§tzlichen bbox_inches = 'tight' Parameter, der sicherstellt, dass die Boundingbox auch wirklich alle Komponenten des Plots genau umschlie√üt.\n\n\nimport matplotlib as mpl\nimport matplotlib.font_manager as font_manager\nfrom IPython.core.display import HTML\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom directory_tree import display_tree\n\n\nplt.rcParams.update({'font.size': 12,\n                     'lines.linewidth': 1,\n                     'lines.markersize': 10,\n                     'axes.labelsize': 11,\n                     'xtick.labelsize' : 10,\n                     'ytick.labelsize' : 10,\n                     'xtick.top' : True,\n                     'xtick.direction' : 'in',\n                     'ytick.right' : True,\n                     'ytick.direction' : 'in',\n                     'figure.dpi': 150})\n\n\ndef get_size(w,h):\n    return((w/2.54,h/2.54))\n\n\nplt.figure(figsize=get_size(7,6))\nx=np.linspace(0,np.pi*4,200)\nplt.plot(x,np.sin(x),color='k')\nplt.xlabel(r\"angle $\\theta$ in [rad]\")\nplt.ylabel(r\"$\\sin(\\theta)$\")\nplt.tight_layout()\nplt.savefig(\"figure_example3.pdf\", transparent=True)\nplt.show()\n\n\n\n\n\n\n\n\nWenn Sie dieses Bild in eine beliebige Software laden, erhalten Sie ein Bild mit einer Gr√∂√üe, die der eingestellten Breite entspricht.\n\n\n\n\n\n\nFigure¬†1"
  },
  {
    "objectID": "seminars/seminar06/25_publication_ready_figures.html#erstellen-eines-diagramms-mit-einer-bestimmten-achsenrahmengr√∂√üe",
    "href": "seminars/seminar06/25_publication_ready_figures.html#erstellen-eines-diagramms-mit-einer-bestimmten-achsenrahmengr√∂√üe",
    "title": "Erstellung ver√∂ffentlichungsreifer Diagramme",
    "section": "Erstellen eines Diagramms mit einer bestimmten Achsenrahmengr√∂√üe",
    "text": "Erstellen eines Diagramms mit einer bestimmten Achsenrahmengr√∂√üe\nDer Achsenrahmen ist die Box des Rahmens, der die Achsen bereitstellt. Beim Erstellen einer Figur mit dem Befehl plt.figure() wird der Achsenrahmen von matplotlib so berechnet, dass er innerhalb der durch figsize angegebenen Boundingbox liegt, so dass alle Achsenbeschriftungen ebenfalls hineinpassen. Der Achsenrahmen ist daher kleiner als die angegebene Bounding Box und h√§ngt oft von den Achsenbeschriftungen und weiteren Dingen ab. Wenn Sie einen Plot mit einer festen Gr√∂√üe des Achsenrahmens erstellen wollen, ist es sinnvoll, eine Funktion in Ihrem Code unterzubringen, die die Gr√∂√üe des Achsenrahmens festlegt. Diese Funktion k√∂nnte lauten\ndef set_size(w,h, ax=None):\n    \"\"\" w, h: width, height in inches \"\"\"\n    if not ax: ax=plt.gca()\n      l = ax.figure.subplotpars.left\n      r = ax.figure.subplotpars.right\n    t = ax.figure.subplotpars.top\n    b = ax.figure.subplotpars.bottom\n    figw = float(w)/(r-l)\n    figh = float(h)/(t-b)\n    ax.figure.set_size_inches(figw, figh)\nwobei Sie die gew√ºnschte Breite und H√∂he (in Zoll) der aktuellen Achse ax angeben m√ºssen. Die Funktion gibt nichts zur√ºck, sondern legt direkt die Gr√∂√üe fest.\n\ndef set_size(w,h, ax=None):\n    \"\"\" w, h: width, height in inches \"\"\"\n    if not ax: ax=plt.gca()\n    l = ax.figure.subplotpars.left\n    r = ax.figure.subplotpars.right\n    t = ax.figure.subplotpars.top\n    b = ax.figure.subplotpars.bottom\n    figw = float(w)/(r-l)\n    figh = float(h)/(t-b)\n    ax.figure.set_size_inches(figw, figh)\n\nfig=plt.figure(dpi=150)\nax=plt.axes()\nax.plot(x,np.sin(x),color='k')\nax.set_xlabel(r\"angle $\\theta$ in [rad]\")\nax.set_ylabel(r\"$\\sin(\\theta)$\")\nset_size(3,2)\nplt.savefig(\"figure_example2.pdf\", bbox_inches='tight', transparent=True)\nplt.show()\n\n\n\n\n\n\n\n\nWenn Sie diese Abbildung in ein Grafikprogramm oder eine Textverarbeitungssoftware laden, sollte das Abbildungsfeld eine Gr√∂√üe von 7,62 cm mal 5,08 cm haben, ohne dass eine Neuskalierung erfolgt:"
  },
  {
    "objectID": "seminars/seminar06/25_publication_ready_figures.html#auswahl-der-schriftarten",
    "href": "seminars/seminar06/25_publication_ready_figures.html#auswahl-der-schriftarten",
    "title": "Erstellung ver√∂ffentlichungsreifer Diagramme",
    "section": "Auswahl der Schriftarten",
    "text": "Auswahl der Schriftarten\nMatplotlib kann auf eine Reihe von verschiedenen Schriftarten zugreifen. Es kann schwierig sein, die passende Schriftart f√ºr den Formelstil Ihres Dokuments oder Ihrer Publikation zu finden. Eine Liste der Schriftarten, die Matplotlib zur Verf√ºgung stehen, kann mit dem folgenden Codeschnipsel abgerufen werden, den ich hier gefunden habe.\n\nfrom IPython.display import HTML, display\n\ndef make_html(fontname):\n    return \"&lt;p&gt;{font}: &lt;span style='font-family:{font}; font-size: 24px;'&gt;{font}&lt;/p&gt;\".format(font=fontname)\n\ncode = \"\\n\".join([make_html(font) for font in sorted(set([f.name for f in font_manager.fontManager.ttflist]))])\n\ndisplay(HTML(\"&lt;div style='column-count: 2;'&gt;{}&lt;/div&gt;\".format(code)))\n\n.Aqua Kana: .Aqua Kana\n.CJK Symbols Fallback HK: .CJK Symbols Fallback HK\n.Keyboard: .Keyboard\n.New York: .New York\n.SF Arabic: .SF Arabic\n.SF Arabic Rounded: .SF Arabic Rounded\n.SF Armenian: .SF Armenian\n.SF Armenian Rounded: .SF Armenian Rounded\n.SF Camera: .SF Camera\n.SF Compact Rounded: .SF Compact Rounded\n.SF Georgian: .SF Georgian\n.SF Georgian Rounded: .SF Georgian Rounded\n.SF Hebrew: .SF Hebrew\n.SF Hebrew Rounded: .SF Hebrew Rounded\n.SF NS Mono: .SF NS Mono\n.SF NS Rounded: .SF NS Rounded\n.SF Soft Numeric: .SF Soft Numeric\n.ThonburiUI: .ThonburiUI\nAcademy Engraved LET: Academy Engraved LET\nAdelle Sans Devanagari: Adelle Sans Devanagari\nAkayaKanadaka: AkayaKanadaka\nAkayaTelivigala: AkayaTelivigala\nAl Bayan: Al Bayan\nAl Nile: Al Nile\nAl Tarikh: Al Tarikh\nAmerican Typewriter: American Typewriter\nAndale Mono: Andale Mono\nAnnai MN: Annai MN\nApple Braille: Apple Braille\nApple Chancery: Apple Chancery\nApple LiGothic: Apple LiGothic\nApple LiSung: Apple LiSung\nApple SD Gothic Neo: Apple SD Gothic Neo\nApple Symbols: Apple Symbols\nAppleGothic: AppleGothic\nAppleMyungjo: AppleMyungjo\nArial: Arial\nArial Black: Arial Black\nArial Hebrew: Arial Hebrew\nArial Narrow: Arial Narrow\nArial Rounded MT Bold: Arial Rounded MT Bold\nArial Unicode MS: Arial Unicode MS\nArima Koshi: Arima Koshi\nArima Madurai: Arima Madurai\nAthelas: Athelas\nAvenir: Avenir\nAvenir Next: Avenir Next\nAvenir Next Condensed: Avenir Next Condensed\nAyuthaya: Ayuthaya\nBM Dohyeon: BM Dohyeon\nBM Hanna 11yrs Old: BM Hanna 11yrs Old\nBM Hanna Air: BM Hanna Air\nBM Hanna Pro: BM Hanna Pro\nBM Jua: BM Jua\nBM Kirang Haerang: BM Kirang Haerang\nBM Yeonsung: BM Yeonsung\nBaghdad: Baghdad\nBai Jamjuree: Bai Jamjuree\nBaloo 2: Baloo 2\nBaloo Bhai 2: Baloo Bhai 2\nBaloo Bhaijaan: Baloo Bhaijaan\nBaloo Bhaina 2: Baloo Bhaina 2\nBaloo Chettan 2: Baloo Chettan 2\nBaloo Da 2: Baloo Da 2\nBaloo Paaji 2: Baloo Paaji 2\nBaloo Tamma 2: Baloo Tamma 2\nBaloo Tammudu 2: Baloo Tammudu 2\nBaloo Thambi 2: Baloo Thambi 2\nBangla MN: Bangla MN\nBangla Sangam MN: Bangla Sangam MN\nBaoli SC: Baoli SC\nBaskerville: Baskerville\nBeirut: Beirut\nBiauKaiHK: BiauKaiHK\nBig Caslon: Big Caslon\nBodoni 72: Bodoni 72\nBodoni 72 Oldstyle: Bodoni 72 Oldstyle\nBodoni 72 Smallcaps: Bodoni 72 Smallcaps\nBodoni Ornaments: Bodoni Ornaments\nBradley Hand: Bradley Hand\nBrush Script MT: Brush Script MT\nCambay Devanagari: Cambay Devanagari\nChakra Petch: Chakra Petch\nChalkboard: Chalkboard\nChalkboard SE: Chalkboard SE\nChalkduster: Chalkduster\nCharm: Charm\nCharmonman: Charmonman\nCharter: Charter\nCochin: Cochin\nComic Sans MS: Comic Sans MS\nCopperplate: Copperplate\nCorsiva Hebrew: Corsiva Hebrew\nCourier: Courier\nCourier New: Courier New\nDIN Alternate: DIN Alternate\nDIN Condensed: DIN Condensed\nDamascus: Damascus\nDecoType Naskh: DecoType Naskh\nDejaVu Sans: DejaVu Sans\nDejaVu Sans Display: DejaVu Sans Display\nDejaVu Sans Mono: DejaVu Sans Mono\nDejaVu Serif: DejaVu Serif\nDejaVu Serif Display: DejaVu Serif Display\nDevanagari MT: Devanagari MT\nDevanagari Sangam MN: Devanagari Sangam MN\nDidot: Didot\nDiwan Kufi: Diwan Kufi\nDiwan Thuluth: Diwan Thuluth\nEuphemia UCAS: Euphemia UCAS\nFahkwang: Fahkwang\nFarah: Farah\nFarisi: Farisi\nFutura: Futura\nGalvji: Galvji\nGeeza Pro: Geeza Pro\nGeneva: Geneva\nGeorgia: Georgia\nGill Sans: Gill Sans\nGotu: Gotu\nGujarati MT: Gujarati MT\nGujarati Sangam MN: Gujarati Sangam MN\nGungSeo: GungSeo\nGurmukhi MN: Gurmukhi MN\nGurmukhi MT: Gurmukhi MT\nGurmukhi Sangam MN: Gurmukhi Sangam MN\nHannotate SC: Hannotate SC\nHanziPen SC: HanziPen SC\nHeadLineA: HeadLineA\nHei: Hei\nHeiti TC: Heiti TC\nHelvetica: Helvetica\nHelvetica Neue: Helvetica Neue\nHerculanum: Herculanum\nHiragino Maru Gothic Pro: Hiragino Maru Gothic Pro\nHiragino Mincho ProN: Hiragino Mincho ProN\nHiragino Sans: Hiragino Sans\nHiragino Sans GB: Hiragino Sans GB\nHiragino Sans TC: Hiragino Sans TC\nHoefler Text: Hoefler Text\nHubballi: Hubballi\nITF Devanagari: ITF Devanagari\nImpact: Impact\nInaiMathi: InaiMathi\nIowan Old Style: Iowan Old Style\nJaini: Jaini\nJaini Purva: Jaini Purva\nK2D: K2D\nKai: Kai\nKailasa: Kailasa\nKaiti SC: Kaiti SC\nKannada MN: Kannada MN\nKannada Sangam MN: Kannada Sangam MN\nKatari: Katari\nKavivanar: Kavivanar\nKefa: Kefa\nKhmer MN: Khmer MN\nKhmer Sangam MN: Khmer Sangam MN\nKlee: Klee\nKoHo: KoHo\nKodchasan: Kodchasan\nKohinoor Bangla: Kohinoor Bangla\nKohinoor Devanagari: Kohinoor Devanagari\nKohinoor Gujarati: Kohinoor Gujarati\nKohinoor Telugu: Kohinoor Telugu\nKokonor: Kokonor\nKrub: Krub\nKrungthep: Krungthep\nKufiStandardGK: KufiStandardGK\nLahore Gurmukhi: Lahore Gurmukhi\nLantinghei SC: Lantinghei SC\nLao MN: Lao MN\nLao Sangam MN: Lao Sangam MN\nLava Devanagari: Lava Devanagari\nLava Kannada: Lava Kannada\nLava Telugu: Lava Telugu\nLiHei Pro: LiHei Pro\nLiSong Pro: LiSong Pro\nLibian SC: Libian SC\nLingWai SC: LingWai SC\nLingWai TC: LingWai TC\nLucida Grande: Lucida Grande\nLuminari: Luminari\nMaku: Maku\nMalayalam MN: Malayalam MN\nMalayalam Sangam MN: Malayalam Sangam MN\nMali: Mali\nMarion: Marion\nMarker Felt: Marker Felt\nMenlo: Menlo\nMicrosoft Sans Serif: Microsoft Sans Serif\nMishafi: Mishafi\nMishafi Gold: Mishafi Gold\nModak: Modak\nMonaco: Monaco\nMshtakan: Mshtakan\nMukta: Mukta\nMukta Mahee: Mukta Mahee\nMukta Malar: Mukta Malar\nMukta Vaani: Mukta Vaani\nMuna: Muna\nMyanmar MN: Myanmar MN\nMyanmar Sangam MN: Myanmar Sangam MN\nNadeem: Nadeem\nNanum Brush Script: Nanum Brush Script\nNanum Gothic: Nanum Gothic\nNanum Myeongjo: Nanum Myeongjo\nNew Peninim MT: New Peninim MT\nNiramit: Niramit\nNoteworthy: Noteworthy\nNoto Nastaliq Urdu: Noto Nastaliq Urdu\nNoto Sans Adlam: Noto Sans Adlam\nNoto Sans Armenian: Noto Sans Armenian\nNoto Sans Avestan: Noto Sans Avestan\nNoto Sans Bamum: Noto Sans Bamum\nNoto Sans Bassa Vah: Noto Sans Bassa Vah\nNoto Sans Batak: Noto Sans Batak\nNoto Sans Bhaiksuki: Noto Sans Bhaiksuki\nNoto Sans Brahmi: Noto Sans Brahmi\nNoto Sans Buginese: Noto Sans Buginese\nNoto Sans Buhid: Noto Sans Buhid\nNoto Sans Canadian Aboriginal: Noto Sans Canadian Aboriginal\nNoto Sans Carian: Noto Sans Carian\nNoto Sans Caucasian Albanian: Noto Sans Caucasian Albanian\nNoto Sans Chakma: Noto Sans Chakma\nNoto Sans Cham: Noto Sans Cham\nNoto Sans Coptic: Noto Sans Coptic\nNoto Sans Cuneiform: Noto Sans Cuneiform\nNoto Sans Cypriot: Noto Sans Cypriot\nNoto Sans Duployan: Noto Sans Duployan\nNoto Sans Egyptian Hieroglyphs: Noto Sans Egyptian Hieroglyphs\nNoto Sans Elbasan: Noto Sans Elbasan\nNoto Sans Glagolitic: Noto Sans Glagolitic\nNoto Sans Gothic: Noto Sans Gothic\nNoto Sans Gunjala Gondi: Noto Sans Gunjala Gondi\nNoto Sans Hanifi Rohingya: Noto Sans Hanifi Rohingya\nNoto Sans Hanunoo: Noto Sans Hanunoo\nNoto Sans Hatran: Noto Sans Hatran\nNoto Sans Imperial Aramaic: Noto Sans Imperial Aramaic\nNoto Sans Inscriptional Pahlavi: Noto Sans Inscriptional Pahlavi\nNoto Sans Inscriptional Parthian: Noto Sans Inscriptional Parthian\nNoto Sans Javanese: Noto Sans Javanese\nNoto Sans Kaithi: Noto Sans Kaithi\nNoto Sans Kannada: Noto Sans Kannada\nNoto Sans Kayah Li: Noto Sans Kayah Li\nNoto Sans Kharoshthi: Noto Sans Kharoshthi\nNoto Sans Khojki: Noto Sans Khojki\nNoto Sans Khudawadi: Noto Sans Khudawadi\nNoto Sans Lepcha: Noto Sans Lepcha\nNoto Sans Limbu: Noto Sans Limbu\nNoto Sans Linear A: Noto Sans Linear A\nNoto Sans Linear B: Noto Sans Linear B\nNoto Sans Lisu: Noto Sans Lisu\nNoto Sans Lycian: Noto Sans Lycian\nNoto Sans Lydian: Noto Sans Lydian\nNoto Sans Mahajani: Noto Sans Mahajani\nNoto Sans Mandaic: Noto Sans Mandaic\nNoto Sans Manichaean: Noto Sans Manichaean\nNoto Sans Marchen: Noto Sans Marchen\nNoto Sans Masaram Gondi: Noto Sans Masaram Gondi\nNoto Sans Meetei Mayek: Noto Sans Meetei Mayek\nNoto Sans Mende Kikakui: Noto Sans Mende Kikakui\nNoto Sans Meroitic: Noto Sans Meroitic\nNoto Sans Miao: Noto Sans Miao\nNoto Sans Modi: Noto Sans Modi\nNoto Sans Mongolian: Noto Sans Mongolian\nNoto Sans Mro: Noto Sans Mro\nNoto Sans Multani: Noto Sans Multani\nNoto Sans Myanmar: Noto Sans Myanmar\nNoto Sans NKo: Noto Sans NKo\nNoto Sans Nabataean: Noto Sans Nabataean\nNoto Sans New Tai Lue: Noto Sans New Tai Lue\nNoto Sans Newa: Noto Sans Newa\nNoto Sans Ol Chiki: Noto Sans Ol Chiki\nNoto Sans Old Hungarian: Noto Sans Old Hungarian\nNoto Sans Old Italic: Noto Sans Old Italic\nNoto Sans Old North Arabian: Noto Sans Old North Arabian\nNoto Sans Old Permic: Noto Sans Old Permic\nNoto Sans Old Persian: Noto Sans Old Persian\nNoto Sans Old South Arabian: Noto Sans Old South Arabian\nNoto Sans Old Turkic: Noto Sans Old Turkic\nNoto Sans Oriya: Noto Sans Oriya\nNoto Sans Osage: Noto Sans Osage\nNoto Sans Osmanya: Noto Sans Osmanya\nNoto Sans Pahawh Hmong: Noto Sans Pahawh Hmong\nNoto Sans Palmyrene: Noto Sans Palmyrene\nNoto Sans Pau Cin Hau: Noto Sans Pau Cin Hau\nNoto Sans PhagsPa: Noto Sans PhagsPa\nNoto Sans Phoenician: Noto Sans Phoenician\nNoto Sans Psalter Pahlavi: Noto Sans Psalter Pahlavi\nNoto Sans Rejang: Noto Sans Rejang\nNoto Sans Samaritan: Noto Sans Samaritan\nNoto Sans Saurashtra: Noto Sans Saurashtra\nNoto Sans Sharada: Noto Sans Sharada\nNoto Sans Siddham: Noto Sans Siddham\nNoto Sans Sora Sompeng: Noto Sans Sora Sompeng\nNoto Sans Sundanese: Noto Sans Sundanese\nNoto Sans Syloti Nagri: Noto Sans Syloti Nagri\nNoto Sans Syriac: Noto Sans Syriac\nNoto Sans Tagalog: Noto Sans Tagalog\nNoto Sans Tagbanwa: Noto Sans Tagbanwa\nNoto Sans Tai Le: Noto Sans Tai Le\nNoto Sans Tai Tham: Noto Sans Tai Tham\nNoto Sans Tai Viet: Noto Sans Tai Viet\nNoto Sans Takri: Noto Sans Takri\nNoto Sans Thaana: Noto Sans Thaana\nNoto Sans Tifinagh: Noto Sans Tifinagh\nNoto Sans Tirhuta: Noto Sans Tirhuta\nNoto Sans Ugaritic: Noto Sans Ugaritic\nNoto Sans Vai: Noto Sans Vai\nNoto Sans Wancho: Noto Sans Wancho\nNoto Sans Warang Citi: Noto Sans Warang Citi\nNoto Sans Yi: Noto Sans Yi\nNoto Serif Ahom: Noto Serif Ahom\nNoto Serif Balinese: Noto Serif Balinese\nNoto Serif Hmong Nyiakeng: Noto Serif Hmong Nyiakeng\nNoto Serif Kannada: Noto Serif Kannada\nNoto Serif Myanmar: Noto Serif Myanmar\nNoto Serif Yezidi: Noto Serif Yezidi\nOctober Compressed Devanagari: October Compressed Devanagari\nOctober Compressed Tamil: October Compressed Tamil\nOctober Condensed Devanagari: October Condensed Devanagari\nOctober Condensed Tamil: October Condensed Tamil\nOctober Devanagari: October Devanagari\nOctober Tamil: October Tamil\nOptima: Optima\nOriya MN: Oriya MN\nOriya Sangam MN: Oriya Sangam MN\nOsaka: Osaka\nPCMyungjo: PCMyungjo\nPSL Ornanong Pro: PSL Ornanong Pro\nPT Mono: PT Mono\nPT Sans: PT Sans\nPT Serif: PT Serif\nPT Serif Caption: PT Serif Caption\nPadyakke Expanded One: Padyakke Expanded One\nPalatino: Palatino\nPapyrus: Papyrus\nParty LET: Party LET\nPhosphate: Phosphate\nPilGi: PilGi\nPlantagenet Cherokee: Plantagenet Cherokee\nRaanana: Raanana\nRockwell: Rockwell\nSTFangsong: STFangsong\nSTHeiti: STHeiti\nSTIX Two Math: STIX Two Math\nSTIX Two Text: STIX Two Text\nSTIXGeneral: STIXGeneral\nSTIXIntegralsD: STIXIntegralsD\nSTIXIntegralsSm: STIXIntegralsSm\nSTIXIntegralsUp: STIXIntegralsUp\nSTIXIntegralsUpD: STIXIntegralsUpD\nSTIXIntegralsUpSm: STIXIntegralsUpSm\nSTIXNonUnicode: STIXNonUnicode\nSTIXSizeFiveSym: STIXSizeFiveSym\nSTIXSizeFourSym: STIXSizeFourSym\nSTIXSizeOneSym: STIXSizeOneSym\nSTIXSizeThreeSym: STIXSizeThreeSym\nSTIXSizeTwoSym: STIXSizeTwoSym\nSTIXVariants: STIXVariants\nSama Devanagari: Sama Devanagari\nSama Gujarati: Sama Gujarati\nSama Gurmukhi: Sama Gurmukhi\nSama Kannada: Sama Kannada\nSama Malayalam: Sama Malayalam\nSama Tamil: Sama Tamil\nSana: Sana\nSarabun: Sarabun\nSathu: Sathu\nSavoye LET: Savoye LET\nSeravek: Seravek\nShobhika: Shobhika\nShree Devanagari 714: Shree Devanagari 714\nSignPainter: SignPainter\nSilom: Silom\nSimSong: SimSong\nSinhala MN: Sinhala MN\nSinhala Sangam MN: Sinhala Sangam MN\nSkia: Skia\nSnell Roundhand: Snell Roundhand\nSongti SC: Songti SC\nSrisakdi: Srisakdi\nSukhumvit Set: Sukhumvit Set\nSuperclarendon: Superclarendon\nSymbol: Symbol\nSystem Font: System Font\nTahoma: Tahoma\nTamil MN: Tamil MN\nTamil Sangam MN: Tamil Sangam MN\nTelugu MN: Telugu MN\nTelugu Sangam MN: Telugu Sangam MN\nThonburi: Thonburi\nTimes: Times\nTimes New Roman: Times New Roman\nTiro Bangla: Tiro Bangla\nTiro Devanagari Hindi: Tiro Devanagari Hindi\nTiro Devanagari Marathi: Tiro Devanagari Marathi\nTiro Devanagari Sanskrit: Tiro Devanagari Sanskrit\nTiro Gurmukhi: Tiro Gurmukhi\nTiro Kannada: Tiro Kannada\nTiro Tamil: Tiro Tamil\nTiro Telugu: Tiro Telugu\nToppan Bunkyu Gothic: Toppan Bunkyu Gothic\nToppan Bunkyu Midashi Gothic: Toppan Bunkyu Midashi Gothic\nToppan Bunkyu Midashi Mincho: Toppan Bunkyu Midashi Mincho\nToppan Bunkyu Mincho: Toppan Bunkyu Mincho\nTrattatello: Trattatello\nTrebuchet MS: Trebuchet MS\nTsukushi A Round Gothic: Tsukushi A Round Gothic\nTsukushi B Round Gothic: Tsukushi B Round Gothic\nVerdana: Verdana\nWaseem: Waseem\nWawati SC: Wawati SC\nWawati TC: Wawati TC\nWebdings: Webdings\nWingdings: Wingdings\nWingdings 2: Wingdings 2\nWingdings 3: Wingdings 3\nXingkai SC: Xingkai SC\nYuGothic: YuGothic\nYuKyokasho Yoko: YuKyokasho Yoko\nYuMincho: YuMincho\nYuanti SC: Yuanti SC\nYuppy SC: Yuppy SC\nYuppy TC: Yuppy TC\nZapf Dingbats: Zapf Dingbats\nZapfino: Zapfino\ncmb10: cmb10\ncmex10: cmex10\ncmmi10: cmmi10\ncmr10: cmr10\ncmss10: cmss10\ncmsy10: cmsy10\ncmtt10: cmtt10\n\n\nFalls Sie Ihr Dokument in LaTeX schreiben, k√∂nnten die cmXXXX-Schriften f√ºr Sie von Interesse sein, da sie den in LaTeX-Dokumenten verwendeten Schriften entsprechen. Hier ist ein Beispiel:\n\ncmfont = font_manager.FontProperties(fname=mpl.get_data_path() + '/fonts/ttf/cmr10.ttf')\nplt.rcParams.update({'font.size': 12,\n                     'axes.titlesize': 12,\n                     'axes.labelsize': 12,\n                     'axes.labelpad': 12,\n                     'lines.linewidth': 1,\n                     'lines.markersize': 10,\n                     'xtick.labelsize' : 10,\n                     'ytick.labelsize' : 10,\n                     'xtick.top' : True,\n                     'xtick.direction' : 'in',\n                     'ytick.right' : True,\n                     'ytick.direction' : 'in',\n                     'font.family' : 'serif',\n                     'font.serif' : cmfont.get_name(),\n                     \"axes.formatter.use_mathtext\": True,\n                     'text.usetex': True,\n                     'mathtext.fontset' : 'cm'\n                    })\n\n\nx=np.linspace(0,np.pi,100)\n\n\nplt.figure(figsize=get_size(6,5),dpi=150)\nplt.plot(x,np.sin(x))\nplt.xlabel(r\"velocity $v$\")\nplt.ylabel(r\"position $r$\")\nplt.show()"
  },
  {
    "objectID": "seminars/seminar06/25_publication_ready_figures.html#ein-dokument-vorbereiten",
    "href": "seminars/seminar06/25_publication_ready_figures.html#ein-dokument-vorbereiten",
    "title": "Erstellung ver√∂ffentlichungsreifer Diagramme",
    "section": "Ein Dokument vorbereiten",
    "text": "Ein Dokument vorbereiten\nWenn man ein Dokument (Bachelorarbeit z.B.) erstellt, ist es n√ºtzlich, seine Daten und Texte geschickt zur organisieren, um sich Arbeit zu ersparen. Hier ist ein Beipiel,\n\ndisplay_tree(\"Report\")\n\nReport/\n‚îú‚îÄ‚îÄ Figures/\n‚îÇ   ‚îú‚îÄ‚îÄ Figure1/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Figure1.ipynb\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Figure1_files/\n‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ libs/\n‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ bootstrap/\n‚îÇ   ‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-icons.css\n‚îÇ   ‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-icons.woff\n‚îÇ   ‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ bootstrap.min.css\n‚îÇ   ‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ bootstrap.min.js\n‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ clipboard/\n‚îÇ   ‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ clipboard.min.js\n‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ quarto-contrib/\n‚îÇ   ‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ live-runtime/\n‚îÇ   ‚îÇ           ‚îÇ       ‚îú‚îÄ‚îÄ live-runtime.css\n‚îÇ   ‚îÇ           ‚îÇ       ‚îú‚îÄ‚îÄ live-runtime.js\n‚îÇ   ‚îÇ           ‚îÇ       ‚îî‚îÄ‚îÄ pyodide-worker.js\n‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ quarto-html/\n‚îÇ   ‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ anchor.min.js\n‚îÇ   ‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ popper.min.js\n‚îÇ   ‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ quarto-syntax-highlighting.css\n‚îÇ   ‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ quarto.js\n‚îÇ   ‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ tippy.css\n‚îÇ   ‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ tippy.umd.min.js\n‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ quarto-ojs/\n‚îÇ   ‚îÇ               ‚îú‚îÄ‚îÄ quarto-ojs-runtime.js\n‚îÇ   ‚îÇ               ‚îî‚îÄ‚îÄ quarto-ojs.css\n‚îÇ   ‚îú‚îÄ‚îÄ figure1.pdf\n‚îÇ   ‚îú‚îÄ‚îÄ Figure2/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Figure2.ipynb\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Figure2_files/\n‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ libs/\n‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ bootstrap/\n‚îÇ   ‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-icons.css\n‚îÇ   ‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-icons.woff\n‚îÇ   ‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ bootstrap.min.css\n‚îÇ   ‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ bootstrap.min.js\n‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ clipboard/\n‚îÇ   ‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ clipboard.min.js\n‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ quarto-contrib/\n‚îÇ   ‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ live-runtime/\n‚îÇ   ‚îÇ           ‚îÇ       ‚îú‚îÄ‚îÄ live-runtime.css\n‚îÇ   ‚îÇ           ‚îÇ       ‚îú‚îÄ‚îÄ live-runtime.js\n‚îÇ   ‚îÇ           ‚îÇ       ‚îî‚îÄ‚îÄ pyodide-worker.js\n‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ quarto-html/\n‚îÇ   ‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ anchor.min.js\n‚îÇ   ‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ popper.min.js\n‚îÇ   ‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ quarto-syntax-highlighting.css\n‚îÇ   ‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ quarto.js\n‚îÇ   ‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ tippy.css\n‚îÇ   ‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ tippy.umd.min.js\n‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ quarto-ojs/\n‚îÇ   ‚îÇ               ‚îú‚îÄ‚îÄ quarto-ojs-runtime.js\n‚îÇ   ‚îÇ               ‚îî‚îÄ‚îÄ quarto-ojs.css\n‚îÇ   ‚îî‚îÄ‚îÄ figure2.pdf\n‚îî‚îÄ‚îÄ Text/\n    ‚îú‚îÄ‚îÄ article.aux\n    ‚îú‚îÄ‚îÄ article.fdb_latexmk\n    ‚îú‚îÄ‚îÄ article.fls\n    ‚îú‚îÄ‚îÄ article.log\n    ‚îú‚îÄ‚îÄ article.pdf\n    ‚îú‚îÄ‚îÄ article.synctex.gz\n    ‚îú‚îÄ‚îÄ article.tex\n    ‚îú‚îÄ‚îÄ content/\n    ‚îÇ   ‚îú‚îÄ‚îÄ 01_introduction.tex\n    ‚îÇ   ‚îú‚îÄ‚îÄ 02_theory.tex\n    ‚îÇ   ‚îú‚îÄ‚îÄ 03_results.tex\n    ‚îÇ   ‚îî‚îÄ‚îÄ 04_conclusions.tex\n    ‚îú‚îÄ‚îÄ paper.aux\n    ‚îú‚îÄ‚îÄ paper.bbl\n    ‚îú‚îÄ‚îÄ paper.blg\n    ‚îú‚îÄ‚îÄ paper.fdb_latexmk\n    ‚îú‚îÄ‚îÄ paper.fls\n    ‚îú‚îÄ‚îÄ paper.log\n    ‚îú‚îÄ‚îÄ paper.pdf\n    ‚îú‚îÄ‚îÄ paper.qmd\n    ‚îú‚îÄ‚îÄ paper.synctex.gz\n    ‚îú‚îÄ‚îÄ paper.tex\n    ‚îú‚îÄ‚îÄ paper_files/\n    ‚îÇ   ‚îî‚îÄ‚îÄ libs/\n    ‚îÇ       ‚îú‚îÄ‚îÄ bootstrap/\n    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-icons.css\n    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-icons.woff\n    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ bootstrap.min.css\n    ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ bootstrap.min.js\n    ‚îÇ       ‚îú‚îÄ‚îÄ clipboard/\n    ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ clipboard.min.js\n    ‚îÇ       ‚îú‚îÄ‚îÄ quarto-contrib/\n    ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ live-runtime/\n    ‚îÇ       ‚îÇ       ‚îú‚îÄ‚îÄ live-runtime.css\n    ‚îÇ       ‚îÇ       ‚îú‚îÄ‚îÄ live-runtime.js\n    ‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ pyodide-worker.js\n    ‚îÇ       ‚îú‚îÄ‚îÄ quarto-html/\n    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ anchor.min.js\n    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ popper.min.js\n    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ quarto-syntax-highlighting.css\n    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ quarto.js\n    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ tippy.css\n    ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ tippy.umd.min.js\n    ‚îÇ       ‚îî‚îÄ‚îÄ quarto-ojs/\n    ‚îÇ           ‚îú‚îÄ‚îÄ quarto-ojs-runtime.js\n    ‚îÇ           ‚îî‚îÄ‚îÄ quarto-ojs.css\n    ‚îî‚îÄ‚îÄ paperNotes.bib\n\n\n/var/folders/t4/_9qps8wj56jc60nwkr3nrcr00000gn/T/ipykernel_20220/929351223.py:1: DeprecationWarning: The `display_tree` Function is Deprecated and will be Removed in a Future Release. Please use `DirectoryTree` Instead. End of Life Date is \"31st December 2024\".\n  display_tree(\"Report\")"
  },
  {
    "objectID": "seminars/seminar01/seminar11.html#what-are-neural-networks",
    "href": "seminars/seminar01/seminar11.html#what-are-neural-networks",
    "title": "Neural Networks",
    "section": "What are Neural Networks?",
    "text": "What are Neural Networks?\nNeural networks are computational models inspired by how our brains process information. Just like our brain consists of interconnected neurons that process and transmit signals, artificial neural networks consist of mathematical ‚Äúneurons‚Äù that process numerical information. They‚Äôre particularly powerful for:\n\nRecognizing patterns in data\nMaking predictions\nClassifying information\nSolving complex problems"
  },
  {
    "objectID": "seminars/seminar01/seminar11.html#why-neural-networks-in-physics",
    "href": "seminars/seminar01/seminar11.html#why-neural-networks-in-physics",
    "title": "Neural Networks",
    "section": "Why Neural Networks in Physics?",
    "text": "Why Neural Networks in Physics?\nIn physics, we often encounter problems where:\n\nTraditional mathematical models become too complex\nWe need to analyze large amounts of experimental data\nWe want to make predictions based on incomplete information\n\nNeural networks help us with these challenges! Some real-world applications include:\n\nParticle physics: Identifying particles in detector data\nAstronomy: Classifying galaxies\nMaterials science: Predicting material properties\nQuantum mechanics: Solving many-body problems\nBiological physics: Modeling neural activity\nActive matter: Predicting collective behavior"
  },
  {
    "objectID": "seminars/seminar01/seminar11.html#a-single-neuron-building-our-first-ai-unit",
    "href": "seminars/seminar01/seminar11.html#a-single-neuron-building-our-first-ai-unit",
    "title": "Neural Networks",
    "section": "A Single Neuron: Building Our First AI Unit",
    "text": "A Single Neuron: Building Our First AI Unit\n\nThe Big Picture\nBefore diving into the details, let‚Äôs understand what we‚Äôre trying to build. Imagine you‚Äôre creating a smart device that can recognize handwritten numbers. The most basic version of this device would be a single artificial neuron - think of it as an electronic version of a brain cell that can make simple yes/no decisions.\n\n\nFrom Biology to Mathematics\nJust like a biological neuron receives signals from other neurons, our artificial neuron processes numerical inputs through mathematical operations to produce a single output value.\nThe neuron performs three distinct steps:\n\nInput Weighting Each input value gets multiplied by a weight parameter:\n\n\\[\\begin{eqnarray}\nx_{1}\\rightarrow x_{1} w_{1}\\\\\nx_{2}\\rightarrow x_{2} w_{2}\n\\end{eqnarray}\\]\n\nBias Addition A bias value \\(b\\) is added to the weighted sum:\n\n\\[\\begin{equation}\nx_{1} w_{1}+ x_{2} w_{2}+b\n\\end{equation}\\]\n\nActivation Function The final step applies an activation function \\(\\sigma()\\):\n\n\\[\\begin{equation}\ny=\\sigma( x_{1} w_{1}+ x_{2} w_{2}+b)\n\\end{equation}\\]\nFor mathematical convenience, we can write this more compactly using vector notation:\n\\[\\begin{equation*}\n\\hat{y} = \\sigma(w^{\\rm T} x + b)\n\\end{equation*}\\]\nThe sigmoid function has the following mathematical form: \\[\\begin{equation*}\n\\sigma(z) = \\frac{1}{1+{\\rm e}^{-z}}\n\\end{equation*}\\]\nLet‚Äôs implement the sigmoid function and visualize how it transforms inputs:\n\n\n\n\n\n\n\n\n\n\n\n\nNow let‚Äôs see how a neuron processes inputs through these operations. Given:\n\\[\\begin{eqnarray}\nw=[0,1]\\\\\nb=4\n\\end{eqnarray}\\]\nAnd input values:\n\\[\\begin{eqnarray}\nx=[2,3]\n\\end{eqnarray}\\]\nThe computation becomes:\n\n\n\n\n\n\nFor computational efficiency, these calculations can be performed on multiple inputs simultaneously using matrix operations:\n\\[\\begin{equation*}\n\\hat{y} = \\sigma(w^{\\rm T} X + b)\n\\end{equation*}\\]"
  },
  {
    "objectID": "seminars/seminar01/seminar11.html#loss-function-measuring-our-networks-mistakes",
    "href": "seminars/seminar01/seminar11.html#loss-function-measuring-our-networks-mistakes",
    "title": "Neural Networks",
    "section": "Loss Function: Measuring Our Network‚Äôs Mistakes",
    "text": "Loss Function: Measuring Our Network‚Äôs Mistakes\n\nWhy Do We Need a Loss Function?\nJust like we need a way to measure error in physics experiments, we need a way to measure how wrong our neural network‚Äôs predictions are. The loss function serves this purpose - it tells us how far our predictions are from the true values.\n\n\nUnderstanding Cross-Entropy Loss\nWhile we could use simpler measures like mean squared error:\n\\[\\begin{equation}\nMSE(y,\\hat{y})=\\frac{1}{n}\\sum_{i=1}^{n}(y-\\hat{y})^2\n\\end{equation}\\]\nWe‚Äôll use a more sophisticated measure called cross-entropy loss. For a single training example, the formula is:\n\\[\\begin{equation*}\nL(y,\\hat{y}) = -y\\log(\\hat{y})-(1-y)\\log(1-\\hat{y})\n\\end{equation*}\\]\nPhysics Analogy: This is similar to entropy in thermodynamics - it measures the disorder or uncertainty in our predictions.\nWhen dealing with multiple training examples (\\(m\\) of them), we average the loss:\n\\[\\begin{equation*}\nL(Y,\\hat{Y}) = -\\frac{1}{m}\\sum_{i = 0}^{m}y^{(i)}\\log(\\hat{y}^{(i)})-(1-y^{(i)})\\log(1-\\hat{y}^{(i)})\n\\end{equation*}\\]\nLet‚Äôs implement this in code:\n\n\n\n\n\n\n\n\nTraining the Network: The Big Picture\nThe goal of training is to minimize this loss function. We do this by adjusting the weights and biases of our network. Let‚Äôs see how this works:\n\n\n\n\n\n\n\n\nBackward Propagation: Finding the Path to Improvement\nThe loss function \\(L\\) depends on all our weights and biases:\n\\[\nL(w_{1},w_{2},w_{3},\\ldots ,b_{1},b_{2},b_{3},\\ldots)\n\\]\nTo minimize the loss, we need to know how it changes when we adjust each weight. This is where partial derivatives come in:\n\\[\n\\frac{\\partial L}{\\partial w_j}\n\\]\nBreaking this down step by step, we use the chain rule:\n\\[\\begin{align*}\n\\frac{\\partial L}{\\partial w_j} = \\frac{\\partial L}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial z}\\frac{\\partial z}{\\partial w_j}\n\\end{align*}\\]\nLet‚Äôs calculate each term:\n\n\\(\\partial L/\\partial\\hat{y}\\): \\[\\begin{align*}\n\\frac{\\partial L}{\\partial\\hat{y}} &= \\frac{\\partial}{\\partial\\hat{y}}\\left(-y\\log(\\hat{y})-(1-y)\\log(1-\\hat{y})\\right) \\\\\n&= -\\frac{y}{\\hat{y}} +\\frac{(1 - y)}{1-\\hat{y}} \\\\\n&= \\frac{\\hat{y} - y}{\\hat{y}(1-\\hat{y})}\n\\end{align*}\\]\n\\(\\partial \\hat{y}/\\partial z\\): \\[\\begin{align*}\n\\frac{\\partial }{\\partial z}\\sigma(z) &= \\sigma(z)(1-\\sigma(z)) \\\\\n&= \\hat{y}(1-\\hat{y})\n\\end{align*}\\]\n\\(\\partial z/\\partial w_j\\): \\[\\begin{align*}\n\\frac{\\partial }{\\partial w_j}(w^{\\rm T} x + b) = x_j\n\\end{align*}\\]\n\nPutting it all together: \\[\\begin{align*}\n\\frac{\\partial L}{\\partial w_j} = (\\hat{y} - y)x_j\n\\end{align*}\\]\nFor multiple training examples, in vector form: \\[\\begin{align*}\n\\frac{\\partial L}{\\partial w} = \\frac{1}{m} X(\\hat{y} - y)^{\\rm T}\n\\end{align*}\\]\nSimilarly for the bias: \\[\\begin{align*}\n\\frac{\\partial L}{\\partial b} = \\frac{1}{m}\\sum_{i=1}^{m}{(\\hat{y}^{(i)} - y^{(i)})}\n\\end{align*}\\]\n\n\nInteractive Example: Watching Gradients\n\n\n\n\n\n\n\n\nKey Points to Remember:\n\nThe loss function measures prediction errors\nCross-entropy loss is particularly suitable for classification problems\nGradients tell us how to adjust weights and biases\nThe chain rule helps us compute these gradients efficiently"
  },
  {
    "objectID": "seminars/seminar01/seminar11.html#training-the-network-putting-it-all-together",
    "href": "seminars/seminar01/seminar11.html#training-the-network-putting-it-all-together",
    "title": "Neural Networks",
    "section": "Training the Network: Putting It All Together",
    "text": "Training the Network: Putting It All Together\n\nStochastic Gradient Descent (SGD)\nNow that we understand how to compute gradients, we can use them to train our network. The basic idea is simple: 1. Calculate how wrong we are (loss) 2. Calculate how to improve (gradients) 3. Take a small step in the right direction\nThis process is called Stochastic Gradient Descent (SGD). The mathematical update rule is:\n\\[\nw\\leftarrow w-\\eta\\frac{\\partial L}{\\partial w}\n\\]\nwhere \\(\\eta\\) is the learning rate - a small number that controls how big our improvement steps are.\nPhysics Analogy: This is similar to finding the minimum of a potential well. The gradient tells us which way is ‚Äúdownhill‚Äù, and we take small steps in that direction.\n\n\nBuilding Our First Complete Network\nLet‚Äôs implement a complete training loop. We‚Äôll use: - Learning rate \\(\\eta = 1\\) - 200 training epochs (complete passes through the data)\n\n\n\n\n\n\n\n\nEvaluating Our Network: The Confusion Matrix\nTo understand how well our network performs, we use a confusion matrix. This shows: - True Positives (TP): Correctly predicted positive cases - False Positives (FP): Incorrectly predicted positive cases - True Negatives (TN): Correctly predicted negative cases - False Negatives (FN): Incorrectly predicted negative cases\n\n\n\nconfusion_matrix\n\n\nLet‚Äôs evaluate our model on the test data:\n\n\n\n\n\n\n\n\nTesting Individual Predictions\nLet‚Äôs visualize how our network performs on a single test image:\n\n\n\n\n\n\n\n\nUnderstanding the Results:\n\nThe confusion matrix shows us where our model makes mistakes\nThe classification report gives us metrics like:\n\nPrecision: How many of our positive predictions were correct\nRecall: How many actual positive cases did we catch\nF1-score: A balanced measure of precision and recall\n\n\n\n\nKey Points to Remember:\n\nTraining is an iterative process of:\n\nForward propagation\nLoss calculation\nBackward propagation\nParameter updates\n\nThe learning rate controls how quickly we update our parameters\nWe evaluate performance using confusion matrices and classification metrics"
  },
  {
    "objectID": "seminars/seminar01/2_reinforcement_learning.html",
    "href": "seminars/seminar01/2_reinforcement_learning.html",
    "title": "Machine Learning and Neural Networks ü§ñ",
    "section": "",
    "text": "Welcome to the first of our advanced seminars! Throughout this course, we‚Äôve explored various applications of Python to physical problems. The course has focused not on teaching physics itself, but on exercising and developing your Python skills through physically motivated examples. Now, as we move into these advanced seminars, we‚Äôll explore a field that is increasingly important in physics: machine learning. Machine learning is the umbrella term for a variety of computational procedures designed to extract useful information from data. In this first advanced seminar, we‚Äôll introduce you to a foundational aspect of machine learning‚Äîreinforcement learning. True to our course philosophy, we‚Äôll implement this in a way that emphasizes understanding by calculating as much as possible in pure Python without relying heavily on specialized packages.",
    "crumbs": [
      "üõ†Ô∏è Hands-On Seminars",
      "Advanced Topics",
      "Advanced: Reinforcement Learning"
    ]
  },
  {
    "objectID": "seminars/seminar01/2_reinforcement_learning.html#overview",
    "href": "seminars/seminar01/2_reinforcement_learning.html#overview",
    "title": "Machine Learning and Neural Networks ü§ñ",
    "section": "Overview üåê",
    "text": "Overview üåê\nMachine learning has its origins long time ago and many of the currently very popular approaches have been developed in the past century. Two things have been stimulating the current hype of machine learning techniques. One is the computational power that is available already at the level of your smartphone. The second one is the availability of data. Machine learning is divided into different areas: Supervised learning üë®‚Äçüè´ involves telling the system what is right or wrong through labeled training data, Semi-supervised learning üéØ works with only sparse information on what is right or wrong, using a mixture of labeled and unlabeled data, and Unsupervised learning üîç lets the system figure out patterns and structures without any labels at all.\nThe graphics below gives a small summary. In our course, we cannot cover all methods. We will focus on Reinforcement Learning and Neural Networks just to show you, how things could look in Python.\n\nImage taken from F. Cichos et al.¬†Nature Machine Intelligence (2020).",
    "crumbs": [
      "üõ†Ô∏è Hands-On Seminars",
      "Advanced Topics",
      "Advanced: Reinforcement Learning"
    ]
  },
  {
    "objectID": "seminars/seminar01/2_reinforcement_learning.html#reinforcement-learning",
    "href": "seminars/seminar01/2_reinforcement_learning.html#reinforcement-learning",
    "title": "Machine Learning and Neural Networks ü§ñ",
    "section": "Reinforcement Learning üéÆ",
    "text": "Reinforcement Learning üéÆ\nReinforcement learning is learning what to do‚Äîhow to map situations to actions‚Äîso as to maximize a numerical reward signal. The learner or agent is not told which actions to take, as in most forms of machine learning, but instead must discover which actions yield the most reward by trying them. In the most interesting and challenging cases, actions may affect not only the immediate reward but also the next situation and, through that, all subsequent rewards. These two characteristics‚Äîtrial-and-error search and delayed reward‚Äîare the two most important distinguishing features of reinforcement learning.\n\n\n\n\n\n\nüéØ Think of it Like This\n\n\n\nImagine teaching a dog a new trick. You don‚Äôt tell the dog exactly how to move its paws. Instead, you give treats (rewards) when it does something right. The dog learns through trial and error, and eventually, it figures out the sequence of actions that gets treats! Reinforcement learning works the same way with computers‚Äîthe agent explores different actions, receives feedback in the form of rewards or penalties, and gradually discovers the optimal behavior through repeated experience.\n\n\nIt has been around since the 1950s but gained momentum only in 2013 with the demonstrations of DeepMind on how to learn play Atari games like pong. The graphic below shows some of its applications in the field of robotics and gaming.\n\n\n\noverview_rl\n\n\n\nüî¨ Applications in Physics\nReinforcement learning offers particularly exciting applications in physics and related fields. Researchers are using RL for üß™ optimizing experimental design parameters to maximize information gain from limited experimental resources, ‚öõÔ∏è controlling quantum systems and preparing specific quantum states with high fidelity, and üå°Ô∏è finding energy-efficient paths through phase space in complex dynamical systems. Additional applications include üî¨ optimizing molecular dynamics simulations to accelerate sampling of rare events or transition states, and üíé discovering new materials with desired properties by navigating the vast space of possible chemical compositions and crystal structures.\n\n\n\n\n\n\nüîó Connection to Physics\n\n\n\nThe mathematical framework of reinforcement learning shares conceptual connections with statistical physics, particularly in how systems evolve toward equilibrium states that maximize certain potentials. The exploration-exploitation tradeoff in RL has parallels to thermodynamic concepts like entropy maximization under constraints.\n\n\n\n\nMarkov Decision Process üé≤\nThe key element of reinforcement learning is the so-called Markov Decision Process (MDP). The Markov decision process denotes a formalism of planning actions in the face of uncertainty. A MDP consists formally of several components: \\(S\\) represents a set of accessible states in the world üó∫Ô∏è, while \\(D\\) defines an initial distribution describing the probability of starting in each state üéØ. The transition probability between states is given by \\(P_{sa}\\) ‚û°Ô∏è, and \\(A\\) represents the set of possible actions that can be taken in each state üéÆ. The discount factor \\(\\gamma\\), which is a number between 0 and 1 ‚è∞, determines how much we value future rewards compared to immediate ones. Finally, \\(R\\) is the reward function üéÅ that assigns a numerical value to being in each state or taking each action.\n\n\n\n\n\n\nüîó Physics Analogies\n\n\n\nIt‚Äôs worth noting the connection to concepts you‚Äôre likely familiar with from physics. The state space \\(S\\) is directly analogous to phase space in classical mechanics, where each point represents a complete specification of the system‚Äôs configuration. The transition probabilities \\(P_{sa}\\) resemble stochastic processes that appear throughout statistical physics, such as those described by the Fokker-Planck equation for systems subject to random fluctuations. The Markov property‚Äîwhereby future states depend only on the current state and not on the history of how we arrived there‚Äîis similar to memoryless processes in statistical mechanics, where equilibrium distributions depend only on current conditions. Finally, the reward function \\(R\\) plays a role conceptually similar to Hamiltonians or Lagrangians in physics, in that the system ‚Äúseeks‚Äù to optimize it through its dynamics.\n\n\nWe begin in an initial state \\(s_{i,j}\\) drawn from the distribution \\(D\\). At each time step \\(t\\), we then have to pick an action, for example \\(a_1(t)\\), as a result of which our state transitions to some state \\(s_{i,j+1}\\). The states do not necessarily correspond to spatial positions, however, as we talk about the gridworld later we may use this example to understand the procedures.\n\nBy repeatedly picking actions, we traverse some sequence of states\n\\[\ns_{0,0}\\rightarrow s_{0,1}\\rightarrow s_{1,1}+\\ldots\n\\]\nOur total reward is then the sum of discounted rewards along this sequence of states\n\\[\nR(s_{0,0})+\\gamma R(s_{0,1})+ \\gamma^2 R(s_{1,1})+ \\ldots\n\\]\n\n\n\n\n\n\nüí∞ Why Discount Future Rewards?\n\n\n\nThe discount factor \\(\\gamma\\) (typically between 0 and 1) makes rewards obtained immediately more valuable than those obtained in the future. This makes sense for several reasons. First, uncertainty generally increases with time, making future rewards less certain than immediate ones. Second, prioritizing immediate gratification helps the algorithm converge faster to a solution. The value of \\(\\gamma\\) dramatically affects behavior: if \\(\\gamma = 0\\), the agent only cares about immediate rewards and becomes very myopic, while if \\(\\gamma = 1\\), all future rewards count equally, which can lead to instability in the learning process. In practice, typical values are \\(\\gamma = 0.9\\) or \\(\\gamma = 0.95\\), striking a balance between short-term and long-term planning. Think of it like compound interest, but in reverse‚Äîrewards lose value the further into the future they occur!\n\n\nIn reinforcement learning, our goal is to find a way of choosing actions \\(a_0\\), \\(a_1, \\ldots\\) over time, so as to maximize the expected value of the rewards. The sequence of actions that realizes the maximum reward is called the optimal policy \\(\\pi^{*}\\). A sequence of actions in general is called a policy \\(\\pi\\).\n\n\n\n\n\n\n‚öñÔ∏è Physics Analogy: Principle of Least Action\n\n\n\nThis optimization can be viewed as analogous to the principle of least action in classical mechanics, where a system evolves along paths that minimize the action integral. The key difference is that in RL, we maximize rewards rather than minimize action.\n\n\n\nMethods of RL üõ†Ô∏è\nThere are different methods available to find the optimal policy:\nModel-based algorithms üìä: If we know the transition probabilities \\(P_{sa}\\), we can use methods like value iteration. Think of this as having a map before you start navigating.\nModel-free algorithms üó∫Ô∏è: If we don‚Äôt know the transition probabilities, we use methods like Q-learning. This is like exploring a city without a map‚Äîyou learn as you go!\nWe will focus on Q-learning, a model-free algorithm.\n\n\n\n\n\n\nüî¨ Common RL Methods in Physics\n\n\n\nFor physics applications, several reinforcement learning methods have proven particularly valuable. Deep Q-Networks (DQN) extend the basic Q-learning approach by incorporating neural networks to handle high-dimensional state spaces, making them suitable for complex physical systems. Policy Gradient methods take a different approach by directly optimizing the policy rather than learning value functions, which can be advantageous when the action space is continuous or very large. Actor-Critic methods combine the strengths of both approaches, using value function approximation (the critic) to guide policy optimization (the actor). Finally, Monte Carlo Tree Search methods, which gained fame through their use in AlphaGo, are particularly effective for planning in systems with well-defined forward models, making them useful in certain physics simulations and control problems.\n\n\n\n\n\nUnderstanding Q-Learning üß†\nIn Q-learning, the value of an action in a state is measured by its Q-value. The expectation value \\(E\\) of the rewards with an initial state and action for a given policy is the Q-function or Q-value.\n\\[\nQ^{\\pi}(s,a)=E[R(s_{0},a_{0})+\\gamma R(s_{1},a_{1})+ \\gamma^2 R(s_{2},a_{2})+ \\ldots | s_{0}=s,a_{0}=a,a_{t}=\\pi(s_{t})]\n\\]\n\n\n\n\n\n\nüí° What is a Q-Value?\n\n\n\nThis sounds complicated but is in principle easy. Think of a Q-value as answering the question ‚Äúhow good is taking action \\(a\\) in state \\(s\\)?‚Äù A high Q-value means ‚Äúthis action looks promising!‚Äù ‚úÖ, while a low Q-value indicates ‚Äúthis action is probably bad‚Äù ‚ùå. The key insight is that there is a separate Q-value for all actions in each state. Thus if we have 4 possible actions and 25 states in our environment, we need to store a total of 100 Q-values, which we conveniently organize in a matrix with dimensions (states √ó actions).\n\n\nFor the optimal sequence of actions‚Äîfor the best way to go‚Äîthis Q value becomes a maximum:\n\\[\nQ^{*}(s,a)=\\max_{\\pi}Q^{\\pi}(s,a)\n\\]\nThe policy which gives the sequence of actions to be carried out to get the maximum reward is then calculated by:\n\\[\n\\pi^{*}(s)=\\arg\\max_{a}Q^{*}(s,a)\n\\]\nThis simply means: ‚ÄúIn state \\(s\\), choose the action \\(a\\) with the highest Q-value!‚Äù\n\n\n\n\n\n\nüìê The Bellman Equation: The Foundation of RL\n\n\n\n\n\nThe Bellman equation is the cornerstone of reinforcement learning and dynamic programming. Named after mathematician Richard Bellman, it expresses the fundamental recursive relationship between the value of a state and the values of its successor states. This elegant mathematical framework allows us to break down complex sequential decision-making problems into simpler, more manageable pieces.\n\nThe Value Function\nAt the heart of reinforcement learning lies the state-value function \\(V^{\\pi}(s)\\), which represents the expected return we can achieve starting from state \\(s\\) and following a particular policy \\(\\pi\\) thereafter. The Bellman equation for this value function is:\n\\[\nV^{\\pi}(s) = E_{\\pi}[R_{t+1} + \\gamma V^{\\pi}(s_{t+1}) | s_t = s]\n\\]\nThis deceptively simple equation encapsulates a profound idea: the value of being in a particular state equals the immediate reward we expect to receive, plus the discounted value of wherever we‚Äôll end up next. In other words, we don‚Äôt need to look infinitely far into the future to determine a state‚Äôs value‚Äîwe only need to consider one step ahead and trust that the value of the next state already captures everything beyond that.\n\n\nThe Bellman Optimality Equation\nWhen we seek the optimal behavior rather than following a fixed policy, we arrive at the Bellman optimality equation. For the optimal state-value function \\(V^{*}(s)\\), this becomes:\n\\[\nV^{*}(s) = \\max_{a} \\left[ R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^{*}(s') \\right]\n\\]\nThe corresponding equation for optimal Q-values (the action-value function) is:\n\\[\nQ^{*}(s,a) = R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) \\max_{a'} Q^{*}(s',a')\n\\]\nThis second equation is particularly important because it forms the theoretical foundation of Q-learning. The Q-learning update rule we‚Äôll implement is essentially an iterative approximation method designed to solve this equation through repeated experience rather than analytical computation.\n\n\nüî¨ Connection to Physics\nThe Bellman equation reveals deep connections to fundamental concepts in physics that may surprise you. In classical mechanics, the Hamilton-Jacobi equation describes how a system evolves through phase space, serving as a bridge between Lagrangian and Hamiltonian formulations. The Bellman equation is essentially a discrete-time, stochastic generalization of this same principle, adapted for decision-making under uncertainty.\nThe connection extends even further to quantum mechanics through Feynman‚Äôs path integral formulation. Just as the path integral sums over all possible trajectories a particle might take, weighting each by its action, the Bellman equation implicitly considers all possible future trajectories through the expectation value, weighting them by their probability and discounted reward. Both frameworks recognize that the optimal or most likely behavior emerges from considering the totality of possibilities.\nPerhaps most fundamentally, both the Bellman equation and the principle of least action in physics exploit what computer scientists call ‚Äúoptimal substructure.‚Äù If the optimal path from point A to point C passes through point B, then the segment from A to B must itself be optimal. This isn‚Äôt just a mathematical convenience‚Äîit‚Äôs a deep principle about how optimal solutions compose, whether we‚Äôre minimizing action in physics or maximizing reward in reinforcement learning.\n\n\nWhy Does Q-Learning Work?\nUnderstanding the Bellman equation helps us appreciate why Q-learning is so powerful. Q-learning is fundamentally a model-free method, meaning it learns the optimal Q-values \\(Q^{*}(s,a)\\) without ever needing to know the transition probabilities \\(P(s'|s,a)\\) that appear in the Bellman optimality equation. This is remarkable because it means we don‚Äôt need a mathematical model of how the environment behaves.\nInstead of solving the Bellman equation directly through analytical methods or dynamic programming, Q-learning takes a different approach. It samples actual experiences from the environment‚Äîtaking actions, observing outcomes, and receiving rewards. It then uses these samples to iteratively update its Q-values, gradually refining its estimates. Under appropriate conditions (including sufficient exploration and a suitable learning rate schedule), these Q-values provably converge to the optimal values that would be obtained by solving the Bellman equation exactly.\nThis sampling-based approach is extraordinarily powerful in practice. We can learn optimal behavior in environments that are too complex to model analytically, too large to solve with traditional dynamic programming, or simply unknown to us. The agent learns from experience, much like a physicist conducting experiments to understand a system whose governing equations are not yet known.\n\n\n\n\n\n\nThe Q-Learning Update Rule üìù\nThe Q-learning algorithm is an iterative procedure of updating the Q-value of each state and action which converges to the optimal policy \\(\\pi^{*}\\). It is given by:\n\\[\nQ_{t+\\Delta t}(s,a)  = Q_t(s,a) + \\alpha\\big[R(s) + \\gamma \\max_{a'}Q_t(s',a')-Q_t(s,a)\\big]\n\\]\n\n\n\n\n\n\nüîç Breaking Down This Equation\n\n\n\nLet‚Äôs understand each term in the Q-learning update equation. The term \\(Q_t(s,a)\\) represents our current estimate of how good this action is. When we take the action, we receive an immediate reward \\(R(s)\\), which gives us instant feedback. Looking ahead, \\(\\gamma \\max_{a'}Q_t(s',a')\\) represents the discounted value of the best action we could take in the next state. The crucial part is the bracketed term \\(R(s) + \\gamma \\max_{a'}Q_t(s',a') - Q_t(s,a)\\), which is our prediction error‚Äîessentially comparing where we thought we‚Äôd end up versus where we actually could end up.\nThe learning rate \\(\\alpha\\) (ranging from 0 to 1) controls how much we trust this new information. If \\(\\alpha = 0\\), we never learn anything and simply ignore new information. If \\(\\alpha = 1\\), we completely replace our old estimate with the new observation, ignoring everything we learned before. In practice, typical values range from \\(\\alpha = 0.1\\) to \\(0.5\\), providing a balance between stability and adaptability. The key insight is that we update our Q-value based on the difference between what we expected and what we observed, allowing the agent to gradually refine its understanding through experience!\n\n\n\n\n\n\n\n\n‚öõÔ∏è Physics Perspective\n\n\n\nFrom a physics perspective, this update rule resembles a relaxation method for finding equilibrium states. The term in brackets can be interpreted as a ‚Äúforce‚Äù that drives the Q-values toward their optimal values, with \\(\\alpha\\) controlling the rate of convergence, similar to a damping constant in physics. The Bellman equation, which underpins this update rule, is also a form of dynamic programming that shares mathematical similarities with the Hamilton-Jacobi-Bellman equation in control theory.",
    "crumbs": [
      "üõ†Ô∏è Hands-On Seminars",
      "Advanced Topics",
      "Advanced: Reinforcement Learning"
    ]
  },
  {
    "objectID": "seminars/seminar01/2_reinforcement_learning.html#navigating-a-grid-world",
    "href": "seminars/seminar01/2_reinforcement_learning.html#navigating-a-grid-world",
    "title": "Machine Learning and Neural Networks ü§ñ",
    "section": "Navigating a Grid World üó∫Ô∏è",
    "text": "Navigating a Grid World üó∫Ô∏è\nFor our Python course we will have a look at the standard problem of reinforcement learning, which is the navigation in a grid world. This is like teaching an agent to find its way home!\nEach of the grid cells below represents a state \\(s\\) in which an object could reside. In each of these states, the object can take several actions. If it may step to left, right, up or down, there are 4 actions, which we may call \\(a_{1},a_{2},a_{3}\\) and \\(a_{4}\\).\nThis image below shows our gridworld, with 25 states, where the shaded state is the goal state üéØ where we want the agent to go to independent of its initial state.\n\nIn each of these states, we have 4 possible actions as depicted below:\n\n\n\n\n\n\n\nüéÆ The Game Plan\n\n\n\nOur agent will follow a simple but effective learning strategy. It will start at a random position in the grid, then take actions by moving up, down, left, or right. For each move, it gets a penalty of -1, which encourages finding the shortest path rather than wandering aimlessly. When it finally reaches the goal, it receives a big reward of +10! üéâ Through many iterations of this process, the agent gradually learns which actions lead to the goal fastest from any starting position.\n\n\n\nStep 1: Initialize Reinforcement Learning üöÄ\nAt first we would like to initialize our problem. We have as depicted above 25 states, where one state is the goal state. We would like to use 4 actions to move between the states so our Q-value matrix has 100 entries.\n\n\n\n\n\n\nüéØ Setting up Rewards and Penalties\n\n\n\nWe would like to give a penalty of \\(R=-1\\) for all states except for the goal state where we give a reward of \\(R=10\\). The reason for the -1 penalty is important: it encourages the agent to find the shortest path to the goal. Since every step costs something, the agent naturally learns to minimize the number of steps taken. Without this penalty, the agent might wander around inefficiently, since reaching the goal eventually would still yield the same total reward regardless of path length.\n\n\nOur agent shall learn with a learning rate of \\(\\alpha=0.5\\) and we will discount future rewards with \\(\\gamma=0.5\\).\n\n\n\n\n\n\n\n\n\n\n\n\nüé≤ The Œµ-Greedy Strategy\n\n\n\nThere is one tiny detail which is crucial to understand: the Œµ-greedy factor. The problem is this: if we always choose the action with the highest Q-value, we might get stuck in a suboptimal strategy. Imagine your initial random Q-values happen to favor ‚Äúalways go right‚Äù‚Äîyou might never discover that ‚Äúgo left‚Äù leads to a shortcut!\nThe solution is the Œµ-greedy strategy, which introduces controlled randomness into the decision-making process. Specifically, 80% of the time (when a random number exceeds 0.2), we choose the best known action based on our current Q-values, exploiting what we‚Äôve learned. However, 20% of the time (when the random number is ‚â§ 0.2), we choose a completely random action to explore new possibilities.\nThis balancing act is called the exploration-exploitation tradeoff. Exploitation üéØ means using what you already know by choosing the best action according to current estimates, while Exploration üîç means trying new things through random actions to discover potentially better strategies. By setting Œµ = 0.2, we ensure that 20% of actions are chosen randomly to explore new possibilities, preventing the agent from getting trapped in local optima while still making progress toward the goal most of the time.\n\n\n\n\nStep 2: Define the Actions üéÆ\nThe actions, which we can take in each state are defined by 2-D vectors here which increase either the row or the column index in our gridworld.\n\n\n\n\n\n\n\n\nStep 3: Choose Initial State üé≤\nWe choose the initial state from which we start randomly. We also initialize a list, where we register the sum of all Q-values. This is helpful to monitor the convergence of our algorithm.\n\n\n\n\n\n\n\n\n\n\n\n\nüìä What to Expect\n\n\n\nAfter running the learning loop:\n\nThe sum of Q-values should converge to a stable negative value\nMost Q-values will be negative (because most states give -1 penalty)\nOnly Q-values leading toward the goal will become positive\nThe convergence plot will show: initial fluctuations ‚Üí stabilization\n\n\n\n\n\nStep 4: The Learning Loop! üîÑ\nThe cell below is all you need for learning how to navigate the grid world. This is where the magic happens! ‚ú®\n\n\n\n\n\n\n\n\n\n\n\n\nüîç What‚Äôs Happening in the Loop?\n\n\n\nLet‚Äôs trace through one typical iteration to understand the learning process. Suppose the agent is in state (2, 3). It chooses an action‚Äîsay ‚Äúmove right‚Äù‚Äîusing the Œµ-greedy strategy (80% chance of the best known action, 20% chance of random exploration). The agent then moves to the new state (2, 4) and receives a reward of -1 as a penalty for the move. Based on this experience, it updates the Q-value for the action ‚Äúmove right‚Äù in state (2, 3), incorporating both the immediate reward and the discounted future reward expected from the new state. The agent then repeats this process from the new position. After 10,000 iterations, the agent has explored many different paths through the grid and learned which actions lead to the goal most efficiently from every possible starting position!\n\n\n\n\nStep 5: Visualize Convergence üìà\nThe convergence of our learning is best judged from the sum of all Q-values in the matrix. This should converge to a negative value as most of the time our agent is getting the penalty \\(R=-1\\) and only sparsely \\(R=10\\) at the goal.\n\n\n\n\n\n\n\n\n\n\n\n\nüîç Interpreting the Convergence Plot\n\n\n\nThe convergence plot reveals three distinct phases of learning. In the early phase (first ~2000 transitions), you‚Äôll see large fluctuations as the agent explores randomly and Q-values are being updated rapidly based on new discoveries. The middle phase (2000-5000 transitions) shows decreasing fluctuations as patterns emerge and the agent begins finding better paths more consistently. Finally, in the late phase (5000+ transitions), the values stabilize, indicating that the optimal policy has been learned! ‚úÖ\nIf your plot doesn‚Äôt stabilize as expected, there are several things you can try. Increasing the number of iterations gives the agent more time to explore and converge. Adjusting the learning rate Œ± can help‚Äîif it‚Äôs too high, learning may be unstable; if too low, convergence will be very slow. Of course, also check carefully for bugs in the code, particularly in the action selection and Q-value update logic.\n\n\n\n\nStep 6: Extract the Policy üó∫Ô∏è\nThe policy is obtained by taking the best actions with the largest Q-value from our Q-matrix.\n\\[\n\\pi^{*}(s)=\\arg\\max_{a}Q^{*}(s,a)\n\\]\nIn plain English: ‚ÄúFor each state, find which action has the highest Q-value‚Äù\n\n\n\n\n\n\n\n\nStep 7: Visualize the Learned Policy! üé®\nNow let‚Äôs see the beautiful result‚Äîa visual map showing the optimal action to take in each state!\n\n\n\n\n\n\n\n\n\n\n\n\nüéâ What You Should See\n\n\n\nThe plot displays arrows in each grid cell indicating the optimal action to take from that position. You should observe several key features: arrows point toward the goal from all positions, demonstrating that the agent has learned a complete navigation strategy. The paths represented are optimal in the sense of being shortest, meaning from any starting position, following the arrows will reach the goal in the minimum number of steps. The green square marks the goal location in the bottom-right corner.\nThis visualization beautifully demonstrates the power of reinforcement learning‚Äîthe agent discovered these optimal paths entirely through trial and error! No explicit programming of paths was needed; the agent simply learned from the rewards and penalties it experienced during exploration.",
    "crumbs": [
      "üõ†Ô∏è Hands-On Seminars",
      "Advanced Topics",
      "Advanced: Reinforcement Learning"
    ]
  },
  {
    "objectID": "seminars/seminar01/2_reinforcement_learning.html#experiments-and-challenges",
    "href": "seminars/seminar01/2_reinforcement_learning.html#experiments-and-challenges",
    "title": "Machine Learning and Neural Networks ü§ñ",
    "section": "üéØ Experiments and Challenges",
    "text": "üéØ Experiments and Challenges\nNow that you understand the basics, try these modifications to deepen your understanding:\n\n\n\n\n\n\nüü¢ Easy Challenge: Change the Goal Location\n\n\n\n\n\nTask: Move the goal to a different corner (e.g., top-left at position [0, 0])\nWhat to change:\n# Instead of:\nR[n_rows-1, n_columns-1] = 10\n\n# Try:\nR[0, 0] = 10\nWhat to observe: Does the policy adapt? Do all arrows now point to the new goal?\n\n\n\n\n\n\n\n\n\nüü° Medium Challenge: Add a Second Goal\n\n\n\n\n\nTask: Add a second goal state with a smaller reward (e.g., R=5)\nWhat to change:\nR[0, n_columns-1] = 5  # Top-right corner\nWhat to observe: Notice which goal the agent prefers‚Äîdoes it consistently choose the higher reward, or does the preference depend on starting position? Pay attention to how the arrows in different regions of the grid show this preference, potentially creating a boundary between regions that lead to each goal.\n\n\n\n\n\n\n\n\n\nüü† Advanced Challenge: Add Walls (Obstacles)\n\n\n\n\n\nTask: Make some cells impassable ‚Äúwalls‚Äù\nWhat to change:\n# Add large negative rewards to wall cells\nR[2, 2] = -100  # Wall in center\nR[2, 3] = -100  # Another wall\nWhat to observe: Does the policy route around the walls? Try creating a maze!\n\n\n\n\n\n\n\n\n\nüî¥ Expert Challenge: Stochastic Environment\n\n\n\n\n\nTask: Make actions succeed only 80% of the time (20% random slip)\nWhat to change: Modify the action selection to sometimes execute a different action than intended:\n# After choosing action, add randomness:\nif np.random.random() &lt; 0.2:  # 20% chance of slip\n    action = np.random.randint(4)  # Random direction instead\nWhat to observe: How does uncertainty affect the learned policy? Do paths change?\n\n\n\n\n\n\n\n\n\nüéì Research Challenge: Parameter Sensitivity\n\n\n\n\n\nTask: Systematically vary parameters and observe effects\nParameters to explore: Experiment with the learning rate Œ± by trying values like 0.1, 0.5, and 0.9 to see how this affects convergence speed. Test different values of the discount factor Œ≥ (0.1, 0.5, 0.9) to observe how it affects the learned policy, particularly the agent‚Äôs preference for shorter versus longer paths. Also vary epsilon Œµ with values like 0.0, 0.2, and 0.5 to understand how exploration rate impacts learning stability and final performance.\nCreate plots showing convergence speed for different Œ± values, the final policy learned with different Œ≥ values, and learning stability (perhaps using running averages of episode rewards) for different Œµ values. These visualizations will help you develop intuition for how these hyperparameters interact with the learning process.",
    "crumbs": [
      "üõ†Ô∏è Hands-On Seminars",
      "Advanced Topics",
      "Advanced: Reinforcement Learning"
    ]
  },
  {
    "objectID": "seminars/seminar01/2_reinforcement_learning.html#what-youve-learned",
    "href": "seminars/seminar01/2_reinforcement_learning.html#what-youve-learned",
    "title": "Machine Learning and Neural Networks ü§ñ",
    "section": "üéì What You‚Äôve Learned",
    "text": "üéì What You‚Äôve Learned\nCongratulations! You‚Äôve just implemented reinforcement learning from scratch! üéâ\n\nKey Concepts ‚úÖ\nYou‚Äôve mastered several fundamental concepts in reinforcement learning. You now understand Markov Decision Processes, including the roles of states, actions, rewards, and policies in sequential decision-making. You‚Äôve implemented Q-Learning, which learns action values through trial and error without needing a model of the environment. You‚Äôve experienced the importance of balancing Exploration vs.¬†Exploitation through the Œµ-greedy strategy, ensuring the agent both learns new strategies and leverages known good actions. You‚Äôve seen how the Bellman Equation provides the theoretical foundation for updating Q-values based on immediate and future rewards. Finally, you‚Äôve learned to monitor Convergence by tracking the sum of Q-values over time, giving you insight into when the agent has finished learning.\n\n\nProgramming Skills üíª\nYou‚Äôve also developed several practical programming skills. You can now work with multi-dimensional NumPy arrays to store and manipulate Q-values efficiently. You‚Äôve implemented iterative update algorithms that gradually refine estimates through repeated experience. You‚Äôve learned to balance exploration and exploitation in decision-making systems using randomization strategies. You can visualize policies with matplotlib, creating intuitive graphical representations of learned behaviors. And you‚Äôve gained experience tracking and plotting convergence metrics to monitor learning progress over time.\n\n\nPhysics Connections üî¨\nThroughout this seminar, you‚Äôve encountered numerous connections to physics. The Q-learning process relates to Statistical Mechanics through concepts like ensemble averages and the approach to equilibrium states. The Optimization problem of finding the best policy parallels finding minimum energy configurations in physical systems. The agent‚Äôs random exploration and probabilistic decisions exemplify Stochastic Processes commonly found in statistical physics. And the iterative Q-value updates resemble Relaxation Methods used to find equilibrium solutions in computational physics.",
    "crumbs": [
      "üõ†Ô∏è Hands-On Seminars",
      "Advanced Topics",
      "Advanced: Reinforcement Learning"
    ]
  },
  {
    "objectID": "seminars/seminar01/2_reinforcement_learning.html#where-to-go-from-here",
    "href": "seminars/seminar01/2_reinforcement_learning.html#where-to-go-from-here",
    "title": "Machine Learning and Neural Networks ü§ñ",
    "section": "üöÄ Where to Go From Here",
    "text": "üöÄ Where to Go From Here\n\nImmediate Next Steps\n\n\n\n\n\n\n\n\nüìö Further Reading\nIf you want to know more about Reinforcement Learning, several excellent resources are available. The classic textbook is üìñ Sutton and Barto‚Äôs Reinforcement Learning: An Introduction, which is freely available as a PDF and covers everything from basics to advanced topics. For hands-on practice, üéÆ OpenAI Gym provides a standardized interface to work with more complex RL environments. In upcoming seminars, we‚Äôll explore üß† Deep RL, where Q-learning is combined with neural networks to handle high-dimensional problems. For physics-specific applications, üî¨ RL in Physics papers cover fascinating topics like quantum control, molecular dynamics optimization, and materials discovery‚Äîsearch for these in journals like Nature Machine Intelligence or Physical Review Letters.\n\n\nReal-World Applications üåç\nThe simple grid world you just solved may seem toy-like, but the same principles scale up to remarkably sophisticated applications. These include ü§ñ robot navigation in complex real-world environments with obstacles and dynamic conditions, üéØ optimal control of quantum systems where precise manipulation of quantum states is required, and üî¨ experimental design for physics measurements to determine the most informative sequence of experiments. RL is also being used for üíé materials discovery through high-dimensional searches of chemical composition spaces, and ‚öõÔ∏è particle accelerator tuning and optimization where thousands of parameters must be adjusted simultaneously.\n\n\nDebugging Tips üêõ\nIf your implementation doesn‚Äôt converge as expected, here‚Äôs a systematic debugging approach. First, check if Q-values are exploding to very large positive or negative numbers‚Äîif so, reduce the learning rate Œ± to make updates more gradual. If the policy seems random even after many iterations, you likely need more training iterations to allow the agent to explore sufficiently. If convergence is happening but very slowly, try adjusting the exploration rate Œµ or increasing Œ± to speed up learning. If the agent seems to be going in circles or taking obviously suboptimal paths, verify that your action vectors are correctly defined and actually move the agent where you intend. Finally, if arrows aren‚Äôt pointing toward the goal, double-check that the reward is indeed highest at the goal state and that goal-reaching is being detected properly in your code.\n\n\n\n\n\n\nüéØ The Big Picture\n\n\n\nWhat you‚Äôve learned here is remarkably powerful. Consider the journey from randomness to intelligence: you started with completely random Q-values and random actions, yet through nothing but trial, error, and a simple update rule, your agent discovered the optimal path! This happened with no explicit programming of the solution‚Äîyou never told the agent ‚Äúgo to the goal‚Äù or specified which path to take. You only defined rewards, and the agent figured out the strategy itself through experience.\nThe approach is also highly generalizable. The same algorithm works for different grid sizes, multiple goals, 3D spaces, continuous state spaces (with appropriate modifications), and complex physics problems. The underlying principles remain the same regardless of the specific application.\nThis is the essence of machine learning: letting the computer discover solutions through experience rather than explicit programming. Rather than encoding our understanding of the optimal solution, we define the objective and let the learning algorithm find its own path to achieving it.",
    "crumbs": [
      "üõ†Ô∏è Hands-On Seminars",
      "Advanced Topics",
      "Advanced: Reinforcement Learning"
    ]
  },
  {
    "objectID": "seminars/seminar01/2_reinforcement_learning.html#summary",
    "href": "seminars/seminar01/2_reinforcement_learning.html#summary",
    "title": "Machine Learning and Neural Networks ü§ñ",
    "section": "üéä Summary",
    "text": "üéä Summary\nYou‚Äôve successfully:\n\n‚úÖ Understood the fundamentals of reinforcement learning\n‚úÖ Implemented Q-learning from scratch in pure Python\n‚úÖ Trained an agent to navigate a gridworld optimally\n‚úÖ Visualized the learning process and final policy\n‚úÖ Connected RL concepts to physics principles\n\nNext seminar: We‚Äôll explore neural networks and see how they can be combined with reinforcement learning to solve even more complex problems! üß†üöÄ\n\n\n\n\n\n\nüåü Final Thought\n\n\n\nThe gridworld we explored is deliberately simple for pedagogical purposes, but don‚Äôt be fooled‚Äîthe principles you learned apply directly to cutting-edge research across multiple domains. Scientists are using these exact same reinforcement learning ideas to design better solar cells by optimizing material compositions and layer thicknesses, to optimize quantum computers through careful control of qubit interactions, and to control fusion reactors where real-time decisions must balance dozens of competing objectives. The same techniques are being applied to discover new drugs by navigating vast chemical spaces, and to understand complex physical systems whose behavior emerges from intricate interactions.\nYou‚Äôre now equipped with a fundamental tool of modern computational physics! üéâ The Q-learning algorithm you implemented today forms the foundation for much more sophisticated approaches that are actively advancing scientific knowledge and technological capabilities.",
    "crumbs": [
      "üõ†Ô∏è Hands-On Seminars",
      "Advanced Topics",
      "Advanced: Reinforcement Learning"
    ]
  },
  {
    "objectID": "seminars/1_input_output.html",
    "href": "seminars/1_input_output.html",
    "title": "Input and output",
    "section": "",
    "text": "Python has a function called input for getting input from the user and assigning it a variable name.\n\nvalue=input(\"Tell me a number: \")\ntype(value)\n\nTell me a number:  78898.9\n\n\nstr\n\n\nThe value contains the keyboard input as expected, but it is a string. We want to use a number and not a string, so we need to convert it from a string to a number.\n\nv = eval(value)\ntype(v)\n\nfloat\n\n\n\n\n\nScreen output is possible by using the print command. The argument of the print function can be of different type.\n\n\nYou can format your output by modifying the string given to the print function by str.format(), The str contains text that is written to be the screen, as well as certain format specifiers contained in curly braces {}. The format function contains the list of variables that are to be printed.\n\nstring1 = \"How\"\nstring2 = \"are you my friend?\"\nint1 = 34\nint2 = 942885\nfloat1 = -3.0\nfloat2 = 3.141592653589793e-14\nprint(' ***')\n\nprint(string1)\nprint(string1 + ' ' + string2)\n\nprint(' 1. {} {}'.format(string1, string2)) \n\nprint(' 2. {0:s} {1:s}'.format(string1, string2))\nprint(' 3. {0:s} {0:s} {1:s} - {0:s} {1:s}'.format(string1, string2)) \n\nprint(' 4. {0:10s}{1:5s}'.format(string1, string2))\nprint(' ***')\nprint(int1, int2)\nprint(' 6. {0:d} {1:d}'.format(int1, int2)) \nprint(' 7. {0:8d} {1:10d}'.format(int1, int2)) \nprint(' ***')\nprint(' 8. {0:0.3f}'.format(float1))\nprint(' 9. {0:6.3f}'.format(float1)) \nprint('10. {0:8.3f}'.format(float1)) \nprint(2*' 11. {0:8.3f}'.format(float1))\nprint(' ***')\nprint('12. {0:0.3e}'.format(float2)) \nprint('13. {0:10.3e}'.format(float2)) \nprint('14. {0:10.3f}'.format(float2))\nprint(' ***')\nprint('15. 12345678901234567890')\nprint('16. {0:s}--{1:8d},{2:10.3e}'.format(string2, int1, float2))\n\n ***\nHow\nHow are you my friend?\n 1. How are you my friend?\n 2. How are you my friend?\n 3. How How are you my friend? - How are you my friend?\n 4. How       are you my friend?\n ***\n34 942885\n 6. 34 942885\n 7.       34     942885\n ***\n 8. -3.000\n 9. -3.000\n10.   -3.000\n 11.   -3.000 11.   -3.000\n ***\n12. 3.142e-14\n13.  3.142e-14\n14.      0.000\n ***\n15. 12345678901234567890\n16. are you my friend?--      34, 3.142e-14\n\n\n\n\n\nA very similar formatting can be achieved with the %operator.\n\nname = \"Frank\"\nprint(\"Hello, %s.\" % name)\n\nHello, Frank.\n\n\n\n\n\nFormatted string literals are the string literals that start with an f at the beginning and use curly braces {} to enclose the expressions that will be replaced with other values.\n\nname = \"Python Lecture\"\nnumber = 3\nfstring = f\"I'm here for the {number}. time and this {name} is awesome!\"\nprint(fstring)\n\nI'm here for the 3. time and this Python Lecture is awesome!\n\n\n\n\ntimes = 100\nfstring = f\"You just have to sent me {times:10.3f} Euros.\"\nprint(fstring)\n\nYou just have to sent me    100.000 Euros.\n\n\n\n\n\n\nFile input and output is one of the most important features. We will have a look at reading and writing of text files with numpy and pandas. Python itself also allows you to open files and the file object provides the methods read, write and close.\n\nimport numpy as np\n\nwith open('a.txt', 'r') as file_1,open('b.txt','r') as file_2:\n    for a,b in zip(file_1,file_2):\n        print(int(a)+int(b))\n\n\nfile_1.close()\nfile_2.close()\n\n10\n12\n14\n97\n9\n9\n\n\n\n\nMost of the time we want import numbers from text files. So direct connection to NumPy seems useful and we will study that first.\n\nimport numpy as np # don't forget to import numpy\n\n\n\nOften you would like to analyze data that you have stored in a text file. Consider, for example, the data file below for an experiment measuring the free fall of a mass.\nData for falling mass experiment\nDate: 16-Aug-2013\nData taken by Frank and Ralf\ndata point  time (sec)  height (mm) uncertainty (mm)\n0       0.0     180     3.5\n1       0.5     182     4.5\n2       1.0     178     4.0\n3       1.5     165     5.5\n4       2.0     160     2.5\n5       2.5     148     3.0\n6       3.0     136     2.5\nSuppose that the name of the text file is MyData.txt. Then we can read the data into four different arrays with the following NumPy statement:\n\ndataPt, time, height, error = np.loadtxt(\"MyData.txt\", skiprows=4 , unpack=True)\n\nIf you don‚Äôt want to read in all the columns of data, you can specify which columns to read in using the usecols key word. For example, the call\n\ntime, height = np.loadtxt(\"MyData.txt\", skiprows=5 , usecols = (1,2), unpack=True)\n\nreads in only columns 1 and 2; columns 0 and 3 are skipped.\n\n\n\nThere are plenty of ways to write data to a data file in Python. We will stick to one very simple one that‚Äôs suitable for writing data files in text format. It uses the NumPy savetxt routine, which is the counterpart of the loadtxt routine introduced in the previous section. The general form of the routine is\nsavetxt(filename, array, fmt=\"%0.18e\", delimiter=\" \", newline=\"\\n\", header=\"\", footer=\"\", comments=\"# \")\nWe illustrate savetext below with a script that first creates four arrays by reading in the data file MyData.txt, as discussed in the previous section, and then writes that same data set to another file MyDataOut.txt.\n\ndataPt, time, height, error = np.loadtxt(\"MyData.txt\", skiprows=5 , unpack=True)\n\n\nlist(zip(dataPt, time, height, error))\n\n[(1.0, 0.5, 182.0, 4.5),\n (2.0, 1.0, 178.0, 4.0),\n (3.0, 1.5, 165.0, 5.5),\n (4.0, 2.0, 160.0, 2.5),\n (5.0, 2.5, 148.0, 3.0),\n (6.0, 3.0, 136.0, 2.5)]\n\n\n\nnp.savetxt('MyDataOut.txt',list(zip(dataPt, time, height, error)), fmt=\"%12.3f\")\n\n\ncat MyDataOut.txt\n\n       1.000        0.500      182.000        4.500\n       2.000        1.000      178.000        4.000\n       3.000        1.500      165.000        5.500\n       4.000        2.000      160.000        2.500\n       5.000        2.500      148.000        3.000\n       6.000        3.000      136.000        2.500\n\n\n\n\n\n\nPandas is a software library written for the Python programming language. It is used for data manipulation and analysis. It provides special data structures and operations for the manipulation of numerical tables and time series and builds on top of numpy.\n\nEasy handling of missing data\nIntelligent label-based slicing, fancy indexing, and subsetting of large data sets\n\nThe data formats provided by the pandas module are used by several other modules, such as the trackpy which is a moduly for feature tracking and analysis in image series.\n\n\n\nimport pandas as pd # import the pandas module\n\nPandas provides two data structures\n\nSeries\nData Frames\n\nA Series is a one-dimensional labeled array capable of holding any data type (integers, strings, floating point numbers, Python objects, etc.). The axis labels are collectively referred to as the index.\n\nmy_simple_series = pd.Series(np.random.randn(7), index=['a', 'b', 'c', 'd', 'e','f','g'])\nmy_simple_series\n\na    1.160711\nb   -0.296427\nc    1.881074\nd   -1.197978\ne    0.280311\nf    1.538339\ng    1.681957\ndtype: float64\n\n\n\nmy_simple_series\n\n-0.2964266043928443\n\n\nThere is a whole lot of functionality built into pandas data types. You may of course also obtain the same functionality using numpy commands, but you may find the pandas abbrevations very useful.\n\nmy_simple_series.agg(['min','max','sum','mean']) # aggregate a number of properties into a single array\n\nmin    -1.523075\nmax     0.525265\nsum    -2.119315\nmean   -0.302759\ndtype: float64\n\n\nA DataFrame is a two-dimensional size-mutable, potentially heterogeneous tabular data structure with labeled axes (rows and columns). The example below shows how such a DataFrame can be generated from the scratch. In addition to the data supplied to the DataFrame method, an index column is generated when creating a DataFrame. As in the case of Series there is a whole lot of functionality integrated into the DataFrame data type which you may explore on the website.\n\ndf = pd.DataFrame()\n\n\ndf = pd.DataFrame(np.random.randint(low=0, high=10, size=(5, 5)),columns=['column 1', 'column 2', 'columns 3', 'column 4', 'column 5'])\ndf.head()\n\n\n\n\n\n\n\n\ncolumn 1\ncolumn 2\ncolumns 3\ncolumn 4\ncolumn 5\n\n\n\n\n0\n3\n9\n3\n7\n4\n\n\n1\n2\n2\n4\n6\n7\n\n\n2\n6\n1\n7\n4\n5\n\n\n3\n3\n4\n4\n3\n0\n\n\n4\n5\n6\n8\n0\n2\n\n\n\n\n\n\n\nDue to the labelling of the columns, each column may be accessed by its column label. Labeling by names improves readability considerably.\n\ndf['column 4']\n\n0    7\n1    6\n2    4\n3    3\n4    0\nName: column 4, dtype: int64\n\n\nIf you don‚Äôt like this format, you can always return to a simple numpy array with the as_matrix() method.\n\ndf.values\n\narray([[3, 9, 3, 7, 4],\n       [2, 2, 4, 6, 7],\n       [6, 1, 7, 4, 5],\n       [3, 4, 4, 3, 0],\n       [5, 6, 8, 0, 2]])\n\n\n\n\n\nDataFrames may also be populated by text files such as comma separated value files (short .csv). These files contain data in text format but also a column label, which can be read by the pandas method read_csv(). You can find an example below, which reads the data from the dust sensor on my balcony from April, 11th. You see the different columns, where P1 and P2 correspond to the PM10 and PM2.5 dust values in \\(\\mu g/m^3\\).\n\ndata = pd.DataFrame()\ndata = pd.read_csv(\"2018-04-11_sds011_sensor_12253.csv\",delimiter=\";\",parse_dates=False)\ndata.head()\n\n\n\n\n\n\n\n\nsensor_id\nsensor_type\nlocation\nlat\nlon\ntimestamp\nP1\ndurP1\nratioP1\nP2\ndurP2\nratioP2\n\n\n\n\n0\n12253\nSDS011\n6189\n52.527\n13.39\n2018-04-11T00:01:58\n25.87\nNaN\nNaN\n19.37\nNaN\nNaN\n\n\n1\n12253\nSDS011\n6189\n52.527\n13.39\n2018-04-11T00:04:24\n25.63\nNaN\nNaN\n20.53\nNaN\nNaN\n\n\n2\n12253\nSDS011\n6189\n52.527\n13.39\n2018-04-11T00:06:55\n26.30\nNaN\nNaN\n22.00\nNaN\nNaN\n\n\n3\n12253\nSDS011\n6189\n52.527\n13.39\n2018-04-11T00:09:23\n24.60\nNaN\nNaN\n20.30\nNaN\nNaN\n\n\n4\n12253\nSDS011\n6189\n52.527\n13.39\n2018-04-11T00:11:51\n25.17\nNaN\nNaN\n20.23\nNaN\nNaN\n\n\n\n\n\n\n\n\n(data['P1']/data['P2']).plot()"
  },
  {
    "objectID": "seminars/1_input_output.html#keyboard-input",
    "href": "seminars/1_input_output.html#keyboard-input",
    "title": "Input and output",
    "section": "",
    "text": "Python has a function called input for getting input from the user and assigning it a variable name.\n\nvalue=input(\"Tell me a number: \")\ntype(value)\n\nTell me a number:  78898.9\n\n\nstr\n\n\nThe value contains the keyboard input as expected, but it is a string. We want to use a number and not a string, so we need to convert it from a string to a number.\n\nv = eval(value)\ntype(v)\n\nfloat"
  },
  {
    "objectID": "seminars/1_input_output.html#screen-output",
    "href": "seminars/1_input_output.html#screen-output",
    "title": "Input and output",
    "section": "",
    "text": "Screen output is possible by using the print command. The argument of the print function can be of different type.\n\n\nYou can format your output by modifying the string given to the print function by str.format(), The str contains text that is written to be the screen, as well as certain format specifiers contained in curly braces {}. The format function contains the list of variables that are to be printed.\n\nstring1 = \"How\"\nstring2 = \"are you my friend?\"\nint1 = 34\nint2 = 942885\nfloat1 = -3.0\nfloat2 = 3.141592653589793e-14\nprint(' ***')\n\nprint(string1)\nprint(string1 + ' ' + string2)\n\nprint(' 1. {} {}'.format(string1, string2)) \n\nprint(' 2. {0:s} {1:s}'.format(string1, string2))\nprint(' 3. {0:s} {0:s} {1:s} - {0:s} {1:s}'.format(string1, string2)) \n\nprint(' 4. {0:10s}{1:5s}'.format(string1, string2))\nprint(' ***')\nprint(int1, int2)\nprint(' 6. {0:d} {1:d}'.format(int1, int2)) \nprint(' 7. {0:8d} {1:10d}'.format(int1, int2)) \nprint(' ***')\nprint(' 8. {0:0.3f}'.format(float1))\nprint(' 9. {0:6.3f}'.format(float1)) \nprint('10. {0:8.3f}'.format(float1)) \nprint(2*' 11. {0:8.3f}'.format(float1))\nprint(' ***')\nprint('12. {0:0.3e}'.format(float2)) \nprint('13. {0:10.3e}'.format(float2)) \nprint('14. {0:10.3f}'.format(float2))\nprint(' ***')\nprint('15. 12345678901234567890')\nprint('16. {0:s}--{1:8d},{2:10.3e}'.format(string2, int1, float2))\n\n ***\nHow\nHow are you my friend?\n 1. How are you my friend?\n 2. How are you my friend?\n 3. How How are you my friend? - How are you my friend?\n 4. How       are you my friend?\n ***\n34 942885\n 6. 34 942885\n 7.       34     942885\n ***\n 8. -3.000\n 9. -3.000\n10.   -3.000\n 11.   -3.000 11.   -3.000\n ***\n12. 3.142e-14\n13.  3.142e-14\n14.      0.000\n ***\n15. 12345678901234567890\n16. are you my friend?--      34, 3.142e-14\n\n\n\n\n\nA very similar formatting can be achieved with the %operator.\n\nname = \"Frank\"\nprint(\"Hello, %s.\" % name)\n\nHello, Frank.\n\n\n\n\n\nFormatted string literals are the string literals that start with an f at the beginning and use curly braces {} to enclose the expressions that will be replaced with other values.\n\nname = \"Python Lecture\"\nnumber = 3\nfstring = f\"I'm here for the {number}. time and this {name} is awesome!\"\nprint(fstring)\n\nI'm here for the 3. time and this Python Lecture is awesome!\n\n\n\n\ntimes = 100\nfstring = f\"You just have to sent me {times:10.3f} Euros.\"\nprint(fstring)\n\nYou just have to sent me    100.000 Euros."
  },
  {
    "objectID": "seminars/1_input_output.html#file-inputoutput",
    "href": "seminars/1_input_output.html#file-inputoutput",
    "title": "Input and output",
    "section": "",
    "text": "File input and output is one of the most important features. We will have a look at reading and writing of text files with numpy and pandas. Python itself also allows you to open files and the file object provides the methods read, write and close.\n\nimport numpy as np\n\nwith open('a.txt', 'r') as file_1,open('b.txt','r') as file_2:\n    for a,b in zip(file_1,file_2):\n        print(int(a)+int(b))\n\n\nfile_1.close()\nfile_2.close()\n\n10\n12\n14\n97\n9\n9\n\n\n\n\nMost of the time we want import numbers from text files. So direct connection to NumPy seems useful and we will study that first.\n\nimport numpy as np # don't forget to import numpy\n\n\n\nOften you would like to analyze data that you have stored in a text file. Consider, for example, the data file below for an experiment measuring the free fall of a mass.\nData for falling mass experiment\nDate: 16-Aug-2013\nData taken by Frank and Ralf\ndata point  time (sec)  height (mm) uncertainty (mm)\n0       0.0     180     3.5\n1       0.5     182     4.5\n2       1.0     178     4.0\n3       1.5     165     5.5\n4       2.0     160     2.5\n5       2.5     148     3.0\n6       3.0     136     2.5\nSuppose that the name of the text file is MyData.txt. Then we can read the data into four different arrays with the following NumPy statement:\n\ndataPt, time, height, error = np.loadtxt(\"MyData.txt\", skiprows=4 , unpack=True)\n\nIf you don‚Äôt want to read in all the columns of data, you can specify which columns to read in using the usecols key word. For example, the call\n\ntime, height = np.loadtxt(\"MyData.txt\", skiprows=5 , usecols = (1,2), unpack=True)\n\nreads in only columns 1 and 2; columns 0 and 3 are skipped.\n\n\n\nThere are plenty of ways to write data to a data file in Python. We will stick to one very simple one that‚Äôs suitable for writing data files in text format. It uses the NumPy savetxt routine, which is the counterpart of the loadtxt routine introduced in the previous section. The general form of the routine is\nsavetxt(filename, array, fmt=\"%0.18e\", delimiter=\" \", newline=\"\\n\", header=\"\", footer=\"\", comments=\"# \")\nWe illustrate savetext below with a script that first creates four arrays by reading in the data file MyData.txt, as discussed in the previous section, and then writes that same data set to another file MyDataOut.txt.\n\ndataPt, time, height, error = np.loadtxt(\"MyData.txt\", skiprows=5 , unpack=True)\n\n\nlist(zip(dataPt, time, height, error))\n\n[(1.0, 0.5, 182.0, 4.5),\n (2.0, 1.0, 178.0, 4.0),\n (3.0, 1.5, 165.0, 5.5),\n (4.0, 2.0, 160.0, 2.5),\n (5.0, 2.5, 148.0, 3.0),\n (6.0, 3.0, 136.0, 2.5)]\n\n\n\nnp.savetxt('MyDataOut.txt',list(zip(dataPt, time, height, error)), fmt=\"%12.3f\")\n\n\ncat MyDataOut.txt\n\n       1.000        0.500      182.000        4.500\n       2.000        1.000      178.000        4.000\n       3.000        1.500      165.000        5.500\n       4.000        2.000      160.000        2.500\n       5.000        2.500      148.000        3.000\n       6.000        3.000      136.000        2.500\n\n\n\n\n\n\nPandas is a software library written for the Python programming language. It is used for data manipulation and analysis. It provides special data structures and operations for the manipulation of numerical tables and time series and builds on top of numpy.\n\nEasy handling of missing data\nIntelligent label-based slicing, fancy indexing, and subsetting of large data sets\n\nThe data formats provided by the pandas module are used by several other modules, such as the trackpy which is a moduly for feature tracking and analysis in image series.\n\n\n\nimport pandas as pd # import the pandas module\n\nPandas provides two data structures\n\nSeries\nData Frames\n\nA Series is a one-dimensional labeled array capable of holding any data type (integers, strings, floating point numbers, Python objects, etc.). The axis labels are collectively referred to as the index.\n\nmy_simple_series = pd.Series(np.random.randn(7), index=['a', 'b', 'c', 'd', 'e','f','g'])\nmy_simple_series\n\na    1.160711\nb   -0.296427\nc    1.881074\nd   -1.197978\ne    0.280311\nf    1.538339\ng    1.681957\ndtype: float64\n\n\n\nmy_simple_series\n\n-0.2964266043928443\n\n\nThere is a whole lot of functionality built into pandas data types. You may of course also obtain the same functionality using numpy commands, but you may find the pandas abbrevations very useful.\n\nmy_simple_series.agg(['min','max','sum','mean']) # aggregate a number of properties into a single array\n\nmin    -1.523075\nmax     0.525265\nsum    -2.119315\nmean   -0.302759\ndtype: float64\n\n\nA DataFrame is a two-dimensional size-mutable, potentially heterogeneous tabular data structure with labeled axes (rows and columns). The example below shows how such a DataFrame can be generated from the scratch. In addition to the data supplied to the DataFrame method, an index column is generated when creating a DataFrame. As in the case of Series there is a whole lot of functionality integrated into the DataFrame data type which you may explore on the website.\n\ndf = pd.DataFrame()\n\n\ndf = pd.DataFrame(np.random.randint(low=0, high=10, size=(5, 5)),columns=['column 1', 'column 2', 'columns 3', 'column 4', 'column 5'])\ndf.head()\n\n\n\n\n\n\n\n\ncolumn 1\ncolumn 2\ncolumns 3\ncolumn 4\ncolumn 5\n\n\n\n\n0\n3\n9\n3\n7\n4\n\n\n1\n2\n2\n4\n6\n7\n\n\n2\n6\n1\n7\n4\n5\n\n\n3\n3\n4\n4\n3\n0\n\n\n4\n5\n6\n8\n0\n2\n\n\n\n\n\n\n\nDue to the labelling of the columns, each column may be accessed by its column label. Labeling by names improves readability considerably.\n\ndf['column 4']\n\n0    7\n1    6\n2    4\n3    3\n4    0\nName: column 4, dtype: int64\n\n\nIf you don‚Äôt like this format, you can always return to a simple numpy array with the as_matrix() method.\n\ndf.values\n\narray([[3, 9, 3, 7, 4],\n       [2, 2, 4, 6, 7],\n       [6, 1, 7, 4, 5],\n       [3, 4, 4, 3, 0],\n       [5, 6, 8, 0, 2]])\n\n\n\n\n\nDataFrames may also be populated by text files such as comma separated value files (short .csv). These files contain data in text format but also a column label, which can be read by the pandas method read_csv(). You can find an example below, which reads the data from the dust sensor on my balcony from April, 11th. You see the different columns, where P1 and P2 correspond to the PM10 and PM2.5 dust values in \\(\\mu g/m^3\\).\n\ndata = pd.DataFrame()\ndata = pd.read_csv(\"2018-04-11_sds011_sensor_12253.csv\",delimiter=\";\",parse_dates=False)\ndata.head()\n\n\n\n\n\n\n\n\nsensor_id\nsensor_type\nlocation\nlat\nlon\ntimestamp\nP1\ndurP1\nratioP1\nP2\ndurP2\nratioP2\n\n\n\n\n0\n12253\nSDS011\n6189\n52.527\n13.39\n2018-04-11T00:01:58\n25.87\nNaN\nNaN\n19.37\nNaN\nNaN\n\n\n1\n12253\nSDS011\n6189\n52.527\n13.39\n2018-04-11T00:04:24\n25.63\nNaN\nNaN\n20.53\nNaN\nNaN\n\n\n2\n12253\nSDS011\n6189\n52.527\n13.39\n2018-04-11T00:06:55\n26.30\nNaN\nNaN\n22.00\nNaN\nNaN\n\n\n3\n12253\nSDS011\n6189\n52.527\n13.39\n2018-04-11T00:09:23\n24.60\nNaN\nNaN\n20.30\nNaN\nNaN\n\n\n4\n12253\nSDS011\n6189\n52.527\n13.39\n2018-04-11T00:11:51\n25.17\nNaN\nNaN\n20.23\nNaN\nNaN\n\n\n\n\n\n\n\n\n(data['P1']/data['P2']).plot()"
  },
  {
    "objectID": "seminars/seminar01_24/md1.html",
    "href": "seminars/seminar01_24/md1.html",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "",
    "text": "Real molecular dynamics (MD) simulations are complex and computationally expensive but very cool, as they give you a glimpse into the world of atoms and molecules. Here, we will develop a simple MD simulation from scratch in Python. The goal is to understand the basic concepts and algorithms behind MD simulations and get something running which can be extended later but also what we are proud of at the end of the course.\nBefore we can start with implementing a simulation, we need to understand the basic concepts and algorithms behind MD simulations. The following sections will guide you through the development of a simple MD simulation."
  },
  {
    "objectID": "seminars/seminar01_24/md1.html#molecular-dynamics-simulations",
    "href": "seminars/seminar01_24/md1.html#molecular-dynamics-simulations",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "",
    "text": "Real molecular dynamics (MD) simulations are complex and computationally expensive but very cool, as they give you a glimpse into the world of atoms and molecules. Here, we will develop a simple MD simulation from scratch in Python. The goal is to understand the basic concepts and algorithms behind MD simulations and get something running which can be extended later but also what we are proud of at the end of the course.\nBefore we can start with implementing a simulation, we need to understand the basic concepts and algorithms behind MD simulations. The following sections will guide you through the development of a simple MD simulation."
  },
  {
    "objectID": "seminars/seminar01_24/md1.html#basic-physical-concepts",
    "href": "seminars/seminar01_24/md1.html#basic-physical-concepts",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "Basic Physical Concepts",
    "text": "Basic Physical Concepts\n\nNewton‚Äôs Equations of Motion\nThe motion of particles in a molecular dynamics simulation is governed by Newton‚Äôs equations of motion:\n\\[m_i \\frac{d^2\\vec{r}_i}{dt^2} = \\vec{F}_i\\]\nwhere:\n\n\\(m_i\\) is the mass of particle \\(i\\)\n\\(\\vec{r}_i\\) is the position of particle \\(i\\)\n\\(\\vec{F}_i\\) is the force acting on particle \\(i\\)\n\nThe force acting on a particle is the sum of all forces acting on it:\n\\[\\vec{F}_i = \\sum_{j \\neq i} \\vec{F}_{ij}\\]\nwhere \\(\\vec{F}_{ij}\\) is the force acting on particle \\(i\\) due to particle \\(j\\).\n\n\nPotential Energy Functions and Forces\nThe force \\(\\vec{F}_{ij}\\) is usually derived from a potential energy function and may result from a variety of interactions, such as:\n\nBonded interactions\n\nbond stretching \nbond angle bending \ntorsional interactions \n\nNon-bonded interactions\n\nelectrostatic interactions\nvan der Waals interactions\n\nExternal forces\n\nWe will implement some of them but not all of them.\n\nLennard-Jones Potential\nThe most common potential energy function used in MD simulations is the Lennard-Jones potential. It is belonging to the class of non-bonded interactions. The force and the potential energy of the Lennard-Jones potential are given by:\n\\[V_{LJ}(r) = 4\\epsilon \\left[\\left(\\frac{\\sigma}{r}\\right)^{12} - \\left(\\frac{\\sigma}{r}\\right)^6\\right]\\]\nand\n\\[F_{LJ}(r) = -\\frac{dV_{LJ}}{dr} = 24\\epsilon \\left[2\\left(\\frac{\\sigma}{r}\\right)^{12} - \\left(\\frac{\\sigma}{r}\\right)^6\\right]\\frac{\\vec{r}}{r^2}\\]\nwhere:\n\n\\(\\epsilon\\) is the depth of the potential well\n\\(\\sigma\\) is the distance at which the potential is zero\n\\(r\\) is the distance between particles\n\nThe Lenard Jones potential is good for describing the interaction of non-bonded atoms in a molecular system e.g.¬†in a gas or a liquid and is therefore well suited if we first want to simulate a gas or a liquid.\n\n\nCode\ndef lennard_jones(r, epsilon=1, sigma=1):\n    return 4 * epsilon * ((sigma/r)**12 - (sigma/r)**6)\n\nr = np.linspace(0.8, 3, 1000)\nV = lennard_jones(r)\n\nplt.figure(figsize=get_size(8, 6),dpi=150)\nplt.plot(r, V, 'b-', linewidth=2)\nplt.grid(True)\nplt.xlabel('r/œÉ')\nplt.ylabel('V/Œµ')\nplt.title('Lennard-Jones Potential')\nplt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\nplt.ylim(-1.5, 3)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe figure above shows the Lenard-Jones potential as a function of the distance between particles. The potential energy is zero at the equilibrium distance \\(r = \\sigma\\) and has a minimum at \\(r = 2^{1/6}\\sigma\\). The potential energy is positive for \\(r &lt; \\sigma\\) and negative for \\(r &gt; \\sigma\\).\n\n\n\n\n\n\nValues for atomic hydrogen\n\n\n\nFor atomic hydrogen (H), typical Lennard-Jones parameters are:\n\n\\(\\sigma \\approx 2.38\\) √Ö = \\(2.38 \\times 10^{-10}\\) meters\n\\(\\epsilon \\approx 0.0167\\) kcal/mol = \\(1.16 \\times 10^{-21}\\) joules\n\n\n\nLater, if we manage to advance to some more complicated systems, we may want to introduce:\n\nforce in bonds between two atoms\nforce in bond angles between three atoms\nforce in dihedral angles between four atoms\n\nBut for now, we will stick to the Lennard-Jones potential."
  },
  {
    "objectID": "seminars/seminar01_24/md1.html#integrating-newtons-euqation-of-motion",
    "href": "seminars/seminar01_24/md1.html#integrating-newtons-euqation-of-motion",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "Integrating Newtons Euqation of Motion",
    "text": "Integrating Newtons Euqation of Motion\nWhen we have the forces on a particle we have in principle its acceleration. To get the velocity and the position of the particle we need to integrate the equations of motion. There are several methods to do this, but we will start with the simplest one, the Euler method.\n\nEuler Method\nTo obtain this one first needs to know about the Taylor expansion of a function in general. The Taylor expansion of a function \\(f(x)\\) around a point \\(x_0\\) is providing an approximation of the function in the vicinity of \\(x_0\\). It is given by:\n\\[f(x) = f(x_0) + f'(x_0)(x - x_0) + \\frac{1}{2}f''(x_0)(x - x_0)^2 + \\cdots\\]\nwhere \\(f'(x_0)\\) is the first derivative of \\(f(x)\\) at \\(x_0\\), \\(f''(x_0)\\) is the second derivative of \\(f(x)\\) at \\(x_0\\), and so on. We can demonstrate that by expanding a sine function around \\(x_0 = 0\\):\n\\[\\sin(x) = \\sin(0) + \\cos(0)x - \\frac{1}{2}\\sin(0)x^2 + \\cdots = x - \\frac{1}{6}x^3 + \\cdots\\]\nPlotting this yields:\n\n\nCode\nx = np.linspace(-2*np.pi, 2*np.pi, 1000)\ny = np.sin(x)\ny_taylor = x - 1/6*x**3\n\nplt.figure(figsize=get_size(8, 6),dpi=150)\nplt.plot(x, y, 'b-', label='sin(x)', linewidth=2)\nplt.plot(x, y_taylor, 'r--', label='Taylor expansion', linewidth=2)\nplt.grid(True)\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.xlim(-2,2)\nplt.ylim(-2,2)\nplt.title('Taylor Expansion of sin(x)')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe expansion is therefore a good approximation in a region close to \\(x_0\\).\n\n\nVelocity Verlet Algorithm\nThe velocity Verlet algorithm is a second-order algorithm that is more accurate than the Euler method. It can be derived from the Taylor expansion of the position and velocity vectors\n\\[\\mathbf{r}(t + \\Delta t) = \\mathbf{r}(t) + \\mathbf{v}(t)\\Delta t + \\frac{1}{2}\\frac{\\mathbf{F}(t)}{m}\\Delta t^2+ O(\\Delta t^3)\\]\nThe higher order terms in the Taylor expansion are neglected, which results in an error of order \\(\\Delta t^3\\). As compared to that the Euler method is obtained by neglecting the higher order terms in the Taylor expansion of the velocity vector:\n\\[\\mathbf{v}(t + \\Delta t) = \\mathbf{v}(t) + \\frac{\\mathbf{F}(t)}{m}\\Delta t + O(\\Delta t^2)\\]\nand is therefore only first order accurate with an error of order \\(\\Delta t^2\\).\nThe velocity Verlet algorithm consists of three steps:\n\nUpdate positions: \\(\\mathbf{r}(t + \\Delta t) = \\mathbf{r}(t) + \\mathbf{v}(t)\\Delta t + \\frac{1}{2}\\frac{\\mathbf{F}(t)}{m}\\Delta t^2\\)\nCalculate new forces: \\(\\mathbf{F}(t + \\Delta t) = \\mathbf{F}(\\mathbf{r}(t + \\Delta t))\\)\nUpdate velocities: \\(\\mathbf{v}(t + \\Delta t) = \\mathbf{v}(t) + \\frac{1}{2}\\frac{\\mathbf{F}(t) + \\mathbf{F}(t + \\Delta t)}{m}\\Delta t\\)\n\nwhere: - \\(\\mathbf{r}\\) is the position vector - \\(\\mathbf{v}\\) is the velocity vector - \\(\\mathbf{F}\\) is the force vector - \\(m\\) is the mass - \\(\\Delta t\\) is the timestep\n\n\nSimple Integration Example: Free Fall\nLet‚Äôs start and try to integrate the equation of motion for a particle in free fall with the help of the Velocity Verlet algorithm. The only force acting on the particle is gravity. The equation of motion is:\nNewton‚Äôs equation of motion: \\(\\mathbf{F} = m\\mathbf{a}\\)\nFor gravity: \\(\\mathbf{F} = -mg\\hat{\\mathbf{y}}\\)\nTherefore: \\(\\ddot{y} = -g\\)\nThe analytical solution is:\n\nPosition: \\(y(t) = y_0 + v_0t - \\frac{1}{2}gt^2\\)\nVelocity: \\(v(t) = v_0 - gt\\)\n\n\n\nCode\n# Parameters\n\ng = 9.81  # m/s^2\ndt = 0.01  # time step\nt_max = 2.0  # total simulation time\nsteps = int(t_max/dt)\n\n# Initial conditions\ny0 = 20.0  # initial height\nv0 = 0.0   # initial velocity\n\n\n# Arrays to store results\nt = np.zeros(steps)\ny = np.zeros(steps)\nv = np.zeros(steps)\na = np.zeros(steps)\n\n# Initial values\ny[0] = y0\nv[0] = v0\na[0] = -g\n\n# Velocity Verlet integration\nfor i in range(1, steps):\n    t[i] = i * dt\n    y[i] = y[i-1] + v[i-1] * dt + 0.5 * a[i-1] * dt**2  # update position\n    a_new = -g                                          # new acceleration (assuming constant gravity)\n    v[i] = v[i-1] + 0.5 * (a[i-1] + a_new) * dt         # update velocity\n    a[i] = a_new                                        # store new acceleration\n\ny_analytical = y0 + v0*t - 0.5*g*t**2\nplt.figure(figsize=get_size(8, 6), dpi=150)\nplt.plot(t, y)\nplt.plot(t, y_analytical, 'r--')\n\nplt.xlabel('Time (s)')\nplt.ylabel('Height (m)')\nplt.title('Free Fall Motion')\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "seminars/seminar01_24/mdX.html",
    "href": "seminars/seminar01_24/mdX.html",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "The Lennard-Jones potential describes the interaction between two atoms:\n\\[V_{LJ}(r) = 4\\epsilon \\left[\\left(\\frac{\\sigma}{r}\\right)^{12} - \\left(\\frac{\\sigma}{r}\\right)^6\\right]\\]\nThe corresponding force:\n\\[F_{LJ}(r) = -\\frac{dV_{LJ}}{dr} = 24\\epsilon \\left[2\\left(\\frac{\\sigma}{r}\\right)^{12} - \\left(\\frac{\\sigma}{r}\\right)^6\\right]\\frac{\\vec{r}}{r^2}\\]\nwhere: - \\(\\epsilon\\) is the depth of the potential well - \\(\\sigma\\) is the distance at which the potential is zero - \\(r\\) is the distance between particles\n\n\n\nimport numpy as np\n\ndef lennard_jones_force(pos1, pos2, epsilon=1.0, sigma=1.0):\n    r_vec = pos2 - pos1\n    r = np.linalg.norm(r_vec)\n\n    # Force magnitude\n    force_mag = 24 * epsilon * (2 * (sigma/r)**12 - (sigma/r)**6) / r\n\n    # Force vector\n    force_vec = force_mag * r_vec / r\n\n    return force_vec"
  },
  {
    "objectID": "seminars/seminar01_24/mdX.html#simple-atomic-system-with-lennard-jones-potential",
    "href": "seminars/seminar01_24/mdX.html#simple-atomic-system-with-lennard-jones-potential",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "The Lennard-Jones potential describes the interaction between two atoms:\n\\[V_{LJ}(r) = 4\\epsilon \\left[\\left(\\frac{\\sigma}{r}\\right)^{12} - \\left(\\frac{\\sigma}{r}\\right)^6\\right]\\]\nThe corresponding force:\n\\[F_{LJ}(r) = -\\frac{dV_{LJ}}{dr} = 24\\epsilon \\left[2\\left(\\frac{\\sigma}{r}\\right)^{12} - \\left(\\frac{\\sigma}{r}\\right)^6\\right]\\frac{\\vec{r}}{r^2}\\]\nwhere: - \\(\\epsilon\\) is the depth of the potential well - \\(\\sigma\\) is the distance at which the potential is zero - \\(r\\) is the distance between particles\n\n\n\nimport numpy as np\n\ndef lennard_jones_force(pos1, pos2, epsilon=1.0, sigma=1.0):\n    r_vec = pos2 - pos1\n    r = np.linalg.norm(r_vec)\n\n    # Force magnitude\n    force_mag = 24 * epsilon * (2 * (sigma/r)**12 - (sigma/r)**6) / r\n\n    # Force vector\n    force_vec = force_mag * r_vec / r\n\n    return force_vec"
  },
  {
    "objectID": "seminars/seminar01_24/mdX.html#boundary-conditions",
    "href": "seminars/seminar01_24/mdX.html#boundary-conditions",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "2. Boundary Conditions",
    "text": "2. Boundary Conditions\n\n2.1 Periodic Boundary Conditions (PBC)\nParticles that exit one side of the box re-enter from the opposite side.\ndef apply_periodic_bc(positions, box_length):\n    return positions - box_length * np.floor(positions/box_length)\n\ndef minimum_image_distance(pos1, pos2, box_length):\n    dr = pos2 - pos1\n    dr = dr - box_length * np.round(dr/box_length)\n    return dr\n\n\n2.2 Reflective Boundaries\nParticles bounce off the walls:\ndef apply_reflective_bc(positions, velocities, box_length):\n    for i in range(len(positions)):\n        for dim in range(3):\n            if positions[i,dim] &lt; 0:\n                positions[i,dim] = -positions[i,dim]\n                velocities[i,dim] = -velocities[i,dim]\n            elif positions[i,dim] &gt; box_length:\n                positions[i,dim] = 2*box_length - positions[i,dim]\n                velocities[i,dim] = -velocities[i,dim]\n    return positions, velocities"
  },
  {
    "objectID": "seminars/seminar01_24/mdX.html#basic-simulation-loop",
    "href": "seminars/seminar01_24/mdX.html#basic-simulation-loop",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "3. Basic Simulation Loop",
    "text": "3. Basic Simulation Loop\n\n3.1 Initial Implementation with Just LJ Forces\nclass MDSimulation:\n    def __init__(self, positions, velocities, mass, box_length, dt):\n        self.positions = positions\n        self.velocities = velocities\n        self.mass = mass\n        self.box_length = box_length\n        self.dt = dt\n\n    def calculate_forces(self):\n        n_particles = len(self.positions)\n        forces = np.zeros_like(self.positions)\n\n        for i in range(n_particles):\n            for j in range(i+1, n_particles):\n                r_ij = minimum_image_distance(\n                    self.positions[i],\n                    self.positions[j],\n                    self.box_length\n                )\n                f_ij = lennard_jones_force(np.zeros(3), r_ij)\n                forces[i] += f_ij\n                forces[j] -= f_ij  # Newton's third law\n\n        return forces\n\n    def velocity_verlet_step(self):\n        # Calculate initial forces\n        forces = self.calculate_forces()\n\n        # Update positions\n        self.positions += self.velocities * self.dt + \\\n                         0.5 * forces / self.mass * self.dt**2\n\n        # Apply boundary conditions\n        self.positions = apply_periodic_bc(self.positions, self.box_length)\n\n        # Calculate new forces\n        new_forces = self.calculate_forces()\n\n        # Update velocities\n        self.velocities += 0.5 * (forces + new_forces) / self.mass * self.dt"
  },
  {
    "objectID": "seminars/seminar01_24/mdX.html#adding-molecular-forces",
    "href": "seminars/seminar01_24/mdX.html#adding-molecular-forces",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "4. Adding Molecular Forces",
    "text": "4. Adding Molecular Forces\n\n4.1 Bond Forces\nAdding harmonic bond potential:\n\\[V_{bond}(r) = \\frac{k}{2}(r - r_0)^2\\]\ndef bond_force(pos1, pos2, k_bond, r0):\n    r_vec = pos2 - pos1\n    r = np.linalg.norm(r_vec)\n    force_mag = -k_bond * (r - r0)\n    return force_mag * r_vec / r\n\n\n4.2 Angle Forces\nThree-body angle potential:\n\\[V_{angle}(\\theta) = \\frac{k_\\theta}{2}(\\theta - \\theta_0)^2\\]\ndef angle_force(pos1, pos2, pos3, k_angle, theta0):\n    # Calculate vectors\n    v1 = pos1 - pos2\n    v2 = pos3 - pos2\n\n    # Calculate angle\n    cos_theta = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n    theta = np.arccos(np.clip(cos_theta, -1.0, 1.0))\n\n    # Calculate forces (simplified version)\n    force_magnitude = -k_angle * (theta - theta0)\n\n    return force_magnitude * v1, -force_magnitude * v2\n\n\n4.3 Enhanced Simulation Class\nclass MolecularMDSimulation(MDSimulation):\n    def __init__(self, *args, bonds=None, angles=None):\n        super().__init__(*args)\n        self.bonds = bonds or []  # [(i, j, k, r0), ...]\n        self.angles = angles or []  # [(i, j, k, k_angle, theta0), ...]\n\n    def calculate_forces(self):\n        # Start with non-bonded forces\n        forces = super().calculate_forces()\n\n        # Add bond forces\n        for bond in self.bonds:\n            i, j, k_bond, r0 = bond\n            f_ij = bond_force(\n                self.positions[i],\n                self.positions[j],\n                k_bond, r0\n            )\n            forces[i] += f_ij\n            forces[j] -= f_ij\n\n        # Add angle forces\n        for angle in self.angles:\n            i, j, k, k_angle, theta0 = angle\n            f_i, f_k = angle_force(\n                self.positions[i],\n                self.positions[j],\n                self.positions[k],\n                k_angle, theta0\n            )\n            forces[i] += f_i\n            forces[k] += f_k\n            forces[j] -= (f_i + f_k)\n\n        return forces"
  },
  {
    "objectID": "seminars/seminar01_24/mdX.html#example-usage",
    "href": "seminars/seminar01_24/mdX.html#example-usage",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "5. Example Usage",
    "text": "5. Example Usage\n# Initialize system\nn_particles = 100\nbox_length = 10.0\npositions = np.random.rand(n_particles, 3) * box_length\nvelocities = np.zeros((n_particles, 3))\nmass = 1.0\ndt = 0.001\n\n# Create simulation\nsim = MolecularMDSimulation(\n    positions, velocities, mass, box_length, dt,\n    bonds=[(0, 1, 1000.0, 1.0)],  # Example bond\n    angles=[(0, 1, 2, 100.0, np.pi)]  # Example angle\n)\n\n# Run simulation\nn_steps = 1000\nfor step in range(n_steps):\n    sim.velocity_verlet_step()\nHere‚Äôs a suggested basic structure:\nclass Atom:\n    def __init__(self, atom_id, atom_type, position, velocity=None, mass=None):\n        self.id = atom_id          # unique identifier\n        self.type = atom_type      # e.g., 'H', 'C', 'O'\n        self.position = position   # numpy array [x, y, z]\n        self.velocity = velocity if velocity is not None else np.zeros(3)\n        self.mass = mass\n        self.force = np.zeros(3)   # current force on atom\n\n        # Optional attributes that might be useful later:\n        self.bonded_atoms = []     # list of atoms this atom is bonded to\n        self.charges = 0.0         # for electrostatic interactions\nThen you can later create classes for: 1. Bond (connects two Atom objects)\nclass Bond:\n    def __init__(self, atom1, atom2, k, r0):\n        self.atom1 = atom1\n        self.atom2 = atom2\n        self.k = k      # force constant\n        self.r0 = r0    # equilibrium distance\n\nAngle (three Atom objects)\n\nclass Angle:\n    def __init__(self, atom1, atom2, atom3, k, theta0):\n        self.atoms = [atom1, atom2, atom3]\n        self.k = k          # force constant\n        self.theta0 = theta0  # equilibrium angle\n\nDihedral (four Atom objects)\n\nThis modular approach makes it easier to: - Add features incrementally - Debug each interaction type separately - Keep track of connectivity - Calculate forces systematically\nclass Atom:\n    def __init__(self, atom_id, atom_type, position, velocity=None, mass=None):\n        self.id = atom_id\n        self.type = atom_type\n        self.position = position\n        self.velocity = velocity if velocity is not None else np.zeros(3)\n        self.mass = mass\n        self.force = np.zeros(3)\n\n    def add_force(self, force):\n        \"\"\"Add force contribution to total force on atom\"\"\"\n        self.force += force\n\n    def reset_force(self):\n        \"\"\"Reset force to zero at start of each step\"\"\"\n        self.force = np.zeros(3)\n\n    def update_position(self, dt):\n        \"\"\"First step of velocity Verlet: update position\"\"\"\n        self.position += self.velocity * dt + 0.5 * (self.force/self.mass) * dt**2\n\n    def update_velocity(self, dt, new_force):\n        \"\"\"Second step of velocity Verlet: update velocity using average force\"\"\"\n        self.velocity += 0.5 * (new_force + self.force)/self.mass * dt\n        self.force = new_force\n\nclass ForceField:\n    def __init__(self, sigma, epsilon):\n        self.sigma = sigma\n        self.epsilon = epsilon\n\n    def calculate_lj_force(self, atom1, atom2):\n        \"\"\"Calculate LJ force between two atoms\"\"\"\n        r = atom1.position - atom2.position\n        r_mag = np.linalg.norm(r)\n        # LJ force calculation\n        force = 24 * self.epsilon * (2 * (self.sigma/r_mag)**12\n                                   - (self.sigma/r_mag)**6) * r/r_mag**2\n        return force\n\nclass MDSimulation:\n    def __init__(self, atoms, forcefield):\n        self.atoms = atoms\n        self.forcefield = forcefield\n\n    def calculate_forces(self):\n        # Reset all forces\n        for atom in self.atoms:\n            atom.reset_force()\n\n        # Calculate forces between all pairs\n        for i, atom1 in enumerate(self.atoms):\n            for atom2 in self.atoms[i+1:]:\n                force = self.forcefield.calculate_lj_force(atom1, atom2)\n                atom1.add_force(force)\n                atom2.add_force(-force)  # Newton's third law\n\nNew lecture\nclass Atom: def init(self, atom_id, atom_type, position, velocity=None, mass=None): self.id = atom_id self.type = atom_type # e.g., ‚ÄòA‚Äô, ‚ÄòB‚Äô, etc. self.position = position self.velocity = velocity if velocity is not None else np.zeros(2) self.mass = mass self.force = np.zeros(2)\nclass ForceField: def init(self): # Dictionary to store interaction parameters between atom types self.pair_parameters = {}\ndef add_pair_parameters(self, type1, type2, epsilon, sigma):\n    \"\"\"Add interaction parameters for a pair of atom types\"\"\"\n    # Store parameters symmetrically\n    key = tuple(sorted([type1, type2]))\n    self.pair_parameters[key] = {'epsilon': epsilon, 'sigma': sigma}\n\ndef get_pair_parameters(self, type1, type2):\n    \"\"\"Get interaction parameters for a pair of atom types\"\"\"\n    key = tuple(sorted([type1, type2]))\n    if key not in self.pair_parameters:\n        raise ValueError(f\"No parameters defined for atom types {type1} and {type2}\")\n    return self.pair_parameters[key]\n\ndef calculate_lj_force(self, atom1, atom2, r_vec, r_mag):\n    \"\"\"Calculate LJ force with type-specific parameters\"\"\"\n    params = self.get_pair_parameters(atom1.type, atom2.type)\n    epsilon = params['epsilon']\n    sigma = params['sigma']\n\n    force_mag = 24.0 * epsilon * (2.0 * (sigma/r_mag)**13\n                                - (sigma/r_mag)**7)\n    return force_mag * r_vec / r_mag\n\n\n\n\n    def setup_simulation():\n        # Create force field and add parameters\n        ff = ForceField()\n\n        # Add parameters for different combinations\n        ff.add_pair_parameters('A', 'A', epsilon=1.0, sigma=1.0)  # A-A interaction\n        ff.add_pair_parameters('B', 'B', epsilon=0.5, sigma=1.2)  # B-B interaction\n        ff.add_pair_parameters('A', 'B', epsilon=0.7, sigma=1.1)  # A-B interaction\n\n        # Create atoms of different types\n        atoms = [\n            Atom(0, 'A', np.array([1.0, 1.0]), mass=1.0),\n            Atom(1, 'A', np.array([2.0, 2.0]), mass=1.0),\n            Atom(2, 'B', np.array([3.0, 3.0]), mass=1.5),\n            Atom(3, 'B', np.array([4.0, 4.0]), mass=1.5)\n        ]\n\n        return ff, atoms"
  },
  {
    "objectID": "seminars/seminar01_24/01-seminar01.html",
    "href": "seminars/seminar01_24/01-seminar01.html",
    "title": "Seminar 1",
    "section": "",
    "text": "Calculate the average of all of the integers from 1 to 10.\n\n\n\n\n\n\n\nuse the sum() and len() functions\nn = range(1, 11)\n\n\n1n = range(1, 11)\n2sum(n)/len(n)\n\n1\n\ncreate the range of numbers from 1 to 10.\n\n2\n\ncalculate the sum of the numbers and divide by the number of elements in the list."
  },
  {
    "objectID": "seminars/seminar03/md3.html",
    "href": "seminars/seminar03/md3.html",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "",
    "text": "We define a class Atom that contains the properties of an atom. The class Atom has the following attributes:\nclass Atom:\n    dimension = 2\n\n    def __init__(self, atom_id, atom_type, position, velocity=None, mass=None):\n        self.id = atom_id\n        self.type = atom_type\n        self.position = position\n        self.velocity = velocity if velocity is not None else np.zeros(dimension)\n        self.mass = mass\n        self.force = np.zeros(dimension)\nThe class Atom has the following attributes:\n\nid: The unique identifier of the atom\ntype: The type of the atom (hydrogen or oxygen or ‚Ä¶)\nposition: The position of the atom in 3D space (x, y, z)\nvelocity: The velocity of the atom in 3D space (vx, vy, vz)\nmass: The mass of the atom\nforce: The force acting on the atom in 3D space (fx, fy, fz)\n\nIn addition, we will need some information on the other atoms that are bound to the atom. We will store this information later in a list of atoms called boundto. Since we start with a monoatomic gas, we will not need this information for now. Note that position, velocity, and force are 3D vectors and we store them in numpy arrays. This is a very convenient way to handle vectors and matrices in Python.\nThe class Atom should further implement a number of functions, called methods in object-oriented programming, that allow us to interact with the atom. The following methods are implemented in the Atom class:\n\n\ndef add_force(self, force):\n    \"\"\"Add force contribution to total force on atom\"\"\"\n    self.force += force\n\n\n\ndef reset_force(self):\n    \"\"\"Reset force to zero at start of each step\"\"\"\n    self.force = np.zeros(dimension)\n\n\n\ndef update_position(self, dt):\n    \"\"\"First step of velocity Verlet: update position\"\"\"\n    self.position += self.velocity * dt + 0.5 * (self.force/self.mass) * dt**2\n\n\n\ndef update_velocity(self, dt, new_force):\n    \"\"\"Second step of velocity Verlet: update velocity using average force\"\"\"\n    self.velocity += 0.5 * (new_force + self.force)/self.mass * dt\n    self.force = new_force\n\n\n\ndef apply_periodic_boundaries(self, box_size):\n        \"\"\"Apply periodic boundary conditions\"\"\"\n        self.position = self.position % box_size\n\n\n\n\n\n\nComplete Atom class\n\n\n\n\n\nclass Atom:\n    def __init__(self, atom_id, atom_type, position, velocity=None, mass=None):\n        self.id = atom_id\n        self.type = atom_type\n        self.position = position\n        self.velocity = velocity if velocity is not None else np.random.randn(2)*20\n        self.mass = mass\n        self.force = np.zeros(2)\n\n\n    def add_force(self, force):\n        \"\"\"Add force contribution to total force on atom\"\"\"\n        self.force += force\n\n    def reset_force(self):\n        \"\"\"Reset force to zero at start of each step\"\"\"\n        self.force = np.zeros(2)\n\n    def update_position(self, dt):\n        \"\"\"First step of velocity Verlet: update position\"\"\"\n        self.position += self.velocity * dt + 0.5 * (self.force/self.mass) * dt**2\n\n    def update_velocity(self, dt, new_force):\n        \"\"\"Second step of velocity Verlet: update velocity using average force\"\"\"\n        self.velocity += 0.5 * (new_force + self.force)/self.mass * dt\n        self.force = new_force\n\n    def apply_periodic_boundaries(self, box_size):\n            \"\"\"Apply periodic boundary conditions\"\"\"\n            self.position = self.position % box_size\n\n\n\nThis would be a good time to do something simple with the atom class. Let‚Äôs create a bunch of atoms and plot them in a 2D space."
  },
  {
    "objectID": "seminars/seminar03/md3.html#implementations",
    "href": "seminars/seminar03/md3.html#implementations",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "",
    "text": "We define a class Atom that contains the properties of an atom. The class Atom has the following attributes:\nclass Atom:\n    dimension = 2\n\n    def __init__(self, atom_id, atom_type, position, velocity=None, mass=None):\n        self.id = atom_id\n        self.type = atom_type\n        self.position = position\n        self.velocity = velocity if velocity is not None else np.zeros(dimension)\n        self.mass = mass\n        self.force = np.zeros(dimension)\nThe class Atom has the following attributes:\n\nid: The unique identifier of the atom\ntype: The type of the atom (hydrogen or oxygen or ‚Ä¶)\nposition: The position of the atom in 3D space (x, y, z)\nvelocity: The velocity of the atom in 3D space (vx, vy, vz)\nmass: The mass of the atom\nforce: The force acting on the atom in 3D space (fx, fy, fz)\n\nIn addition, we will need some information on the other atoms that are bound to the atom. We will store this information later in a list of atoms called boundto. Since we start with a monoatomic gas, we will not need this information for now. Note that position, velocity, and force are 3D vectors and we store them in numpy arrays. This is a very convenient way to handle vectors and matrices in Python.\nThe class Atom should further implement a number of functions, called methods in object-oriented programming, that allow us to interact with the atom. The following methods are implemented in the Atom class:\n\n\ndef add_force(self, force):\n    \"\"\"Add force contribution to total force on atom\"\"\"\n    self.force += force\n\n\n\ndef reset_force(self):\n    \"\"\"Reset force to zero at start of each step\"\"\"\n    self.force = np.zeros(dimension)\n\n\n\ndef update_position(self, dt):\n    \"\"\"First step of velocity Verlet: update position\"\"\"\n    self.position += self.velocity * dt + 0.5 * (self.force/self.mass) * dt**2\n\n\n\ndef update_velocity(self, dt, new_force):\n    \"\"\"Second step of velocity Verlet: update velocity using average force\"\"\"\n    self.velocity += 0.5 * (new_force + self.force)/self.mass * dt\n    self.force = new_force\n\n\n\ndef apply_periodic_boundaries(self, box_size):\n        \"\"\"Apply periodic boundary conditions\"\"\"\n        self.position = self.position % box_size\n\n\n\n\n\n\nComplete Atom class\n\n\n\n\n\nclass Atom:\n    def __init__(self, atom_id, atom_type, position, velocity=None, mass=None):\n        self.id = atom_id\n        self.type = atom_type\n        self.position = position\n        self.velocity = velocity if velocity is not None else np.random.randn(2)*20\n        self.mass = mass\n        self.force = np.zeros(2)\n\n\n    def add_force(self, force):\n        \"\"\"Add force contribution to total force on atom\"\"\"\n        self.force += force\n\n    def reset_force(self):\n        \"\"\"Reset force to zero at start of each step\"\"\"\n        self.force = np.zeros(2)\n\n    def update_position(self, dt):\n        \"\"\"First step of velocity Verlet: update position\"\"\"\n        self.position += self.velocity * dt + 0.5 * (self.force/self.mass) * dt**2\n\n    def update_velocity(self, dt, new_force):\n        \"\"\"Second step of velocity Verlet: update velocity using average force\"\"\"\n        self.velocity += 0.5 * (new_force + self.force)/self.mass * dt\n        self.force = new_force\n\n    def apply_periodic_boundaries(self, box_size):\n            \"\"\"Apply periodic boundary conditions\"\"\"\n            self.position = self.position % box_size\n\n\n\nThis would be a good time to do something simple with the atom class. Let‚Äôs create a bunch of atoms and plot them in a 2D space."
  },
  {
    "objectID": "seminars/Assignment 1/Assignment 1.html",
    "href": "seminars/Assignment 1/Assignment 1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Calculate the mean of integers 1 to 10 (inclusive) using range(), sum(), and len().\nNote: Do NOT print the results.\n\nn=range(1,11)\nmean=sum(n)/len(n)\n\nWrite a Python program that swaps the values of two variables x = 5 and y = 10 without using a third variable.\n\nx = 5\ny = 10\n\nx, y = y, x\n\nWrite a Python program that calculates the area and perimeter of a rectangle with the¬†length = 10 and width = 5.\nNote: Do NOT print the results.\n\nlength = 10\nwidth = 5\n\narea = length * width\nperimeter = 2 * (length + width)\n\n(50, 30)\n\n\nGiven the temperature in Celsius temp1_in_C = 25 and the temperature in Fahrenheit temp2_in_F = 86, write a Python program to convert these temperatures. Use variables temp1_in_F and temp2_in_C to store the converted temperatures using appropriate conversion formulas.\nNote: Do NOT print the results.\n\ntemp1_in_C = 25\ntemp1_in_F = (temp1_in_C * 9/5) + 32\n\ntemp2_in_F = 86\ntemp2_in_C = (temp2_in_F - 32) * 5/9\n\nWrite a Python program that: - Assigns the string ‚ÄúHello, World!‚Äù to a variable my_string. - Performs the following string operations:\nconcatenation: add ‚Äù How are you?‚Äù to my_string and put it in concatenated_string\nslicing: slice my_string from position 7 to 12 and put in sliced_string\nupper case my_string and put it in upper_case_string\nlower case my_string and put it in lower_case_string\n\nmy_string = \"Hello, World!\"\n\nconcatenated_string = my_string + \" How are you?\"\nsliced_string = my_string[7:12]\nupper_case_string = my_string.upper()\nlower_case_string = my_string.lower()"
  },
  {
    "objectID": "seminars/seminar02/MD Simulation.html",
    "href": "seminars/seminar02/MD Simulation.html",
    "title": "Molecular Dynamics Simulations in EMPP 2024",
    "section": "",
    "text": "Before we implement all classes, we will first visualize the particles moving in a 2D box. We will use the matplotlib library to create an animation of the particles moving in the box. We will also implement periodic boundary conditions, so that particles that leave the box on one side re-enter on the opposite side.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nfrom scipy.spatial.distance import cdist\n\nplt.rcParams.update({'font.size': 8,\n                     'lines.linewidth': 1,\n                     'lines.markersize': 10,\n                     'axes.labelsize': 10,\n                     'axes.titlesize': 10,\n                     'xtick.labelsize' : 10,\n                     'ytick.labelsize' : 10,\n                     'xtick.top' : True,\n                     'xtick.direction' : 'in',\n                     'ytick.right' : True,\n                     'ytick.direction' : 'in',})\n\ndef get_size(w,h):\n    return((w/2.54,h/2.54))"
  },
  {
    "objectID": "seminars/seminar02/MD Simulation.html#part-2",
    "href": "seminars/seminar02/MD Simulation.html#part-2",
    "title": "Molecular Dynamics Simulations in EMPP 2024",
    "section": "",
    "text": "Before we implement all classes, we will first visualize the particles moving in a 2D box. We will use the matplotlib library to create an animation of the particles moving in the box. We will also implement periodic boundary conditions, so that particles that leave the box on one side re-enter on the opposite side.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nfrom scipy.spatial.distance import cdist\n\nplt.rcParams.update({'font.size': 8,\n                     'lines.linewidth': 1,\n                     'lines.markersize': 10,\n                     'axes.labelsize': 10,\n                     'axes.titlesize': 10,\n                     'xtick.labelsize' : 10,\n                     'ytick.labelsize' : 10,\n                     'xtick.top' : True,\n                     'xtick.direction' : 'in',\n                     'ytick.right' : True,\n                     'ytick.direction' : 'in',})\n\ndef get_size(w,h):\n    return((w/2.54,h/2.54))"
  },
  {
    "objectID": "seminars/seminar02/MD Simulation.html#create-a-particle-array",
    "href": "seminars/seminar02/MD Simulation.html#create-a-particle-array",
    "title": "Molecular Dynamics Simulations in EMPP 2024",
    "section": "Create a particle array",
    "text": "Create a particle array\nWe will start creating an array of particles in 2D using numpy. Out box will be of size (1,1).\n\n# number of particle per edge\nn_side =20\n\n# particle x and y coordinates\nx = np.linspace(0.05, 0.95, n_side)\ny = np.linspace(0.05, 0.95, n_side)\n\n# meshgrid of them to have points per particle\nxx, yy = np.meshgrid(x, y)\n\n# flatten the 2D array\nparticles = np.vstack([xx.ravel(), yy.ravel()]).T\n\nJust have a look at the array xx."
  },
  {
    "objectID": "seminars/seminar02/MD Simulation.html#create-particle-velocities",
    "href": "seminars/seminar02/MD Simulation.html#create-particle-velocities",
    "title": "Molecular Dynamics Simulations in EMPP 2024",
    "section": "Create particle velocities",
    "text": "Create particle velocities\n\nvelocities = np.random.normal(scale=0.005, size=(n_side**2, 2))\n\n\nvelocities.shape\n\n(400, 2)\n\n\n\nplt.figure(figsize=(5,5))\nplt.hist(velocities[:,1],density=True,bins=50)\nplt.xlabel(r\"$v_x$\")\nplt.xlabel(r\"$p(v_x)$\")\nplt.show()"
  },
  {
    "objectID": "seminars/seminar02/MD Simulation.html#do-one-step-by-hand-and-apply-the-boundary-conditions",
    "href": "seminars/seminar02/MD Simulation.html#do-one-step-by-hand-and-apply-the-boundary-conditions",
    "title": "Molecular Dynamics Simulations in EMPP 2024",
    "section": "Do one step by hand and apply the boundary conditions",
    "text": "Do one step by hand and apply the boundary conditions\nWe choose perdiodic boundary conditions. To apply them, the modulo operator % is a good choice.\n\n\n# Update particle positions based on their velocities\nparticles += velocities\n\n# Apply periodic boundary conditions in x direction (wrap around at 0 and 1)\nparticles[:, 0] = particles[:, 0] % 1\nparticles[:, 1] = particles[:, 1] % 1\n\n\n6 % 2\n\n0\n\n\nWe also need to handle collisions. This requires to calculate all pairwaise distances between all atoms. This could be quite time-consuming.\n\n\n# Calculate distances between all pairs of particles\ndistances = cdist(particles, particles)\n\n# Calculate collisions using the upper triangle of the distance matrix\n# distances &lt; 2*radius gives a boolean matrix where True means collision\n# np.triu takes only the upper triangle to avoid counting collisions twice\ncollisions = np.triu(distances &lt; 2*radius , 1)\n\n\ncollisions\n\narray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       ...,\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])\n\n\nThen we need to carry out the collision. We simplify everything assuming a central collision of masses of the same size. This allows us to swap the velocities and to remove the overlap.\n\n# Handle collisions between particles\nfor i, j in zip(*np.nonzero(collisions)):\n    # Exchange velocities between colliding particles (elastic collision)\n    velocities[i], velocities[j] = velocities[j], velocities[i].copy()\n\n    # Calculate how much particles overlap\n    overlap = 2*radius - distances[i, j]\n\n    # Calculate unit vector pointing from j to i\n    direction = particles[i] - particles[j]\n    direction /= np.linalg.norm(direction)\n\n    # Move particles apart to prevent overlap\n    particles[i] += 0.5 * overlap * direction\n    particles[j] -= 0.5 * overlap * direction\n\nThis is now carrying out the simulation loop all together handles the drawing. This is the key in this loop.\nClear the canvas for drawing\nclear_output(wait=True)\nDraw the particles\nax.scatter(particles[:, 0], particles[:, 1], s=100, edgecolors='r', facecolors='none')\nDisplay the figure\ndisplay(fig)\nplt.pause(0.01)\nax.clear()\n\nradius = 0.0177\nfig, ax = plt.subplots(figsize=(6,6))\n\nn_steps = 200\n\nfor _ in range(n_steps):\n    clear_output(wait=True)\n\n    # Update particle positions based on their velocities\n    particles += velocities\n    # Apply periodic boundary conditions in x direction (wrap around at 0 and 1)\n    particles[:, 0] = particles[:, 0] % 1\n    # Apply periodic boundary conditions in y direction (wrap around at 0 and 1)\n    particles[:, 1] = particles[:, 1] % 1\n    # Calculate distances between all pairs of particles\n    distances = cdist(particles, particles)\n\n    # Calculate collisions using the upper triangle of the distance matrix\n    # distances &lt; 2*radius gives a boolean matrix where True means collision\n    # np.triu takes only the upper triangle to avoid counting collisions twice\n    collisions = np.triu(distances &lt; 2*radius, 1)\n\n    # Handle collisions between particles\n    for i, j in zip(*np.nonzero(collisions)):\n        # Exchange velocities between colliding particles (elastic collision)\n        velocities[i], velocities[j] = velocities[j], velocities[i].copy()\n\n        # Calculate how much particles overlap\n        overlap = 2*radius - distances[i, j]\n\n        # Calculate unit vector pointing from j to i\n        direction = particles[i] - particles[j]\n        direction /= np.linalg.norm(direction)\n\n        # Move particles apart to prevent overlap\n        particles[i] += 0.5 * overlap * direction\n        particles[j] -= 0.5 * overlap * direction\n\n    ax.scatter(particles[:, 0], particles[:, 1], s=100, edgecolors='r', facecolors='none')\n\n\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.axis(\"off\")\n\n    display(fig)\n    plt.pause(0.01)\n    ax.clear()"
  },
  {
    "objectID": "seminars/seminar05/mdsim copy.html",
    "href": "seminars/seminar05/mdsim copy.html",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "",
    "text": "# %% Load  modules and initialize\nfrom typing_extensions import ParamSpec\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nimport matplotlib.patches as patches\nplt.rcParams.update({'font.size': 8,\n                     'lines.linewidth': 1,\n                     'lines.markersize': 10,\n                     'axes.labelsize': 10,\n                     'axes.titlesize': 10,\n                     'xtick.labelsize' : 10,\n                     'ytick.labelsize' : 10,\n                     'xtick.top' : True,\n                     'xtick.direction' : 'in',\n                     'ytick.right' : True,\n                     'ytick.direction' : 'in',\n                     'figure.facecolor' : 'white',})\n\ndef get_size(w,h):\n    return((w/2.54,h/2.54))\n\n\n# %% Load the atom class we did already in the previous seminar\nclass Atom:\n    def __init__(self, atom_id, atom_type, position, velocity=None, mass=None):\n        self.id = atom_id\n        self.type = atom_type\n        self.position = position\n        self.velocity = velocity if velocity is not None else np.random.randn(2)*20\n        self.mass = mass\n        self.force = np.zeros(2)\n\n\n    def add_force(self, force):\n        \"\"\"Add force contribution to total force on atom\"\"\"\n        self.force += force\n\n    def reset_force(self):\n        \"\"\"Reset force to zero at start of each step\"\"\"\n        self.force = np.zeros(2)\n\n    def update_position(self, dt):\n        \"\"\"First step of velocity Verlet: update position\"\"\"\n        self.position += self.velocity * dt + 0.5 * (self.force/self.mass) * dt**2\n\n    def update_velocity(self, dt, new_force):\n        \"\"\"Second step of velocity Verlet: update velocity using average force\"\"\"\n        self.velocity += 0.5 * (new_force + self.force)/self.mass * dt\n        self.force = new_force\n\n    def apply_periodic_boundaries(self, box_size):\n            \"\"\"Apply periodic boundary conditions\"\"\"\n            self.position = self.position % box_size\n\nclass ForceField:\n    def __init__(self):\n        self.parameters = {\n            'C': {'epsilon': 1.615, 'sigma': 1.36},\n            'H': {'epsilon': 1.0, 'sigma': 1.0 },\n            'O': {'epsilon': 1.846, 'sigma': 3.0},\n        }\n        self.box_size = None  # Will be set when initializing the simulation\n\n    def get_pair_parameters(self, type1, type2):\n        # Apply mixing rules when needed\n        eps1 = self.parameters[type1]['epsilon']\n        eps2 = self.parameters[type2]['epsilon']\n        sig1 = self.parameters[type1]['sigma']\n        sig2 = self.parameters[type2]['sigma']\n\n        # Lorentz-Berthelot mixing rules\n        epsilon = np.sqrt(eps1 * eps2)\n        sigma = (sig1 + sig2) / 2\n\n        return epsilon, sigma\n\n    def minimum_image_distance(self, pos1, pos2):\n        \"\"\"Calculate minimum image distance between two positions\"\"\"\n        delta = pos1 - pos2\n        # Apply minimum image convention\n        delta = delta - self.box_size * np.round(delta / self.box_size)\n        return delta\n\n    def calculate_lj_force(self, atom1, atom2):\n        epsilon, sigma = self.get_pair_parameters(atom1.type, atom2.type)\n        r = self.minimum_image_distance(atom1.position, atom2.position)\n        r_mag = np.linalg.norm(r)\n\n        # Add cutoff distance for stability\n        if r_mag &gt; 3.5*sigma:\n            return np.zeros(2)\n\n        force_mag = 24 * epsilon * (\n            2 * (sigma/r_mag)**13\n            - (sigma/r_mag)**7\n        )\n        force = force_mag * r/r_mag\n        return force\n\n\n\n\n# %% Define the MD Simulation master controller class\n\nclass MDSimulation:\n    def __init__(self, atoms, forcefield, timestep, box_size):\n        self.atoms = atoms\n        self.forcefield = forcefield\n        self.forcefield.box_size = box_size  # Set box size in forcefield\n        self.timestep = timestep\n        self.box_size = np.array(box_size)\n        self.initial_energy = None\n        self.energy_history = []\n\n\n    def calculate_forces(self):\n        # Reset all forces\n        for atom in self.atoms:\n            atom.reset_force()\n\n        # Calculate forces between all pairs\n        for i, atom1 in enumerate(self.atoms):\n            for atom2 in self.atoms[i+1:]:\n                force = self.forcefield.calculate_lj_force(atom1, atom2)\n                atom1.add_force(force)\n                atom2.add_force(-force)  # Newton's third law\n\n    def update_positions_and_velocities(self):\n        # First step: Update positions using current forces\n        for atom in self.atoms:\n            atom.update_position(self.timestep)\n            # Apply periodic boundary conditions\n            atom.apply_periodic_boundaries(self.box_size)\n\n        # Recalculate forces with new positions\n        self.calculate_forces()\n\n        # Second step: Update velocities using average of old and new forces\n        for atom in self.atoms:\n            atom.update_velocity(self.timestep, atom.force)\n\n\n# %% Cell 6\ndef create_grid_atoms(num_atoms, box_size, type=\"H\",mass=1.0, random_offset=0.1):\n    box_size = np.array(box_size)\n\n    # Calculate grid dimensions\n    n = int(np.ceil(np.sqrt(num_atoms)))\n    spacing = np.min(box_size) / n\n\n    atoms = []\n    for i in range(num_atoms):\n        # Calculate grid position\n        row = i // n\n        col = i % n\n\n        # Base position\n        pos = np.array([col * spacing + spacing/2,\n                       row * spacing + spacing/2])\n\n        # Add random offset\n        pos += (np.random.rand(2) - 0.5) * spacing * random_offset\n\n        # Create atom\n        atoms.append(Atom(i, type, pos, mass=mass))\n\n    return atoms\n\n\n# %% Cell 7\ndef set_temperature(atoms, target_temperature):\n    N = len(atoms)      # number of atoms\n    Nf = 2 * N         # degrees of freedom in 2D\n\n    # Calculate current kinetic energy\n    current_ke = sum(0.5 * atom.mass * np.sum(atom.velocity**2) for atom in atoms)\n    current_temperature = 2 * current_ke / Nf  # kb = 1 in reduced units\n    print(current_temperature)\n    # Calculate scaling factor\n    scale_factor = np.sqrt(target_temperature / current_temperature)\n\n    # Scale velocities\n    for atom in atoms:\n        atom.velocity *= scale_factor\n\n\ndef initialize_velocities(atoms, temperature, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n\n    N = len(atoms)  # number of atoms\n    dim = 2         # 2D simulation\n\n    # Generate random velocities from normal distribution\n    velocities = np.random.normal(0, np.sqrt(temperature), size=(N, dim))\n\n    # Remove center of mass motion\n    total_momentum = np.sum([atom.mass * velocities[i] for i, atom in enumerate(atoms)], axis=0)\n    total_mass = np.sum([atom.mass for atom in atoms])\n    cm_velocity = total_momentum / total_mass\n\n    # Assign velocities to atoms\n    for i, atom in enumerate(atoms):\n        atom.velocity = velocities[i] - cm_velocity\n\n    # Scale velocities to exact temperature\n    set_temperature(atoms, temperature)\n\n    return atoms\n\n# %% run the simulation w\n\nbox_size = np.array([50.0, 50.0])  # Box dimensions\nnum_atoms = 200\n\nT=5\ndt = 0.01\n\n# Create atoms and set initial velocities\natoms = create_grid_atoms(num_atoms, box_size, type=\"H\",mass=1.0, random_offset=0.1)\natoms = initialize_velocities(atoms, temperature=T)\n\n\n# Create force field\nff = ForceField()\n\n\n# Create simulation with periodic boundaries\nsim = MDSimulation(atoms, ff, dt, box_size)\n\nfig, ax = plt.subplots(1,1,figsize=(6,6))\n\nfor step in range(1000):\n    clear_output(wait=True)\n    set_temperature(atoms, target_temperature=T)\n    sim.update_positions_and_velocities()\n\n\n    positions = [atom.position for atom in sim.atoms]\n    x_coords = [pos[0] for pos in positions]\n    y_coords = [pos[1] for pos in positions]\n\n    circle=patches.Circle((x_coords[0],y_coords[0]),ff.parameters[atoms[0].type][\"sigma\"],edgecolor=\"white\",fill=False)\n    ax.add_patch(circle)\n    ax.scatter(x_coords, y_coords,color=\"red\")\n    ax.set_xlim(0, box_size[0])\n    ax.set_ylim(0, box_size[1])\n    ax.axis(\"off\")\n\n    display(fig)\n\n    ax.clear()\n# %% Cell 8\n#\n\nvx=np.array([atom.velocity for atom in atoms])\n\nvx.reshape(200,2)\nplt.hist(vx[:,0],bins=20)"
  },
  {
    "objectID": "seminars/seminar05/mdsim copy.html#here-is-the-complete-code-for-the-molecular-dynamics-simulation",
    "href": "seminars/seminar05/mdsim copy.html#here-is-the-complete-code-for-the-molecular-dynamics-simulation",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "",
    "text": "# %% Load  modules and initialize\nfrom typing_extensions import ParamSpec\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nimport matplotlib.patches as patches\nplt.rcParams.update({'font.size': 8,\n                     'lines.linewidth': 1,\n                     'lines.markersize': 10,\n                     'axes.labelsize': 10,\n                     'axes.titlesize': 10,\n                     'xtick.labelsize' : 10,\n                     'ytick.labelsize' : 10,\n                     'xtick.top' : True,\n                     'xtick.direction' : 'in',\n                     'ytick.right' : True,\n                     'ytick.direction' : 'in',\n                     'figure.facecolor' : 'white',})\n\ndef get_size(w,h):\n    return((w/2.54,h/2.54))\n\n\n# %% Load the atom class we did already in the previous seminar\nclass Atom:\n    def __init__(self, atom_id, atom_type, position, velocity=None, mass=None):\n        self.id = atom_id\n        self.type = atom_type\n        self.position = position\n        self.velocity = velocity if velocity is not None else np.random.randn(2)*20\n        self.mass = mass\n        self.force = np.zeros(2)\n\n\n    def add_force(self, force):\n        \"\"\"Add force contribution to total force on atom\"\"\"\n        self.force += force\n\n    def reset_force(self):\n        \"\"\"Reset force to zero at start of each step\"\"\"\n        self.force = np.zeros(2)\n\n    def update_position(self, dt):\n        \"\"\"First step of velocity Verlet: update position\"\"\"\n        self.position += self.velocity * dt + 0.5 * (self.force/self.mass) * dt**2\n\n    def update_velocity(self, dt, new_force):\n        \"\"\"Second step of velocity Verlet: update velocity using average force\"\"\"\n        self.velocity += 0.5 * (new_force + self.force)/self.mass * dt\n        self.force = new_force\n\n    def apply_periodic_boundaries(self, box_size):\n            \"\"\"Apply periodic boundary conditions\"\"\"\n            self.position = self.position % box_size\n\nclass ForceField:\n    def __init__(self):\n        self.parameters = {\n            'C': {'epsilon': 1.615, 'sigma': 1.36},\n            'H': {'epsilon': 1.0, 'sigma': 1.0 },\n            'O': {'epsilon': 1.846, 'sigma': 3.0},\n        }\n        self.box_size = None  # Will be set when initializing the simulation\n\n    def get_pair_parameters(self, type1, type2):\n        # Apply mixing rules when needed\n        eps1 = self.parameters[type1]['epsilon']\n        eps2 = self.parameters[type2]['epsilon']\n        sig1 = self.parameters[type1]['sigma']\n        sig2 = self.parameters[type2]['sigma']\n\n        # Lorentz-Berthelot mixing rules\n        epsilon = np.sqrt(eps1 * eps2)\n        sigma = (sig1 + sig2) / 2\n\n        return epsilon, sigma\n\n    def minimum_image_distance(self, pos1, pos2):\n        \"\"\"Calculate minimum image distance between two positions\"\"\"\n        delta = pos1 - pos2\n        # Apply minimum image convention\n        delta = delta - self.box_size * np.round(delta / self.box_size)\n        return delta\n\n    def calculate_lj_force(self, atom1, atom2):\n        epsilon, sigma = self.get_pair_parameters(atom1.type, atom2.type)\n        r = self.minimum_image_distance(atom1.position, atom2.position)\n        r_mag = np.linalg.norm(r)\n\n        # Add cutoff distance for stability\n        if r_mag &gt; 3.5*sigma:\n            return np.zeros(2)\n\n        force_mag = 24 * epsilon * (\n            2 * (sigma/r_mag)**13\n            - (sigma/r_mag)**7\n        )\n        force = force_mag * r/r_mag\n        return force\n\n\n\n\n# %% Define the MD Simulation master controller class\n\nclass MDSimulation:\n    def __init__(self, atoms, forcefield, timestep, box_size):\n        self.atoms = atoms\n        self.forcefield = forcefield\n        self.forcefield.box_size = box_size  # Set box size in forcefield\n        self.timestep = timestep\n        self.box_size = np.array(box_size)\n        self.initial_energy = None\n        self.energy_history = []\n\n\n    def calculate_forces(self):\n        # Reset all forces\n        for atom in self.atoms:\n            atom.reset_force()\n\n        # Calculate forces between all pairs\n        for i, atom1 in enumerate(self.atoms):\n            for atom2 in self.atoms[i+1:]:\n                force = self.forcefield.calculate_lj_force(atom1, atom2)\n                atom1.add_force(force)\n                atom2.add_force(-force)  # Newton's third law\n\n    def update_positions_and_velocities(self):\n        # First step: Update positions using current forces\n        for atom in self.atoms:\n            atom.update_position(self.timestep)\n            # Apply periodic boundary conditions\n            atom.apply_periodic_boundaries(self.box_size)\n\n        # Recalculate forces with new positions\n        self.calculate_forces()\n\n        # Second step: Update velocities using average of old and new forces\n        for atom in self.atoms:\n            atom.update_velocity(self.timestep, atom.force)\n\n\n# %% Cell 6\ndef create_grid_atoms(num_atoms, box_size, type=\"H\",mass=1.0, random_offset=0.1):\n    box_size = np.array(box_size)\n\n    # Calculate grid dimensions\n    n = int(np.ceil(np.sqrt(num_atoms)))\n    spacing = np.min(box_size) / n\n\n    atoms = []\n    for i in range(num_atoms):\n        # Calculate grid position\n        row = i // n\n        col = i % n\n\n        # Base position\n        pos = np.array([col * spacing + spacing/2,\n                       row * spacing + spacing/2])\n\n        # Add random offset\n        pos += (np.random.rand(2) - 0.5) * spacing * random_offset\n\n        # Create atom\n        atoms.append(Atom(i, type, pos, mass=mass))\n\n    return atoms\n\n\n# %% Cell 7\ndef set_temperature(atoms, target_temperature):\n    N = len(atoms)      # number of atoms\n    Nf = 2 * N         # degrees of freedom in 2D\n\n    # Calculate current kinetic energy\n    current_ke = sum(0.5 * atom.mass * np.sum(atom.velocity**2) for atom in atoms)\n    current_temperature = 2 * current_ke / Nf  # kb = 1 in reduced units\n    print(current_temperature)\n    # Calculate scaling factor\n    scale_factor = np.sqrt(target_temperature / current_temperature)\n\n    # Scale velocities\n    for atom in atoms:\n        atom.velocity *= scale_factor\n\n\ndef initialize_velocities(atoms, temperature, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n\n    N = len(atoms)  # number of atoms\n    dim = 2         # 2D simulation\n\n    # Generate random velocities from normal distribution\n    velocities = np.random.normal(0, np.sqrt(temperature), size=(N, dim))\n\n    # Remove center of mass motion\n    total_momentum = np.sum([atom.mass * velocities[i] for i, atom in enumerate(atoms)], axis=0)\n    total_mass = np.sum([atom.mass for atom in atoms])\n    cm_velocity = total_momentum / total_mass\n\n    # Assign velocities to atoms\n    for i, atom in enumerate(atoms):\n        atom.velocity = velocities[i] - cm_velocity\n\n    # Scale velocities to exact temperature\n    set_temperature(atoms, temperature)\n\n    return atoms\n\n# %% run the simulation w\n\nbox_size = np.array([50.0, 50.0])  # Box dimensions\nnum_atoms = 200\n\nT=5\ndt = 0.01\n\n# Create atoms and set initial velocities\natoms = create_grid_atoms(num_atoms, box_size, type=\"H\",mass=1.0, random_offset=0.1)\natoms = initialize_velocities(atoms, temperature=T)\n\n\n# Create force field\nff = ForceField()\n\n\n# Create simulation with periodic boundaries\nsim = MDSimulation(atoms, ff, dt, box_size)\n\nfig, ax = plt.subplots(1,1,figsize=(6,6))\n\nfor step in range(1000):\n    clear_output(wait=True)\n    set_temperature(atoms, target_temperature=T)\n    sim.update_positions_and_velocities()\n\n\n    positions = [atom.position for atom in sim.atoms]\n    x_coords = [pos[0] for pos in positions]\n    y_coords = [pos[1] for pos in positions]\n\n    circle=patches.Circle((x_coords[0],y_coords[0]),ff.parameters[atoms[0].type][\"sigma\"],edgecolor=\"white\",fill=False)\n    ax.add_patch(circle)\n    ax.scatter(x_coords, y_coords,color=\"red\")\n    ax.set_xlim(0, box_size[0])\n    ax.set_ylim(0, box_size[1])\n    ax.axis(\"off\")\n\n    display(fig)\n\n    ax.clear()\n# %% Cell 8\n#\n\nvx=np.array([atom.velocity for atom in atoms])\n\nvx.reshape(200,2)\nplt.hist(vx[:,0],bins=20)"
  },
  {
    "objectID": "seminars/seminar10/1_deep_learning.html",
    "href": "seminars/seminar10/1_deep_learning.html",
    "title": "Neural Networks",
    "section": "",
    "text": "Neural networks are one of the most commonly used machine learning objects nowadays. Mostly these systems are known as deep neural networks, which just says something about how many layers in which neurons are arranged exist. We will in this lecture have a look at the basic unit, the neuron, and how to connect and train a network. We will do all ourselves, that means, we will not use one of the many existing python modules, that simplifies the task. This notebook has been largely developed by Martin Fr√§nzl.\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\nplt.rcParams.update({'font.size': 18,\n                     'axes.titlesize': 20,\n                     'axes.labelsize': 20,\n                     'axes.labelpad': 1,\n                     'lines.linewidth': 2,\n                     'lines.markersize': 10,\n                     'xtick.labelsize' : 18,\n                     'ytick.labelsize' : 18,\n                     'xtick.top' : True,\n                     'xtick.direction' : 'in',\n                     'ytick.right' : True,\n                     'ytick.direction' : 'in'\n                    })\nIn this lecture we are going to build a neural network from scratch using Python and NumPy (The high-level libaries like Keras and TensorFlow will be covered in Part 2). We will build a network to recognize hand-written digits, using the famous MNIST data set.\nWe will start with the simplest possible ‚Äúnetwork‚Äù: A single node that recognizes just the digit 0. This is actually just an implementation of logistic regression, but it will help us understand some of the key components before things get more complicated. Then we‚Äôll extend that into a network with one hidden layer, still recognizing just 0. Finally, we will extend the network to recognize all the digits 0 through 9. That will give us a 92% accurate digit-recognizer."
  },
  {
    "objectID": "seminars/seminar10/1_deep_learning.html#the-mnist-data-set",
    "href": "seminars/seminar10/1_deep_learning.html#the-mnist-data-set",
    "title": "Neural Networks",
    "section": "The MNIST Data Set",
    "text": "The MNIST Data Set\nThe MNIST data set contains 70,000 images of hand-written digits, each 28 x 28 pixels, in greyscale with pixel-values from 0 to 255. We could download and preprocess the data ourselves, but the makers of the module sklearn already did that for us:\n\nLoad the data\n\nfrom sklearn.datasets import fetch_openml\nX, y = fetch_openml('mnist_784', version=1, return_X_y=True,as_frame=False)\n\nThe images are now contained in the array X, while the labels (so which number it is) are contained in y. Let‚Äôs have a look at a random image and label.\n\ni = 33419\nplt.imshow(np.array(X)[i].reshape(28, 28), cmap='gray')\nplt.colorbar()\nplt.show()\nprint('label: ',y[i])\n\n\n\n\n\n\n\n\nlabel:  8\n\n\n\n\nNormalize the data\nTo use data in neural networks as training data, it is always useful to normalize the data to the interval [0, 1].\n\nX = X/255\n\n\n\nPreparing training and testing data\nThe default MNIST labels say ‚Äò1‚Äô for an image of a one, ‚Äò2‚Äô for an image of a two, etc., but we are just building a zero classifier for now. So we want our labels to say 1 when we have a zero, and 0 otherwise. So we overwrite the labels accordingly:\n\ny_new = np.zeros(y.shape)\ny_new[np.where(y == '0')[0]] = 1\ny = y_new\n\nWe now split the data in a train and test set. The MNIST images are pre-arranged so that the first 60,000 can be used for training, and the last 10,000 for testing. We‚Äôll also transform the data into the shape we want, with each example in a column (instead of a row):\n\nm = 60000\nm_test = X.shape[0] - m\n\nX_train, X_test = X[:m].T, X[m:].T\ny_train, y_test = y[:m].reshape(1,m), y[m:].reshape(1, m_test)\n\nFinally, we shuffle the training set:\n\nnp.random.seed(1)\nshuffle_index = np.random.permutation(m)\nX_train, y_train = X_train[:,shuffle_index], y_train[:,shuffle_index]\n\nLet‚Äôs again have a look at random image and label just to check\n\ni = 39\nplt.imshow(X_train[:,i].reshape(28, 28), cmap='gray')\nplt.colorbar()\nplt.show()\nprint(y_train[:,i])\n\n\n\n\n\n\n\n\n[0.]\n\n\nTry to find a zero to check whether the corresponding label is a 1."
  },
  {
    "objectID": "seminars/seminar10/1_deep_learning.html#a-single-neuron",
    "href": "seminars/seminar10/1_deep_learning.html#a-single-neuron",
    "title": "Neural Networks",
    "section": "A Single Neuron",
    "text": "A Single Neuron\nThe basic unit of a neural network is a neuron. A neuron takes inputs, does some math with them, and produces one output. The neuron below does that with two inputs.\n\n\n\nimage\n\n\n\nForward Propogation\nThe neuron does now three things.\n\nTake input values and multipy by weights\n\n\\[\\begin{eqnarray}\nx_{1}\\rightarrow x_{1} w_{1}\\\\\nx_{2}\\rightarrow x_{2} w_{2}\n\\end{eqnarray}\\]\n\nAll the weighted inputs are the added to a bias value \\(b\\)\n\n\\[\\begin{equation}\nx_{1} w_{1}+ x_{2} w_{2}+b\n\\end{equation}\\]\n\nThe output is generated by applying a function \\(\\sigma()\\) \\[\\begin{equation}\ny=\\sigma( x_{1} w_{1}+ x_{2} w_{2}+b)\n\\end{equation}\\]\n\nThis function is called activation function. The activation function is used to turn an unbounded input value into a bounded output value with a predictable range. A commonly used activation function is the sigmoid function.\nFor a single input dataset \\(x\\) a more compact writing of the math above is\n\\[\\begin{equation*}\n\\hat{y} = \\sigma(w^{\\rm T} x + b)\\ .\n\\end{equation*}\\]\nHere \\(\\sigma\\) is the sigmoid function: \\[\\begin{equation*}\n\\sigma(z) = \\frac{1}{1+{\\rm e}^{-z}}\\ .\n\\end{equation*}\\]\nThe sigmoid function is something we can already define and plot.\n\ndef sigmoid(z):\n    return 1/(1 + np.exp(-z))\n\n\nx=np.linspace(-5,5,100)\nplt.figure(figsize=(5,3))\nplt.plot(x,sigmoid(x))\nplt.xlabel('input')\nplt.ylabel('output')\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nIf we now have this kind of two input neuron with the weights \\(w\\) and the bias value \\(b\\)\n\\[\\begin{eqnarray}\nw=[0,1]\\\\\nb=4\n\\end{eqnarray}\\]\nwe may supply and input\n\\[\\begin{eqnarray}\nx=[2,3]\n\\end{eqnarray}\\]\nwhich gives writing it a s a dot product\n\\[\\begin{equation}\ny=f(w\\cdot x+b)=f(7)=0.999\n\\end{equation}\\]\nThis procedure of propagating the input values to obtain and output value is called feedforward or forward propagation. Our first goal is now to create a network with a single neuron with 784 inputs (28 x 28), and a single sigmoid unit generating the output.\nThe above examples can be written and executed more efficiently in a vectorized form. Generating the output We‚Äôll vectorize by stacking examples side-by-side, so that our input matrix \\(X\\) has an example in each column. The vectorized form of the forward pass is then\n\\[\\begin{equation*}\n\\hat{y} = \\sigma(w^{\\rm T} X + b)\\ .\n\\end{equation*}\\]\nNote that \\(\\hat{y}\\) is now a vector, not a scalar as it was in the previous equation.\nIn our code we will compute this in two stages: Z = np.matmul(W.T, X) + b and then A = sigmoid(Z) (A for Activation). Breaking things up into stages like this is just for clarity - It will make our forward pass computations mirror the steps in our backward propagation computation.\n\n\nLoss Function\nSince we have now data and we also know how to propagate (at least in principle) the input through the single neuron here, we also need to define a measure for how far the output deviates from the input. This measure is called loss. The many different ways of defining a suitable loss. The mean squared error, as it appeared already during our fitting lecture, could be a suitable loss function\n\\[\\begin{equation}\nMSE(y,\\hat{y})=\\frac{1}{n}\\sum_{i=1}^{n}(y-\\hat{y})^2\n\\end{equation}\\]\nfor a number of \\(n\\) datasets. Here \\(\\hat{y}\\) is the data that is predicted by the network and \\(y\\) is the value which represents the so called ground truth, i.e.¬†the data provided by the training set.\nWe will not use the mean squared error bu the cross-entropy for our loss function. The formula for a single training example (one input image) is:\n\\[\\begin{equation*}\nL(y,\\hat{y}) = -y\\log(\\hat{y})-(1-y)\\log(1-\\hat{y})\\ .\n\\end{equation*}\\]\nThis error definition comes from the Shannon entropy definition, which you may look up in the web if you are interested. Averaging over a training set of \\(m\\) examples we then have:\n\\[\\begin{equation*}\nL(Y,\\hat{Y}) = -\\frac{1}{m}\\sum_{i = 0}^{m}y^{(i)}\\log(\\hat{y}^{(i)})-(1-y^{(i)})\\log(1-\\hat{y}^{(i)})\\ .\n\\end{equation*}\\]\nIn Python code, this looks like\n\ndef compute_loss(Y, Y_hat):\n    m = Y.shape[1]\n    L = -(1./m)*(np.sum(np.multiply(np.log(Y_hat), Y)) + np.sum(np.multiply(np.log(1 - Y_hat), (1 - Y))))\n    return L"
  },
  {
    "objectID": "seminars/seminar10/1_deep_learning.html#trainging-the-network",
    "href": "seminars/seminar10/1_deep_learning.html#trainging-the-network",
    "title": "Neural Networks",
    "section": "Trainging the Network",
    "text": "Trainging the Network\nThe goal of all neural network training procedures is to minimize the loss and we have to find a way to minimize that loss. This is not so much different from our fitting of function values before.\n\nBackward Propagation\nThe output of the network is determined by the input values and how we have distributed the weights \\(w\\) and the biases \\(b\\). We can write the loss function therefore as a function of the weights and losses\n\\[\nL(w_{1},w_{2},w_{3},\\ldots ,b_{1},b_{2},b_{3},\\ldots)\n\\]\nTo train the network, we would now try to find out, by how much the output values change if we do change a specific weight \\(w_j\\). This can be expressed by the partial derivative\n\\[\n\\frac{\\partial L}{\\partial w_j}\n\\]\nWe may then take a tiny step and correct the current value of \\(w_j\\) such that the network yields a new output. This way back from the current output of the network and its current loss to a correction of the weights to yield a smaller loss is called back propagation.\nCalculating derivatives\nFocusing on a single input image will make it easier to derive the formulas we need. Holding all values except \\(w_j\\) fixed, we can think of \\(L\\) as being computed in three steps: \\(w_j\\rightarrow z \\rightarrow \\hat{y} \\rightarrow L\\). The formulas for these steps are: \\[\\begin{align*}\nz &= w^{\\rm T} x + b\\ , \\\\\n\\hat{y} &= \\sigma(z)\\ , \\\\\nL(y,\\hat{y}) &= -y\\log(\\hat{y})-(1-y)\\log(1-\\hat{y})\\ .\n\\end{align*}\\]\nThe change of the loss function with the weights can then be split up by the chain rule into\n\\[\\begin{align*}\n\\frac{\\partial L}{\\partial w_j} = \\frac{\\partial L}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial z}\\frac{\\partial z}{\\partial w_j}\n\\end{align*}\\] \nThere we have a product of three individual partial derivatives, which are a bit tedius to write down, but not to complicated. The read like\n\\(\\partial L/\\partial\\hat{y}\\): \\[\\begin{align*}\n\\frac{\\partial L}{\\partial\\hat{y}} &= \\frac{\\partial}{\\partial\\hat{y}}\\left(-y\\log(\\hat{y})-(1-y)\\log(1-\\hat{y})\\right) \\\\\n&= -y\\frac{\\partial}{\\partial\\hat{y}}\\log(\\hat{y})-(1-y)\\frac{\\partial}{\\partial\\hat{y}}\\log(1-\\hat{y}) \\\\\n&= -\\frac{y}{\\hat{y}} +\\frac{(1 - y)}{1-\\hat{y}} \\\\\n&= \\frac{\\hat{y} - y}{\\hat{y}(1-\\hat{y})}\n\\end{align*}\\]\n\\(\\partial \\hat{y}/\\partial z\\): \\[\\begin{align*}\n\\frac{\\partial }{\\partial z}\\sigma(z)\n&= \\frac{\\partial }{\\partial z}\\left(\\frac{1}{1 + {\\rm e}^{-z}}\\right) \\\\\n&= \\frac{1}{(1 + {\\rm e}^{-z})^2}\\frac{\\partial }{\\partial z}(1 + {\\rm e}^{-z}) \\\\\n&= \\frac{{\\rm e}^{-z}}{(1 + {\\rm e}^{-z})^2} \\\\\n&= \\frac{1}{1 + {\\rm e}^{-z}}\\frac{{\\rm e}^{-z}}{1 + {\\rm e}^{-z}} \\\\\n&= \\frac{1}{1 + {\\rm e}^{-z}}\\left(1 - \\frac{1}{1 + {\\rm e}^{-z}}\\right) \\\\\n&= \\sigma(z)(1-\\sigma(z)) \\\\\n&= \\hat{y}(1-\\hat{y})\n\\end{align*}\\]\n\\(\\partial z/\\partial w_j\\): \\[\\begin{align*}\n\\frac{\\partial }{\\partial w_j}(w^{\\rm T} x + b) &= \\frac{\\partial }{\\partial w_j}(w_0x_0 + \\dots + w_nx_n + b) \\\\\n&= x_j\n\\end{align*}\\]\nSubstituting back into the chain rule yields: \\[\\begin{align*}\n\\frac{\\partial L}{\\partial w_j}\n&= \\frac{\\partial L}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial z}\\frac{\\partial z}{\\partial w_j} \\\\\n&= \\frac{\\hat{y} - y}{\\hat{y}(1-\\hat{y})}\\hat{y}(1-\\hat{y}) x_j \\\\\n&= (\\hat{y} - y)x_j\\ .\n\\end{align*}\\]\nwhich does not look that unfriendly anymore.\nIn vectorized form with \\(m\\) training examples this gives us \\[\\begin{align*}\n\\frac{\\partial L}{\\partial w} = \\frac{1}{m} X(\\hat{y} - y)^{\\rm T}\\ .\n\\end{align*}\\]\nA very similar derivation of \\(\\partial L/\\partial b\\) yields, for a single example: \\[\\begin{align*}\n\\frac{\\partial L}{\\partial b} = (\\hat{y} - y)\\ .\n\\end{align*}\\]\nIn vectorized form we get \\[\\begin{align*}\n\\frac{\\partial L}{\\partial b} = \\frac{1}{m}\\sum_{i=1}^{m}{(\\hat{y}^{(i)} - y^{(i)})}\\ .\n\\end{align*}\\]\nIn our code we label these gradients according to their denominators, as dW and db. So for backpropagation we compute dW = (1/m) * np.matmul(X, (A-Y).T) and db = (1/m)*np.sum(A-Y, axis=1, keepdims=True).\n\n\nStochastic Gradient Descent\nWe have all the tools we need to train a neural network now! We‚Äôll use an optimization algorithm called stochastic gradient descent (SGD) that tells us how to change our weights and biases to minimize loss. It is a simple umpdate of the weights and biases, which would read for the weights like\n\\[\nw\\leftarrow w-\\eta\\frac{\\partial L}{\\partial w}\n\\]\nwhere \\(\\eta\\) is a constant called the learning rate that controls how fast we train. All we‚Äôre doing is subtracting \\(\\eta \\partial L/\\partial w\\) from \\(w\\)\n\nIf \\(\\partial L/\\partial w\\) is positive, \\(w\\) will decrease, which makes L decrease.\nIf \\(\\partial L/\\partial w\\) is negative, \\(w\\) will increase, which makes L decrease.\n\nThe equations look equivalent for the bias \\(b\\). Our back propagation procedure will do that for as many steps we want, i.e.¬†until we feel that the output is close enough to the ground truth. Each back propagation step is called and epoch.\n\n\nBuild an Train\nNow we have all things together to create a single neuron network doing the analysis of the MNIST numbers. This type of data processing is called logistic regression based on the sigmoid function, which is a logistic function. So let‚Äôs create all in python code and train the network for 100 epochs.\n\nlearning_rate = 1\n\nX = np.array(X_train)\nY = np.array(y_train)\n\nn_x = X.shape[0]\nm = X.shape[1]\n\nW = np.random.randn(n_x, 1) * 0.01\nb = np.zeros((1, 1))\n\nfor i in range(200):\n    Z = np.matmul(W.T, X) + b\n    A = sigmoid(Z)\n\n    loss = compute_loss(Y, A)\n\n    dW = (1/m)*np.matmul(X, (A-Y).T)\n    db = (1/m)*np.sum(A-Y, axis=1, keepdims=True)\n\n    W = W - learning_rate * dW\n    b = b - learning_rate * db\n\n    if i % 10 == 0:\n        print(\"Epoch\", i, \" loss: \", loss)\n\nprint(\"Final loss:\", loss)\n\nEpoch 0  loss:  0.7471125121616977\nEpoch 10  loss:  0.07308269582929021\nEpoch 20  loss:  0.06131832354627721\nEpoch 30  loss:  0.05523011981200572\nEpoch 40  loss:  0.0513243202361425\nEpoch 50  loss:  0.04854004196371184\nEpoch 60  loss:  0.04642485272904433\nEpoch 70  loss:  0.04474722082574825\nEpoch 80  loss:  0.043374333931969114\nEpoch 90  loss:  0.04222371551840797\nEpoch 100  loss:  0.04124104599832092\nEpoch 110  loss:  0.04038888651110667\nEpoch 120  loss:  0.03964048057966332\nEpoch 130  loss:  0.0389761317586134\nEpoch 140  loss:  0.038380979206568466\nEpoch 150  loss:  0.03784357458020544\nEpoch 160  loss:  0.03735493964481513\nEpoch 170  loss:  0.03690792357698068\nEpoch 180  loss:  0.03649675336766015\nEpoch 190  loss:  0.036116712255247\nFinal loss: 0.03579805734492837\n\n\nWe do not really now how to judge the quality of our trained network. At least we saw that the loss is decreasing, which is good. We may judge the quality of our trained network by calculating the so-called confusion matrix. The confusion matrix is creating a matrix giving reports the actual values in the rows and the predicted values in the columns.\n\n\n\nconfusion_matrix\n\n\nThe entries in the matrix are called true positives (TP), false positives (FP), false negatives (FN), and true negatives (TN). Fortunately we can use a method of the sklearn module to calculate the confusion matrix. We just have to supply the predictions and the actual labels to it. To do so, we use the testing data set X_test which we have splitted earlier.\n\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nZ = np.matmul(W.T,X_test) + b\nA = sigmoid(Z)\n\npredictions = (A&gt;.5)[0,:]\nlabels = (y_test == 1)[0,:]\n\nprint(confusion_matrix(predictions, labels))\n\n[[8973   33]\n [  47  947]]\n\n\n\nprint(classification_report(predictions, labels))\n\n              precision    recall  f1-score   support\n\n       False       0.99      1.00      1.00      9006\n        True       0.97      0.95      0.96       994\n\n    accuracy                           0.99     10000\n   macro avg       0.98      0.97      0.98     10000\nweighted avg       0.99      0.99      0.99     10000\n\n\n\n\n\nTesting our model\nWe can check a single image of our testing data with the following line. If the output number is bigger than 0.5, our number is likely a 0.\n\ni=200\nbool(sigmoid(np.matmul(W.T, np.array(X_test)[:,i])+b)&gt;0.5)\n\nFalse\n\n\n\nplt.imshow(np.array(X_test)[:,i].reshape(28,28),cmap='gray')"
  },
  {
    "objectID": "seminars/seminar10/1_deep_learning.html#network-with-hidden-layers",
    "href": "seminars/seminar10/1_deep_learning.html#network-with-hidden-layers",
    "title": "Neural Networks",
    "section": "Network with Hidden Layers",
    "text": "Network with Hidden Layers\nIn our example above, we just had an input layer and a single output neuron. More complex neural networks are containing many layers between the input layer and the output layer. These inbetween layers are called hidden layers. Here is a simple example of a neural network with a single hidden layer.\n\n\n\nhidden\n\n\nSo we have now and input layer with 784 inputs that are connected to 64 units in the hidden layer and 1 neuron in the output layer. We will not go through the derivations of all the formulas for the forward and backward passes this time. The code is a simple extension of what we did before and I hope easy to read.\n\nX = X_train\nY = y_train\n\nn_x = X.shape[0]\nn_h = 64\nlearning_rate = 1\n\nW1 = np.random.randn(n_h, n_x)\nb1 = np.zeros((n_h, 1))\nW2 = np.random.randn(1, n_h)\nb2 = np.zeros((1, 1))\n\nfor i in range(100):\n\n    Z1 = np.matmul(W1, X) + b1\n    A1 = sigmoid(Z1)\n    Z2 = np.matmul(W2, A1) + b2\n    A2 = sigmoid(Z2)\n\n    loss = compute_loss(Y, A2)\n\n    dZ2 = A2-Y\n    dW2 = (1./m) * np.matmul(dZ2, A1.T)\n    db2 = (1./m) * np.sum(dZ2, axis=1, keepdims=True)\n\n    dA1 = np.matmul(W2.T, dZ2)\n    dZ1 = dA1 * sigmoid(Z1) * (1 - sigmoid(Z1))\n    dW1 = (1./m) * np.matmul(dZ1, X.T)\n    db1 = (1./m) * np.sum(dZ1, axis=1, keepdims=True)\n\n    W2 = W2 - learning_rate * dW2\n    b2 = b2 - learning_rate * db2\n    W1 = W1 - learning_rate * dW1\n    b1 = b1 - learning_rate * db1\n\n    if i % 10 == 0:\n        print(\"Epoch\", i, \"loss: \", loss)\n\nprint(\"Final loss:\", loss)\n\nEpoch 0 loss:  2.395166635058746\nEpoch 10 loss:  0.22074168759268953\nEpoch 20 loss:  0.1660154822272753\nEpoch 30 loss:  0.13990677867922952\nEpoch 40 loss:  0.12390102523919129\nEpoch 50 loss:  0.11269161497108851\nEpoch 60 loss:  0.10421329497723456\nEpoch 70 loss:  0.09747959072905935\nEpoch 80 loss:  0.09194898313097832\nEpoch 90 loss:  0.0872943606401609\nFinal loss: 0.08367740628296327\n\n\nTo judge the newtork quality we do use again the confusion matrix.\n\nZ1 = np.matmul(W1, X_test) + b1\nA1 = sigmoid(Z1)\nZ2 = np.matmul(W2, A1) + b2\nA2 = sigmoid(Z2)\n\npredictions = (A2&gt;.5)[0,:]\nlabels = (y_test == 1)[0,:]\n\nprint(confusion_matrix(predictions, labels))\nprint(classification_report(predictions, labels))\n\n[[8905  178]\n [ 115  802]]\n              precision    recall  f1-score   support\n\n       False       0.99      0.98      0.98      9083\n        True       0.82      0.87      0.85       917\n\n    accuracy                           0.97     10000\n   macro avg       0.90      0.93      0.91     10000\nweighted avg       0.97      0.97      0.97     10000"
  },
  {
    "objectID": "seminars/seminar10/1_deep_learning.html#multiclass-network",
    "href": "seminars/seminar10/1_deep_learning.html#multiclass-network",
    "title": "Neural Networks",
    "section": "Multiclass Network",
    "text": "Multiclass Network\nSo far we did only classify if the number we feed to the network is just a 0 or not. We would like to recognize the different number now and therefore need a multiclass network. Each number is then a class and per class, we have multiple realizations of handwritten numbers. We therefore have to create an output layer, which is not only containing a single neuron, but 10 neurons. Each of these neuron can output a value between 0 and 1. Whenever the output is 1, the index of the neuron represents the number predicted.\nThe output array\n[0,1,0,0,0,0,0,0,0,0]\nwould therefore correspond to the value 1.\nFor this purpose, we need to reload the right labels.\n\nfrom sklearn.datasets import fetch_openml\nX, y = fetch_openml('mnist_784', version=1, return_X_y=True,as_frame=False)\n\nX = X / 255\n\nThen we‚Äôll one-hot encode MNIST‚Äôs labels, to get a 10 x 70,000 array.\n\ndigits = 10\nexamples = y.shape[0]\n\ny = y.reshape(1, examples)\n\nY_new = np.eye(digits)[y.astype('int32')]\nY_new = Y_new.T.reshape(digits, examples)\n\n\nY_new.shape\n\n(10, 70000)\n\n\nWe also seperate into trainging and testing data\n\nm = 60000\nm_test = X.shape[0] - m\n\nX_train, X_test = X[:m].T, X[m:].T\nY_train, Y_test = Y_new[:,:m], Y_new[:,m:]\n\nshuffle_index = np.random.permutation(m)\nX_train, Y_train = X_train[:, shuffle_index], Y_train[:, shuffle_index]\n\n\ni = 58\nplt.imshow(X_train[:,i].reshape(28,28), cmap='gray')\nplt.colorbar()\nplt.show()\nY_train[:,i]\n\n\n\n\n\n\n\n\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n\n\n\nChanges to the model\nOK, so let‚Äôs consider what changes we need to make to the model itself.\n\nForward Pass\nOnly the last layer of our network is changing. To add the softmax, we have to replace our lone, final node with a 10 unit layer. Its final activations are the exponentials of its z-values, normalized across all ten such exponentials. So instead of just computing \\(\\sigma(z)\\), we compute the activation for each unit \\(i\\) using the softmax function: \\[\\begin{align*}\n\\sigma(z)_i = \\frac{{\\rm e}^{z_i}}{\\sum_{j=0}^9{\\rm e}^{z_i}}\\ .\n\\end{align*}\\]\nSo, in our vectorized code, the last line of forward propagation will be A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0).\n\n\nLoss Function\nOur loss function now has to generalize to more than two classes. The general formula for \\(n\\) classes is: \\[\\begin{align*}\nL(y,\\hat{y}) = -\\sum_{i=0}^n y_i\\log(\\hat{y}_i)\\ .\n\\end{align*}\\] Averaging over \\(m\\) training examples this becomes: \\[\\begin{align*}\nL(y,\\hat{y}) = -\\frac{1}{m}\\sum_{j=0}^m\\sum_{i=0}^n y_i^{(i)}\\log(\\hat{y}_i^{(i)})\\ .\n\\end{align*}\\]\nSo let‚Äôs define:\n\ndef compute_multiclass_loss(Y, Y_hat):\n    L_sum = np.sum(np.multiply(Y, np.log(Y_hat)))\n    m = Y.shape[1]\n    L = -(1/m) * L_sum\n    return L\n\n\n\nBack Propagation\nLuckily it turns out that back propagation isn‚Äôt really affected by the switch to a softmax. A softmax generalizes the sigmoid activiation we‚Äôve been using, and in such a way that the code we wrote earlier still works. We could verify this by deriving: \\[\\begin{align*}\n\\frac{\\partial L}{\\partial z_i} = \\hat{y}_i - y_i\\ .\n\\end{align*}\\]\nBut we won‚Äôt walk through the steps here. Let‚Äôs just go ahead and build our final network.\n\n\n\nBuild and Train\nAs we have now more weights and classes, the training takes longer and we actually need also more episodes to achieve a good accuracy.\n\nn_x = X_train.shape[0]\nn_h = 64\nlearning_rate = 1\n\nW1 = np.random.randn(n_h, n_x)\nb1 = np.zeros((n_h, 1))\nW2 = np.random.randn(digits, n_h)\nb2 = np.zeros((digits, 1))\n\nX = X_train\nY = Y_train\n\nfor i in range(200):\n\n    Z1 = np.matmul(W1,X) + b1\n    A1 = sigmoid(Z1)\n    Z2 = np.matmul(W2,A1) + b2\n    A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0)\n\n    loss = compute_multiclass_loss(Y, A2)\n\n    dZ2 = A2-Y\n    dW2 = (1./m) * np.matmul(dZ2, A1.T)\n    db2 = (1./m) * np.sum(dZ2, axis=1, keepdims=True)\n\n    dA1 = np.matmul(W2.T, dZ2)\n    dZ1 = dA1 * sigmoid(Z1) * (1 - sigmoid(Z1))\n    dW1 = (1./m) * np.matmul(dZ1, X.T)\n    db1 = (1./m) * np.sum(dZ1, axis=1, keepdims=True)\n\n    W2 = W2 - learning_rate * dW2\n    b2 = b2 - learning_rate * db2\n    W1 = W1 - learning_rate * dW1\n    b1 = b1 - learning_rate * db1\n\n    if (i % 10 == 0):\n        print(\"Epoch\", i, \"loss: \", loss)\n\nprint(\"Final loss:\", loss)\n\nEpoch 0 loss:  9.359409945262723\nEpoch 10 loss:  2.480915410750769\nEpoch 20 loss:  1.674432764227767\nEpoch 30 loss:  1.3330104308788548\nEpoch 40 loss:  1.1447842302497118\nEpoch 50 loss:  1.0230964725181804\nEpoch 60 loss:  0.9368747323694273\nEpoch 70 loss:  0.871957389404843\nEpoch 80 loss:  0.8208795576102073\nEpoch 90 loss:  0.7793325725168161\nEpoch 100 loss:  0.7446649543545801\nEpoch 110 loss:  0.7151537041535516\nEpoch 120 loss:  0.6896258244540621\nEpoch 130 loss:  0.6672519100025255\nEpoch 140 loss:  0.6474268213495038\nEpoch 150 loss:  0.6296970416913454\nEpoch 160 loss:  0.6137147676333653\nEpoch 170 loss:  0.5992079750548168\nEpoch 180 loss:  0.5859603076597457\nEpoch 190 loss:  0.5737971945414019\nFinal loss: 0.5636592880338959\n\n\nLet‚Äôs see how we did:\n\nZ1 = np.matmul(W1, X_test) + b1\nA1 = sigmoid(Z1)\nZ2 = np.matmul(W2, A1) + b2\nA2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0)\n\npredictions = np.argmax(A2, axis=0)\nlabels = np.argmax(Y_test, axis=0)\n\n\nModel performance\n\nprint(confusion_matrix(predictions, labels))\nprint(classification_report(predictions, labels))\n\n[[ 896    0   26    8    8   22   30    4   14   10]\n [   0 1076   15    6    2    7    3    6   14    2]\n [  14   13  815   30   10   13   23   33   24   13]\n [  10   12   47  820    4   60    5   18   40   17]\n [   0    1   17    0  790   22   30   17   14  106]\n [  27    4    7   56    5  669   23    6   58   16]\n [  13    5   30    7   23   28  822    0   21    1]\n [   6    2   18   25   12   10    3  866   18   37]\n [  12   22   47   42   19   44   16   15  739   28]\n [   2    0   10   16  109   17    3   63   32  779]]\n              precision    recall  f1-score   support\n\n           0       0.91      0.88      0.90      1018\n           1       0.95      0.95      0.95      1131\n           2       0.79      0.82      0.81       988\n           3       0.81      0.79      0.80      1033\n           4       0.80      0.79      0.80       997\n           5       0.75      0.77      0.76       871\n           6       0.86      0.87      0.86       950\n           7       0.84      0.87      0.86       997\n           8       0.76      0.75      0.75       984\n           9       0.77      0.76      0.76      1031\n\n    accuracy                           0.83     10000\n   macro avg       0.82      0.83      0.82     10000\nweighted avg       0.83      0.83      0.83     10000\n\n\n\nWe are at 84% accuray across all digits, which could be of course better. We may now plot image and the corresponding prediction."
  },
  {
    "objectID": "seminars/seminar10/1_deep_learning.html#test-the-model",
    "href": "seminars/seminar10/1_deep_learning.html#test-the-model",
    "title": "Neural Networks",
    "section": "Test the model",
    "text": "Test the model\n\ni=2003\nplt.imshow(X_test[:,i].reshape(28,28), cmap='gray')\npredictions[i]\n\nnp.int64(5)"
  },
  {
    "objectID": "seminars/seminar10/seminar10.html#what-are-neural-networks",
    "href": "seminars/seminar10/seminar10.html#what-are-neural-networks",
    "title": "Neural Networks",
    "section": "What are Neural Networks?",
    "text": "What are Neural Networks?\nNeural networks are computational models inspired by how our brains process information. Just like our brain consists of interconnected neurons that process and transmit signals, artificial neural networks consist of mathematical ‚Äúneurons‚Äù that process numerical information. They‚Äôre particularly powerful for:\n\nRecognizing patterns in data\nMaking predictions\nClassifying information\nSolving complex problems"
  },
  {
    "objectID": "seminars/seminar10/seminar10.html#why-neural-networks-in-physics",
    "href": "seminars/seminar10/seminar10.html#why-neural-networks-in-physics",
    "title": "Neural Networks",
    "section": "Why Neural Networks in Physics?",
    "text": "Why Neural Networks in Physics?\nIn physics, we often encounter situations that push the boundaries of traditional approaches. Sometimes the mathematical models grow too complex to solve directly, while in other cases we face the challenge of analyzing vast amounts of experimental data. We frequently need to make predictions even when we have incomplete information about a system. These scenarios represent key areas where neural networks can provide valuable solutions.\nNeural networks help us with these challenges! Some real-world applications include:\n\nParticle physics: Identifying particles in detector data\nAstronomy: Classifying galaxies\nMaterials science: Predicting material properties\nQuantum mechanics: Solving many-body problems\nBiological physics: Modeling neural activity\nActive matter: Predicting collective behavior"
  },
  {
    "objectID": "seminars/seminar10/seminar10.html#data-for-neural-networks-teaching-computers-to-read-numbers",
    "href": "seminars/seminar10/seminar10.html#data-for-neural-networks-teaching-computers-to-read-numbers",
    "title": "Neural Networks",
    "section": "Data for Neural Networks: Teaching Computers to Read Numbers",
    "text": "Data for Neural Networks: Teaching Computers to Read Numbers\nLet‚Äôs start our journey into neural networks with an exciting challenge: teaching a computer to read handwritten numbers! We‚Äôll build this step by step using Python, starting with the basics and working our way up to something quite impressive.\nThink of this like teaching a child to recognize numbers - we‚Äôll start by teaching our computer to recognize just one number (zero), and then build up to recognizing all digits from 0 to 9.\n\n\n\nMNIS\n\n\nWe‚Äôll use a famous collection of handwritten numbers called the MNIST dataset. Imagine asking 70,000 different people to write down numbers - that‚Äôs what this dataset is! Each number is written on a small 28 x 28 grid (like graph paper), where each square (or pixel) is shaded in grayscale from white (0) to black (255)."
  },
  {
    "objectID": "seminars/seminar10/seminar10.html#getting-started-with-mnist",
    "href": "seminars/seminar10/seminar10.html#getting-started-with-mnist",
    "title": "Neural Networks",
    "section": "Getting Started with MNIST",
    "text": "Getting Started with MNIST\nJust like we need data from experiments in physics, we need data to train our neural network. Fortunately, other scientists have already collected this data for us:\n\nLoading Our Training Data\n\n\n\n\n\n\nHere, X contains all our images, and y contains the correct answer for each image (which number it is). Let‚Äôs look at one:\n\n\n\n\n\n\n\n\nMaking the Data Easier to Work With\nJust like we often normalize measurements in physics experiments (like dividing by the maximum value), we‚Äôll normalize our image data to be between 0 and 1:\n\n\n\n\n\n\n\n\nPreparing Our Training and Testing Sets\nFor now, we‚Äôll start simple: we‚Äôll just teach our network to recognize zeros. We‚Äôll mark zeros with a 1 and all other numbers with a 0:\n\n\n\n\n\n\nLike any good scientific experiment, we need both training data (to teach the network) and testing data (to check how well it learned). We‚Äôll use: - 60,000 images for training - 10,000 images for testing\n\n\n\n\n\n\nFinally, we shuffle our training data (like shuffling flashcards when studying):\n\n\n\n\n\n\nLet‚Äôs check our work by looking at one of our training images:\n\n\n\n\n\n\nTry looking at different images (change the number 39 above) until you find a zero - its label should be 1!"
  },
  {
    "objectID": "seminars/seminar10/seminar10.html#a-single-neuron",
    "href": "seminars/seminar10/seminar10.html#a-single-neuron",
    "title": "Neural Networks",
    "section": "A Single Neuron",
    "text": "A Single Neuron\nThe basic building block of any neural network is an artificial neuron. Similar to neurons in the human brain that process incoming signals and decide whether to fire or not, an artificial neuron processes numerical inputs through mathematical operations to produce a single output value. The following diagram shows a simple artificial neuron with two input values:\n\n\nUnderstanding Forward Propagation\nAn artificial neuron processes information in three distinct steps that together form what is called forward propagation:\n\nInput Weighting Each input value gets multiplied by a weight parameter. These weights determine how much influence each input has on the final output, similar to how synapses in biological neurons can be stronger or weaker. For two inputs, this operation looks like:\n\n\\[\\begin{eqnarray}\nx_{1}\\rightarrow x_{1} w_{1}\\\\\nx_{2}\\rightarrow x_{2} w_{2}\n\\end{eqnarray}\\]\n\nBias Addition After weighting the inputs, a bias value \\(b\\) is added to the sum. The bias helps the neuron learn by shifting the weighted sum up or down, making it easier or harder for the neuron to produce a strong output signal:\n\n\\[\\begin{equation}\nx_{1} w_{1}+ x_{2} w_{2}+b\n\\end{equation}\\]\n\nActivation Function The final step applies an activation function \\(\\sigma()\\) to the weighted sum plus bias. This function introduces non-linearity into the network, allowing it to learn complex patterns:\n\n\\[\\begin{equation}\ny=\\sigma( x_{1} w_{1}+ x_{2} w_{2}+b)\n\\end{equation}\\]\nThe activation function used in this example is called the sigmoid function. This function is particularly useful because it takes any input number (positive or negative, large or small) and transforms it into an output between 0 and 1. This property makes the sigmoid function ideal for tasks where the output should represent a probability or a binary decision.\nFor mathematical convenience, the above steps can be written more compactly using vector notation:\n\\[\\begin{equation*}\n\\hat{y} = \\sigma(w^{\\rm T} x + b)\\ .\n\\end{equation*}\\]\nThe sigmoid function has the following mathematical form: \\[\\begin{equation*}\n\\sigma(z) = \\frac{1}{1+{\\rm e}^{-z}}\\ .\n\\end{equation*}\\]\nHere is a Python implementation of the sigmoid function:\n\n\n\n\n\n\nThe following code visualizes how the sigmoid function transforms input values:\n\n\n\n\n\n\nTo understand how these components work together, consider a simple example with two inputs. Given:\n\\[\\begin{eqnarray}\nw=[0,1]\\\\\nb=4\n\\end{eqnarray}\\]\nAnd input values:\n\\[\\begin{eqnarray}\nx=[2,3]\n\\end{eqnarray}\\]\nThe computation becomes:\n\\[\\begin{equation}\ny=f(w\\cdot x+b)=f(7)=0.999\n\\end{equation}\\]\nThis process of moving from inputs to output through these mathematical operations is called forward propagation. The goal is to extend this concept to create a network capable of processing images, which requires 784 inputs (one for each pixel in a 28 x 28 image) and producing meaningful outputs.\nFor computational efficiency, these calculations can be performed on multiple inputs simultaneously using matrix operations. The forward pass equation becomes:\n\\[\\begin{equation*}\n\\hat{y} = \\sigma(w^{\\rm T} X + b)\\ .\n\\end{equation*}\\]\nIn this matrix form, \\(\\hat{y}\\) represents a vector of outputs rather than a single value. The implementation splits this computation into two parts:\n\nCalculate the weighted sum: Z = np.matmul(W.T, X) + b\nApply the activation function: A = sigmoid(Z)\n\nThis separation into distinct steps makes the code clearer and prepares for the more complex calculations needed in the backward propagation phase.\n\n\nLoss Function: Measuring How Wrong We Are\nNow that our network can make predictions, we need a way to measure how accurate those predictions are. Just like we measure error in physics experiments, we need to measure the error (or ‚Äúloss‚Äù) in our neural network‚Äôs predictions.\nThe simplest way would be to use mean squared error, which you‚Äôve seen before in data fitting:\n\\[\\begin{equation}\nMSE(y,\\hat{y})=\\frac{1}{n}\\sum_{i=1}^{n}(y-\\hat{y})^2\n\\end{equation}\\]\nHere, \\(y\\) is the true value (what we know is correct) and \\(\\hat{y}\\) is our network‚Äôs prediction.\nHowever, for this type of classification problem, we‚Äôll use a different measure called cross-entropy. For a single training example, it looks like this:\n\\[\\begin{equation*}\nL(y,\\hat{y}) = -y\\log(\\hat{y})-(1-y)\\log(1-\\hat{y})\\ .\n\\end{equation*}\\]\nWhen we have many training examples (\\(m\\) of them), we take the average:\n\\[\\begin{equation*}\nL(Y,\\hat{Y}) = -\\frac{1}{m}\\sum_{i = 0}^{m}y^{(i)}\\log(\\hat{y}^{(i)})-(1-y^{(i)})\\log(1-\\hat{y}^{(i)})\\ .\n\\end{equation*}\\]\nHere‚Äôs how we implement this in code:"
  },
  {
    "objectID": "seminars/seminar10/seminar10.html#training-the-network-making-our-network-learn",
    "href": "seminars/seminar10/seminar10.html#training-the-network-making-our-network-learn",
    "title": "Neural Networks",
    "section": "Training the Network: Making Our Network Learn",
    "text": "Training the Network: Making Our Network Learn\nThink of training a neural network like teaching a student - we need to: 1. See how well they‚Äôre doing (measure the loss) 2. Give feedback on what to improve 3. Let them practice and improve\n\nBackward Propagation: Learning from Mistakes\nJust like we adjust our aim when throwing a ball based on how far we missed, our network needs to adjust its weights and biases based on its errors. The loss function depends on all our weights and biases:\n\\[\nL(w_{1},w_{2},w_{3},\\ldots ,b_{1},b_{2},b_{3},\\ldots)\n\\]\nTo improve, we need to know how changing each weight affects our error. We can find this using partial derivatives:\n\\[\n\\frac{\\partial L}{\\partial w_j}\n\\]\nThis tells us ‚Äúif we change weight \\(w_j\\) a little bit, how much will our error change?‚Äù This process of calculating how to adjust weights based on errors is called back propagation.\nCalculating How to Improve\nLet‚Äôs break this down into steps. For a single image, we can follow how changes flow through the network: \\[\\begin{align*}\nz &= w^{\\rm T} x + b\\ , \\\\\n\\hat{y} &= \\sigma(z)\\ , \\\\\nL(y,\\hat{y}) &= -y\\log(\\hat{y})-(1-y)\\log(1-\\hat{y})\\ .\n\\end{align*}\\]\nUsing the chain rule from calculus:\n\\[\\begin{align*}\n\\frac{\\partial L}{\\partial w_j} = \\frac{\\partial L}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial z}\\frac{\\partial z}{\\partial w_j}\n\\end{align*}\\] \nAfter working through the calculus (which we won‚Äôt detail here), we get three parts, each representing how different components of our network affect the final loss:\nFirst, we calculate how the loss changes with respect to our prediction (\\(\\hat{y}\\)):\n\\(\\partial L/\\partial\\hat{y}\\): \\[\\begin{align*}\n\\frac{\\partial L}{\\partial\\hat{y}} &= \\frac{\\hat{y} - y}{\\hat{y}(1-\\hat{y})}\n\\end{align*}\\]\nNext, we find how our prediction changes with respect to the weighted input (\\(z\\)). This is just the derivative of the sigmoid function:\n\\(\\partial \\hat{y}/\\partial z\\): \\[\\begin{align*}\n\\frac{\\partial }{\\partial z}\\sigma(z) &= \\hat{y}(1-\\hat{y})\n\\end{align*}\\]\nFinally, we calculate how the weighted input changes with respect to each weight. This is simply the corresponding input value:\n\\(\\partial z/\\partial w_j\\): \\[\\begin{align*}\n\\frac{\\partial }{\\partial w_j}(w^{\\rm T} x + b) &= x_j\n\\end{align*}\\]\nWhen we multiply these three components together using the chain rule, something remarkable happens - most terms cancel out, leaving us with this elegantly simple result: \\[\\begin{align*}\n\\frac{\\partial L}{\\partial w_j} = (\\hat{y} - y)x_j\\ .\n\\end{align*}\\]\nThis tells us that the adjustment to each weight should be proportional to both the prediction error (\\(\\hat{y} - y\\)) and the input value (\\(x_j\\)).\nWhen dealing with multiple training examples, we need to average these gradients:\n\\[\\begin{align*}\n\\frac{\\partial L}{\\partial w} = \\frac{1}{m} X(\\hat{y} - y)^{\\rm T}\\ .\n\\end{align*}\\]\nThe bias term follows a similar pattern. For a single example: \\[\\begin{align*}\n\\frac{\\partial L}{\\partial b} = (\\hat{y} - y)\\ .\n\\end{align*}\\]\nAnd for multiple training examples, we average the gradients: \\[\\begin{align*}\n\\frac{\\partial L}{\\partial b} = \\frac{1}{m}\\sum_{i=1}^{m}{(\\hat{y}^{(i)} - y^{(i)})}\\ .\n\\end{align*}\\]\nIn our code, these mathematical formulas translate directly into matrix operations: dW = (1/m) * np.matmul(X, (A-Y).T) and db = (1/m)*np.sum(A-Y, axis=1, keepdims=True).\n\n\nStochastic Gradient Descent: Teaching Our Network to Learn\nNow comes the exciting part - making our network learn! Just like how you adjust your throw when playing catch based on whether you threw too far or too short, our network needs to adjust its weights and biases based on its mistakes.\nWe‚Äôll use a learning method called stochastic gradient descent (SGD). Don‚Äôt let the fancy name scare you - it‚Äôs actually quite simple! Think of it like walking down a hill:\n\nLook around to see which way is steepest downhill (that‚Äôs the gradient)\nTake a small step in that direction\nRepeat until you reach the bottom\n\nMathematically, we update each weight using this formula:\n\\[\nw\\leftarrow w-\\eta\\frac{\\partial L}{\\partial w}\n\\]\nHere, \\(\\eta\\) (eta) is called the learning rate - it controls how big our steps are:\n\nToo large: We might overshoot the bottom\nToo small: Learning will take forever\n\nThe term \\(\\frac{\\partial L}{\\partial w}\\) tells us which direction to step:\n\nIf positive: The weight is too large, so decrease it\nIf negative: The weight is too small, so increase it\n\nWe do the same thing for the bias \\(b\\). Each complete pass through all our training data is called an ‚Äúepoch‚Äù.\nPhysics Connection: This is similar to finding the minimum of a potential well - we follow the direction where the potential decreases most rapidly!\n\n\nBuilding and Training Our First Network\nLet‚Äôs put everything together to create a network that can recognize handwritten numbers. We‚Äôll train it for 200 epochs (learning cycles) and watch how the loss decreases:\n\n\n\n\n\n\n\n\nEvaluating Our Network: How Well Did We Do?\nJust like in physics experiments, we need ways to measure how well our model performs. One powerful tool is the confusion matrix. Think of it as a report card for our network:\n\n\n\nconfusion_matrix\n\n\nThe confusion matrix shows: - True Positives (TP): We predicted ‚Äúyes‚Äù and were right - False Positives (FP): We predicted ‚Äúyes‚Äù but were wrong - True Negatives (TN): We predicted ‚Äúno‚Äù and were right - False Negatives (FN): We predicted ‚Äúno‚Äù but were wrong\nLet‚Äôs calculate this for our network:\n\n\n\n\n\n\n\n\nTesting Individual Images\nLet‚Äôs see our network in action! We can test it on individual images:"
  },
  {
    "objectID": "seminars/seminar10/seminar10.html#network-with-hidden-layers",
    "href": "seminars/seminar10/seminar10.html#network-with-hidden-layers",
    "title": "Neural Networks",
    "section": "Network with Hidden Layers",
    "text": "Network with Hidden Layers\nIn our example above, we just had an input layer and a single output neuron. More complex neural networks are containing many layers between the input layer and the output layer. These inbetween layers are called hidden layers. Here is a simple example of a neural network with a single hidden layer.\n\n\n\nhidden\n\n\nSo we have now and input layer with 784 inputs that are connected to 64 units in the hidden layer and 1 neuron in the output layer. We will not go through the derivations of all the formulas for the forward and backward passes this time. The code is a simple extension of what we did before and I hope easy to read.\n\n\n\n\n\n\nTo judge the newtork quality we do use again the confusion matrix."
  },
  {
    "objectID": "seminars/seminar10/seminar10.html#multiclass-network",
    "href": "seminars/seminar10/seminar10.html#multiclass-network",
    "title": "Neural Networks",
    "section": "Multiclass Network",
    "text": "Multiclass Network\nSo far we did only classify if the number we feed to the network is just a 0 or not. We would like to recognize the different number now and therefore need a multiclass network. Each number is then a class and per class, we have multiple realizations of handwritten numbers. We therefore have to create an output layer, which is not only containing a single neuron, but 10 neurons. Each of these neuron can output a value between 0 and 1. Whenever the output is 1, the index of the neuron represents the number predicted.\nThe output array\n[0,1,0,0,0,0,0,0,0,0]\nwould therefore correspond to the value 1.\nFor this purpose, we need to reload the right labels.\n\n\n\n\n\n\nThen we‚Äôll one-hot encode MNIST‚Äôs labels, to get a 10 x 70,000 array.\n\n\n\n\n\n\n\n\n\n\n\n\nWe also seperate into trainging and testing data\n\n\n\n\n\n\n\n\n\n\n\n\n\nChanges to the model\nOK, so let‚Äôs consider what changes we need to make to the model itself.\n\nForward Pass\nOnly the last layer of our network is changing. To add the softmax, we have to replace our lone, final node with a 10 unit layer. Its final activations are the exponentials of its z-values, normalized across all ten such exponentials. So instead of just computing \\(\\sigma(z)\\), we compute the activation for each unit \\(i\\) using the softmax function: \\[\\begin{align*}\n\\sigma(z)_i = \\frac{{\\rm e}^{z_i}}{\\sum_{j=0}^9{\\rm e}^{z_i}}\\ .\n\\end{align*}\\]\nSo, in our vectorized code, the last line of forward propagation will be A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0).\n\n\nLoss Function\nOur loss function now has to generalize to more than two classes. The general formula for \\(n\\) classes is: \\[\\begin{align*}\nL(y,\\hat{y}) = -\\sum_{i=0}^n y_i\\log(\\hat{y}_i)\\ .\n\\end{align*}\\] Averaging over \\(m\\) training examples this becomes: \\[\\begin{align*}\nL(y,\\hat{y}) = -\\frac{1}{m}\\sum_{j=0}^m\\sum_{i=0}^n y_i^{(i)}\\log(\\hat{y}_i^{(i)})\\ .\n\\end{align*}\\]\nSo let‚Äôs define:\n\n\n\n\n\n\n\n\nBack Propagation\nLuckily it turns out that back propagation isn‚Äôt really affected by the switch to a softmax. A softmax generalizes the sigmoid activiation we‚Äôve been using, and in such a way that the code we wrote earlier still works. We could verify this by deriving: \\[\\begin{align*}\n\\frac{\\partial L}{\\partial z_i} = \\hat{y}_i - y_i\\ .\n\\end{align*}\\]\nBut we won‚Äôt walk through the steps here. Let‚Äôs just go ahead and build our final network.\n\n\n\nBuild and Train\nAs we have now more weights and classes, the training takes longer and we actually need also more episodes to achieve a good accuracy.\n\n\n\n\n\n\nLet‚Äôs see how we did:\n\n\n\n\n\n\n\nModel performance\n\n\n\n\n\n\nWe are at 84% accuray across all digits, which could be of course better. We may now plot image and the corresponding prediction."
  },
  {
    "objectID": "seminars/seminar10/seminar10.html#test-the-model",
    "href": "seminars/seminar10/seminar10.html#test-the-model",
    "title": "Neural Networks",
    "section": "Test the model",
    "text": "Test the model"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "Summary\nIn summary, this book has no content whatsoever."
  },
  {
    "objectID": "course-info/schedule.html",
    "href": "course-info/schedule.html",
    "title": "Zeitplan f√ºr den Kurs",
    "section": "",
    "text": "Der Kurs wird w√∂chentlich mit dem Zeitplan der Vorlesungen aktualisiert. Erwarten Sie also jeden\nDienstag ab 15. Oktober 2024, jeweils um 11:15\neine neue Vorlesung und eine neue Aufgabe ab 13:00.\nErfahrungsgem√§√ü werden die besten Ergebnisse erzielt, wenn Sie bei den Vorlesungen im H√∂rsaal anwesend sind. Das gesamte Material wird jedoch auch online zur Verf√ºgung stehen, sodass Sie jederzeit darauf zugreifen k√∂nnen, um zu lernen, wann immer es Ihnen passt."
  },
  {
    "objectID": "course-info/resources.html",
    "href": "course-info/resources.html",
    "title": "Quellen",
    "section": "",
    "text": "Vor allen anderen Sachen ist es f√ºr diesen Kurs wichtig, dass Sie w√§hrend der Vorlesung einen Internetzugang haben. Wir werden viele Beispiele und √úbungen durchf√ºhren, die auf Online-Ressourcen verweisen.\nInnerhalb der Universit√§t k√∂nnen die Eduroam-Netzwerke verwendet werden. Die notwendigen Profildaten k√∂nnen Sie hier finden.\nWeiterhin gibt es eine Menge weiterer gut strukturierter Ressourcen zu Python im Netz. Nachfolgend finden Sie nur eine sehr kleine Auswahl.",
    "crumbs": [
      "üìã Course Info",
      "Ressourcen"
    ]
  },
  {
    "objectID": "course-info/resources.html#hypothesis-annotation-tool",
    "href": "course-info/resources.html#hypothesis-annotation-tool",
    "title": "Quellen",
    "section": "Hypothesis Annotation Tool",
    "text": "Hypothesis Annotation Tool\n\nInvite to the Hypothesis annotation tool",
    "crumbs": [
      "üìã Course Info",
      "Ressourcen"
    ]
  },
  {
    "objectID": "course-info/resources.html#molecular-nanophotonics-group",
    "href": "course-info/resources.html#molecular-nanophotonics-group",
    "title": "Quellen",
    "section": "Molecular Nanophotonics Group",
    "text": "Molecular Nanophotonics Group\n\nMolecular Nanophotonics Group Website\nHypothesis Annotation Tool Invite",
    "crumbs": [
      "üìã Course Info",
      "Ressourcen"
    ]
  },
  {
    "objectID": "course-info/resources.html#additional-advanced-courses",
    "href": "course-info/resources.html#additional-advanced-courses",
    "title": "Quellen",
    "section": "Additional Advanced Courses",
    "text": "Additional Advanced Courses\n\nRosenow Group (Theory), Master Course on Statistical Mechanics of Deep Learning",
    "crumbs": [
      "üìã Course Info",
      "Ressourcen"
    ]
  },
  {
    "objectID": "course-info/resources.html#python-documentation",
    "href": "course-info/resources.html#python-documentation",
    "title": "Quellen",
    "section": "Python Documentation",
    "text": "Python Documentation\n\nPython\nMatplotlib\nPandas",
    "crumbs": [
      "üìã Course Info",
      "Ressourcen"
    ]
  },
  {
    "objectID": "course-info/resources.html#python-tutorials",
    "href": "course-info/resources.html#python-tutorials",
    "title": "Quellen",
    "section": "Python Tutorials",
    "text": "Python Tutorials\n\nIntroduction to Python for Science\nNice MatPlotLib tutorial",
    "crumbs": [
      "üìã Course Info",
      "Ressourcen"
    ]
  },
  {
    "objectID": "course-info/resources.html#julia-tutorial",
    "href": "course-info/resources.html#julia-tutorial",
    "title": "Quellen",
    "section": "Julia Tutorial",
    "text": "Julia Tutorial\n\nJulia Programming Language",
    "crumbs": [
      "üìã Course Info",
      "Ressourcen"
    ]
  },
  {
    "objectID": "course-info/resources.html#pluto-notebook",
    "href": "course-info/resources.html#pluto-notebook",
    "title": "Quellen",
    "section": "Pluto NoteBook",
    "text": "Pluto NoteBook\n\nPluto GitHub Webpage",
    "crumbs": [
      "üìã Course Info",
      "Ressourcen"
    ]
  },
  {
    "objectID": "course-info/how_to_quiz.html",
    "href": "course-info/how_to_quiz.html",
    "title": "Interactive Python Quiz",
    "section": "",
    "text": "In this quiz, you can write and execute Python code directly in your browser.\n\n\nWrite a function square(n) that returns the square of a number.\n\n\n\n\n\n# Write your Python code here\ndef square(n):\n    return n * n\n\nprint(square(5))\n\n\nRun Code"
  },
  {
    "objectID": "course-info/how_to_quiz.html#question-1",
    "href": "course-info/how_to_quiz.html#question-1",
    "title": "Interactive Python Quiz",
    "section": "",
    "text": "Write a function square(n) that returns the square of a number.\n\n\n\n\n\n# Write your Python code here\ndef square(n):\n    return n * n\n\nprint(square(5))\n\n\nRun Code"
  },
  {
    "objectID": "course-info/intructors.html",
    "href": "course-info/intructors.html",
    "title": "Instructor",
    "section": "",
    "text": "Linn√©str. 5, 04103 Leipzig\nOffice: 322\nPhone: +49 341 97 32571\nEmail: lastname@physik.uni-leipzig.de",
    "crumbs": [
      "üìã Course Info",
      "Vorlesender"
    ]
  },
  {
    "objectID": "course-info/intructors.html#prof.-dr.-frank-cichos",
    "href": "course-info/intructors.html#prof.-dr.-frank-cichos",
    "title": "Instructor",
    "section": "",
    "text": "Linn√©str. 5, 04103 Leipzig\nOffice: 322\nPhone: +49 341 97 32571\nEmail: lastname@physik.uni-leipzig.de",
    "crumbs": [
      "üìã Course Info",
      "Vorlesender"
    ]
  },
  {
    "objectID": "NiceFigures.html",
    "href": "NiceFigures.html",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "import matplotlib as mpl\nimport matplotlib.font_manager as font_manager\nfrom IPython.core.display import HTML\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom directory_tree import display_tree\n\n\nplt.rcParams.update({'font.size': 12,\n                     'lines.linewidth': 1,\n                     'lines.markersize': 10,\n                     'axes.labelsize': 11,\n                     'xtick.labelsize' : 10,\n                     'ytick.labelsize' : 10,\n                     'xtick.top' : True,\n                     'xtick.direction' : 'in',\n                     'ytick.right' : True,\n                     'ytick.direction' : 'in',\n                     'figure.dpi': 150})\n\n\ndef get_size(w,h):\n    return((w/2.54,h/2.54))\n\n\nplt.figure(figsize=get_size(7,5))\nx=np.linspace(0,np.pi*4,200)\n\nplt.plot(x,np.sin(x),color='k')\nplt.xlabel(r\"angle $\\theta$ in [rad]\")\nplt.ylabel(r\"$\\sin(\\theta)$\")\nplt.tight_layout()\nplt.savefig(\"figure_example3.png\", transparent=True,dpi=300)\nplt.show()\n\n\n\n\n\n\n\n\n\ndef set_size(w,h, ax=None):\n    \"\"\" w, h: width, height in inches \"\"\"\n    if not ax: ax=plt.gca()\n    l = ax.figure.subplotpars.left\n    r = ax.figure.subplotpars.right\n    t = ax.figure.subplotpars.top\n    b = ax.figure.subplotpars.bottom\n    figw = float(w)/(r-l)\n    figh = float(h)/(t-b)\n    ax.figure.set_size_inches(figw, figh)\n\nfig=plt.figure(dpi=150)\nax=plt.axes()\nax.plot(x,np.sin(x),color='k')\nax.set_xlabel(r\"angle $\\theta$ in [rad]\")\nax.set_ylabel(r\"$\\sin(\\theta)$\")\nset_size(3,2)\nplt.savefig(\"figure_example2.pdf\", bbox_inches='tight', transparent=True)\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom IPython.display import HTML, display\n\ndef make_html(fontname):\n    return \"&lt;p&gt;{font}: &lt;span style='font-family:{font}; font-size: 24px;'&gt;{font}&lt;/p&gt;\".format(font=fontname)\n\ncode = \"\\n\".join([make_html(font) for font in sorted(set([f.name for f in font_manager.fontManager.ttflist]))])\n\ndisplay(HTML(\"&lt;div style='column-count: 2;'&gt;{}&lt;/div&gt;\".format(code)))\n\n.Aqua Kana: .Aqua Kana\n.CJK Symbols Fallback HK: .CJK Symbols Fallback HK\n.Keyboard: .Keyboard\n.New York: .New York\n.SF Arabic: .SF Arabic\n.SF Arabic Rounded: .SF Arabic Rounded\n.SF Armenian: .SF Armenian\n.SF Armenian Rounded: .SF Armenian Rounded\n.SF Camera: .SF Camera\n.SF Compact: .SF Compact\n.SF Compact Rounded: .SF Compact Rounded\n.SF Georgian: .SF Georgian\n.SF Georgian Rounded: .SF Georgian Rounded\n.SF Hebrew: .SF Hebrew\n.SF Hebrew Rounded: .SF Hebrew Rounded\n.SF NS Mono: .SF NS Mono\n.SF NS Rounded: .SF NS Rounded\n.SF Soft Numeric: .SF Soft Numeric\n.ThonburiUI: .ThonburiUI\nAcademy Engraved LET: Academy Engraved LET\nAgency FB: Agency FB\nAl Bayan: Al Bayan\nAl Nile: Al Nile\nAl Tarikh: Al Tarikh\nAmerican Typewriter: American Typewriter\nAndale Mono: Andale Mono\nApple Braille: Apple Braille\nApple Chancery: Apple Chancery\nApple SD Gothic Neo: Apple SD Gothic Neo\nApple Symbols: Apple Symbols\nAppleGothic: AppleGothic\nAppleMyungjo: AppleMyungjo\nArial: Arial\nArial Black: Arial Black\nArial Hebrew: Arial Hebrew\nArial Narrow: Arial Narrow\nArial Rounded MT Bold: Arial Rounded MT Bold\nArial Unicode MS: Arial Unicode MS\nArtifakt Element: Artifakt Element\nAthelas: Athelas\nAvenir: Avenir\nAvenir Next: Avenir Next\nAvenir Next Condensed: Avenir Next Condensed\nAyuthaya: Ayuthaya\nBaghdad: Baghdad\nBangla MN: Bangla MN\nBangla Sangam MN: Bangla Sangam MN\nBaskerville: Baskerville\nBeirut: Beirut\nBig Caslon: Big Caslon\nBodoni 72: Bodoni 72\nBodoni 72 Oldstyle: Bodoni 72 Oldstyle\nBodoni 72 Smallcaps: Bodoni 72 Smallcaps\nBodoni Ornaments: Bodoni Ornaments\nBradley Hand: Bradley Hand\nBrush Script MT: Brush Script MT\nChalkboard: Chalkboard\nChalkboard SE: Chalkboard SE\nChalkduster: Chalkduster\nCharter: Charter\nCochin: Cochin\nComic Sans MS: Comic Sans MS\nCopperplate: Copperplate\nCorsiva Hebrew: Corsiva Hebrew\nCourier: Courier\nCourier New: Courier New\nDIN Alternate: DIN Alternate\nDIN Condensed: DIN Condensed\nDamascus: Damascus\nDecoType Naskh: DecoType Naskh\nDejaVu Sans: DejaVu Sans\nDejaVu Sans Display: DejaVu Sans Display\nDejaVu Sans Mono: DejaVu Sans Mono\nDejaVu Serif: DejaVu Serif\nDejaVu Serif Display: DejaVu Serif Display\nDevanagari MT: Devanagari MT\nDevanagari Sangam MN: Devanagari Sangam MN\nDidot: Didot\nDiwan Kufi: Diwan Kufi\nDiwan Thuluth: Diwan Thuluth\nEuphemia UCAS: Euphemia UCAS\nFarah: Farah\nFarisi: Farisi\nFutura: Futura\nGalvji: Galvji\nGeeza Pro: Geeza Pro\nGeneva: Geneva\nGeorgia: Georgia\nGill Sans: Gill Sans\nGujarati MT: Gujarati MT\nGujarati Sangam MN: Gujarati Sangam MN\nGurmukhi MN: Gurmukhi MN\nGurmukhi MT: Gurmukhi MT\nGurmukhi Sangam MN: Gurmukhi Sangam MN\nHeiti TC: Heiti TC\nHelvetica: Helvetica\nHelvetica Neue: Helvetica Neue\nHerculanum: Herculanum\nHiragino Maru Gothic Pro: Hiragino Maru Gothic Pro\nHiragino Mincho ProN: Hiragino Mincho ProN\nHiragino Sans: Hiragino Sans\nHiragino Sans GB: Hiragino Sans GB\nHoefler Text: Hoefler Text\nITF Devanagari: ITF Devanagari\nImpact: Impact\nInaiMathi: InaiMathi\nIowan Old Style: Iowan Old Style\nKailasa: Kailasa\nKannada MN: Kannada MN\nKannada Sangam MN: Kannada Sangam MN\nKefa: Kefa\nKhmer MN: Khmer MN\nKhmer Sangam MN: Khmer Sangam MN\nKohinoor Bangla: Kohinoor Bangla\nKohinoor Devanagari: Kohinoor Devanagari\nKohinoor Gujarati: Kohinoor Gujarati\nKohinoor Telugu: Kohinoor Telugu\nKokonor: Kokonor\nKrungthep: Krungthep\nKufiStandardGK: KufiStandardGK\nLao MN: Lao MN\nLao Sangam MN: Lao Sangam MN\nLucida Grande: Lucida Grande\nLuminari: Luminari\nMalayalam MN: Malayalam MN\nMalayalam Sangam MN: Malayalam Sangam MN\nMarion: Marion\nMarker Felt: Marker Felt\nMenlo: Menlo\nMicrosoft Sans Serif: Microsoft Sans Serif\nMishafi: Mishafi\nMishafi Gold: Mishafi Gold\nMonaco: Monaco\nMshtakan: Mshtakan\nMukta Mahee: Mukta Mahee\nMuna: Muna\nMyanmar MN: Myanmar MN\nMyanmar Sangam MN: Myanmar Sangam MN\nNadeem: Nadeem\nNew Peninim MT: New Peninim MT\nNoteworthy: Noteworthy\nNoto Nastaliq Urdu: Noto Nastaliq Urdu\nNoto Sans Adlam: Noto Sans Adlam\nNoto Sans Armenian: Noto Sans Armenian\nNoto Sans Avestan: Noto Sans Avestan\nNoto Sans Bamum: Noto Sans Bamum\nNoto Sans Bassa Vah: Noto Sans Bassa Vah\nNoto Sans Batak: Noto Sans Batak\nNoto Sans Bhaiksuki: Noto Sans Bhaiksuki\nNoto Sans Brahmi: Noto Sans Brahmi\nNoto Sans Buginese: Noto Sans Buginese\nNoto Sans Buhid: Noto Sans Buhid\nNoto Sans Canadian Aboriginal: Noto Sans Canadian Aboriginal\nNoto Sans Carian: Noto Sans Carian\nNoto Sans Caucasian Albanian: Noto Sans Caucasian Albanian\nNoto Sans Chakma: Noto Sans Chakma\nNoto Sans Cham: Noto Sans Cham\nNoto Sans Coptic: Noto Sans Coptic\nNoto Sans Cuneiform: Noto Sans Cuneiform\nNoto Sans Cypriot: Noto Sans Cypriot\nNoto Sans Duployan: Noto Sans Duployan\nNoto Sans Egyptian Hieroglyphs: Noto Sans Egyptian Hieroglyphs\nNoto Sans Elbasan: Noto Sans Elbasan\nNoto Sans Glagolitic: Noto Sans Glagolitic\nNoto Sans Gothic: Noto Sans Gothic\nNoto Sans Gunjala Gondi: Noto Sans Gunjala Gondi\nNoto Sans Hanifi Rohingya: Noto Sans Hanifi Rohingya\nNoto Sans Hanunoo: Noto Sans Hanunoo\nNoto Sans Hatran: Noto Sans Hatran\nNoto Sans Imperial Aramaic: Noto Sans Imperial Aramaic\nNoto Sans Inscriptional Pahlavi: Noto Sans Inscriptional Pahlavi\nNoto Sans Inscriptional Parthian: Noto Sans Inscriptional Parthian\nNoto Sans Javanese: Noto Sans Javanese\nNoto Sans Kaithi: Noto Sans Kaithi\nNoto Sans Kannada: Noto Sans Kannada\nNoto Sans Kayah Li: Noto Sans Kayah Li\nNoto Sans Kharoshthi: Noto Sans Kharoshthi\nNoto Sans Khojki: Noto Sans Khojki\nNoto Sans Khudawadi: Noto Sans Khudawadi\nNoto Sans Lepcha: Noto Sans Lepcha\nNoto Sans Limbu: Noto Sans Limbu\nNoto Sans Linear A: Noto Sans Linear A\nNoto Sans Linear B: Noto Sans Linear B\nNoto Sans Lisu: Noto Sans Lisu\nNoto Sans Lycian: Noto Sans Lycian\nNoto Sans Lydian: Noto Sans Lydian\nNoto Sans Mahajani: Noto Sans Mahajani\nNoto Sans Mandaic: Noto Sans Mandaic\nNoto Sans Manichaean: Noto Sans Manichaean\nNoto Sans Marchen: Noto Sans Marchen\nNoto Sans Masaram Gondi: Noto Sans Masaram Gondi\nNoto Sans Meetei Mayek: Noto Sans Meetei Mayek\nNoto Sans Mende Kikakui: Noto Sans Mende Kikakui\nNoto Sans Meroitic: Noto Sans Meroitic\nNoto Sans Miao: Noto Sans Miao\nNoto Sans Modi: Noto Sans Modi\nNoto Sans Mongolian: Noto Sans Mongolian\nNoto Sans Mro: Noto Sans Mro\nNoto Sans Multani: Noto Sans Multani\nNoto Sans Myanmar: Noto Sans Myanmar\nNoto Sans NKo: Noto Sans NKo\nNoto Sans Nabataean: Noto Sans Nabataean\nNoto Sans New Tai Lue: Noto Sans New Tai Lue\nNoto Sans Newa: Noto Sans Newa\nNoto Sans Ol Chiki: Noto Sans Ol Chiki\nNoto Sans Old Hungarian: Noto Sans Old Hungarian\nNoto Sans Old Italic: Noto Sans Old Italic\nNoto Sans Old North Arabian: Noto Sans Old North Arabian\nNoto Sans Old Permic: Noto Sans Old Permic\nNoto Sans Old Persian: Noto Sans Old Persian\nNoto Sans Old South Arabian: Noto Sans Old South Arabian\nNoto Sans Old Turkic: Noto Sans Old Turkic\nNoto Sans Oriya: Noto Sans Oriya\nNoto Sans Osage: Noto Sans Osage\nNoto Sans Osmanya: Noto Sans Osmanya\nNoto Sans Pahawh Hmong: Noto Sans Pahawh Hmong\nNoto Sans Palmyrene: Noto Sans Palmyrene\nNoto Sans Pau Cin Hau: Noto Sans Pau Cin Hau\nNoto Sans PhagsPa: Noto Sans PhagsPa\nNoto Sans Phoenician: Noto Sans Phoenician\nNoto Sans Psalter Pahlavi: Noto Sans Psalter Pahlavi\nNoto Sans Rejang: Noto Sans Rejang\nNoto Sans Samaritan: Noto Sans Samaritan\nNoto Sans Saurashtra: Noto Sans Saurashtra\nNoto Sans Sharada: Noto Sans Sharada\nNoto Sans Siddham: Noto Sans Siddham\nNoto Sans Sora Sompeng: Noto Sans Sora Sompeng\nNoto Sans Sundanese: Noto Sans Sundanese\nNoto Sans Syloti Nagri: Noto Sans Syloti Nagri\nNoto Sans Syriac: Noto Sans Syriac\nNoto Sans Tagalog: Noto Sans Tagalog\nNoto Sans Tagbanwa: Noto Sans Tagbanwa\nNoto Sans Tai Le: Noto Sans Tai Le\nNoto Sans Tai Tham: Noto Sans Tai Tham\nNoto Sans Tai Viet: Noto Sans Tai Viet\nNoto Sans Takri: Noto Sans Takri\nNoto Sans Thaana: Noto Sans Thaana\nNoto Sans Tifinagh: Noto Sans Tifinagh\nNoto Sans Tirhuta: Noto Sans Tirhuta\nNoto Sans Ugaritic: Noto Sans Ugaritic\nNoto Sans Vai: Noto Sans Vai\nNoto Sans Wancho: Noto Sans Wancho\nNoto Sans Warang Citi: Noto Sans Warang Citi\nNoto Sans Yi: Noto Sans Yi\nNoto Serif Ahom: Noto Serif Ahom\nNoto Serif Balinese: Noto Serif Balinese\nNoto Serif Hmong Nyiakeng: Noto Serif Hmong Nyiakeng\nNoto Serif Myanmar: Noto Serif Myanmar\nNoto Serif Yezidi: Noto Serif Yezidi\nOptima: Optima\nOriya MN: Oriya MN\nOriya Sangam MN: Oriya Sangam MN\nPT Mono: PT Mono\nPT Sans: PT Sans\nPT Serif: PT Serif\nPT Serif Caption: PT Serif Caption\nPalatino: Palatino\nPapyrus: Papyrus\nParty LET: Party LET\nPhosphate: Phosphate\nPlantagenet Cherokee: Plantagenet Cherokee\nRaanana: Raanana\nRockwell: Rockwell\nSF Grandezza: SF Grandezza\nSTIX Two Math: STIX Two Math\nSTIX Two Text: STIX Two Text\nSTIXGeneral: STIXGeneral\nSTIXIntegralsD: STIXIntegralsD\nSTIXIntegralsSm: STIXIntegralsSm\nSTIXIntegralsUp: STIXIntegralsUp\nSTIXIntegralsUpD: STIXIntegralsUpD\nSTIXIntegralsUpSm: STIXIntegralsUpSm\nSTIXNonUnicode: STIXNonUnicode\nSTIXSizeFiveSym: STIXSizeFiveSym\nSTIXSizeFourSym: STIXSizeFourSym\nSTIXSizeOneSym: STIXSizeOneSym\nSTIXSizeThreeSym: STIXSizeThreeSym\nSTIXSizeTwoSym: STIXSizeTwoSym\nSTIXVariants: STIXVariants\nSana: Sana\nSathu: Sathu\nSavoye LET: Savoye LET\nSeravek: Seravek\nShree Devanagari 714: Shree Devanagari 714\nSignPainter: SignPainter\nSilom: Silom\nSinhala MN: Sinhala MN\nSinhala Sangam MN: Sinhala Sangam MN\nSkia: Skia\nSnell Roundhand: Snell Roundhand\nSongti SC: Songti SC\nSukhumvit Set: Sukhumvit Set\nSuperclarendon: Superclarendon\nSymbol: Symbol\nSystem Font: System Font\nTahoma: Tahoma\nTamil MN: Tamil MN\nTamil Sangam MN: Tamil Sangam MN\nTelugu MN: Telugu MN\nTelugu Sangam MN: Telugu Sangam MN\nThonburi: Thonburi\nTimes: Times\nTimes New Roman: Times New Roman\nTrattatello: Trattatello\nTrebuchet MS: Trebuchet MS\nVerdana: Verdana\nWaseem: Waseem\nWebdings: Webdings\nWingdings: Wingdings\nWingdings 2: Wingdings 2\nWingdings 3: Wingdings 3\nZapf Dingbats: Zapf Dingbats\nZapfino: Zapfino\ncmb10: cmb10\ncmex10: cmex10\ncmmi10: cmmi10\ncmr10: cmr10\ncmss10: cmss10\ncmsy10: cmsy10\ncmtt10: cmtt10\n\n\n\ncmfont = font_manager.FontProperties(fname=mpl.get_data_path() + '/fonts/ttf/cmr10.ttf')\nplt.rcParams.update({'font.size': 12,\n                     'axes.titlesize': 12,\n                     'axes.labelsize': 12,\n                     'axes.labelpad': 12,\n                     'lines.linewidth': 1,\n                     'lines.markersize': 10,\n                     'xtick.labelsize' : 10,\n                     'ytick.labelsize' : 10,\n                     'xtick.top' : True,\n                     'xtick.direction' : 'in',\n                     'ytick.right' : True,\n                     'ytick.direction' : 'in',\n                     'font.family' : 'serif',\n                     'font.serif' : cmfont.get_name(),\n                     \"axes.formatter.use_mathtext\": True,\n                     'text.usetex': True,\n                     'mathtext.fontset' : 'cm'\n                    })\n\n\nx=np.linspace(0,np.pi,100)\n\n\nplt.figure(figsize=get_size(6,5),dpi=150)\nplt.plot(x,np.sin(x))\nplt.xlabel(r\"velocity $v$\")\nplt.ylabel(r\"position $r$\")\nplt.show()"
  },
  {
    "objectID": "course-info/assignments.html",
    "href": "course-info/assignments.html",
    "title": "√úbungsaufgaben",
    "section": "",
    "text": "Es werden insgesamt 6 √úbungsbl√§tter zur Verf√ºgung gestellt. Die √úbungsbl√§tter sind Teil der Pr√ºfungsleistung! Die √úbungsbl√§tter werden nicht benotet, aber die Bearbeitung ist f√ºr den erfolgreichen Abschluss des Moduls erforderlich. Genauere Informationen zur Wertung der √úbungsbl√§tter finden sie auf der Seite zur Pr√ºfung.\n\nBereitstellung und Abgabe der √úbungsaufgaben\n\nVer√∂ffentlichung: Jeden Dienstag um 13:00 Uhr\nAbgabefrist: Bis zum folgenden Dienstag um 12:00 Uhr\nBearbeitungszeitraum: Eine Woche (minus eine Stunde)\nPlattform: Moodle der Universit√§t\n\nSowohl f√ºr die Bearbeitung als auch f√ºr die Abgabe\n\nWichtig:\n\nAchten Sie auf die p√ºnktliche Abgabe innerhalb der angegebenen Frist.\nNach Ablauf der Frist ist die Aufgabe nicht mehr verf√ºgbar und kann nicht mehr eingereicht werden.\nAchten sie darauf, dass mehrmaliges Einreichen derselben Aufgabe die Punktzahl mit jeder Einreichung um 10% verringert",
    "crumbs": [
      "üìã Course Info",
      "√úbungsaufgaben"
    ]
  },
  {
    "objectID": "course-info/website.html",
    "href": "course-info/website.html",
    "title": "Diese Webseiten",
    "section": "",
    "text": "Diese Webseiten\nDiese Website enth√§lt alle Informationen, die f√ºr unseren Kurs Einf√ºhrung in die Modellierung Physikalischer Prozesse erforderlich sind. Sie werden hier jede Woche eine neue Vorlesung und eine neue Aufgabe finden. Die Vorlesungshefte werden von Videos begleitet, die den Inhalt der Vorlesung auf Englisch erkl√§ren, aber Sie k√∂nnen auch mit dem Lesen auskommen. Die Vorlesungen in Person, werden auf Deutsch stattfinden. Von diesen Webseiten aus werden Sie zu verschiedenen Ressourcen gef√ºhrt, die Sie nutzen k√∂nnen, um das Programmieren in Python zu lernen. Dabei werden wir einige gro√üartige Tools aus dem Internet nutzen, wie\n\nGoogle Colab Dienst, um auch Jupyter Notebooks (https://colab.research.google.com) zu hosten. Das Google Colab-Projekt bietet eine n√ºtzliche Umgebung zur gemeinsamen Nutzung von Notebooks.\n\n\n\ngoogle colab screen\n\n\nWenn Sie die folgende Website besuchen, werden Sie an mehreren Stellen das folgende Symbol sehen.\n![Substitution Name1]\nDieses Symbol zeigt an, dass diese Webseite auf einem Jupyter Notebook basiert. Anstatt nur die Website zu betrachten, k√∂nnen Sie auf das Symbol klicken und der Google Colab-Dienst wird ge√∂ffnet, damit Sie das Notizbuch interaktiv nutzen k√∂nnen. Google Colab √∂ffnet sich viel schneller als myBinder, aber die Notizb√ºcher sind f√ºr die Arbeit mit myBinder gemacht und nicht alle Funktionen funktionieren mit Colab. Ich arbeite jedoch an der Kompatibilit√§t.\nGitHub and GitHub Pages Dienst zum Hosting von Websites (https://github.com). GitHub ist ein gro√üartiger Ort, um Ihre kollaborativen Coding-Projekte einschlie√ülich Versionskontrolle zu hosten. In der oberen rechten Ecke finden Sie auch einen Link zum GitHub-Repository, in dem die Notebooks gehostet werden.\n\n\n\ngithub screen\n\n\nAnaconda Jupyter package f√ºr Notebooks auf dem eigenen Computer (https://www.anaconda.com/distribution/). Das Paket anaconda stellt Ihnen die Jupyter Notebook-Umgebung einschlie√ülich Python zur Verf√ºgung. Wenn Sie Jupyter zu Hause ohne Online-Zugang verwenden m√∂chten, ist dies ein gutes Paket zur Installation.\n\n\n\nanaconda screen"
  },
  {
    "objectID": "course-info/exam.html",
    "href": "course-info/exam.html",
    "title": "Pr√ºfung",
    "section": "",
    "text": "Die Pr√ºfungsleistung in diesem Modul ist eine Portfolio-Pr√ºfung und besteht aus zwei Portfolio-Teilen:\n\n√úbungsaufgaben:\n\n6 Serien von Aufgaben, die nicht benotet werden\nMindestens 50% der Gesamtpunktzahl aller √úbungsaufgaben muss erreicht werden\n\nZwei Tests:\n\nJeder Test dauert 45 Minuten\nFinden in Person w√§hrend zwei √úbungsseminare statt\nDie Tests werden vorher angek√ºndigt\nDie Punkte beider Tests werden addiert und ergeben eine Gesamtnote\n\n\nWichtige Hinweise: - Beide Portfolio-Teile (√úbungsaufgaben und Tests) m√ºssen bestanden werden, um das Modul erfolgreich abzuschlie√üen. - Die Gesamtnote der Tests entspricht der Abschlussnote des Moduls.",
    "crumbs": [
      "üìã Course Info",
      "Pr√ºfungen"
    ]
  },
  {
    "objectID": "course-info/exam.html#erstteilnehmer",
    "href": "course-info/exam.html#erstteilnehmer",
    "title": "Pr√ºfung",
    "section": "",
    "text": "Die Pr√ºfungsleistung in diesem Modul ist eine Portfolio-Pr√ºfung und besteht aus zwei Portfolio-Teilen:\n\n√úbungsaufgaben:\n\n6 Serien von Aufgaben, die nicht benotet werden\nMindestens 50% der Gesamtpunktzahl aller √úbungsaufgaben muss erreicht werden\n\nZwei Tests:\n\nJeder Test dauert 45 Minuten\nFinden in Person w√§hrend zwei √úbungsseminare statt\nDie Tests werden vorher angek√ºndigt\nDie Punkte beider Tests werden addiert und ergeben eine Gesamtnote\n\n\nWichtige Hinweise: - Beide Portfolio-Teile (√úbungsaufgaben und Tests) m√ºssen bestanden werden, um das Modul erfolgreich abzuschlie√üen. - Die Gesamtnote der Tests entspricht der Abschlussnote des Moduls.",
    "crumbs": [
      "üìã Course Info",
      "Pr√ºfungen"
    ]
  },
  {
    "objectID": "course-info/exam.html#wiederholer",
    "href": "course-info/exam.html#wiederholer",
    "title": "Pr√ºfung",
    "section": "Wiederholer",
    "text": "Wiederholer\nF√ºr Studierende, die das Modul bereits im WS 2023/24 belegt haben und nun wiederholen:\n\n√úbungsaufgaben:\n\nWenn Sie im WS 2023/24 ‚â•50% erreicht haben: Nur Projektabgabe erforderlich\nWenn Sie im WS 2023/24 &lt;50% erreicht haben: √úbungsaufgaben und Projektabgabe erforderlich\nEine Liste der erreichten √úbungspunkte wird im internen Bereich der MONA Webseite ver√∂ffentlicht\n\nEinschreibung:\n\nErneute Moduleinschreibung nicht notwendig\nF√ºr Moodle-Zugang: Kontaktieren Sie Andrea Kramer per Uni-E-Mail\n\nProjektabgabe f√ºr Wiederholer:\n\nFrist: 12.03.2025, 13:00 Uhr\nSp√§tere Abgaben werden nicht ber√ºcksichtigt",
    "crumbs": [
      "üìã Course Info",
      "Pr√ºfungen"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EMPP",
    "section": "",
    "text": "Willkommen zum Kurs Einf√ºhrung in die Modellierung Physikalischer Prozesse!\nDie Programmiersprache Python ist f√ºr alle Arten von wissenschaftlichen und technischen Aufgaben n√ºtzlich. Sie k√∂nnen mit ihr Daten analysieren und darstellen. Sie k√∂nnen mit ihr auch wissenschaftliche Probleme numerisch l√∂sen, die analytisch nur schwer oder gar nicht zu l√∂sen sind. Python ist frei verf√ºgbar und wurde aufgrund seines modularen Aufbaus um eine nahezu unendliche Anzahl von Modulen f√ºr verschiedene Zwecke erweitert.\nDieser Kurs soll Sie in die Programmierung mit Python einf√ºhren. Er richtet sich eher an den Anf√§nger, wir hoffen aber, dass er auch f√ºr Fortgeschrittene interessant ist. Wir beginnen den Kurs mit einer Einf√ºhrung in die Jupyter Notebook-Umgebung, die wir w√§hrend des gesamten Kurses verwenden werden. Danach werden wir eine Einf√ºhrung in Python geben und Ihnen einige grundlegende Funktionen zeigen, wie z.B. das Plotten und Analysieren von Daten durch Kurvenanpassung, das Lesen und Schreiben von Dateien, was einige der Aufgaben sind, die Ihnen w√§hrend Ihres Physikstudiums begegnen werden. Wir zeigen Ihnen auch einige fortgeschrittene Themen wie die Animation in Jupyter und die Simulation von physikalischen Prozessen in\n\nMechanik\nElektrostatik\nWellen\nOptik\n\nFalls am Ende des Kurses Zeit bleibt, werden wir auch einen Blick auf Verfahren des maschinellen Lernens werfen, das mittlerweile auch in der Physik zu einem wichtigen Werkzeug geworden ist.\nWir werden keine umfassende Liste von numerischen Simulationsschemata pr√§sentieren, sondern die Beispiele nutzen, um Ihre Neugierde zu wecken. Da es leichte Unterschiede in der Syntax der verschiedenen Python-Versionen gibt, werden wir uns im Folgenden immer auf den Python 3-Standard beziehen.\nDer Kurs wird auf Deutsch gehalten werden. Die Webseiten, die Sie f√ºr den √úberblick zu Python zur Verf√ºgung gestellt bekommen, werden allerdings auf Englisch sein. √úbungsaufgaben werden werden auf Deutsch gestellt.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "WEEK1_CHANGES.html",
    "href": "WEEK1_CHANGES.html",
    "title": "Week 1 Restructuring - Summary of Changes",
    "section": "",
    "text": "Week 1 has been restructured to follow a physics-first, motivation-driven approach. Instead of overwhelming students with Jupyter documentation before they write any code, we now get them coding and visualizing physics immediately.\n\n\n\n\n\nWeek 1:\n  1. Jupyter Notebooks (350+ lines of technical documentation)\n  2. Variables & Numbers (technical Python details)\n  3. [Plotting way later in Week 4]\n\n\n\nWeek 1: Your First Physics Code\n  1. Setup: Jupyter Notebooks (streamlined, 15-20 min)\n  2. Quick Win: Plotting Your First Graph (moved from Week 4)\n  3. Variables & Numbers (What You Just Used) - with physics context\n\n\n\n\n\n\nBefore: - 350+ lines of comprehensive Jupyter documentation - Deep technical details about kernels, JSON format, nbconvert - Advanced features (magic commands, debugger, widgets) upfront - No physics motivation - Reference manual style\nAfter: - Streamlined to ~200 lines focused on getting started - Physics motivation first: Shows what students will create - Embedded example: Projectile motion plot in the introduction - First calculation: Gravitational potential energy (within 5 minutes) - Essential keyboard shortcuts only - Advanced topics moved to collapsible sections - Clear ‚ÄúNext Steps‚Äù pointing to plotting lesson\nKey Improvements: - ‚úÖ Shows physics visualization in first 2 minutes - ‚úÖ Students run their first physics calculation within 10 minutes - ‚úÖ Two types of cells explained with physics examples - ‚úÖ Practice exercise: Calculate kinetic energy with proper LaTeX documentation - ‚úÖ ‚ÄúTry It Yourself‚Äù section with free fall calculation - ‚úÖ Quick reference card for essential shortcuts\n\n\n\nStatus: File kept as-is but moved up in the course sequence\nWhy: This file already contains good content. By moving it to Week 1 position #2, students: - See visual results immediately (motivation!) - Learn by doing before learning theory - Understand why variables matter (they‚Äôll use them in plots)\nCurrent content includes: - Simple line plots - Anatomy of matplotlib figures - Axis labels, legends, error bars - Multiple plot types (scatter, histogram) - Physics examples throughout\n\n\n\nBefore: - Generic programming tutorial - No physics context - Technical type explanations - Reserved keywords warning with lambda mention but no physics connection\nAfter: - Opens with ‚ÄúWhat You Just Used!‚Äù - references the previous plotting lesson - Physics-motivated variable naming conventions - Real physics examples for each number type: - Integers: Particle counts, quantum numbers, timesteps - Floats: Mass, energy, position (with physical constants) - Complex: Wave functions, AC circuits, quantum mechanics - Physics constant examples (c, h, electron mass) - Scientific notation with astronomy/atomic scales - Practical tips: ‚ÄúWhy lambda_ instead of lambda‚Äù for wavelength - Complex numbers tied to quantum mechanics applications\nKey Improvements: - ‚úÖ Every code example uses physics variables (mass, velocity, energy) - ‚úÖ Explains when to use each type in physics context - ‚úÖ Real constants: Speed of light, Planck‚Äôs constant, electron mass - ‚úÖ Complex number section connected to quantum mechanics and wave functions - ‚úÖ Practical examples: Photon energy calculation, wave function probability density - ‚úÖ Tips box: ‚ÄúWhich Type for Physics?‚Äù table\n\n\n\n\n\n\nStudents see a beautiful physics plot in the first 2 minutes, not after hours of documentation reading.\n\n\n\nThey plot data before understanding all the theory. This is how physicists actually work in research.\n\n\n\nVariables are taught after students have used them, so they understand why they‚Äôre learning it.\n\n\n\nEvery example uses physics notation, units, and real-world constants. Students see Python as a physics tool, not just a programming language.\n\n\n\nWeek 1 focuses on three things: 1. Running Jupyter 2. Making plots 3. Understanding what variables are\nAdvanced topics (kernels, magic commands, etc.) are in collapsible sections for reference.\n\n\n\n\n\n\nHour 1: Read about Jupyter architecture\nHour 2: Learn about kernels and JSON format\nHour 3: Markdown syntax\nHour 4: Still reading documentation...\nHour 5: Finally start programming\nResult: \"This is boring, when do we do physics?\"\n\n\n\nMinute 1: \"Wow, look at that physics plot!\"\nMinute 10: \"I just calculated potential energy!\"\nMinute 30: \"I made my first graph!\"\nMinute 45: \"I can plot projectile motion!\"\nHour 1-2: \"Now I understand what variables are and why they matter\"\nResult: \"This is cool, I'm doing real physics!\"\n\n\n\n\n\nContent quality: All original material is preserved (moved to collapsible sections)\nDepth: Advanced users can still access detailed Jupyter documentation\nFile organization: All files remain in their original locations\nLater weeks: Only Week 1 structure was modified\n\n\n\n\nThe proposed _quarto.yml section for Week 1:\n- section: \"üöÄ Week 1: Your First Physics Code\"\n  contents:\n    - text: \"Setup: Jupyter Notebooks\"\n      href: lectures/lecture01/01-lecture01.qmd\n    - text: \"Quick Win: Plotting Your First Graph\"\n      href: lectures/lecture04/04-plotting.qmd  # MOVED UP!\n    - text: \"Variables & Numbers (What You Just Used)\"\n      href: lectures/lecture01/02-lecture01.qmd\n\n\n\n\n\n\n‚Üë Higher engagement in first week\n‚Üë ‚ÄúAha!‚Äù moments earlier\n‚Üë Confidence: ‚ÄúI can do this!‚Äù\n‚Üì Dropout in first two weeks\n\n\n\n\n\nBetter retention (learned in context)\nStronger connection between Python and physics\nMore independent experimentation\nClearer understanding of ‚Äúwhy‚Äù before ‚Äúhow‚Äù\n\n\n\n\n\n\n\n\nAdd a ‚ÄúWeek Overview‚Äù page for each week with learning objectives\nCreate a cheat sheet PDF for quick reference during lectures\nAdd more interactive exercises throughout the lessons\nInclude short video demos (5 min) showing key concepts\nStudent project gallery to showcase what‚Äôs possible\n\n\n\n\nApply similar principles to other weeks: - Week 2: Lead with Brownian Motion before classes - Week 4: Show planetary orbits before ODEs - Week 8: Wave animations before Fourier theory\n\n\n\n\n\nlectures/lecture01/01-lecture01.qmd - Completely restructured\nlectures/lecture01/02-lecture01.qmd - Enhanced with physics context\nlectures/lecture04/04-plotting.qmd - Moved up (content unchanged)\n\n\n\n\nBefore deploying to students: 1. ‚úÖ Run through Week 1 as a student would 2. ‚úÖ Time each section (should be ~2 hours total) 3. ‚úÖ Test all code examples in fresh Jupyter environment 4. ‚úÖ Verify all links work in the rendered Quarto site 5. ‚úÖ Ask a colleague to review for clarity\n\n\n\nBottom line: Week 1 is now exciting, physics-focused, and gets students creating visualizations in minutes instead of hours. The technical details are still there, but they‚Äôre optional reference material instead of required reading.\nStudents will leave Week 1 thinking ‚ÄúI can do physics with code!‚Äù instead of ‚ÄúI learned about Jupyter‚Äôs architecture.‚Äù\n\nDate of Changes: Generated during course restructuring consultation Philosophy: Physics first, technical details second, engagement always"
  },
  {
    "objectID": "WEEK1_CHANGES.html#overview",
    "href": "WEEK1_CHANGES.html#overview",
    "title": "Week 1 Restructuring - Summary of Changes",
    "section": "",
    "text": "Week 1 has been restructured to follow a physics-first, motivation-driven approach. Instead of overwhelming students with Jupyter documentation before they write any code, we now get them coding and visualizing physics immediately."
  },
  {
    "objectID": "WEEK1_CHANGES.html#new-week-1-structure",
    "href": "WEEK1_CHANGES.html#new-week-1-structure",
    "title": "Week 1 Restructuring - Summary of Changes",
    "section": "",
    "text": "Week 1:\n  1. Jupyter Notebooks (350+ lines of technical documentation)\n  2. Variables & Numbers (technical Python details)\n  3. [Plotting way later in Week 4]\n\n\n\nWeek 1: Your First Physics Code\n  1. Setup: Jupyter Notebooks (streamlined, 15-20 min)\n  2. Quick Win: Plotting Your First Graph (moved from Week 4)\n  3. Variables & Numbers (What You Just Used) - with physics context"
  },
  {
    "objectID": "WEEK1_CHANGES.html#key-changes-made",
    "href": "WEEK1_CHANGES.html#key-changes-made",
    "title": "Week 1 Restructuring - Summary of Changes",
    "section": "",
    "text": "Before: - 350+ lines of comprehensive Jupyter documentation - Deep technical details about kernels, JSON format, nbconvert - Advanced features (magic commands, debugger, widgets) upfront - No physics motivation - Reference manual style\nAfter: - Streamlined to ~200 lines focused on getting started - Physics motivation first: Shows what students will create - Embedded example: Projectile motion plot in the introduction - First calculation: Gravitational potential energy (within 5 minutes) - Essential keyboard shortcuts only - Advanced topics moved to collapsible sections - Clear ‚ÄúNext Steps‚Äù pointing to plotting lesson\nKey Improvements: - ‚úÖ Shows physics visualization in first 2 minutes - ‚úÖ Students run their first physics calculation within 10 minutes - ‚úÖ Two types of cells explained with physics examples - ‚úÖ Practice exercise: Calculate kinetic energy with proper LaTeX documentation - ‚úÖ ‚ÄúTry It Yourself‚Äù section with free fall calculation - ‚úÖ Quick reference card for essential shortcuts\n\n\n\nStatus: File kept as-is but moved up in the course sequence\nWhy: This file already contains good content. By moving it to Week 1 position #2, students: - See visual results immediately (motivation!) - Learn by doing before learning theory - Understand why variables matter (they‚Äôll use them in plots)\nCurrent content includes: - Simple line plots - Anatomy of matplotlib figures - Axis labels, legends, error bars - Multiple plot types (scatter, histogram) - Physics examples throughout\n\n\n\nBefore: - Generic programming tutorial - No physics context - Technical type explanations - Reserved keywords warning with lambda mention but no physics connection\nAfter: - Opens with ‚ÄúWhat You Just Used!‚Äù - references the previous plotting lesson - Physics-motivated variable naming conventions - Real physics examples for each number type: - Integers: Particle counts, quantum numbers, timesteps - Floats: Mass, energy, position (with physical constants) - Complex: Wave functions, AC circuits, quantum mechanics - Physics constant examples (c, h, electron mass) - Scientific notation with astronomy/atomic scales - Practical tips: ‚ÄúWhy lambda_ instead of lambda‚Äù for wavelength - Complex numbers tied to quantum mechanics applications\nKey Improvements: - ‚úÖ Every code example uses physics variables (mass, velocity, energy) - ‚úÖ Explains when to use each type in physics context - ‚úÖ Real constants: Speed of light, Planck‚Äôs constant, electron mass - ‚úÖ Complex number section connected to quantum mechanics and wave functions - ‚úÖ Practical examples: Photon energy calculation, wave function probability density - ‚úÖ Tips box: ‚ÄúWhich Type for Physics?‚Äù table"
  },
  {
    "objectID": "WEEK1_CHANGES.html#pedagogical-rationale",
    "href": "WEEK1_CHANGES.html#pedagogical-rationale",
    "title": "Week 1 Restructuring - Summary of Changes",
    "section": "",
    "text": "Students see a beautiful physics plot in the first 2 minutes, not after hours of documentation reading.\n\n\n\nThey plot data before understanding all the theory. This is how physicists actually work in research.\n\n\n\nVariables are taught after students have used them, so they understand why they‚Äôre learning it.\n\n\n\nEvery example uses physics notation, units, and real-world constants. Students see Python as a physics tool, not just a programming language.\n\n\n\nWeek 1 focuses on three things: 1. Running Jupyter 2. Making plots 3. Understanding what variables are\nAdvanced topics (kernels, magic commands, etc.) are in collapsible sections for reference."
  },
  {
    "objectID": "WEEK1_CHANGES.html#student-experience-comparison",
    "href": "WEEK1_CHANGES.html#student-experience-comparison",
    "title": "Week 1 Restructuring - Summary of Changes",
    "section": "",
    "text": "Hour 1: Read about Jupyter architecture\nHour 2: Learn about kernels and JSON format\nHour 3: Markdown syntax\nHour 4: Still reading documentation...\nHour 5: Finally start programming\nResult: \"This is boring, when do we do physics?\"\n\n\n\nMinute 1: \"Wow, look at that physics plot!\"\nMinute 10: \"I just calculated potential energy!\"\nMinute 30: \"I made my first graph!\"\nMinute 45: \"I can plot projectile motion!\"\nHour 1-2: \"Now I understand what variables are and why they matter\"\nResult: \"This is cool, I'm doing real physics!\""
  },
  {
    "objectID": "WEEK1_CHANGES.html#what-hasnt-changed",
    "href": "WEEK1_CHANGES.html#what-hasnt-changed",
    "title": "Week 1 Restructuring - Summary of Changes",
    "section": "",
    "text": "Content quality: All original material is preserved (moved to collapsible sections)\nDepth: Advanced users can still access detailed Jupyter documentation\nFile organization: All files remain in their original locations\nLater weeks: Only Week 1 structure was modified"
  },
  {
    "objectID": "WEEK1_CHANGES.html#integration-with-your-existing-_quarto.yml",
    "href": "WEEK1_CHANGES.html#integration-with-your-existing-_quarto.yml",
    "title": "Week 1 Restructuring - Summary of Changes",
    "section": "",
    "text": "The proposed _quarto.yml section for Week 1:\n- section: \"üöÄ Week 1: Your First Physics Code\"\n  contents:\n    - text: \"Setup: Jupyter Notebooks\"\n      href: lectures/lecture01/01-lecture01.qmd\n    - text: \"Quick Win: Plotting Your First Graph\"\n      href: lectures/lecture04/04-plotting.qmd  # MOVED UP!\n    - text: \"Variables & Numbers (What You Just Used)\"\n      href: lectures/lecture01/02-lecture01.qmd"
  },
  {
    "objectID": "WEEK1_CHANGES.html#expected-outcomes",
    "href": "WEEK1_CHANGES.html#expected-outcomes",
    "title": "Week 1 Restructuring - Summary of Changes",
    "section": "",
    "text": "‚Üë Higher engagement in first week\n‚Üë ‚ÄúAha!‚Äù moments earlier\n‚Üë Confidence: ‚ÄúI can do this!‚Äù\n‚Üì Dropout in first two weeks\n\n\n\n\n\nBetter retention (learned in context)\nStronger connection between Python and physics\nMore independent experimentation\nClearer understanding of ‚Äúwhy‚Äù before ‚Äúhow‚Äù"
  },
  {
    "objectID": "WEEK1_CHANGES.html#next-steps-optional",
    "href": "WEEK1_CHANGES.html#next-steps-optional",
    "title": "Week 1 Restructuring - Summary of Changes",
    "section": "",
    "text": "Add a ‚ÄúWeek Overview‚Äù page for each week with learning objectives\nCreate a cheat sheet PDF for quick reference during lectures\nAdd more interactive exercises throughout the lessons\nInclude short video demos (5 min) showing key concepts\nStudent project gallery to showcase what‚Äôs possible\n\n\n\n\nApply similar principles to other weeks: - Week 2: Lead with Brownian Motion before classes - Week 4: Show planetary orbits before ODEs - Week 8: Wave animations before Fourier theory"
  },
  {
    "objectID": "WEEK1_CHANGES.html#files-modified",
    "href": "WEEK1_CHANGES.html#files-modified",
    "title": "Week 1 Restructuring - Summary of Changes",
    "section": "",
    "text": "lectures/lecture01/01-lecture01.qmd - Completely restructured\nlectures/lecture01/02-lecture01.qmd - Enhanced with physics context\nlectures/lecture04/04-plotting.qmd - Moved up (content unchanged)"
  },
  {
    "objectID": "WEEK1_CHANGES.html#testing-recommendations",
    "href": "WEEK1_CHANGES.html#testing-recommendations",
    "title": "Week 1 Restructuring - Summary of Changes",
    "section": "",
    "text": "Before deploying to students: 1. ‚úÖ Run through Week 1 as a student would 2. ‚úÖ Time each section (should be ~2 hours total) 3. ‚úÖ Test all code examples in fresh Jupyter environment 4. ‚úÖ Verify all links work in the rendered Quarto site 5. ‚úÖ Ask a colleague to review for clarity"
  },
  {
    "objectID": "WEEK1_CHANGES.html#summary",
    "href": "WEEK1_CHANGES.html#summary",
    "title": "Week 1 Restructuring - Summary of Changes",
    "section": "",
    "text": "Bottom line: Week 1 is now exciting, physics-focused, and gets students creating visualizations in minutes instead of hours. The technical details are still there, but they‚Äôre optional reference material instead of required reading.\nStudents will leave Week 1 thinking ‚ÄúI can do physics with code!‚Äù instead of ‚ÄúI learned about Jupyter‚Äôs architecture.‚Äù\n\nDate of Changes: Generated during course restructuring consultation Philosophy: Physics first, technical details second, engagement always"
  },
  {
    "objectID": "seminars/seminar10/seminar11.html#what-are-neural-networks",
    "href": "seminars/seminar10/seminar11.html#what-are-neural-networks",
    "title": "Neural Networks",
    "section": "What are Neural Networks?",
    "text": "What are Neural Networks?\nNeural networks are computational models inspired by how our brains process information. Just like our brain consists of interconnected neurons that process and transmit signals, artificial neural networks consist of mathematical ‚Äúneurons‚Äù that process numerical information. They‚Äôre particularly powerful for:\n\nRecognizing patterns in data\nMaking predictions\nClassifying information\nSolving complex problems"
  },
  {
    "objectID": "seminars/seminar10/seminar11.html#why-neural-networks-in-physics",
    "href": "seminars/seminar10/seminar11.html#why-neural-networks-in-physics",
    "title": "Neural Networks",
    "section": "Why Neural Networks in Physics?",
    "text": "Why Neural Networks in Physics?\nIn physics, we often encounter problems where:\n\nTraditional mathematical models become too complex\nWe need to analyze large amounts of experimental data\nWe want to make predictions based on incomplete information\n\nNeural networks help us with these challenges! Some real-world applications include:\n\nParticle physics: Identifying particles in detector data\nAstronomy: Classifying galaxies\nMaterials science: Predicting material properties\nQuantum mechanics: Solving many-body problems\nBiological physics: Modeling neural activity\nActive matter: Predicting collective behavior"
  },
  {
    "objectID": "seminars/seminar10/seminar11.html#a-single-neuron-building-our-first-ai-unit",
    "href": "seminars/seminar10/seminar11.html#a-single-neuron-building-our-first-ai-unit",
    "title": "Neural Networks",
    "section": "A Single Neuron: Building Our First AI Unit",
    "text": "A Single Neuron: Building Our First AI Unit\n\nThe Big Picture\nBefore diving into the details, let‚Äôs understand what we‚Äôre trying to build. Imagine you‚Äôre creating a smart device that can recognize handwritten numbers. The most basic version of this device would be a single artificial neuron - think of it as an electronic version of a brain cell that can make simple yes/no decisions.\n\n\nFrom Biology to Mathematics\nJust like a biological neuron receives signals from other neurons, our artificial neuron processes numerical inputs through mathematical operations to produce a single output value.\nThe neuron performs three distinct steps:\n\nInput Weighting Each input value gets multiplied by a weight parameter:\n\n\\[\\begin{eqnarray}\nx_{1}\\rightarrow x_{1} w_{1}\\\\\nx_{2}\\rightarrow x_{2} w_{2}\n\\end{eqnarray}\\]\n\nBias Addition A bias value \\(b\\) is added to the weighted sum:\n\n\\[\\begin{equation}\nx_{1} w_{1}+ x_{2} w_{2}+b\n\\end{equation}\\]\n\nActivation Function The final step applies an activation function \\(\\sigma()\\):\n\n\\[\\begin{equation}\ny=\\sigma( x_{1} w_{1}+ x_{2} w_{2}+b)\n\\end{equation}\\]\nFor mathematical convenience, we can write this more compactly using vector notation:\n\\[\\begin{equation*}\n\\hat{y} = \\sigma(w^{\\rm T} x + b)\n\\end{equation*}\\]\nThe sigmoid function has the following mathematical form: \\[\\begin{equation*}\n\\sigma(z) = \\frac{1}{1+{\\rm e}^{-z}}\n\\end{equation*}\\]\nLet‚Äôs implement the sigmoid function and visualize how it transforms inputs:\n\n\n\n\n\n\n\n\n\n\n\n\nNow let‚Äôs see how a neuron processes inputs through these operations. Given:\n\\[\\begin{eqnarray}\nw=[0,1]\\\\\nb=4\n\\end{eqnarray}\\]\nAnd input values:\n\\[\\begin{eqnarray}\nx=[2,3]\n\\end{eqnarray}\\]\nThe computation becomes:\n\n\n\n\n\n\nFor computational efficiency, these calculations can be performed on multiple inputs simultaneously using matrix operations:\n\\[\\begin{equation*}\n\\hat{y} = \\sigma(w^{\\rm T} X + b)\n\\end{equation*}\\]"
  },
  {
    "objectID": "seminars/seminar10/seminar11.html#loss-function-measuring-our-networks-mistakes",
    "href": "seminars/seminar10/seminar11.html#loss-function-measuring-our-networks-mistakes",
    "title": "Neural Networks",
    "section": "Loss Function: Measuring Our Network‚Äôs Mistakes",
    "text": "Loss Function: Measuring Our Network‚Äôs Mistakes\n\nWhy Do We Need a Loss Function?\nJust like we need a way to measure error in physics experiments, we need a way to measure how wrong our neural network‚Äôs predictions are. The loss function serves this purpose - it tells us how far our predictions are from the true values.\n\n\nUnderstanding Cross-Entropy Loss\nWhile we could use simpler measures like mean squared error:\n\\[\\begin{equation}\nMSE(y,\\hat{y})=\\frac{1}{n}\\sum_{i=1}^{n}(y-\\hat{y})^2\n\\end{equation}\\]\nWe‚Äôll use a more sophisticated measure called cross-entropy loss. For a single training example, the formula is:\n\\[\\begin{equation*}\nL(y,\\hat{y}) = -y\\log(\\hat{y})-(1-y)\\log(1-\\hat{y})\n\\end{equation*}\\]\nPhysics Analogy: This is similar to entropy in thermodynamics - it measures the disorder or uncertainty in our predictions.\nWhen dealing with multiple training examples (\\(m\\) of them), we average the loss:\n\\[\\begin{equation*}\nL(Y,\\hat{Y}) = -\\frac{1}{m}\\sum_{i = 0}^{m}y^{(i)}\\log(\\hat{y}^{(i)})-(1-y^{(i)})\\log(1-\\hat{y}^{(i)})\n\\end{equation*}\\]\nLet‚Äôs implement this in code:\n\n\n\n\n\n\n\n\nTraining the Network: The Big Picture\nThe goal of training is to minimize this loss function. We do this by adjusting the weights and biases of our network. Let‚Äôs see how this works:\n\n\n\n\n\n\n\n\nBackward Propagation: Finding the Path to Improvement\nThe loss function \\(L\\) depends on all our weights and biases:\n\\[\nL(w_{1},w_{2},w_{3},\\ldots ,b_{1},b_{2},b_{3},\\ldots)\n\\]\nTo minimize the loss, we need to know how it changes when we adjust each weight. This is where partial derivatives come in:\n\\[\n\\frac{\\partial L}{\\partial w_j}\n\\]\nBreaking this down step by step, we use the chain rule:\n\\[\\begin{align*}\n\\frac{\\partial L}{\\partial w_j} = \\frac{\\partial L}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial z}\\frac{\\partial z}{\\partial w_j}\n\\end{align*}\\]\nLet‚Äôs calculate each term:\n\n\\(\\partial L/\\partial\\hat{y}\\): \\[\\begin{align*}\n\\frac{\\partial L}{\\partial\\hat{y}} &= \\frac{\\partial}{\\partial\\hat{y}}\\left(-y\\log(\\hat{y})-(1-y)\\log(1-\\hat{y})\\right) \\\\\n&= -\\frac{y}{\\hat{y}} +\\frac{(1 - y)}{1-\\hat{y}} \\\\\n&= \\frac{\\hat{y} - y}{\\hat{y}(1-\\hat{y})}\n\\end{align*}\\]\n\\(\\partial \\hat{y}/\\partial z\\): \\[\\begin{align*}\n\\frac{\\partial }{\\partial z}\\sigma(z) &= \\sigma(z)(1-\\sigma(z)) \\\\\n&= \\hat{y}(1-\\hat{y})\n\\end{align*}\\]\n\\(\\partial z/\\partial w_j\\): \\[\\begin{align*}\n\\frac{\\partial }{\\partial w_j}(w^{\\rm T} x + b) = x_j\n\\end{align*}\\]\n\nPutting it all together: \\[\\begin{align*}\n\\frac{\\partial L}{\\partial w_j} = (\\hat{y} - y)x_j\n\\end{align*}\\]\nFor multiple training examples, in vector form: \\[\\begin{align*}\n\\frac{\\partial L}{\\partial w} = \\frac{1}{m} X(\\hat{y} - y)^{\\rm T}\n\\end{align*}\\]\nSimilarly for the bias: \\[\\begin{align*}\n\\frac{\\partial L}{\\partial b} = \\frac{1}{m}\\sum_{i=1}^{m}{(\\hat{y}^{(i)} - y^{(i)})}\n\\end{align*}\\]\n\n\nInteractive Example: Watching Gradients\n\n\n\n\n\n\n\n\nKey Points to Remember:\n\nThe loss function measures prediction errors\nCross-entropy loss is particularly suitable for classification problems\nGradients tell us how to adjust weights and biases\nThe chain rule helps us compute these gradients efficiently"
  },
  {
    "objectID": "seminars/seminar10/seminar11.html#training-the-network-putting-it-all-together",
    "href": "seminars/seminar10/seminar11.html#training-the-network-putting-it-all-together",
    "title": "Neural Networks",
    "section": "Training the Network: Putting It All Together",
    "text": "Training the Network: Putting It All Together\n\nStochastic Gradient Descent (SGD)\nNow that we understand how to compute gradients, we can use them to train our network. The basic idea is simple: 1. Calculate how wrong we are (loss) 2. Calculate how to improve (gradients) 3. Take a small step in the right direction\nThis process is called Stochastic Gradient Descent (SGD). The mathematical update rule is:\n\\[\nw\\leftarrow w-\\eta\\frac{\\partial L}{\\partial w}\n\\]\nwhere \\(\\eta\\) is the learning rate - a small number that controls how big our improvement steps are.\nPhysics Analogy: This is similar to finding the minimum of a potential well. The gradient tells us which way is ‚Äúdownhill‚Äù, and we take small steps in that direction.\n\n\nBuilding Our First Complete Network\nLet‚Äôs implement a complete training loop. We‚Äôll use: - Learning rate \\(\\eta = 1\\) - 200 training epochs (complete passes through the data)\n\n\n\n\n\n\n\n\nEvaluating Our Network: The Confusion Matrix\nTo understand how well our network performs, we use a confusion matrix. This shows: - True Positives (TP): Correctly predicted positive cases - False Positives (FP): Incorrectly predicted positive cases - True Negatives (TN): Correctly predicted negative cases - False Negatives (FN): Incorrectly predicted negative cases\n\n\n\nconfusion_matrix\n\n\nLet‚Äôs evaluate our model on the test data:\n\n\n\n\n\n\n\n\nTesting Individual Predictions\nLet‚Äôs visualize how our network performs on a single test image:\n\n\n\n\n\n\n\n\nUnderstanding the Results:\n\nThe confusion matrix shows us where our model makes mistakes\nThe classification report gives us metrics like:\n\nPrecision: How many of our positive predictions were correct\nRecall: How many actual positive cases did we catch\nF1-score: A balanced measure of precision and recall\n\n\n\n\nKey Points to Remember:\n\nTraining is an iterative process of:\n\nForward propagation\nLoss calculation\nBackward propagation\nParameter updates\n\nThe learning rate controls how quickly we update our parameters\nWe evaluate performance using confusion matrices and classification metrics"
  },
  {
    "objectID": "seminars/seminar05/mdsim.html",
    "href": "seminars/seminar05/mdsim.html",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "",
    "text": "# %% Load  modules and initialize\nfrom typing_extensions import ParamSpec\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nimport matplotlib.patches as patches\nplt.rcParams.update({'font.size': 8,\n                     'lines.linewidth': 1,\n                     'lines.markersize': 10,\n                     'axes.labelsize': 10,\n                     'axes.titlesize': 10,\n                     'xtick.labelsize' : 10,\n                     'ytick.labelsize' : 10,\n                     'xtick.top' : True,\n                     'xtick.direction' : 'in',\n                     'ytick.right' : True,\n                     'ytick.direction' : 'in',\n                     'figure.facecolor' : 'white',})\n\ndef get_size(w,h):\n    return((w/2.54,h/2.54))\n\n\n# %% Load the atom class we did already in the previous seminar\nclass Atom:\n    def __init__(self, atom_id, atom_type, position, velocity=None, mass=None):\n        self.id = atom_id\n        self.type = atom_type\n        self.position = position\n        self.velocity = velocity if velocity is not None else np.random.randn(2)*20\n        self.mass = mass\n        self.force = np.zeros(2)\n\n\n    def add_force(self, force):\n        \"\"\"Add force contribution to total force on atom\"\"\"\n        self.force += force\n\n    def reset_force(self):\n        \"\"\"Reset force to zero at start of each step\"\"\"\n        self.force = np.zeros(2)\n\n    def update_position(self, dt):\n        \"\"\"First step of velocity Verlet: update position\"\"\"\n        self.position += self.velocity * dt + 0.5 * (self.force/self.mass) * dt**2\n\n    def update_velocity(self, dt, new_force):\n        \"\"\"Second step of velocity Verlet: update velocity using average force\"\"\"\n        self.velocity += 0.5 * (new_force + self.force)/self.mass * dt\n        self.force = new_force\n\n    def apply_periodic_boundaries(self, box_size):\n            \"\"\"Apply periodic boundary conditions\"\"\"\n            self.position = self.position % box_size\n\nclass ForceField:\n    def __init__(self):\n        self.parameters = {\n            'C': {'epsilon': 1.615, 'sigma': 1.36},\n            'H': {'epsilon': 1.0, 'sigma': 1.0 },\n            'O': {'epsilon': 1.846, 'sigma': 3.0},\n        }\n        self.bond_parameters = {\n            ('H', 'H'): {'k': 500.0, 'r0': 0.74},  # Example parameters for H2\n            ('O', 'H'): {'k': 550.0, 'r0': 0.96},  # Example parameters for OH bond\n        }\n        self.box_size = None\n\n    def calculate_bond_force(self, bond):\n        \"\"\"Calculate harmonic bond force\"\"\"\n        r = self.minimum_image_distance(bond.atom1.position, bond.atom2.position)\n        r_mag = np.linalg.norm(r)\n\n        # F = -k(r - r0)‚àô(r/|r|)\n        force_mag = -bond.k * (r_mag - bond.r0)\n        force = force_mag * r/r_mag\n        return force\n\n    def get_pair_parameters(self, type1, type2):\n        # Apply mixing rules when needed\n        eps1 = self.parameters[type1]['epsilon']\n        eps2 = self.parameters[type2]['epsilon']\n        sig1 = self.parameters[type1]['sigma']\n        sig2 = self.parameters[type2]['sigma']\n\n        # Lorentz-Berthelot mixing rules\n        epsilon = np.sqrt(eps1 * eps2)\n        sigma = (sig1 + sig2) / 2\n\n        return epsilon, sigma\n\n    def minimum_image_distance(self, pos1, pos2):\n        \"\"\"Calculate minimum image distance between two positions\"\"\"\n        delta = pos1 - pos2\n        # Apply minimum image convention\n        delta = delta - self.box_size * np.round(delta / self.box_size)\n        return delta\n\n    def calculate_lj_force(self, atom1, atom2):\n        epsilon, sigma = self.get_pair_parameters(atom1.type, atom2.type)\n        r = self.minimum_image_distance(atom1.position, atom2.position)\n        r_mag = np.linalg.norm(r)\n\n        # Add cutoff distance for stability\n        if r_mag &gt; 3.5*sigma:\n            return np.zeros(2)\n\n        force_mag = 24 * epsilon * (\n            2 * (sigma/r_mag)**13\n            - (sigma/r_mag)**7\n        )\n        force = force_mag * r/r_mag\n        return force\n\n\n# %% Diatomic Molecule Definition\nclass DiatomicMolecule:\n    def __init__(self, atom1, atom2, bond):\n        self.atom1 = atom1\n        self.atom2 = atom2\n        self.bond = bond\n\n\n# %% Define the MD Simulation master controller class\n\nclass MDSimulation:\n    def __init__(self, molecules, forcefield, timestep, box_size):\n        self.molecules = molecules\n        self.atoms = [atom for mol in molecules for atom in [mol.atom1, mol.atom2]]\n        self.forcefield = forcefield\n        self.forcefield.box_size = box_size\n        self.timestep = timestep\n        self.box_size = np.array(box_size)\n        self.energy_history = []\n\n\n    def calculate_forces(self):\n        # Reset all forces\n        for atom in self.atoms:\n            atom.reset_force()\n\n        # Calculate bonded forces\n        for molecule in self.molecules:\n            force = self.forcefield.calculate_bond_force(molecule.bond)\n            molecule.atom1.add_force(force)\n            molecule.atom2.add_force(-force)\n\n        # Calculate non-bonded forces between molecules\n        for i, mol1 in enumerate(self.molecules):\n            for mol2 in self.molecules[i+1:]:\n                # Calculate forces between atoms of different molecules\n                for atom1 in [mol1.atom1, mol1.atom2]:\n                    for atom2 in [mol2.atom1, mol2.atom2]:\n                        force = self.forcefield.calculate_lj_force(atom1, atom2)\n                        atom1.add_force(force)\n                        atom2.add_force(-force)\n\n    def update_positions_and_velocities(self):\n        # First step: Update positions using current forces\n        for atom in self.atoms:\n            atom.update_position(self.timestep)\n            # Apply periodic boundary conditions\n            atom.apply_periodic_boundaries(self.box_size)\n\n        # Recalculate forces with new positions\n        self.calculate_forces()\n\n        # Second step: Update velocities using average of old and new forces\n        for atom in self.atoms:\n            atom.update_velocity(self.timestep, atom.force)\n\n\n# %% Cell 6\ndef create_grid_atoms(num_atoms, box_size, type=\"H\",mass=1.0, random_offset=0.1):\n    box_size = np.array(box_size)\n\n    # Calculate grid dimensions\n    n = int(np.ceil(np.sqrt(num_atoms)))\n    spacing = np.min(box_size) / n\n\n    atoms = []\n    for i in range(num_atoms):\n        # Calculate grid position\n        row = i // n\n        col = i % n\n\n        # Base position\n        pos = np.array([col * spacing + spacing/2,\n                       row * spacing + spacing/2])\n\n        # Add random offset\n        pos += (np.random.rand(2) - 0.5) * spacing * random_offset\n\n        # Create atom\n        atoms.append(Atom(i, type, pos, mass=mass))\n\n    return atoms\n\n\n# %% Create diatomic Molecules\n#\ndef create_diatomic_molecules(num_molecules, box_size, type1=\"H\", type2=\"H\", mass1=1.0, mass2=1.0):\n    molecules = []\n    spacing = np.min(box_size) / np.ceil(np.sqrt(num_molecules))\n\n    for i in range(num_molecules):\n        # Calculate grid position for molecule center\n        row = i // int(np.ceil(np.sqrt(num_molecules)))\n        col = i % int(np.ceil(np.sqrt(num_molecules)))\n        center = np.array([col * spacing + spacing/2, row * spacing + spacing/2])\n\n        # Create atoms with small random displacement for initial bond length\n        displacement = np.random.randn(2) * 0.1\n        atom1 = Atom(2*i, type1, center + displacement, mass=mass1)\n        atom2 = Atom(2*i+1, type2, center - displacement, mass=mass2)\n\n        # Create bond\n        ff = ForceField()\n        bond_params = ff.bond_parameters[(type1, type2)]\n        bond = Bond(atom1, atom2, bond_params['k'], bond_params['r0'])\n\n        molecules.append(DiatomicMolecule(atom1, atom2, bond))\n\n    return molecules\n\n# %% Cell 7\ndef set_temperature(atoms, target_temperature):\n    N = len(atoms)      # number of atoms\n    Nf = 2 * N         # degrees of freedom in 2D\n\n    # Calculate current kinetic energy\n    current_ke = sum(0.5 * atom.mass * np.sum(atom.velocity**2) for atom in atoms)\n    current_temperature = 2 * current_ke / Nf  # kb = 1 in reduced units\n    print(current_temperature)\n    # Calculate scaling factor\n    scale_factor = np.sqrt(target_temperature / current_temperature)\n\n    # Scale velocities\n    for atom in atoms:\n        atom.velocity *= scale_factor\n\n\ndef initialize_velocities(atoms, temperature, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n\n    N = len(atoms)  # number of atoms\n    dim = 2         # 2D simulation\n\n    # Generate random velocities from normal distribution\n    velocities = np.random.normal(0, np.sqrt(temperature), size=(N, dim))\n\n    # Remove center of mass motion\n    total_momentum = np.sum([atom.mass * velocities[i] for i, atom in enumerate(atoms)], axis=0)\n    total_mass = np.sum([atom.mass for atom in atoms])\n    cm_velocity = total_momentum / total_mass\n\n    # Assign velocities to atoms\n    for i, atom in enumerate(atoms):\n        atom.velocity = velocities[i] - cm_velocity\n\n    # Scale velocities to exact temperature\n    set_temperature(atoms, temperature)\n\n    return atoms\n\n# %% run the simulation w\n\nT=5\ndt = 0.01\nbox_size = np.array([50.0, 50.0])\nnum_molecules = 100\nmolecules = create_diatomic_molecules(num_molecules, box_size, \"H\", \"H\")\nff = ForceField()\nsim = MDSimulation(molecules, ff, dt, box_size)\n\n# Initialize velocities for all atoms\natoms = [atom for mol in molecules for atom in [mol.atom1, mol.atom2]]\ninitialize_velocities(atoms, temperature=T)\n\nfig, ax = plt.subplots(1,1,figsize=(6,6))\n\nfor step in range(1000):\n    clear_output(wait=True)\n    set_temperature(atoms, target_temperature=T)\n    sim.update_positions_and_velocities()\n\n\n    positions = [atom.position for atom in sim.atoms]\n    x_coords = [pos[0] for pos in positions]\n    y_coords = [pos[1] for pos in positions]\n\n    circle=patches.Circle((x_coords[0],y_coords[0]),ff.parameters[atoms[0].type][\"sigma\"],edgecolor=\"white\",fill=False)\n    ax.add_patch(circle)\n    ax.scatter(x_coords, y_coords,color=\"red\")\n    ax.set_xlim(0, box_size[0])\n    ax.set_ylim(0, box_size[1])\n    ax.axis(\"off\")\n\n    display(fig)\n\n    ax.clear()\n# %% Cell 8\n#\n\nvx=np.array([atom.velocity for atom in atoms])\n\nvx.reshape(200,2)\nplt.hist(vx[:,0],bins=20)"
  },
  {
    "objectID": "seminars/seminar05/mdsim.html#here-is-the-complete-code-for-the-molecular-dynamics-simulation",
    "href": "seminars/seminar05/mdsim.html#here-is-the-complete-code-for-the-molecular-dynamics-simulation",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "",
    "text": "# %% Load  modules and initialize\nfrom typing_extensions import ParamSpec\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nimport matplotlib.patches as patches\nplt.rcParams.update({'font.size': 8,\n                     'lines.linewidth': 1,\n                     'lines.markersize': 10,\n                     'axes.labelsize': 10,\n                     'axes.titlesize': 10,\n                     'xtick.labelsize' : 10,\n                     'ytick.labelsize' : 10,\n                     'xtick.top' : True,\n                     'xtick.direction' : 'in',\n                     'ytick.right' : True,\n                     'ytick.direction' : 'in',\n                     'figure.facecolor' : 'white',})\n\ndef get_size(w,h):\n    return((w/2.54,h/2.54))\n\n\n# %% Load the atom class we did already in the previous seminar\nclass Atom:\n    def __init__(self, atom_id, atom_type, position, velocity=None, mass=None):\n        self.id = atom_id\n        self.type = atom_type\n        self.position = position\n        self.velocity = velocity if velocity is not None else np.random.randn(2)*20\n        self.mass = mass\n        self.force = np.zeros(2)\n\n\n    def add_force(self, force):\n        \"\"\"Add force contribution to total force on atom\"\"\"\n        self.force += force\n\n    def reset_force(self):\n        \"\"\"Reset force to zero at start of each step\"\"\"\n        self.force = np.zeros(2)\n\n    def update_position(self, dt):\n        \"\"\"First step of velocity Verlet: update position\"\"\"\n        self.position += self.velocity * dt + 0.5 * (self.force/self.mass) * dt**2\n\n    def update_velocity(self, dt, new_force):\n        \"\"\"Second step of velocity Verlet: update velocity using average force\"\"\"\n        self.velocity += 0.5 * (new_force + self.force)/self.mass * dt\n        self.force = new_force\n\n    def apply_periodic_boundaries(self, box_size):\n            \"\"\"Apply periodic boundary conditions\"\"\"\n            self.position = self.position % box_size\n\nclass ForceField:\n    def __init__(self):\n        self.parameters = {\n            'C': {'epsilon': 1.615, 'sigma': 1.36},\n            'H': {'epsilon': 1.0, 'sigma': 1.0 },\n            'O': {'epsilon': 1.846, 'sigma': 3.0},\n        }\n        self.bond_parameters = {\n            ('H', 'H'): {'k': 500.0, 'r0': 0.74},  # Example parameters for H2\n            ('O', 'H'): {'k': 550.0, 'r0': 0.96},  # Example parameters for OH bond\n        }\n        self.box_size = None\n\n    def calculate_bond_force(self, bond):\n        \"\"\"Calculate harmonic bond force\"\"\"\n        r = self.minimum_image_distance(bond.atom1.position, bond.atom2.position)\n        r_mag = np.linalg.norm(r)\n\n        # F = -k(r - r0)‚àô(r/|r|)\n        force_mag = -bond.k * (r_mag - bond.r0)\n        force = force_mag * r/r_mag\n        return force\n\n    def get_pair_parameters(self, type1, type2):\n        # Apply mixing rules when needed\n        eps1 = self.parameters[type1]['epsilon']\n        eps2 = self.parameters[type2]['epsilon']\n        sig1 = self.parameters[type1]['sigma']\n        sig2 = self.parameters[type2]['sigma']\n\n        # Lorentz-Berthelot mixing rules\n        epsilon = np.sqrt(eps1 * eps2)\n        sigma = (sig1 + sig2) / 2\n\n        return epsilon, sigma\n\n    def minimum_image_distance(self, pos1, pos2):\n        \"\"\"Calculate minimum image distance between two positions\"\"\"\n        delta = pos1 - pos2\n        # Apply minimum image convention\n        delta = delta - self.box_size * np.round(delta / self.box_size)\n        return delta\n\n    def calculate_lj_force(self, atom1, atom2):\n        epsilon, sigma = self.get_pair_parameters(atom1.type, atom2.type)\n        r = self.minimum_image_distance(atom1.position, atom2.position)\n        r_mag = np.linalg.norm(r)\n\n        # Add cutoff distance for stability\n        if r_mag &gt; 3.5*sigma:\n            return np.zeros(2)\n\n        force_mag = 24 * epsilon * (\n            2 * (sigma/r_mag)**13\n            - (sigma/r_mag)**7\n        )\n        force = force_mag * r/r_mag\n        return force\n\n\n# %% Diatomic Molecule Definition\nclass DiatomicMolecule:\n    def __init__(self, atom1, atom2, bond):\n        self.atom1 = atom1\n        self.atom2 = atom2\n        self.bond = bond\n\n\n# %% Define the MD Simulation master controller class\n\nclass MDSimulation:\n    def __init__(self, molecules, forcefield, timestep, box_size):\n        self.molecules = molecules\n        self.atoms = [atom for mol in molecules for atom in [mol.atom1, mol.atom2]]\n        self.forcefield = forcefield\n        self.forcefield.box_size = box_size\n        self.timestep = timestep\n        self.box_size = np.array(box_size)\n        self.energy_history = []\n\n\n    def calculate_forces(self):\n        # Reset all forces\n        for atom in self.atoms:\n            atom.reset_force()\n\n        # Calculate bonded forces\n        for molecule in self.molecules:\n            force = self.forcefield.calculate_bond_force(molecule.bond)\n            molecule.atom1.add_force(force)\n            molecule.atom2.add_force(-force)\n\n        # Calculate non-bonded forces between molecules\n        for i, mol1 in enumerate(self.molecules):\n            for mol2 in self.molecules[i+1:]:\n                # Calculate forces between atoms of different molecules\n                for atom1 in [mol1.atom1, mol1.atom2]:\n                    for atom2 in [mol2.atom1, mol2.atom2]:\n                        force = self.forcefield.calculate_lj_force(atom1, atom2)\n                        atom1.add_force(force)\n                        atom2.add_force(-force)\n\n    def update_positions_and_velocities(self):\n        # First step: Update positions using current forces\n        for atom in self.atoms:\n            atom.update_position(self.timestep)\n            # Apply periodic boundary conditions\n            atom.apply_periodic_boundaries(self.box_size)\n\n        # Recalculate forces with new positions\n        self.calculate_forces()\n\n        # Second step: Update velocities using average of old and new forces\n        for atom in self.atoms:\n            atom.update_velocity(self.timestep, atom.force)\n\n\n# %% Cell 6\ndef create_grid_atoms(num_atoms, box_size, type=\"H\",mass=1.0, random_offset=0.1):\n    box_size = np.array(box_size)\n\n    # Calculate grid dimensions\n    n = int(np.ceil(np.sqrt(num_atoms)))\n    spacing = np.min(box_size) / n\n\n    atoms = []\n    for i in range(num_atoms):\n        # Calculate grid position\n        row = i // n\n        col = i % n\n\n        # Base position\n        pos = np.array([col * spacing + spacing/2,\n                       row * spacing + spacing/2])\n\n        # Add random offset\n        pos += (np.random.rand(2) - 0.5) * spacing * random_offset\n\n        # Create atom\n        atoms.append(Atom(i, type, pos, mass=mass))\n\n    return atoms\n\n\n# %% Create diatomic Molecules\n#\ndef create_diatomic_molecules(num_molecules, box_size, type1=\"H\", type2=\"H\", mass1=1.0, mass2=1.0):\n    molecules = []\n    spacing = np.min(box_size) / np.ceil(np.sqrt(num_molecules))\n\n    for i in range(num_molecules):\n        # Calculate grid position for molecule center\n        row = i // int(np.ceil(np.sqrt(num_molecules)))\n        col = i % int(np.ceil(np.sqrt(num_molecules)))\n        center = np.array([col * spacing + spacing/2, row * spacing + spacing/2])\n\n        # Create atoms with small random displacement for initial bond length\n        displacement = np.random.randn(2) * 0.1\n        atom1 = Atom(2*i, type1, center + displacement, mass=mass1)\n        atom2 = Atom(2*i+1, type2, center - displacement, mass=mass2)\n\n        # Create bond\n        ff = ForceField()\n        bond_params = ff.bond_parameters[(type1, type2)]\n        bond = Bond(atom1, atom2, bond_params['k'], bond_params['r0'])\n\n        molecules.append(DiatomicMolecule(atom1, atom2, bond))\n\n    return molecules\n\n# %% Cell 7\ndef set_temperature(atoms, target_temperature):\n    N = len(atoms)      # number of atoms\n    Nf = 2 * N         # degrees of freedom in 2D\n\n    # Calculate current kinetic energy\n    current_ke = sum(0.5 * atom.mass * np.sum(atom.velocity**2) for atom in atoms)\n    current_temperature = 2 * current_ke / Nf  # kb = 1 in reduced units\n    print(current_temperature)\n    # Calculate scaling factor\n    scale_factor = np.sqrt(target_temperature / current_temperature)\n\n    # Scale velocities\n    for atom in atoms:\n        atom.velocity *= scale_factor\n\n\ndef initialize_velocities(atoms, temperature, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n\n    N = len(atoms)  # number of atoms\n    dim = 2         # 2D simulation\n\n    # Generate random velocities from normal distribution\n    velocities = np.random.normal(0, np.sqrt(temperature), size=(N, dim))\n\n    # Remove center of mass motion\n    total_momentum = np.sum([atom.mass * velocities[i] for i, atom in enumerate(atoms)], axis=0)\n    total_mass = np.sum([atom.mass for atom in atoms])\n    cm_velocity = total_momentum / total_mass\n\n    # Assign velocities to atoms\n    for i, atom in enumerate(atoms):\n        atom.velocity = velocities[i] - cm_velocity\n\n    # Scale velocities to exact temperature\n    set_temperature(atoms, temperature)\n\n    return atoms\n\n# %% run the simulation w\n\nT=5\ndt = 0.01\nbox_size = np.array([50.0, 50.0])\nnum_molecules = 100\nmolecules = create_diatomic_molecules(num_molecules, box_size, \"H\", \"H\")\nff = ForceField()\nsim = MDSimulation(molecules, ff, dt, box_size)\n\n# Initialize velocities for all atoms\natoms = [atom for mol in molecules for atom in [mol.atom1, mol.atom2]]\ninitialize_velocities(atoms, temperature=T)\n\nfig, ax = plt.subplots(1,1,figsize=(6,6))\n\nfor step in range(1000):\n    clear_output(wait=True)\n    set_temperature(atoms, target_temperature=T)\n    sim.update_positions_and_velocities()\n\n\n    positions = [atom.position for atom in sim.atoms]\n    x_coords = [pos[0] for pos in positions]\n    y_coords = [pos[1] for pos in positions]\n\n    circle=patches.Circle((x_coords[0],y_coords[0]),ff.parameters[atoms[0].type][\"sigma\"],edgecolor=\"white\",fill=False)\n    ax.add_patch(circle)\n    ax.scatter(x_coords, y_coords,color=\"red\")\n    ax.set_xlim(0, box_size[0])\n    ax.set_ylim(0, box_size[1])\n    ax.axis(\"off\")\n\n    display(fig)\n\n    ax.clear()\n# %% Cell 8\n#\n\nvx=np.array([atom.velocity for atom in atoms])\n\nvx.reshape(200,2)\nplt.hist(vx[:,0],bins=20)"
  },
  {
    "objectID": "seminars/seminar05/md5.html",
    "href": "seminars/seminar05/md5.html",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "",
    "text": "Previously we wrote our simulation with classes for the atoms, the force-field and the MD simulation.Now we want to implement the velocity initialization. We will use the Maxwell-Boltzmann distribution to generate random velocities for the particles in our system. This ensures that our system starts in a state of thermal equilibrium, reflecting the physical reality of molecular motion.\nThe Maxwell-Boltzmann distribution stands as a cornerstone principle in molecular dynamics (MD) simulations, providing us with a statistical description of particle velocities in a system at thermal equilibrium. This distribution emerged from the kinetic theory of gases and proves invaluable in understanding how molecules move and interact at various temperatures.\n\n\nAt its heart, the Maxwell-Boltzmann distribution tells us the probability of finding a particle moving at a particular velocity in a system at thermal equilibrium. The mathematical expression for this probability density \\(f(v)\\) is:\n\\[f(v) = \\sqrt{\\left(\\frac{m}{2\\pi k_B T}\\right)^3} 4\\pi v^2 \\exp\\left(-\\frac{mv^2}{2k_B T}\\right)\\]\nHere, \\(m\\) represents the particle mass, \\(k_B\\) is Boltzmann‚Äôs constant, \\(T\\) denotes the temperature in Kelvin, and \\(v\\) is the velocity magnitude. We can also write down the Maxwell-Boltzmann distribution in terms of the velocity components \\(v_x\\), \\(v_y\\), and \\(v_z\\). For each of these components, the distribution is given by\n\\[\nf(v_x) = \\sqrt{\\frac{m}{2\\pi k_B T}} \\exp\\left(-\\frac{m v_x^2}{2 k_B T}\\right)\n\\]\nThe Maxwell-Boltzmann distribution has the following properties\nThe mean velocity of the particles is of course zero as the system as a whole does not move. The mean magnitude of the velocity can be calculated from\n\\[\n\\bar{v}=\\int_0^{\\infty} v p(v) \\mathrm{d} v\n\\]\nwhich results in\n\\[\n\\bar{v}=\\sqrt{\\frac{8 k_{\\mathrm{B}} T}{\\pi m}}\n\\]\nWhat is also important is the mean squared velocity which can be calculated by\n\\[\n\\overline{v^2}=\\int_0^{\\infty} v^2 p(v) d v\n\\]\nsince this will provide the mean kinetic energy of the particles. This results in\n\\[\n\\overline{v^2}=\\frac{3 k_{\\mathrm{B}} T}{m}\n\\]\nThis is consisten with a kinetic energy of \\(1/2 k_{\\mathrm{B}} T\\) per degree of freedom.\nSince we use in MD simulations with Lennard-Jones atoms reduced units, we can also express the Maxwell-Boltzmann distribution in reduced units. The reduced temperature is defined as \\(T^{*}=k_{\\mathrm{B}} T / \\varepsilon\\) and the reduced velocity as \\(v^{*}=v / \\sqrt{\\varepsilon / m}\\). The reduced Maxwell-Boltzmann distribution is then\n\\[\nf(v^{*}) = \\sqrt{\\left(\\frac{1}{2\\pi T^{*}}\\right)^3} 4\\pi v^{*2} \\exp\\left(-\\frac{v^{2}}{2T^{*}}\\right)\n\\]\nOur goal is to generate random velocities for particles in our MD simulations that follow this distribution. This ensures that our system starts in a state of thermal equilibrium, reflecting the physical reality of molecular motion.\n\n\n\n\nviewof reducedTemp = Inputs.range([0.1, 5], {\n  step: 0.1,\n  value: 1.0,\n  label: \"Reduced Temperature (T*)\"\n})\n\n// Generate distribution data in reduced units\nfunction generateReducedData() {\n  const data = [];\n  // Generate points for reduced velocities from 0 to 5\n  for (let v = 0; v &lt;= 5; v += 0.05) {\n    const term1 = Math.sqrt((1 / (2 * Math.PI * reducedTemp)) ** 3);\n    const term2 = 4 * Math.PI * v * v;\n    const term3 = Math.exp((-v * v) / (2 * reducedTemp));\n    data.push({\n      velocity: v,\n      probability: term1 * term2 * term3\n    });\n  }\n  return data;\n}\n\nPlot.plot({\n  width: 400,\n  height: 400,\n  margin: 50,\n  grid: true,\n  style: {\n    fontSize: 16\n  },\n  x: {\n    label: \"Reduced Velocity (v*)\",\n    domain: [0, 5],\n  },\n  y: {\n    label: \"Probability Density\",\n  },\n  marks: [\n    Plot.line(generateReducedData(), {\n      x: \"velocity\",\n      y: \"probability\",\n      stroke: \"blue\"\n    })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe interactive plot above demonstrates how the Maxwell-Boltzmann distribution changes with temperature and molecular mass. Try adjusting the sliders to see how:\n\nIncreasing temperature broadens the distribution and shifts the peak to higher velocities\nIncreasing molecular mass narrows the distribution and shifts the peak to lower velocities\n\n\n\n\nWhen we begin an MD simulation, one of our first tasks is to assign initial velocities to all particles in our system. The Maxwell-Boltzmann distribution guides this process, ensuring that our initial configuration reflects physical reality. We typically generate random velocities following this distribution while ensuring that the total momentum of the system remains zero ‚Äì a condition that prevents our system from drifting as a whole.\n\n\n\nHere‚Äôs how we can implement velocity initialization following the Maxwell-Boltzmann distribution:\ndef initialize_velocities(atoms, temperature, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n\n    N = len(atoms)\n    dim = 2\n\n\n    velocities = np.random.normal(0, np.sqrt(temperature), size=(N, dim))\n\n\n    total_momentum = np.sum([atom.mass * velocities[i] for i, atom in enumerate(atoms)], axis=0)\n    total_mass = np.sum([atom.mass for atom in atoms])\n    cm_velocity = total_momentum / total_mass\n\n    for i, atom in enumerate(atoms):\n        atom.velocity = velocities[i] - cm_velocity\n\n    set_temperature(atoms, temperature)\n\n    return atoms\nIn molecular dynamics simulations, it is crucial to control the temperature of the system to ensure that it reflects the desired physical conditions. The temperature of a system in MD simulations is directly related to the kinetic energy of the particles. If the initial velocities of the particles do not correspond to the target temperature, the system will not accurately represent the intended thermodynamic state.\nScaling the velocities of the particles is a common technique to adjust the temperature of the system. By scaling the velocities, we can ensure that the kinetic energy‚Äîand hence the temperature‚Äîmatches the target value. This process is essential for initializing the system correctly and for maintaining the desired temperature during the simulation.\nThe provided code scales the velocities of the atoms to achieve the target temperature. Here‚Äôs a step-by-step explanation of the code:\n\ndef set_temperature(atoms, target_temperature):\n    N = len(atoms)      # number of atoms\n    Nf = 2 * N         # degrees of freedom in 2D\n\n    # Calculate current kinetic energy\n    current_ke = sum(0.5 * atom.mass * np.sum(atom.velocity**2) for atom in atoms)\n    current_temperature = 2 * current_ke / Nf  # kb = 1 in reduced units\n\n    # Calculate scaling factor\n    scale_factor = np.sqrt(target_temperature / current_temperature)\n\n    # Scale velocities\n    for atom in atoms:\n        atom.velocity *= scale_factor\n\nThis code snippet sets the temperature of the system to the target temperature by scaling the velocities of the atoms. Here‚Äôs a breakdown of the key steps:\n\nNumber of Atoms and Degrees of Freedom:\nN = len(atoms)      # number of atoms\nNf = 2 * N         # degrees of freedom in 2D\n\nN is the number of atoms in the system.\nNf is the number of degrees of freedom. In a 2D system, each atom has 2 degrees of freedom (one for each spatial dimension), so Nf = 2 * N.\n\nCalculate Current Kinetic Energy:\ncurrent_ke = sum(0.5 * atom.mass * np.sum(atom.velocity**2) for atom in atoms)\ncurrent_temperature = 2 * current_ke / Nf  # kb = 1 in reduced units\n\nThe current kinetic energy (current_ke) is calculated by summing the kinetic energy of each atom. The kinetic energy of an atom is given by ( m v^2 ), where m is the mass and v is the velocity.\nThe current temperature (current_temperature) is then calculated using the relation ( T = ). Here, the Boltzmann constant ( k_B ) is assumed to be 1 in reduced units.\n\nCalculate Scaling Factor:\nscale_factor = np.sqrt(target_temperature / current_temperature)\n\nThe scaling factor is calculated as the square root of the ratio of the target temperature to the current temperature. This factor will be used to scale the velocities of the atoms.\n\nScale Velocities:\nfor atom in atoms:\n    atom.velocity *= scale_factor\n\nThe velocities of all atoms are scaled by the calculated scaling factor. This adjustment ensures that the kinetic energy‚Äîand thus the temperature‚Äîof the system matches the target temperature."
  },
  {
    "objectID": "seminars/seminar05/md5.html#temperature-and-velocity-initialization",
    "href": "seminars/seminar05/md5.html#temperature-and-velocity-initialization",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "",
    "text": "Previously we wrote our simulation with classes for the atoms, the force-field and the MD simulation.Now we want to implement the velocity initialization. We will use the Maxwell-Boltzmann distribution to generate random velocities for the particles in our system. This ensures that our system starts in a state of thermal equilibrium, reflecting the physical reality of molecular motion.\nThe Maxwell-Boltzmann distribution stands as a cornerstone principle in molecular dynamics (MD) simulations, providing us with a statistical description of particle velocities in a system at thermal equilibrium. This distribution emerged from the kinetic theory of gases and proves invaluable in understanding how molecules move and interact at various temperatures.\n\n\nAt its heart, the Maxwell-Boltzmann distribution tells us the probability of finding a particle moving at a particular velocity in a system at thermal equilibrium. The mathematical expression for this probability density \\(f(v)\\) is:\n\\[f(v) = \\sqrt{\\left(\\frac{m}{2\\pi k_B T}\\right)^3} 4\\pi v^2 \\exp\\left(-\\frac{mv^2}{2k_B T}\\right)\\]\nHere, \\(m\\) represents the particle mass, \\(k_B\\) is Boltzmann‚Äôs constant, \\(T\\) denotes the temperature in Kelvin, and \\(v\\) is the velocity magnitude. We can also write down the Maxwell-Boltzmann distribution in terms of the velocity components \\(v_x\\), \\(v_y\\), and \\(v_z\\). For each of these components, the distribution is given by\n\\[\nf(v_x) = \\sqrt{\\frac{m}{2\\pi k_B T}} \\exp\\left(-\\frac{m v_x^2}{2 k_B T}\\right)\n\\]\nThe Maxwell-Boltzmann distribution has the following properties\nThe mean velocity of the particles is of course zero as the system as a whole does not move. The mean magnitude of the velocity can be calculated from\n\\[\n\\bar{v}=\\int_0^{\\infty} v p(v) \\mathrm{d} v\n\\]\nwhich results in\n\\[\n\\bar{v}=\\sqrt{\\frac{8 k_{\\mathrm{B}} T}{\\pi m}}\n\\]\nWhat is also important is the mean squared velocity which can be calculated by\n\\[\n\\overline{v^2}=\\int_0^{\\infty} v^2 p(v) d v\n\\]\nsince this will provide the mean kinetic energy of the particles. This results in\n\\[\n\\overline{v^2}=\\frac{3 k_{\\mathrm{B}} T}{m}\n\\]\nThis is consisten with a kinetic energy of \\(1/2 k_{\\mathrm{B}} T\\) per degree of freedom.\nSince we use in MD simulations with Lennard-Jones atoms reduced units, we can also express the Maxwell-Boltzmann distribution in reduced units. The reduced temperature is defined as \\(T^{*}=k_{\\mathrm{B}} T / \\varepsilon\\) and the reduced velocity as \\(v^{*}=v / \\sqrt{\\varepsilon / m}\\). The reduced Maxwell-Boltzmann distribution is then\n\\[\nf(v^{*}) = \\sqrt{\\left(\\frac{1}{2\\pi T^{*}}\\right)^3} 4\\pi v^{*2} \\exp\\left(-\\frac{v^{2}}{2T^{*}}\\right)\n\\]\nOur goal is to generate random velocities for particles in our MD simulations that follow this distribution. This ensures that our system starts in a state of thermal equilibrium, reflecting the physical reality of molecular motion.\n\n\n\n\nviewof reducedTemp = Inputs.range([0.1, 5], {\n  step: 0.1,\n  value: 1.0,\n  label: \"Reduced Temperature (T*)\"\n})\n\n// Generate distribution data in reduced units\nfunction generateReducedData() {\n  const data = [];\n  // Generate points for reduced velocities from 0 to 5\n  for (let v = 0; v &lt;= 5; v += 0.05) {\n    const term1 = Math.sqrt((1 / (2 * Math.PI * reducedTemp)) ** 3);\n    const term2 = 4 * Math.PI * v * v;\n    const term3 = Math.exp((-v * v) / (2 * reducedTemp));\n    data.push({\n      velocity: v,\n      probability: term1 * term2 * term3\n    });\n  }\n  return data;\n}\n\nPlot.plot({\n  width: 400,\n  height: 400,\n  margin: 50,\n  grid: true,\n  style: {\n    fontSize: 16\n  },\n  x: {\n    label: \"Reduced Velocity (v*)\",\n    domain: [0, 5],\n  },\n  y: {\n    label: \"Probability Density\",\n  },\n  marks: [\n    Plot.line(generateReducedData(), {\n      x: \"velocity\",\n      y: \"probability\",\n      stroke: \"blue\"\n    })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe interactive plot above demonstrates how the Maxwell-Boltzmann distribution changes with temperature and molecular mass. Try adjusting the sliders to see how:\n\nIncreasing temperature broadens the distribution and shifts the peak to higher velocities\nIncreasing molecular mass narrows the distribution and shifts the peak to lower velocities\n\n\n\n\nWhen we begin an MD simulation, one of our first tasks is to assign initial velocities to all particles in our system. The Maxwell-Boltzmann distribution guides this process, ensuring that our initial configuration reflects physical reality. We typically generate random velocities following this distribution while ensuring that the total momentum of the system remains zero ‚Äì a condition that prevents our system from drifting as a whole.\n\n\n\nHere‚Äôs how we can implement velocity initialization following the Maxwell-Boltzmann distribution:\ndef initialize_velocities(atoms, temperature, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n\n    N = len(atoms)\n    dim = 2\n\n\n    velocities = np.random.normal(0, np.sqrt(temperature), size=(N, dim))\n\n\n    total_momentum = np.sum([atom.mass * velocities[i] for i, atom in enumerate(atoms)], axis=0)\n    total_mass = np.sum([atom.mass for atom in atoms])\n    cm_velocity = total_momentum / total_mass\n\n    for i, atom in enumerate(atoms):\n        atom.velocity = velocities[i] - cm_velocity\n\n    set_temperature(atoms, temperature)\n\n    return atoms\nIn molecular dynamics simulations, it is crucial to control the temperature of the system to ensure that it reflects the desired physical conditions. The temperature of a system in MD simulations is directly related to the kinetic energy of the particles. If the initial velocities of the particles do not correspond to the target temperature, the system will not accurately represent the intended thermodynamic state.\nScaling the velocities of the particles is a common technique to adjust the temperature of the system. By scaling the velocities, we can ensure that the kinetic energy‚Äîand hence the temperature‚Äîmatches the target value. This process is essential for initializing the system correctly and for maintaining the desired temperature during the simulation.\nThe provided code scales the velocities of the atoms to achieve the target temperature. Here‚Äôs a step-by-step explanation of the code:\n\ndef set_temperature(atoms, target_temperature):\n    N = len(atoms)      # number of atoms\n    Nf = 2 * N         # degrees of freedom in 2D\n\n    # Calculate current kinetic energy\n    current_ke = sum(0.5 * atom.mass * np.sum(atom.velocity**2) for atom in atoms)\n    current_temperature = 2 * current_ke / Nf  # kb = 1 in reduced units\n\n    # Calculate scaling factor\n    scale_factor = np.sqrt(target_temperature / current_temperature)\n\n    # Scale velocities\n    for atom in atoms:\n        atom.velocity *= scale_factor\n\nThis code snippet sets the temperature of the system to the target temperature by scaling the velocities of the atoms. Here‚Äôs a breakdown of the key steps:\n\nNumber of Atoms and Degrees of Freedom:\nN = len(atoms)      # number of atoms\nNf = 2 * N         # degrees of freedom in 2D\n\nN is the number of atoms in the system.\nNf is the number of degrees of freedom. In a 2D system, each atom has 2 degrees of freedom (one for each spatial dimension), so Nf = 2 * N.\n\nCalculate Current Kinetic Energy:\ncurrent_ke = sum(0.5 * atom.mass * np.sum(atom.velocity**2) for atom in atoms)\ncurrent_temperature = 2 * current_ke / Nf  # kb = 1 in reduced units\n\nThe current kinetic energy (current_ke) is calculated by summing the kinetic energy of each atom. The kinetic energy of an atom is given by ( m v^2 ), where m is the mass and v is the velocity.\nThe current temperature (current_temperature) is then calculated using the relation ( T = ). Here, the Boltzmann constant ( k_B ) is assumed to be 1 in reduced units.\n\nCalculate Scaling Factor:\nscale_factor = np.sqrt(target_temperature / current_temperature)\n\nThe scaling factor is calculated as the square root of the ratio of the target temperature to the current temperature. This factor will be used to scale the velocities of the atoms.\n\nScale Velocities:\nfor atom in atoms:\n    atom.velocity *= scale_factor\n\nThe velocities of all atoms are scaled by the calculated scaling factor. This adjustment ensures that the kinetic energy‚Äîand thus the temperature‚Äîof the system matches the target temperature."
  },
  {
    "objectID": "seminars/seminar05/md5.html#simulation-setup-and-initialization",
    "href": "seminars/seminar05/md5.html#simulation-setup-and-initialization",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "Simulation Setup and Initialization",
    "text": "Simulation Setup and Initialization\nThis now completes our initial code for the MD simulation and we can put it all together to run a simulation.\nbox_size = np.array([50.0, 50.0])  # Box dimensions\nnum_atoms = 200\n\nT=5\ndt = 0.01\n\n# Create atoms and set initial velocities\natoms = create_grid_atoms(num_atoms, box_size, type=\"H\",mass=1.0, random_offset=0.1)\natoms = initialize_velocities(atoms, temperature=T)\n\n\n# Create force field\nff = ForceField()\n\n\n# Create simulation with periodic boundaries\nsim = MDSimulation(atoms, ff, dt, box_size)\n\nfig, ax = plt.subplots(1,1,figsize=(6,6))\n\nfor step in range(1000):\n    clear_output(wait=True)\n    set_temperature(atoms, target_temperature=T)\n    sim.update_positions_and_velocities()\n\n    positions = [atom.position for atom in sim.atoms]\n    x_coords = [pos[0] for pos in positions]\n    y_coords = [pos[1] for pos in positions]\n\n    circle=patches.Circle((x_coords[0],y_coords[0]),ff.parameters[atoms[0].type][\"sigma\"],edgecolor=\"white\",fill=False)\n    ax.add_patch(circle)\n    ax.scatter(x_coords, y_coords,color=\"red\")\n    ax.set_xlim(0, box_size[0])\n    ax.set_ylim(0, box_size[1])\n    ax.axis(\"off\")\n\n    display(fig)\n\n    ax.clear()"
  },
  {
    "objectID": "seminars/seminar02/md2.html",
    "href": "seminars/seminar02/md2.html",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "",
    "text": "In the previous document, we learned about the key components of a molecular dynamics simulation: - The Lennard-Jones potential describing forces between atoms - The Velocity Verlet algorithm for updating positions and velocities\nNow we‚Äôll implement these concepts in code. To organize our simulation, we‚Äôll have to think about some issues:\n\n\nIn the previous example, we have assumed that the particle is in free fall. That means eventually it would bounce against the floor. In a real simulation, we need to consider boundary conditions as well. For example, if the particle hits the ground we could implement a simple reflection rule. This is called reflecting boundary conditions and would introduce some additional effects to the simulation. On the other side, one could make the system ‚Äúkind of‚Äù infinitely large by introducing periodic boundary conditions. This means that if a particle leaves the simulation box on one side, it re-enters on the opposite side. This is a common approach in molecular dynamics simulations.\n\n\n\nPerdiodic Boundary Conditions\n\n\n\n\n\n\n\n\nThe Minimum Image Convention in Molecular Dynamics\n\n\n\n\n\nWhen we simulate particles in a box with periodic boundary conditions (meaning particles that leave on one side reappear on the opposite side), we need to calculate the forces between them correctly. Imagine two particles near opposite edges of the box: one at position x=1 and another at x=9 in a box of length 10. Without the minimum image convention, we would calculate their distance as 8 units (9-1). However, due to the periodic boundaries, these particles could actually interact across the boundary, with a shorter distance of just 2 units! The minimum image convention automatically finds this shortest distance, ensuring that we calculate the forces between particles correctly. It‚Äôs like taking a shortcut across the periodic boundary instead of walking the longer path through the box.\n\n\n\n\n\n\nThe question we have to think about now is how to implement these formulas in a numerical simulation. The goal is to simulate the motion of many atoms in a box. Each atom is different and has its own position, velocity, and force. Consequently we need to store these quantities for each atom, though the structure in which we store them is the same for each atom. All atoms with their properties actually belong to the same class of objects. We can therefore use a very suitable concept of object-oriented programming, the class.\nA class in object-oriented programming is a blueprint for creating objects (a particular data structure), providing initial values for state (member variables or attributes), and implementations of behavior (member functions or methods). The class is a template for objects, and an object is an instance of a class. The class defines the properties and behavior common to all objects of the class. The objects are the instances of the class that contain the actual data.\nThink of the Atom class as a container for everything we need to know about a single atom:\n\nIts position (where is it?)\nIts velocity (how fast is it moving?)\nThe forces acting on it (what‚Äôs pushing or pulling it?)\nIts type (is it hydrogen, oxygen, etc.?)\nIts mass (how heavy is it?)\n\n\n\n\nWe also have a set of forces, that is acting between the atoms. These forces are calculated based on the positions of the atoms. The force fields are all the same for the atoms only the parameters are different. We can represent the force field as a class as well. We will first implement the Lennard-Jones potential in the class. Later we will implement more complex force fields. We will realize that we will later have to introduce different parameters for the Lenard Jones potential for different atom types. We will store these parameters in a dictionary. This dictionary will be part of the force field class and actually represent the Force Field.\nIf atoms are of the same type, they will have the same parameters. However, if they are of different types we will have to mix the parameters. This is done by the mixing rules. We will implement the Lorentz-Berthelot mixing rules. These rules are used to calculate the parameters for the interaction between two different atom types.\n\n\nFor two different atoms (A and B), the Lennard-Jones parameters \\(\\sigma\\) and \\(\\epsilon\\) are calculated using:\n\nArithmetic mean for \\(\\sigma\\) (Lorentz rule):\n\\[\\sigma_{AB} = \\frac{\\sigma_A + \\sigma_B}{2}\\]\nGeometric mean for \\(\\epsilon\\) (Berthelot rule):\n\\[\\epsilon_{AB} = \\sqrt{\\epsilon_A \\epsilon_B}\\]\n\nThese parameters are then used in the Lennard-Jones potential:\n\\[V_{LJ}(r) = 4\\epsilon_{AB}\\left[\\left(\\frac{\\sigma_{AB}}{r}\\right)^{12} - \\left(\\frac{\\sigma_{AB}}{r}\\right)^6\\right]\\]\n\n\n\n\nIn the previous example, we have started with a particle at rest. In a real simulation, we would like to start with a certain temperature. This means that the particles have a certain velocity distribution. We can introduce this by assigning random velocities to the particles. The velocities should be drawn from a Maxwell-Boltzmann distribution. This is a distribution that describes the velocity distribution of particles in at a certain temperature. The distribution is given by:\n\\[\nf_v\\left(v_x\\right)=\\sqrt{\\frac{m}{2 \\pi k_B T}} \\exp \\left[\\frac{-m v_x^2}{2 k_B T}\\right]\n\\]\nwhere \\(m\\) is the mass of the particle, \\(k_B\\) is Boltzmann‚Äôs constant, and \\(T\\) is the temperature. \\(v_x\\) is the velocity in the x-direction. The velocities in the y and z directions are drawn in the same way. The temperature of the system is related to the kinetic energy of the particles.\n\n\n\n\n\n\nMaxwell-Boltzmann Velocities in 3D\n\n\n\n\n\nThe probability distribution for the velocity magnitude in 3D is:\n\\[f(v) = 4\\pi v^2 \\left(\\frac{m}{2\\pi k_BT}\\right)^{3/2} \\exp\\left(-\\frac{mv^2}{2k_BT}\\right)\\]\n\nMean velocity magnitude:\n\\[\\langle v \\rangle = \\sqrt{\\frac{8k_BT}{\\pi m}}\\]\nMost probable velocity (peak of distribution):\n\\[v_{mp} = \\sqrt{\\frac{2k_BT}{m}}\\]\nRoot mean square velocity:\n\\[v_{rms} = \\sqrt{\\frac{3k_BT}{m}}\\]\n\nThese velocities can also be expressed in terms of the kinetic energy of the particles. The average kinetic energy per particle is:\n\\[\\langle E_{kin} \\rangle = \\frac{3}{2}k_BT\\]\nThen we can express the velocities as:\n\nMean velocity magnitude:\n\\[\\langle v \\rangle = \\sqrt{\\frac{8\\langle E_{kin} \\rangle}{3\\pi m}}\\]\nMost probable velocity:\n\\[v_{mp} = \\sqrt{\\frac{4\\langle E_{kin} \\rangle}{3m}}\\]\nRoot mean square velocity:\n\\[v_{rms} = \\sqrt{\\frac{2\\langle E_{kin} \\rangle}{m}}\\]\n\n\n\n\n\n\nCode\n# Constants\nkb = 1.380649e-23  # Boltzmann constant in J/K\nm_H = 1.6735575e-27  # Mass of hydrogen atom in kg\nT = 300  # Temperature in K\n\n# Velocity range for plotting\nv = np.linspace(-10000, 10000, 1000)  # m/s\nv_mag = np.linspace(0, 10000, 1000)  # m/s\n\n# Maxwell-Boltzmann distribution for x-component\ndef MB_1D(v, m, T):\n    return np.sqrt(m/(2*np.pi*kb*T)) * np.exp(-m*v**2/(2*kb*T))\n\n# Maxwell-Boltzmann distribution for velocity magnitude in 2D\ndef MB_2D_mag(v, m, T):\n    return v * m/(kb*T) * np.exp(-m*v**2/(2*kb*T))\n\n# Create figure\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=get_size(16, 8))\n\n# Plot x-component distribution\nax1.plot(v, MB_1D(v, m_H, T))\nax1.set_xlabel('Velocity [m/s]')\nax1.set_ylabel('Probability density')\n\n# Plot magnitude distribution\nax2.plot(v_mag, MB_2D_mag(v_mag, m_H, T))\nax2.set_xlabel('Velocity magnitude [m/s]')\nax2.set_ylabel('Probability density')\nax2.axvline(np.sqrt(kb*T/m_H), color='r', linestyle='--', label='Most probable velocity')\nax2.axvline(np.sqrt(2)*np.sqrt(kb*T/m_H), color='g', linestyle='--', label='Mean velocity')\n\nplt.tight_layout()\nplt.show()\n\n# Print most probable velocity\nv_mp_1D = 0  # Most probable velocity for 1D is zero\nv_mp_2D = np.sqrt(kb*T/m_H)  # Most probable velocity magnitude in 2D\nprint(f\"Most probable velocity magnitude in 2D: {v_mp_2D:.1f} m/s\")\nprint(f\"Mean velocity magnitude in 2D: {np.sqrt(2)*v_mp_2D:.1f} m/s\")\n\n\n\n\n\n\n\n\n\nMost probable velocity magnitude in 2D: 1573.2 m/s\nMean velocity magnitude in 2D: 2224.8 m/s\n\n\nThe temperature T in a 2D system is related to the kinetic energy by:\n\\[T = \\frac{2K}{N_f k_B}\\]\nwhere:\n\nK is the total kinetic energy: \\(K = \\sum_i \\frac{1}{2}m_i v_i^2\\)\n\\(N_f\\) is the number of degrees of freedom (2N in 2D, where N is number of particles)\n\\(k_B\\) is Boltzmann‚Äôs constant (often set to 1 in reduced units)\n\nTo scale to a target temperature \\(T_{target}\\), we multiply velocities by \\(\\sqrt{\\frac{T_{target}}{T_{current}}}\\)\n\n\n\nFinally, we need a class that controls the simulation. This class will contain the main loop of the simulation, where the integration algorithm is called in each time step. It will also contain the methods to calculate the forces between the atoms.\n\n\n\nBefore we implement all classes, we will first visualize the particles moving in a 2D box. We will use the matplotlib library to create an animation of the particles moving in the box. We will also implement periodic boundary conditions, so that particles that leave the box on one side re-enter on the opposite side.\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nfrom scipy.spatial.distance import cdist\n\nn_side =2\n\n1x = np.linspace(0.05, 0.95, n_side)\ny = np.linspace(0.05, 0.95, n_side)\n2xx, yy = np.meshgrid(x, y)\n3particles = np.vstack([xx.ravel(), yy.ravel()]).T\n\nvelocities = np.random.normal(scale=0.005, size=(n_side**2, 2))\n\nradius = 0.0177\nfig, ax = plt.subplots(figsize=(9,9))\n\nn_steps = 200\n\n4for _ in range(n_steps):\n5    clear_output(wait=True)\n\n    # Update particle positions based on their velocities\n    particles += velocities\n    # Apply periodic boundary conditions in x direction (wrap around at 0 and 1)\n    particles[:, 0] = particles[:, 0] % 1\n    # Apply periodic boundary conditions in y direction (wrap around at 0 and 1)\n    particles[:, 1] = particles[:, 1] % 1\n    # Calculate distances between all pairs of particles\n    distances = cdist(particles, particles)\n\n    # Calculate collisions using the upper triangle of the distance matrix\n    # distances &lt; 2*radius gives a boolean matrix where True means collision\n    # np.triu takes only the upper triangle to avoid counting collisions twice\n    collisions = np.triu(distances &lt; 2*radius, 1)\n\n    # Handle collisions between particles\n    for i, j in zip(*np.nonzero(collisions)):\n        # Exchange velocities between colliding particles (elastic collision)\n6        velocities[i], velocities[j] = velocities[j], velocities[i].copy()\n\n        # Calculate how much particles overlap\n        overlap = 2*radius - distances[i, j]\n\n        # Calculate unit vector pointing from j to i\n        direction = particles[i] - particles[j]\n        direction /= np.linalg.norm(direction)\n\n        # Move particles apart to prevent overlap\n        particles[i] += 0.5 * overlap * direction\n        particles[j] -= 0.5 * overlap * direction\n\n    ax.scatter(particles[:, 0], particles[:, 1], s=100, edgecolors='r', facecolors='none')\n\n\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.axis(\"off\")\n\n    display(fig)\n    plt.pause(0.01)\n\n    # Clear the current plot to prepare for next frame\n    ax.clear()\n\n1\n\nCreate a 1D array of x and y-coordinates for the particles.\n\n2\n\nCreate a meshgrid of x and y-coordinates.\n\n3\n\nFlatten the meshgrid to get a 2D array of particle positions.\n\n4\n\nSimulation loop\n\n5\n\nClear the output to display the animation in a single cell.\n\n6\n\nHandle collisions between particles by exchanging velocities and moving particles apart to prevent overlap. The exchange of velocities in your code works because of the conservation of momentum and energy:\n\n\n\nFor two particles of equal mass m in a head-on elastic collision: Before collision:\n\nMomentum: \\(p = mv_1 + mv_2\\)\nEnergy: \\(E = \\frac{1}{2}mv_1^2 + \\frac{1}{2}mv_2^2\\)\n\nAfter collision (with velocity exchange): - Momentum: \\(p = mv_2 + mv_1\\) (same as before!) - Energy: \\(E = \\frac{1}{2}mv_2^2 + \\frac{1}{2}mv_1^2\\) (same as before!)"
  },
  {
    "objectID": "seminars/seminar02/md2.html#from-theory-to-code",
    "href": "seminars/seminar02/md2.html#from-theory-to-code",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "",
    "text": "In the previous document, we learned about the key components of a molecular dynamics simulation: - The Lennard-Jones potential describing forces between atoms - The Velocity Verlet algorithm for updating positions and velocities\nNow we‚Äôll implement these concepts in code. To organize our simulation, we‚Äôll have to think about some issues:\n\n\nIn the previous example, we have assumed that the particle is in free fall. That means eventually it would bounce against the floor. In a real simulation, we need to consider boundary conditions as well. For example, if the particle hits the ground we could implement a simple reflection rule. This is called reflecting boundary conditions and would introduce some additional effects to the simulation. On the other side, one could make the system ‚Äúkind of‚Äù infinitely large by introducing periodic boundary conditions. This means that if a particle leaves the simulation box on one side, it re-enters on the opposite side. This is a common approach in molecular dynamics simulations.\n\n\n\nPerdiodic Boundary Conditions\n\n\n\n\n\n\n\n\nThe Minimum Image Convention in Molecular Dynamics\n\n\n\n\n\nWhen we simulate particles in a box with periodic boundary conditions (meaning particles that leave on one side reappear on the opposite side), we need to calculate the forces between them correctly. Imagine two particles near opposite edges of the box: one at position x=1 and another at x=9 in a box of length 10. Without the minimum image convention, we would calculate their distance as 8 units (9-1). However, due to the periodic boundaries, these particles could actually interact across the boundary, with a shorter distance of just 2 units! The minimum image convention automatically finds this shortest distance, ensuring that we calculate the forces between particles correctly. It‚Äôs like taking a shortcut across the periodic boundary instead of walking the longer path through the box.\n\n\n\n\n\n\nThe question we have to think about now is how to implement these formulas in a numerical simulation. The goal is to simulate the motion of many atoms in a box. Each atom is different and has its own position, velocity, and force. Consequently we need to store these quantities for each atom, though the structure in which we store them is the same for each atom. All atoms with their properties actually belong to the same class of objects. We can therefore use a very suitable concept of object-oriented programming, the class.\nA class in object-oriented programming is a blueprint for creating objects (a particular data structure), providing initial values for state (member variables or attributes), and implementations of behavior (member functions or methods). The class is a template for objects, and an object is an instance of a class. The class defines the properties and behavior common to all objects of the class. The objects are the instances of the class that contain the actual data.\nThink of the Atom class as a container for everything we need to know about a single atom:\n\nIts position (where is it?)\nIts velocity (how fast is it moving?)\nThe forces acting on it (what‚Äôs pushing or pulling it?)\nIts type (is it hydrogen, oxygen, etc.?)\nIts mass (how heavy is it?)\n\n\n\n\nWe also have a set of forces, that is acting between the atoms. These forces are calculated based on the positions of the atoms. The force fields are all the same for the atoms only the parameters are different. We can represent the force field as a class as well. We will first implement the Lennard-Jones potential in the class. Later we will implement more complex force fields. We will realize that we will later have to introduce different parameters for the Lenard Jones potential for different atom types. We will store these parameters in a dictionary. This dictionary will be part of the force field class and actually represent the Force Field.\nIf atoms are of the same type, they will have the same parameters. However, if they are of different types we will have to mix the parameters. This is done by the mixing rules. We will implement the Lorentz-Berthelot mixing rules. These rules are used to calculate the parameters for the interaction between two different atom types.\n\n\nFor two different atoms (A and B), the Lennard-Jones parameters \\(\\sigma\\) and \\(\\epsilon\\) are calculated using:\n\nArithmetic mean for \\(\\sigma\\) (Lorentz rule):\n\\[\\sigma_{AB} = \\frac{\\sigma_A + \\sigma_B}{2}\\]\nGeometric mean for \\(\\epsilon\\) (Berthelot rule):\n\\[\\epsilon_{AB} = \\sqrt{\\epsilon_A \\epsilon_B}\\]\n\nThese parameters are then used in the Lennard-Jones potential:\n\\[V_{LJ}(r) = 4\\epsilon_{AB}\\left[\\left(\\frac{\\sigma_{AB}}{r}\\right)^{12} - \\left(\\frac{\\sigma_{AB}}{r}\\right)^6\\right]\\]\n\n\n\n\nIn the previous example, we have started with a particle at rest. In a real simulation, we would like to start with a certain temperature. This means that the particles have a certain velocity distribution. We can introduce this by assigning random velocities to the particles. The velocities should be drawn from a Maxwell-Boltzmann distribution. This is a distribution that describes the velocity distribution of particles in at a certain temperature. The distribution is given by:\n\\[\nf_v\\left(v_x\\right)=\\sqrt{\\frac{m}{2 \\pi k_B T}} \\exp \\left[\\frac{-m v_x^2}{2 k_B T}\\right]\n\\]\nwhere \\(m\\) is the mass of the particle, \\(k_B\\) is Boltzmann‚Äôs constant, and \\(T\\) is the temperature. \\(v_x\\) is the velocity in the x-direction. The velocities in the y and z directions are drawn in the same way. The temperature of the system is related to the kinetic energy of the particles.\n\n\n\n\n\n\nMaxwell-Boltzmann Velocities in 3D\n\n\n\n\n\nThe probability distribution for the velocity magnitude in 3D is:\n\\[f(v) = 4\\pi v^2 \\left(\\frac{m}{2\\pi k_BT}\\right)^{3/2} \\exp\\left(-\\frac{mv^2}{2k_BT}\\right)\\]\n\nMean velocity magnitude:\n\\[\\langle v \\rangle = \\sqrt{\\frac{8k_BT}{\\pi m}}\\]\nMost probable velocity (peak of distribution):\n\\[v_{mp} = \\sqrt{\\frac{2k_BT}{m}}\\]\nRoot mean square velocity:\n\\[v_{rms} = \\sqrt{\\frac{3k_BT}{m}}\\]\n\nThese velocities can also be expressed in terms of the kinetic energy of the particles. The average kinetic energy per particle is:\n\\[\\langle E_{kin} \\rangle = \\frac{3}{2}k_BT\\]\nThen we can express the velocities as:\n\nMean velocity magnitude:\n\\[\\langle v \\rangle = \\sqrt{\\frac{8\\langle E_{kin} \\rangle}{3\\pi m}}\\]\nMost probable velocity:\n\\[v_{mp} = \\sqrt{\\frac{4\\langle E_{kin} \\rangle}{3m}}\\]\nRoot mean square velocity:\n\\[v_{rms} = \\sqrt{\\frac{2\\langle E_{kin} \\rangle}{m}}\\]\n\n\n\n\n\n\nCode\n# Constants\nkb = 1.380649e-23  # Boltzmann constant in J/K\nm_H = 1.6735575e-27  # Mass of hydrogen atom in kg\nT = 300  # Temperature in K\n\n# Velocity range for plotting\nv = np.linspace(-10000, 10000, 1000)  # m/s\nv_mag = np.linspace(0, 10000, 1000)  # m/s\n\n# Maxwell-Boltzmann distribution for x-component\ndef MB_1D(v, m, T):\n    return np.sqrt(m/(2*np.pi*kb*T)) * np.exp(-m*v**2/(2*kb*T))\n\n# Maxwell-Boltzmann distribution for velocity magnitude in 2D\ndef MB_2D_mag(v, m, T):\n    return v * m/(kb*T) * np.exp(-m*v**2/(2*kb*T))\n\n# Create figure\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=get_size(16, 8))\n\n# Plot x-component distribution\nax1.plot(v, MB_1D(v, m_H, T))\nax1.set_xlabel('Velocity [m/s]')\nax1.set_ylabel('Probability density')\n\n# Plot magnitude distribution\nax2.plot(v_mag, MB_2D_mag(v_mag, m_H, T))\nax2.set_xlabel('Velocity magnitude [m/s]')\nax2.set_ylabel('Probability density')\nax2.axvline(np.sqrt(kb*T/m_H), color='r', linestyle='--', label='Most probable velocity')\nax2.axvline(np.sqrt(2)*np.sqrt(kb*T/m_H), color='g', linestyle='--', label='Mean velocity')\n\nplt.tight_layout()\nplt.show()\n\n# Print most probable velocity\nv_mp_1D = 0  # Most probable velocity for 1D is zero\nv_mp_2D = np.sqrt(kb*T/m_H)  # Most probable velocity magnitude in 2D\nprint(f\"Most probable velocity magnitude in 2D: {v_mp_2D:.1f} m/s\")\nprint(f\"Mean velocity magnitude in 2D: {np.sqrt(2)*v_mp_2D:.1f} m/s\")\n\n\n\n\n\n\n\n\n\nMost probable velocity magnitude in 2D: 1573.2 m/s\nMean velocity magnitude in 2D: 2224.8 m/s\n\n\nThe temperature T in a 2D system is related to the kinetic energy by:\n\\[T = \\frac{2K}{N_f k_B}\\]\nwhere:\n\nK is the total kinetic energy: \\(K = \\sum_i \\frac{1}{2}m_i v_i^2\\)\n\\(N_f\\) is the number of degrees of freedom (2N in 2D, where N is number of particles)\n\\(k_B\\) is Boltzmann‚Äôs constant (often set to 1 in reduced units)\n\nTo scale to a target temperature \\(T_{target}\\), we multiply velocities by \\(\\sqrt{\\frac{T_{target}}{T_{current}}}\\)\n\n\n\nFinally, we need a class that controls the simulation. This class will contain the main loop of the simulation, where the integration algorithm is called in each time step. It will also contain the methods to calculate the forces between the atoms.\n\n\n\nBefore we implement all classes, we will first visualize the particles moving in a 2D box. We will use the matplotlib library to create an animation of the particles moving in the box. We will also implement periodic boundary conditions, so that particles that leave the box on one side re-enter on the opposite side.\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nfrom scipy.spatial.distance import cdist\n\nn_side =2\n\n1x = np.linspace(0.05, 0.95, n_side)\ny = np.linspace(0.05, 0.95, n_side)\n2xx, yy = np.meshgrid(x, y)\n3particles = np.vstack([xx.ravel(), yy.ravel()]).T\n\nvelocities = np.random.normal(scale=0.005, size=(n_side**2, 2))\n\nradius = 0.0177\nfig, ax = plt.subplots(figsize=(9,9))\n\nn_steps = 200\n\n4for _ in range(n_steps):\n5    clear_output(wait=True)\n\n    # Update particle positions based on their velocities\n    particles += velocities\n    # Apply periodic boundary conditions in x direction (wrap around at 0 and 1)\n    particles[:, 0] = particles[:, 0] % 1\n    # Apply periodic boundary conditions in y direction (wrap around at 0 and 1)\n    particles[:, 1] = particles[:, 1] % 1\n    # Calculate distances between all pairs of particles\n    distances = cdist(particles, particles)\n\n    # Calculate collisions using the upper triangle of the distance matrix\n    # distances &lt; 2*radius gives a boolean matrix where True means collision\n    # np.triu takes only the upper triangle to avoid counting collisions twice\n    collisions = np.triu(distances &lt; 2*radius, 1)\n\n    # Handle collisions between particles\n    for i, j in zip(*np.nonzero(collisions)):\n        # Exchange velocities between colliding particles (elastic collision)\n6        velocities[i], velocities[j] = velocities[j], velocities[i].copy()\n\n        # Calculate how much particles overlap\n        overlap = 2*radius - distances[i, j]\n\n        # Calculate unit vector pointing from j to i\n        direction = particles[i] - particles[j]\n        direction /= np.linalg.norm(direction)\n\n        # Move particles apart to prevent overlap\n        particles[i] += 0.5 * overlap * direction\n        particles[j] -= 0.5 * overlap * direction\n\n    ax.scatter(particles[:, 0], particles[:, 1], s=100, edgecolors='r', facecolors='none')\n\n\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.axis(\"off\")\n\n    display(fig)\n    plt.pause(0.01)\n\n    # Clear the current plot to prepare for next frame\n    ax.clear()\n\n1\n\nCreate a 1D array of x and y-coordinates for the particles.\n\n2\n\nCreate a meshgrid of x and y-coordinates.\n\n3\n\nFlatten the meshgrid to get a 2D array of particle positions.\n\n4\n\nSimulation loop\n\n5\n\nClear the output to display the animation in a single cell.\n\n6\n\nHandle collisions between particles by exchanging velocities and moving particles apart to prevent overlap. The exchange of velocities in your code works because of the conservation of momentum and energy:\n\n\n\nFor two particles of equal mass m in a head-on elastic collision: Before collision:\n\nMomentum: \\(p = mv_1 + mv_2\\)\nEnergy: \\(E = \\frac{1}{2}mv_1^2 + \\frac{1}{2}mv_2^2\\)\n\nAfter collision (with velocity exchange): - Momentum: \\(p = mv_2 + mv_1\\) (same as before!) - Energy: \\(E = \\frac{1}{2}mv_2^2 + \\frac{1}{2}mv_1^2\\) (same as before!)"
  },
  {
    "objectID": "seminars/seminar03/MD Simulation.html",
    "href": "seminars/seminar03/MD Simulation.html",
    "title": "MD Simulation",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'font.size': 8,\n                     'lines.linewidth': 1,\n                     'lines.markersize': 10,\n                     'axes.labelsize': 10,\n                     'axes.titlesize': 10,\n                     'xtick.labelsize' : 10,\n                     'ytick.labelsize' : 10,\n                     'xtick.top' : True,\n                     'xtick.direction' : 'in',\n                     'ytick.right' : True,\n                     'ytick.direction' : 'in',})\n\ndef get_size(w,h):\n    return((w/2.54,h/2.54))"
  },
  {
    "objectID": "seminars/seminar03/MD Simulation.html#atom-class",
    "href": "seminars/seminar03/MD Simulation.html#atom-class",
    "title": "MD Simulation",
    "section": "Atom Class",
    "text": "Atom Class\n\nclass Atom:\n    def __init__(self, atom_id, atom_type, position, velocity=None, mass=None):\n        self.id = atom_id\n        self.type = atom_type\n        self.position = position\n        self.velocity = velocity if velocity is not None else np.random.randn(2)*20\n        self.mass = mass\n        self.force = np.zeros(2)\n\n\n    def add_force(self, force):\n        \"\"\"Add force contribution to total force on atom\"\"\"\n        self.force += force\n\n    def reset_force(self):\n        \"\"\"Reset force to zero at start of each step\"\"\"\n        self.force = np.zeros(2)\n\n    def update_position(self, dt):\n        \"\"\"First step of velocity Verlet: update position\"\"\"\n        self.position += self.velocity * dt + 0.5 * (self.force/self.mass) * dt**2\n\n    def update_velocity(self, dt, new_force):\n        \"\"\"Second step of velocity Verlet: update velocity using average force\"\"\"\n        self.velocity += 0.5 * (new_force + self.force)/self.mass * dt\n        self.force = new_force\n\n    def apply_periodic_boundaries(self, box_size):\n            \"\"\"Apply periodic boundary conditions\"\"\"\n            self.position = self.position % box_size\n\n\nclass ForceField:\n    def __init__(self):\n        self.parameters = {\n            'C': {'epsilon': 1.0, 'sigma': 3.4},\n            'H': {'epsilon': 1, 'sigma': 1},\n            'O': {'epsilon': 0.8, 'sigma': 3.0},\n        }\n        self.box_size = None  # Will be set when initializing the simulation\n\n    def get_pair_parameters(self, type1, type2):\n        # Apply mixing rules when needed\n        eps1 = self.parameters[type1]['epsilon']\n        eps2 = self.parameters[type2]['epsilon']\n        sig1 = self.parameters[type1]['sigma']\n        sig2 = self.parameters[type2]['sigma']\n\n        # Lorentz-Berthelot mixing rules\n        epsilon = np.sqrt(eps1 * eps2)\n        sigma = (sig1 + sig2) / 2\n\n        return epsilon, sigma\n\n    def minimum_image_distance(self, pos1, pos2):\n        \"\"\"Calculate minimum image distance between two positions\"\"\"\n        delta = pos1 - pos2\n        # Apply minimum image convention\n        delta = delta - self.box_size * np.round(delta / self.box_size)\n        return delta\n\n    def calculate_lj_force(self, atom1, atom2):\n        epsilon, sigma = self.get_pair_parameters(atom1.type, atom2.type)\n        r = self.minimum_image_distance(atom1.position, atom2.position)\n        r_mag = np.linalg.norm(r)\n\n        # Add cutoff distance for stability\n        if r_mag &gt; 2.5*sigma:\n            return np.zeros(2)\n\n        force_mag = 24 * epsilon * (\n            2 * (sigma/r_mag)**13\n            - (sigma/r_mag)**7\n        )\n        force = force_mag * r/r_mag\n        return force\n\n\nclass MDSimulation:\n    def __init__(self, atoms, forcefield, timestep, box_size):\n        self.atoms = atoms\n        self.forcefield = forcefield\n        self.forcefield.box_size = box_size  # Set box size in forcefield\n        self.timestep = timestep\n        self.box_size = np.array(box_size)\n        self.initial_energy = None\n        self.energy_history = []\n\n    def minimum_image_distance(self, pos1, pos2):\n        \"\"\"Calculate minimum image distance between two positions\"\"\"\n        delta = pos1 - pos2\n        # Apply minimum image convention\n        delta = delta - self.box_size * np.round(delta / self.box_size)\n        return delta\n\n    def calculate_forces(self):\n        # Reset all forces\n        for atom in self.atoms:\n            atom.reset_force()\n\n        # Calculate forces between all pairs\n        for i, atom1 in enumerate(self.atoms):\n            for atom2 in self.atoms[i+1:]:\n                force = self.forcefield.calculate_lj_force(atom1, atom2)\n                atom1.add_force(force)\n                atom2.add_force(-force)  # Newton's third law\n\n    def update_positions_and_velocities(self):\n        # First step: Update positions using current forces\n        for atom in self.atoms:\n            atom.update_position(self.timestep)\n            # Apply periodic boundary conditions\n            atom.apply_periodic_boundaries(self.box_size)\n\n        # Store current forces for velocity update\n        old_forces = {atom.id: atom.force.copy() for atom in self.atoms}\n\n        # Recalculate forces with new positions\n        self.calculate_forces()\n\n        # Second step: Update velocities using average of old and new forces\n        for atom in self.atoms:\n            atom.update_velocity(self.timestep, atom.force)\n\n\ndef create_grid_atoms(num_atoms, box_size, mass=1.0, random_offset=0.1):\n    box_size = np.array(box_size)\n\n    # Calculate grid dimensions\n    n = int(np.ceil(np.sqrt(num_atoms)))\n    spacing = np.min(box_size) / n\n\n    atoms = []\n    for i in range(num_atoms):\n        # Calculate grid position\n        row = i // n\n        col = i % n\n\n        # Base position\n        pos = np.array([col * spacing + spacing/2,\n                       row * spacing + spacing/2])\n\n        # Add random offset\n        pos += (np.random.rand(2) - 0.5) * spacing * random_offset\n\n        # Create atom\n        atoms.append(Atom(i, 'H', pos, mass=mass))\n\n    return atoms"
  },
  {
    "objectID": "seminars/seminar04/md4.html",
    "href": "seminars/seminar04/md4.html",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "",
    "text": "In the last seminar we have defined the class Atom that represents an atom in the simulation. This time, we would like to a force field to the simulation. We will use for out simulations the Lennard-Jones potential that we have had a look at initiall. We will implement this force field in a class ForceField that will contain the parameters of the force field and the methods to calculate the forces between the atoms."
  },
  {
    "objectID": "seminars/seminar04/md4.html#the-forcefield-class",
    "href": "seminars/seminar04/md4.html#the-forcefield-class",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "The ForceField Class",
    "text": "The ForceField Class\nThe force field is a class that contains the parameters of the force field and the methods to calculate the forces between the atoms. The class ForceField has the following attributes:\n\nsigma: The parameter sigma of the Lennard-Jones potential\nepsilon: The parameter epsilon of the Lennard-Jones potential\n\nThese parameters are specific for each atom type. We will store these parameters in a dictionary where the keys are the atom types and the values are dictionaries containing the parameters sigma and epsilon. The class ForceField also contains the box size of the simulation. This is needed to apply periodic boundary conditions.\nclass ForceField:\n    def __init__(self):\n        self.parameters = {\n            'C': {'epsilon': 1.615, 'sigma': 1.36},\n            'H': {'epsilon': 1.0, 'sigma': 1.0 },\n            'O': {'epsilon': 1.846, 'sigma': 3.0},\n        }\n        self.box_size = None  # Will be set when initializing the simulation\nYou will have certainly noticed that the parameters I defined do not correspond to the real values of the Lennard-Jones potential. Remember that the values for the hydrogen atom are typically\n\n\\(\\sigma \\approx 2.38\\) √Ö = \\(2.38 \\times 10^{-10}\\) meters\n\\(\\epsilon \\approx 0.0167\\) kcal/mol = \\(1.16 \\times 10^{-21}\\) joules\n\nThese are all small numbers and we will use larger values to make the simulation more stable. Actually, the Lenard-Jones potential provides a natural length and energy scale for the simulation. The length scale is the parameter \\(\\sigma\\) and the energy scale is the parameter \\(\\epsilon\\). We can therefore set \\(\\sigma_{LJ}=1\\) and \\(\\epsilon_{LJ}=1\\) and scale all other parameters accordingly. This is a common practice in molecular dynamics simulations.\nDue to this rescaling energy, temperature and time units are also not the same as in the real world. We will use the following units:\n\nEnergy: \\(\\epsilon_{LJ} = \\epsilon_{H}/\\epsilon_{H} = 1\\)\nLength: \\(\\sigma_{LJ} = 1\\)\nMass: \\(m_{LJ} = 1\\)\n\nThis means now that all energies, for example, have to be scales by _{H} also the thermal energy. As thermal energy is related to temperature, then the temperature of the Lennard-Jones system\n\\[\nT_{LJ}=\\frac{k_B T}{\\epsilon_{LJ}}\n\\]\nwhich is, in the case of using the hydrogen energy scale, \\(T_{LJ}=3.571\\). for \\(T=300\\, K\\). For the time scale, we have to consider the mass of the hydrogen atom. The time scale is given by\n\\[\nt_{LJ}=\\frac{t}{\\sigma}\\sqrt{\\frac{\\epsilon}{m_{H}}}\n\\]\nThus a time unit of \\(1\\, fs\\) corresponds to \\(t_{LJ}=0.099\\). Thus using a timestep of 0.01 in reduced units would correspond to a real world timestep of just 1 fs. The table below shows the conversion factors for the different units. Simulating a Lennard-Jones system in reduced units therefore allows you to rescale to a real systems with the help of these conversion factors.\n\\[\n\\begin{array}{c|c}\n\\hline \\mathrm{r}^* & \\mathrm{r} \\sigma^{-1} \\\\\n\\hline \\mathrm{~m}^* & \\mathrm{mM}^{-1} \\\\\n\\hline \\mathrm{t}^* & \\mathrm{t} \\sigma^{-1} \\sqrt{\\epsilon / M} \\\\\n\\hline \\mathrm{~T}^* & \\mathrm{k}_B T \\epsilon^{-1} \\\\\n\\hline \\mathrm{E}^* & \\mathrm{E} \\epsilon^{-1} \\\\\n\\hline \\mathrm{~F}^* & \\mathrm{~F} \\sigma \\epsilon^{-1} \\\\\n\\hline \\mathrm{P}^* & \\mathrm{P} \\sigma^3 \\epsilon^{-1} \\\\\n\\hline \\mathrm{v}^* & \\mathrm{v} \\sqrt{M / \\epsilon} \\\\\n\\hline \\rho^* & \\mathrm{~N} \\sigma^3 V^{-1} \\\\\n\\hline\n\\end{array}\n\\]\n\nApply mixing rules when needed\n\nget_pair_parameters\nWhen we looked at the Lennard-Jones potential we realized that it reflects the pair interaction between the same atoms. However, in a molecular dynamics simulation, we have different atoms interacting with each other. We need to define the parameters of the interaction between different atoms. This is done using mixing rules. The most common mixing rule is the Lorentz-Berthelot mixing rule. The parameters of the interaction between two different atoms are calculated as follows:\ndef get_pair_parameters(self, type1, type2):\n    # Apply mixing rules when needed\n    eps1 = self.parameters[type1]['epsilon']\n    eps2 = self.parameters[type2]['epsilon']\n    sig1 = self.parameters[type1]['sigma']\n    sig2 = self.parameters[type2]['sigma']\n\n    # Lorentz-Berthelot mixing rules\n    epsilon = np.sqrt(eps1 * eps2)\n    sigma = (sig1 + sig2) / 2\n\n    return epsilon, sigma\nWe therefore introduce the method get_pair_parameters that calculates the parameters of the Lennard-Jones potential between two different atoms. The method takes the atom types as arguments and returns the parameters epsilon and sigma of the Lennard-Jones potential between these two atoms. The method applies the Lorentz-Berthelot mixing rules to calculate the parameters. The method returns the parameters epsilon and sigma of the Lennard-Jones potential between the two atoms.\n\n\n\nApply minimum image convention\n\nminimum_image_distance\nSimilarly, we already realized that using a finite box size requires to introduce boundary conditions. We decided that periodic boundary conditions are actually most convinient. However, this is introducing a new problem. When we calculate the distance between two atoms, we have to consider the minimum image distance. This means that we have to consider the distance between two atoms in the nearest image. This is done by applying the minimum image convention. The method minimum_image_distance calculates the minimum image distance between two positions. The method takes the positions of the two atoms as arguments and returns the minimum image distance between the two positions. The method applies the minimum image convention to calculate the minimum image distance.\ndef minimum_image_distance(self, pos1, pos2):\n    \"\"\"Calculate minimum image distance between two positions\"\"\"\n    delta = pos1 - pos2\n    # Apply minimum image convention\n    delta = delta - self.box_size * np.round(delta / self.box_size)\n    return delta\n\n\n\nCalculate the Lennard-Jones force between two atoms\n\ncalculate_lj_force\nFinally we can calculate the Lennard-Jones force between two atoms. The method calculate_lj_force calculates the Lennard-Jones force between two atoms. The method takes the two atoms as arguments and returns the force between the two atoms. The method calculates the Lennard-Jones force between the two atoms using the Lennard-Jones potential. The method returns the force between the two atoms.\ndef calculate_lj_force(self, atom1, atom2):\n    epsilon, sigma = self.get_pair_parameters(atom1.type, atom2.type)\n    r = self.minimum_image_distance(atom1.position, atom2.position)\n    r_mag = np.linalg.norm(r)\n\n    # Add cutoff distance for stability\n    if r_mag &gt; 2.5*sigma:\n        return np.zeros(2)\n\n    force_mag = 24 * epsilon * (\n        2 * (sigma/r_mag)**13\n        - (sigma/r_mag)**7\n    )\n    force = force_mag * r/r_mag\n    return force\nWith these parts we have now a complete force field class which we can add to our simulation code.\n\n\n\n\n\n\nComplete ForceField class\n\n\n\n\n\nclass ForceField:\n    def __init__(self):\n        self.parameters = {\n            'C': {'epsilon': 1.615, 'sigma': 1.36},\n            'H': {'epsilon': 1.0, 'sigma': 1.0 },\n            'O': {'epsilon': 1.846, 'sigma': 3.0},\n        }\n        self.box_size = None  # Will be set when initializing the simulation\n\n    def get_pair_parameters(self, type1, type2):\n        # Apply mixing rules when needed\n        eps1 = self.parameters[type1]['epsilon']\n        eps2 = self.parameters[type2]['epsilon']\n        sig1 = self.parameters[type1]['sigma']\n        sig2 = self.parameters[type2]['sigma']\n\n        # Lorentz-Berthelot mixing rules\n        epsilon = np.sqrt(eps1 * eps2)\n        sigma = (sig1 + sig2) / 2\n\n        return epsilon, sigma\n\n    def minimum_image_distance(self, pos1, pos2):\n        \"\"\"Calculate minimum image distance between two positions\"\"\"\n        delta = pos1 - pos2\n        # Apply minimum image convention\n        delta = delta - self.box_size * np.round(delta / self.box_size)\n        return delta\n\n    def calculate_lj_force(self, atom1, atom2):\n        epsilon, sigma = self.get_pair_parameters(atom1.type, atom2.type)\n        r = self.minimum_image_distance(atom1.position, atom2.position)\n        r_mag = np.linalg.norm(r)\n\n        # Add cutoff distance for stability\n        if r_mag &gt; 2.5*sigma:\n            return np.zeros(2)\n\n        force_mag = 24 * epsilon * (\n            2 * (sigma/r_mag)**13\n            - (sigma/r_mag)**7\n        )\n        force = force_mag * r/r_mag\n        return force"
  },
  {
    "objectID": "seminars/seminar04/md4.html#md-simulation-class",
    "href": "seminars/seminar04/md4.html#md-simulation-class",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "MD Simulation Class",
    "text": "MD Simulation Class\nThe last thing we need to do is to implement the MD simulation class. This class will be responsible for running the simulation. It is the controller of the simulation, who coordinates everything. By keeping this in a class you may even run several simulations simultaneously. This is not the case here, but it is a good practice to keep the simulation in a class.\n\nMDSimulation class constructor\nThis is just the constructor of the MD Simulation class. It takes the atoms, the force field, the timestep, and the box size as input. It initializes the simulation with the given parameters and sets the initial energy of the system to None. It also initializes an empty list to store the energy history of the system. The latter ones are not used for the moment but could be important later.\nclass MDSimulation:\n    def __init__(self, atoms, forcefield, timestep, box_size):\n        self.atoms = atoms\n        self.forcefield = forcefield\n        self.forcefield.box_size = box_size  # Set box size in forcefield\n        self.timestep = timestep\n        self.box_size = np.array(box_size)\n        self.initial_energy = None\n        self.energy_history = []\n\n\ncalculate_forces method\nThe calculate_forces method calculates the forces between all pairs of atoms in the system. It first resets all forces on the atoms to zero. Then, it calculates the forces between all pairs of atoms using the Lennard-Jones force calculation from the force field class. The method updates the forces on the atoms accordingly. The method does not return anything.\n    def calculate_forces(self):\n        # Reset all forces\n        for atom in self.atoms:\n            atom.reset_force()\n\n        # Calculate forces between all pairs\n        for i, atom1 in enumerate(self.atoms):\n            for atom2 in self.atoms[i+1:]:\n                force = self.forcefield.calculate_lj_force(atom1, atom2)\n                atom1.add_force(force)\n                atom2.add_force(-force)  # Newton's third law\n\n\nupdate_positions_and_velocities method\nThe update_positions_and_velocities method does exactly what its name says. It first of all updates the positions by calling the specific method of the atom. Then it is applying periodic boundary conditions. After that, it stores the current forces for the velocity update. Then it recalculates the forces with the new positions. Finally, it updates the velocities using the average of the old and new forces. The method does not return anything.\n    def update_positions_and_velocities(self):\n        # First step: Update positions using current forces\n        for atom in self.atoms:\n            atom.update_position(self.timestep)\n            # Apply periodic boundary conditions\n            atom.apply_periodic_boundaries(self.box_size)\n\n        # Store current forces for velocity update\n        old_forces = {atom.id: atom.force.copy() for atom in self.atoms}\n\n        # Recalculate forces with new positions\n        self.calculate_forces()\n\n        # Second step: Update velocities using average of old and new forces\n        for atom in self.atoms:\n            atom.update_velocity(self.timestep, atom.force)\nWith these methods, we have a complete simulation class that can run a molecular dynamics simulation for a given number of steps. The simulation class will keep track of the energy of the system at each step, which can be used to analyze the behavior of the system over time.\n\n\n\n\n\n\nComplete MDSimulation class\n\n\n\n\n\n\nclass MDSimulation:\n    def __init__(self, atoms, forcefield, timestep, box_size):\n        self.atoms = atoms\n        self.forcefield = forcefield\n        self.forcefield.box_size = box_size  # Set box size in forcefield\n        self.timestep = timestep\n        self.box_size = np.array(box_size)\n        self.initial_energy = None\n        self.energy_history = []\n\n\n    def calculate_forces(self):\n        # Reset all forces\n        for atom in self.atoms:\n            atom.reset_force()\n\n        # Calculate forces between all pairs\n        for i, atom1 in enumerate(self.atoms):\n            for atom2 in self.atoms[i+1:]:\n                force = self.forcefield.calculate_lj_force(atom1, atom2)\n                atom1.add_force(force)\n                atom2.add_force(-force)  # Newton's third law\n\n    def update_positions_and_velocities(self):\n        # First step: Update positions using current forces\n        for atom in self.atoms:\n            atom.update_position(self.timestep)\n            # Apply periodic boundary conditions\n            atom.apply_periodic_boundaries(self.box_size)\n\n        # Store current forces for velocity update\n        old_forces = {atom.id: atom.force.copy() for atom in self.atoms}\n\n        # Recalculate forces with new positions\n        self.calculate_forces()\n\n        # Second step: Update velocities using average of old and new forces\n        for atom in self.atoms:\n            atom.update_velocity(self.timestep, atom.force)\n\n\n\n\nNow we have the atom class, the force field class, and the simulation class. We can use these classes to run a molecular dynamics simulation of a simple Lennard-Jones system. In the next seminar, we still have to find a way to\n\ninitialize the positions of the atoms in an appropriate way\nto provide them with a velocity distribution that matches the temperature of the system\nto run the simulation and keep the temperature constant\nto trace the energy in the system over time"
  },
  {
    "objectID": "seminars/seminar01_24/MDSimulation.html",
    "href": "seminars/seminar01_24/MDSimulation.html",
    "title": "Molecular Dynamics Simulations in EMPP 2024",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'font.size': 8,\n                     'lines.linewidth': 1,\n                     'lines.markersize': 10,\n                     'axes.labelsize': 10,\n                     'axes.titlesize': 10,\n                     'xtick.labelsize' : 10,\n                     'ytick.labelsize' : 10,\n                     'xtick.top' : True,\n                     'xtick.direction' : 'in',\n                     'ytick.right' : True,\n                     'ytick.direction' : 'in',})\n\ndef get_size(w,h):\n    return((w/2.54,h/2.54))"
  },
  {
    "objectID": "seminars/seminar01_24/MDSimulation.html#lenard-jones-potential",
    "href": "seminars/seminar01_24/MDSimulation.html#lenard-jones-potential",
    "title": "Molecular Dynamics Simulations in EMPP 2024",
    "section": "Lenard-Jones Potential",
    "text": "Lenard-Jones Potential\n\ndef lennard_jones(r, epsilon=1, sigma=1):\n    return 4 * epsilon * ((sigma/r)**12 - (sigma/r)**6)\n\nr = np.linspace(0.8, 3, 1000)\nV = lennard_jones(r)\n\nplt.figure(figsize=get_size(8, 6),dpi=150)\nplt.plot(r, V, 'b-', linewidth=2)\nplt.grid(True)\nplt.xlabel('r/œÉ')\nplt.ylabel('V/Œµ')\nplt.title('Lennard-Jones Potential')\nplt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\nplt.ylim(-1.5, 3)\nplt.show()"
  },
  {
    "objectID": "seminars/seminar01_24/MDSimulation.html#taylor-expansion",
    "href": "seminars/seminar01_24/MDSimulation.html#taylor-expansion",
    "title": "Molecular Dynamics Simulations in EMPP 2024",
    "section": "Taylor Expansion",
    "text": "Taylor Expansion\n\nx = np.linspace(-2*np.pi, 2*np.pi, 1000)\ny = np.sin(x)\ny_taylor = x - 1/6*x**3\ny_taylor1 = x - 1/6*x**3+x**5/120\n\n\nplt.figure(figsize=get_size(8, 6),dpi=150)\nplt.plot(x, y, 'b-', label='sin(x)', linewidth=2)\nplt.plot(x, y_taylor, 'r--', label='O(5)', linewidth=2)\nplt.plot(x, y_taylor1, 'g--', label='O(7)', linewidth=2)\nplt.grid(True)\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.xlim(-2,2)\nplt.ylim(-1.3,1.3)\nplt.title('Taylor Expansion of sin(x)')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "seminars/seminar01_24/MDSimulation.html#velocity-verlet",
    "href": "seminars/seminar01_24/MDSimulation.html#velocity-verlet",
    "title": "Molecular Dynamics Simulations in EMPP 2024",
    "section": "Velocity Verlet",
    "text": "Velocity Verlet\n\ng = 9.81  # m/s^2\ndt = 0.01  # time step\nt_max = 2.0  # total simulation time\nsteps = int(t_max/dt)\n\n# Initial conditions\ny0 = 20.0  # initial height\nv0 = 0.0   # initial velocity\n\n\n# Arrays to store results\nt = np.zeros(steps)\ny = np.zeros(steps)\nv = np.zeros(steps)\na = np.zeros(steps)\n\n# Initial values\ny[0] = y0\nv[0] = v0\na[0] = -g\n\n# Velocity Verlet integration\nfor i in range(1, steps):\n    t[i] = i * dt\n    y[i] = y[i-1] + v[i-1] * dt + 0.5 * a[i-1] * dt**2  # update position\n    a_new = -g                                          # new acceleration (assuming constant gravity)\n    v[i] = v[i-1] + 0.5 * (a[i-1] + a_new) * dt  # update velocity\n    a[i] = a_new  # store new acceleration\n\nplt.figure(figsize=get_size(8, 6), dpi=150)\nplt.plot(t, y)\nplt.xlabel('Time (s)')\nplt.ylabel('Height (m)')\nplt.title('Free Fall Motion')\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "seminars/seminar01_24/MDSimulation.html#class-for-an-atom",
    "href": "seminars/seminar01_24/MDSimulation.html#class-for-an-atom",
    "title": "Molecular Dynamics Simulations in EMPP 2024",
    "section": "Class for an Atom",
    "text": "Class for an Atom"
  },
  {
    "objectID": "seminars/Assignment 3.html",
    "href": "seminars/Assignment 3.html",
    "title": "Assignment 3",
    "section": "",
    "text": "Problem 1\nDefine a class with the name particle with a constructor that initializes the property D with twice the value supplied as an argument to the constructor.¬†\n\nclass particle:\n    def __init__(self,R):\n        self.D=2*R\n\nProblem 2\nWrite a class particle that has a class variable id. Implement a constructor that assigns a unique id property to each instance from the class id, increasing by 1 each time an instance (object) is created. The first created object id should be 1.\n\nclass particle:\n    id = 0\n    def __init__(self):\n        particle.id+=1\n        self.id = particle.id\n    \n    def __del__(self):\n        particle.id-=1\n\nProblem 3\nWrite a class particle that is constructed with two parameters that are stored in the properties R¬†and type_t of the object (R goes first). The type_t property should be of type string and be either ‚Äúcircle‚Äù or ‚Äúsquare‚Äù.\nWrite a class method¬†area that calculates the area of the object from the parameter R depending on the property type_t. The result of the area calculation should be stored in the property A, which should be 0 initially.\nCreate an object c which is a circle and compute its area. Create an object s which is a square without computing its area. The R parameter of these objects can be an arbitrary positive number.\nYou have two answer attempts without penalty.\n\nfrom math import pi\n\nclass particle:\n    def __init__(self, R, type_t):\n        self.R = R\n        self.type_t = type_t\n        self.A = 0\n        \n    def area(self):\n        if self.type_t == \"square\":\n            self.A = self.R**2\n        elif self.type_t == \"circle\":\n            self.A = pi*self.R**2\n        else:\n            self.A = 0\n            \nc=particle(3,\"circle\")\nc.area()\ns=particle(3,\"square\")\n\n\nc.A\n\n28.274333882308138"
  },
  {
    "objectID": "seminars/Assignment5.html",
    "href": "seminars/Assignment5.html",
    "title": "Assignment 5",
    "section": "",
    "text": "Write a function \\(G(f, x, ...)\\) that takes a function \\(f(x,...)\\) as an argument with a variable number of additional parameters. The function¬†G shall take the arguments f, x and the parameters of f.¬†It shall calculate the sum of all function values of f.¬†You should import the numpy module as np. A definition of f¬†is not required.¬†\n\nimport numpy as np\n\ndef G(f,x,*params):\n    return np.sum(f(x,*params))\n\n\nx=np.linspace(0,10,100)-5\ny=np.linspace(0,10,100)-5\n\nX,Y=np.meshgrid(x,y)\n\n\nR=np.zeros([len(x),len(y)])\n\nfor i in range(0,len(x)): # gehe durch jede Zeile\n    for j in range(0,len(y)): # gehe durch jede Spalte\n        R[i,j]=np.sqrt(x[j]**2+y[i]**2)\n\n\nimport matplotlib.pyplot as plt\n\n\nplt.contour(x,y,R)\nplt.axhline(y=0)\nplt.axvline(x=0)\nplt.axis(\"square\")\n\n\n\n\n\n\n\n\n\ndef line(x,a,b):\n   return a*x+b\n\nx = np.arange(5,11,1)\nprint(G(line,x,5,5))\n\n255"
  },
  {
    "objectID": "seminars/Assignment5.html#problem-1",
    "href": "seminars/Assignment5.html#problem-1",
    "title": "Assignment 5",
    "section": "",
    "text": "Write a function \\(G(f, x, ...)\\) that takes a function \\(f(x,...)\\) as an argument with a variable number of additional parameters. The function¬†G shall take the arguments f, x and the parameters of f.¬†It shall calculate the sum of all function values of f.¬†You should import the numpy module as np. A definition of f¬†is not required.¬†\n\nimport numpy as np\n\ndef G(f,x,*params):\n    return np.sum(f(x,*params))\n\n\nx=np.linspace(0,10,100)-5\ny=np.linspace(0,10,100)-5\n\nX,Y=np.meshgrid(x,y)\n\n\nR=np.zeros([len(x),len(y)])\n\nfor i in range(0,len(x)): # gehe durch jede Zeile\n    for j in range(0,len(y)): # gehe durch jede Spalte\n        R[i,j]=np.sqrt(x[j]**2+y[i]**2)\n\n\nimport matplotlib.pyplot as plt\n\n\nplt.contour(x,y,R)\nplt.axhline(y=0)\nplt.axvline(x=0)\nplt.axis(\"square\")\n\n\n\n\n\n\n\n\n\ndef line(x,a,b):\n   return a*x+b\n\nx = np.arange(5,11,1)\nprint(G(line,x,5,5))\n\n255"
  },
  {
    "objectID": "seminars/Assignment5.html#problem-2",
    "href": "seminars/Assignment5.html#problem-2",
    "title": "Assignment 5",
    "section": "Problem 2",
    "text": "Problem 2\nWrite a Python program that:\nConstructs a 10x10 matrix D that approximates the first derivative operator using finite differences. Uses for loops to build the matrix. Do not import any library(math, NumPy, etc.). Apply the following finite difference schemes:\n\nForward difference¬†at the first point.\nBackward difference¬†at the last point.\nCentral difference at all interior points.\n\nAssumes a constant grid spacing h (you can set¬†h = 1 for simplicity).\nFinite Difference Schemes:\nForward Difference (at the first point, i=0): \\(f'(x_0) \\approx \\frac{f(x_1) - f(x_0)}{h}\\)\nBackward Difference (at the last point, i=N-1):¬†¬† \\(f'(x_{N-1}) \\approx \\frac{f(x_{N-1}) - f(x_{N-2})}{h}\\)\nCentral Difference (at interior points, 1 ‚â§ i ‚â§ N-2):¬†¬† \\(f'(x_i) \\approx \\frac{f(x_{i+1}) - f(x_{i-1})}{2h}\\)\nInstructions:\n\nInitialize a 10x10 matrix D filled with zeros.\nUse for loops to populate the matrix according to the finite difference schemes.\nDo not print anything.\n\n\nliste=[[(i,j) for i in range(N)] for j in range(N)]\n\n\nN = 10\nh = 1.0\n\nD = [[0.0 for _ in range(N)] for _ in range(N)]\n\n# Forward difference for the first point\nD[0][0] = -1.0 / h\nD[0][1] = 1.0 / h\n\n# Central difference for interior points\nfor i in range(1,N-1):\n    D[i][i-1] = -0.5 / h\n    D[i][i+1] = 0.5 / h\n    \n# Backward difference for the last point\nD[N-1][N-2] = -1.0 / h\nD[N-1][N-1] = 1.0 / h\n\n\\[\nU(\\vec{r},t)=U_0\\exp(i(\\omega t- \\vec{k} \\cdot \\vec{r}))\n\\]\n\nx=np.linspace(0,10,100)\ny=np.linspace(0,10,100)\nZ=np.zeros([10,10])\n\nX,Y=np.meshgrid(x,y)\n\nR=np.array([X,Y,0],dtype=object)\n\n\nplt.imshow(np.real(np.exp(-1j*np.dot(k,R))))\n\n\n\n\n\n\n\n\n\nnp.dot(k,R)plt.imshow()\n\narray([[ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ]])\n\n\n\nnp.dot(k,R)\n\narray([[ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ],\n       [ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,\n         5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ]])\n\n\n\nk=np.array([1,1,0])\n\n\ndef plane(r,omega=1,t=0,k):\n    \n\nnp.complex128(1j)\n\n\n\n\n\narray([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.],\n       [10., 11., 12., 13., 14., 15., 16., 17., 18., 19.],\n       [20., 21., 22., 23., 24., 25., 26., 27., 28., 29.],\n       [30., 31., 32., 33., 34., 35., 36., 37., 38., 39.],\n       [40., 41., 42., 43., 44., 45., 46., 47., 48., 49.],\n       [50., 51., 52., 53., 54., 55., 56., 57., 58., 59.],\n       [60., 61., 62., 63., 64., 65., 66., 67., 68., 69.],\n       [70., 71., 72., 73., 74., 75., 76., 77., 78., 79.],\n       [80., 81., 82., 83., 84., 85., 86., 87., 88., 89.],\n       [90., 91., 92., 93., 94., 95., 96., 97., 98., 99.]])\n\n\n\nimport numpy as np\n\nA = np.array([\n    [1, -1, 3, -2],\n    [-2, 4, -3, 1],\n    [3, -1, 10, -4],\n    [4, -3, 8, -2],\n])\n\nB = np.array([1, 0.5, 2.9, 0.6])\n\nx = np.linalg.solve(A, B)\n\n\ntype(np.round(sum(x),1))\n\nnumpy.float64"
  },
  {
    "objectID": "seminars/seminar01/seminar10.html#what-are-neural-networks",
    "href": "seminars/seminar01/seminar10.html#what-are-neural-networks",
    "title": "Neural Networks",
    "section": "What are Neural Networks?",
    "text": "What are Neural Networks?\nNeural networks are computational models inspired by how our brains process information. Just like our brain consists of interconnected neurons that process and transmit signals, artificial neural networks consist of mathematical ‚Äúneurons‚Äù that process numerical information. They‚Äôre particularly powerful for:\n\nRecognizing patterns in data\nMaking predictions\nClassifying information\nSolving complex problems"
  },
  {
    "objectID": "seminars/seminar01/seminar10.html#why-neural-networks-in-physics",
    "href": "seminars/seminar01/seminar10.html#why-neural-networks-in-physics",
    "title": "Neural Networks",
    "section": "Why Neural Networks in Physics?",
    "text": "Why Neural Networks in Physics?\nIn physics, we often encounter situations that push the boundaries of traditional approaches. Sometimes the mathematical models grow too complex to solve directly, while in other cases we face the challenge of analyzing vast amounts of experimental data. We frequently need to make predictions even when we have incomplete information about a system. These scenarios represent key areas where neural networks can provide valuable solutions.\nNeural networks help us with these challenges! Some real-world applications include:\n\nParticle physics: Identifying particles in detector data\nAstronomy: Classifying galaxies\nMaterials science: Predicting material properties\nQuantum mechanics: Solving many-body problems\nBiological physics: Modeling neural activity\nActive matter: Predicting collective behavior"
  },
  {
    "objectID": "seminars/seminar01/seminar10.html#data-for-neural-networks-teaching-computers-to-read-numbers",
    "href": "seminars/seminar01/seminar10.html#data-for-neural-networks-teaching-computers-to-read-numbers",
    "title": "Neural Networks",
    "section": "Data for Neural Networks: Teaching Computers to Read Numbers",
    "text": "Data for Neural Networks: Teaching Computers to Read Numbers\nLet‚Äôs start our journey into neural networks with an exciting challenge: teaching a computer to read handwritten numbers! We‚Äôll build this step by step using Python, starting with the basics and working our way up to something quite impressive.\nThink of this like teaching a child to recognize numbers - we‚Äôll start by teaching our computer to recognize just one number (zero), and then build up to recognizing all digits from 0 to 9.\n\n\n\nMNIS\n\n\nWe‚Äôll use a famous collection of handwritten numbers called the MNIST dataset. Imagine asking 70,000 different people to write down numbers - that‚Äôs what this dataset is! Each number is written on a small 28 x 28 grid (like graph paper), where each square (or pixel) is shaded in grayscale from white (0) to black (255)."
  },
  {
    "objectID": "seminars/seminar01/seminar10.html#getting-started-with-mnist",
    "href": "seminars/seminar01/seminar10.html#getting-started-with-mnist",
    "title": "Neural Networks",
    "section": "Getting Started with MNIST",
    "text": "Getting Started with MNIST\nJust like we need data from experiments in physics, we need data to train our neural network. Fortunately, other scientists have already collected this data for us:\n\nLoading Our Training Data\n\n\n\n\n\n\nHere, X contains all our images, and y contains the correct answer for each image (which number it is). Let‚Äôs look at one:\n\n\n\n\n\n\n\n\nMaking the Data Easier to Work With\nJust like we often normalize measurements in physics experiments (like dividing by the maximum value), we‚Äôll normalize our image data to be between 0 and 1:\n\n\n\n\n\n\n\n\nPreparing Our Training and Testing Sets\nFor now, we‚Äôll start simple: we‚Äôll just teach our network to recognize zeros. We‚Äôll mark zeros with a 1 and all other numbers with a 0:\n\n\n\n\n\n\nLike any good scientific experiment, we need both training data (to teach the network) and testing data (to check how well it learned). We‚Äôll use: - 60,000 images for training - 10,000 images for testing\n\n\n\n\n\n\nFinally, we shuffle our training data (like shuffling flashcards when studying):\n\n\n\n\n\n\nLet‚Äôs check our work by looking at one of our training images:\n\n\n\n\n\n\nTry looking at different images (change the number 39 above) until you find a zero - its label should be 1!"
  },
  {
    "objectID": "seminars/seminar01/seminar10.html#a-single-neuron",
    "href": "seminars/seminar01/seminar10.html#a-single-neuron",
    "title": "Neural Networks",
    "section": "A Single Neuron",
    "text": "A Single Neuron\nThe basic building block of any neural network is an artificial neuron. Similar to neurons in the human brain that process incoming signals and decide whether to fire or not, an artificial neuron processes numerical inputs through mathematical operations to produce a single output value. The following diagram shows a simple artificial neuron with two input values:\n\n\nUnderstanding Forward Propagation\nAn artificial neuron processes information in three distinct steps that together form what is called forward propagation:\n\nInput Weighting Each input value gets multiplied by a weight parameter. These weights determine how much influence each input has on the final output, similar to how synapses in biological neurons can be stronger or weaker. For two inputs, this operation looks like:\n\n\\[\\begin{eqnarray}\nx_{1}\\rightarrow x_{1} w_{1}\\\\\nx_{2}\\rightarrow x_{2} w_{2}\n\\end{eqnarray}\\]\n\nBias Addition After weighting the inputs, a bias value \\(b\\) is added to the sum. The bias helps the neuron learn by shifting the weighted sum up or down, making it easier or harder for the neuron to produce a strong output signal:\n\n\\[\\begin{equation}\nx_{1} w_{1}+ x_{2} w_{2}+b\n\\end{equation}\\]\n\nActivation Function The final step applies an activation function \\(\\sigma()\\) to the weighted sum plus bias. This function introduces non-linearity into the network, allowing it to learn complex patterns:\n\n\\[\\begin{equation}\ny=\\sigma( x_{1} w_{1}+ x_{2} w_{2}+b)\n\\end{equation}\\]\nThe activation function used in this example is called the sigmoid function. This function is particularly useful because it takes any input number (positive or negative, large or small) and transforms it into an output between 0 and 1. This property makes the sigmoid function ideal for tasks where the output should represent a probability or a binary decision.\nFor mathematical convenience, the above steps can be written more compactly using vector notation:\n\\[\\begin{equation*}\n\\hat{y} = \\sigma(w^{\\rm T} x + b)\\ .\n\\end{equation*}\\]\nThe sigmoid function has the following mathematical form: \\[\\begin{equation*}\n\\sigma(z) = \\frac{1}{1+{\\rm e}^{-z}}\\ .\n\\end{equation*}\\]\nHere is a Python implementation of the sigmoid function:\n\n\n\n\n\n\nThe following code visualizes how the sigmoid function transforms input values:\n\n\n\n\n\n\nTo understand how these components work together, consider a simple example with two inputs. Given:\n\\[\\begin{eqnarray}\nw=[0,1]\\\\\nb=4\n\\end{eqnarray}\\]\nAnd input values:\n\\[\\begin{eqnarray}\nx=[2,3]\n\\end{eqnarray}\\]\nThe computation becomes:\n\\[\\begin{equation}\ny=f(w\\cdot x+b)=f(7)=0.999\n\\end{equation}\\]\nThis process of moving from inputs to output through these mathematical operations is called forward propagation. The goal is to extend this concept to create a network capable of processing images, which requires 784 inputs (one for each pixel in a 28 x 28 image) and producing meaningful outputs.\nFor computational efficiency, these calculations can be performed on multiple inputs simultaneously using matrix operations. The forward pass equation becomes:\n\\[\\begin{equation*}\n\\hat{y} = \\sigma(w^{\\rm T} X + b)\\ .\n\\end{equation*}\\]\nIn this matrix form, \\(\\hat{y}\\) represents a vector of outputs rather than a single value. The implementation splits this computation into two parts:\n\nCalculate the weighted sum: Z = np.matmul(W.T, X) + b\nApply the activation function: A = sigmoid(Z)\n\nThis separation into distinct steps makes the code clearer and prepares for the more complex calculations needed in the backward propagation phase.\n\n\nLoss Function: Measuring How Wrong We Are\nNow that our network can make predictions, we need a way to measure how accurate those predictions are. Just like we measure error in physics experiments, we need to measure the error (or ‚Äúloss‚Äù) in our neural network‚Äôs predictions.\nThe simplest way would be to use mean squared error, which you‚Äôve seen before in data fitting:\n\\[\\begin{equation}\nMSE(y,\\hat{y})=\\frac{1}{n}\\sum_{i=1}^{n}(y-\\hat{y})^2\n\\end{equation}\\]\nHere, \\(y\\) is the true value (what we know is correct) and \\(\\hat{y}\\) is our network‚Äôs prediction.\nHowever, for this type of classification problem, we‚Äôll use a different measure called cross-entropy. For a single training example, it looks like this:\n\\[\\begin{equation*}\nL(y,\\hat{y}) = -y\\log(\\hat{y})-(1-y)\\log(1-\\hat{y})\\ .\n\\end{equation*}\\]\nWhen we have many training examples (\\(m\\) of them), we take the average:\n\\[\\begin{equation*}\nL(Y,\\hat{Y}) = -\\frac{1}{m}\\sum_{i = 0}^{m}y^{(i)}\\log(\\hat{y}^{(i)})-(1-y^{(i)})\\log(1-\\hat{y}^{(i)})\\ .\n\\end{equation*}\\]\nHere‚Äôs how we implement this in code:"
  },
  {
    "objectID": "seminars/seminar01/seminar10.html#training-the-network-making-our-network-learn",
    "href": "seminars/seminar01/seminar10.html#training-the-network-making-our-network-learn",
    "title": "Neural Networks",
    "section": "Training the Network: Making Our Network Learn",
    "text": "Training the Network: Making Our Network Learn\nThink of training a neural network like teaching a student - we need to: 1. See how well they‚Äôre doing (measure the loss) 2. Give feedback on what to improve 3. Let them practice and improve\n\nBackward Propagation: Learning from Mistakes\nJust like we adjust our aim when throwing a ball based on how far we missed, our network needs to adjust its weights and biases based on its errors. The loss function depends on all our weights and biases:\n\\[\nL(w_{1},w_{2},w_{3},\\ldots ,b_{1},b_{2},b_{3},\\ldots)\n\\]\nTo improve, we need to know how changing each weight affects our error. We can find this using partial derivatives:\n\\[\n\\frac{\\partial L}{\\partial w_j}\n\\]\nThis tells us ‚Äúif we change weight \\(w_j\\) a little bit, how much will our error change?‚Äù This process of calculating how to adjust weights based on errors is called back propagation.\nCalculating How to Improve\nLet‚Äôs break this down into steps. For a single image, we can follow how changes flow through the network: \\[\\begin{align*}\nz &= w^{\\rm T} x + b\\ , \\\\\n\\hat{y} &= \\sigma(z)\\ , \\\\\nL(y,\\hat{y}) &= -y\\log(\\hat{y})-(1-y)\\log(1-\\hat{y})\\ .\n\\end{align*}\\]\nUsing the chain rule from calculus:\n\\[\\begin{align*}\n\\frac{\\partial L}{\\partial w_j} = \\frac{\\partial L}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial z}\\frac{\\partial z}{\\partial w_j}\n\\end{align*}\\] \nAfter working through the calculus (which we won‚Äôt detail here), we get three parts, each representing how different components of our network affect the final loss:\nFirst, we calculate how the loss changes with respect to our prediction (\\(\\hat{y}\\)):\n\\(\\partial L/\\partial\\hat{y}\\): \\[\\begin{align*}\n\\frac{\\partial L}{\\partial\\hat{y}} &= \\frac{\\hat{y} - y}{\\hat{y}(1-\\hat{y})}\n\\end{align*}\\]\nNext, we find how our prediction changes with respect to the weighted input (\\(z\\)). This is just the derivative of the sigmoid function:\n\\(\\partial \\hat{y}/\\partial z\\): \\[\\begin{align*}\n\\frac{\\partial }{\\partial z}\\sigma(z) &= \\hat{y}(1-\\hat{y})\n\\end{align*}\\]\nFinally, we calculate how the weighted input changes with respect to each weight. This is simply the corresponding input value:\n\\(\\partial z/\\partial w_j\\): \\[\\begin{align*}\n\\frac{\\partial }{\\partial w_j}(w^{\\rm T} x + b) &= x_j\n\\end{align*}\\]\nWhen we multiply these three components together using the chain rule, something remarkable happens - most terms cancel out, leaving us with this elegantly simple result: \\[\\begin{align*}\n\\frac{\\partial L}{\\partial w_j} = (\\hat{y} - y)x_j\\ .\n\\end{align*}\\]\nThis tells us that the adjustment to each weight should be proportional to both the prediction error (\\(\\hat{y} - y\\)) and the input value (\\(x_j\\)).\nWhen dealing with multiple training examples, we need to average these gradients:\n\\[\\begin{align*}\n\\frac{\\partial L}{\\partial w} = \\frac{1}{m} X(\\hat{y} - y)^{\\rm T}\\ .\n\\end{align*}\\]\nThe bias term follows a similar pattern. For a single example: \\[\\begin{align*}\n\\frac{\\partial L}{\\partial b} = (\\hat{y} - y)\\ .\n\\end{align*}\\]\nAnd for multiple training examples, we average the gradients: \\[\\begin{align*}\n\\frac{\\partial L}{\\partial b} = \\frac{1}{m}\\sum_{i=1}^{m}{(\\hat{y}^{(i)} - y^{(i)})}\\ .\n\\end{align*}\\]\nIn our code, these mathematical formulas translate directly into matrix operations: dW = (1/m) * np.matmul(X, (A-Y).T) and db = (1/m)*np.sum(A-Y, axis=1, keepdims=True).\n\n\nStochastic Gradient Descent: Teaching Our Network to Learn\nNow comes the exciting part - making our network learn! Just like how you adjust your throw when playing catch based on whether you threw too far or too short, our network needs to adjust its weights and biases based on its mistakes.\nWe‚Äôll use a learning method called stochastic gradient descent (SGD). Don‚Äôt let the fancy name scare you - it‚Äôs actually quite simple! Think of it like walking down a hill:\n\nLook around to see which way is steepest downhill (that‚Äôs the gradient)\nTake a small step in that direction\nRepeat until you reach the bottom\n\nMathematically, we update each weight using this formula:\n\\[\nw\\leftarrow w-\\eta\\frac{\\partial L}{\\partial w}\n\\]\nHere, \\(\\eta\\) (eta) is called the learning rate - it controls how big our steps are:\n\nToo large: We might overshoot the bottom\nToo small: Learning will take forever\n\nThe term \\(\\frac{\\partial L}{\\partial w}\\) tells us which direction to step:\n\nIf positive: The weight is too large, so decrease it\nIf negative: The weight is too small, so increase it\n\nWe do the same thing for the bias \\(b\\). Each complete pass through all our training data is called an ‚Äúepoch‚Äù.\nPhysics Connection: This is similar to finding the minimum of a potential well - we follow the direction where the potential decreases most rapidly!\n\n\nBuilding and Training Our First Network\nLet‚Äôs put everything together to create a network that can recognize handwritten numbers. We‚Äôll train it for 200 epochs (learning cycles) and watch how the loss decreases:\n\n\n\n\n\n\n\n\nEvaluating Our Network: How Well Did We Do?\nJust like in physics experiments, we need ways to measure how well our model performs. One powerful tool is the confusion matrix. Think of it as a report card for our network:\n\n\n\nconfusion_matrix\n\n\nThe confusion matrix shows: - True Positives (TP): We predicted ‚Äúyes‚Äù and were right - False Positives (FP): We predicted ‚Äúyes‚Äù but were wrong - True Negatives (TN): We predicted ‚Äúno‚Äù and were right - False Negatives (FN): We predicted ‚Äúno‚Äù but were wrong\nLet‚Äôs calculate this for our network:\n\n\n\n\n\n\n\n\nTesting Individual Images\nLet‚Äôs see our network in action! We can test it on individual images:"
  },
  {
    "objectID": "seminars/seminar01/seminar10.html#network-with-hidden-layers",
    "href": "seminars/seminar01/seminar10.html#network-with-hidden-layers",
    "title": "Neural Networks",
    "section": "Network with Hidden Layers",
    "text": "Network with Hidden Layers\nIn our example above, we just had an input layer and a single output neuron. More complex neural networks are containing many layers between the input layer and the output layer. These inbetween layers are called hidden layers. Here is a simple example of a neural network with a single hidden layer.\n\n\n\nhidden\n\n\nSo we have now and input layer with 784 inputs that are connected to 64 units in the hidden layer and 1 neuron in the output layer. We will not go through the derivations of all the formulas for the forward and backward passes this time. The code is a simple extension of what we did before and I hope easy to read.\n\n\n\n\n\n\nTo judge the newtork quality we do use again the confusion matrix."
  },
  {
    "objectID": "seminars/seminar01/seminar10.html#multiclass-network",
    "href": "seminars/seminar01/seminar10.html#multiclass-network",
    "title": "Neural Networks",
    "section": "Multiclass Network",
    "text": "Multiclass Network\nSo far we did only classify if the number we feed to the network is just a 0 or not. We would like to recognize the different number now and therefore need a multiclass network. Each number is then a class and per class, we have multiple realizations of handwritten numbers. We therefore have to create an output layer, which is not only containing a single neuron, but 10 neurons. Each of these neuron can output a value between 0 and 1. Whenever the output is 1, the index of the neuron represents the number predicted.\nThe output array\n[0,1,0,0,0,0,0,0,0,0]\nwould therefore correspond to the value 1.\nFor this purpose, we need to reload the right labels.\n\n\n\n\n\n\nThen we‚Äôll one-hot encode MNIST‚Äôs labels, to get a 10 x 70,000 array.\n\n\n\n\n\n\n\n\n\n\n\n\nWe also seperate into trainging and testing data\n\n\n\n\n\n\n\n\n\n\n\n\n\nChanges to the model\nOK, so let‚Äôs consider what changes we need to make to the model itself.\n\nForward Pass\nOnly the last layer of our network is changing. To add the softmax, we have to replace our lone, final node with a 10 unit layer. Its final activations are the exponentials of its z-values, normalized across all ten such exponentials. So instead of just computing \\(\\sigma(z)\\), we compute the activation for each unit \\(i\\) using the softmax function: \\[\\begin{align*}\n\\sigma(z)_i = \\frac{{\\rm e}^{z_i}}{\\sum_{j=0}^9{\\rm e}^{z_i}}\\ .\n\\end{align*}\\]\nSo, in our vectorized code, the last line of forward propagation will be A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0).\n\n\nLoss Function\nOur loss function now has to generalize to more than two classes. The general formula for \\(n\\) classes is: \\[\\begin{align*}\nL(y,\\hat{y}) = -\\sum_{i=0}^n y_i\\log(\\hat{y}_i)\\ .\n\\end{align*}\\] Averaging over \\(m\\) training examples this becomes: \\[\\begin{align*}\nL(y,\\hat{y}) = -\\frac{1}{m}\\sum_{j=0}^m\\sum_{i=0}^n y_i^{(i)}\\log(\\hat{y}_i^{(i)})\\ .\n\\end{align*}\\]\nSo let‚Äôs define:\n\n\n\n\n\n\n\n\nBack Propagation\nLuckily it turns out that back propagation isn‚Äôt really affected by the switch to a softmax. A softmax generalizes the sigmoid activiation we‚Äôve been using, and in such a way that the code we wrote earlier still works. We could verify this by deriving: \\[\\begin{align*}\n\\frac{\\partial L}{\\partial z_i} = \\hat{y}_i - y_i\\ .\n\\end{align*}\\]\nBut we won‚Äôt walk through the steps here. Let‚Äôs just go ahead and build our final network.\n\n\n\nBuild and Train\nAs we have now more weights and classes, the training takes longer and we actually need also more episodes to achieve a good accuracy.\n\n\n\n\n\n\nLet‚Äôs see how we did:\n\n\n\n\n\n\n\nModel performance\n\n\n\n\n\n\nWe are at 84% accuray across all digits, which could be of course better. We may now plot image and the corresponding prediction."
  },
  {
    "objectID": "seminars/seminar01/seminar10.html#test-the-model",
    "href": "seminars/seminar01/seminar10.html#test-the-model",
    "title": "Neural Networks",
    "section": "Test the model",
    "text": "Test the model"
  },
  {
    "objectID": "seminars/seminar01/1_deep_learning.html",
    "href": "seminars/seminar01/1_deep_learning.html",
    "title": "Neural Networks",
    "section": "",
    "text": "Neural networks have become one of the most powerful tools in modern computational science, with applications ranging from particle physics to materials science. The term deep neural networks refers to architectures with many layers of interconnected neurons, where ‚Äúdeep‚Äù simply indicates the number of these layers. But what makes these systems particularly fascinating for physicists is their ability to learn complex patterns from data‚Äîmuch like how we extract physical laws from experimental observations.\nIn this lecture, we‚Äôll build neural networks from scratch, gaining deep insight into their inner workings rather than treating them as black boxes. We won‚Äôt rely on high-level libraries like TensorFlow or Keras just yet; instead, we‚Äôll construct everything using NumPy, giving you the same kind of fundamental understanding you‚Äôd gain from deriving equations of motion from first principles rather than just applying them. This hands-on approach will help you understand what‚Äôs really happening under the hood when you train a neural network.\nThink of neural networks as complex function approximators. Just as you might fit a polynomial to experimental data, neural networks can approximate incredibly complicated functions‚Äîfunctions that might describe the relationship between input images and their classifications, or between system parameters and physical outcomes. The difference is that neural networks can handle much higher-dimensional spaces and more complex patterns than traditional fitting methods.\nOur goal today is to build a neural network that can recognize handwritten digits‚Äîa classic problem that will teach us all the essential concepts. We‚Äôll start simple with a single neuron (essentially logistic regression), then add hidden layers to increase the network‚Äôs representational power. By the end, you‚Äôll have a working digit classifier and a solid foundation for understanding deep learning. This notebook has been largely developed by Martin Fr√§nzl and adapted for physics students.\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\nplt.rcParams.update({'font.size': 18,\n                     'axes.titlesize': 20,\n                     'axes.labelsize': 20,\n                     'axes.labelpad': 1,\n                     'lines.linewidth': 2,\n                     'lines.markersize': 10,\n                     'xtick.labelsize' : 18,\n                     'ytick.labelsize' : 18,\n                     'xtick.top' : True,\n                     'xtick.direction' : 'in',\n                     'ytick.right' : True,\n                     'ytick.direction' : 'in'\n                    })",
    "crumbs": [
      "üõ†Ô∏è Hands-On Seminars",
      "Advanced Topics",
      "Advanced: Deep Learning"
    ]
  },
  {
    "objectID": "seminars/seminar01/1_deep_learning.html#overview-building-intelligence-from-scratch",
    "href": "seminars/seminar01/1_deep_learning.html#overview-building-intelligence-from-scratch",
    "title": "Neural Networks",
    "section": "Overview: Building Intelligence from Scratch üß†",
    "text": "Overview: Building Intelligence from Scratch üß†\nWe‚Äôre going to build a neural network from scratch using Python and NumPy, without relying on high-level libraries like Keras or TensorFlow (those will come in Part 2). Our task is to recognize handwritten digits using the famous MNIST dataset‚Äîa benchmark problem in machine learning that has driven much of the field‚Äôs development.\n\n\n\nMNIS\n\n\nOur learning journey proceeds in three stages, each building on the previous one. We‚Äôll start with the simplest possible ‚Äúnetwork‚Äù: a single neuron that learns to recognize just the digit 0. This is actually just logistic regression in disguise, but it will teach us the fundamental concepts of forward propagation, loss functions, and gradient descent. Think of this as the ‚Äúharmonic oscillator‚Äù of neural networks‚Äîthe simplest non-trivial system that captures the essential physics.\nNext, we‚Äôll add a hidden layer to our network, still focusing on recognizing zeros. This is where things get interesting: the hidden layer allows the network to learn intermediate representations, much like how eigenstates provide a natural basis for describing quantum systems. The network can now discover its own ‚Äúfeatures‚Äù rather than working directly with raw pixel values.\nFinally, we‚Äôll extend our network to recognize all digits from 0 through 9, creating a true multiclass classifier. This requires changing our output layer and loss function, but the core principles remain the same. By the end, we‚Äôll have a network that achieves around 84-92% accuracy‚Äînot state-of-the-art by modern standards, but impressive for a network we built entirely from scratch!",
    "crumbs": [
      "üõ†Ô∏è Hands-On Seminars",
      "Advanced Topics",
      "Advanced: Deep Learning"
    ]
  },
  {
    "objectID": "seminars/seminar01/1_deep_learning.html#the-mnist-data-set",
    "href": "seminars/seminar01/1_deep_learning.html#the-mnist-data-set",
    "title": "Neural Networks",
    "section": "The MNIST Data Set üî¢",
    "text": "The MNIST Data Set üî¢\nThe MNIST (Modified National Institute of Standards and Technology) dataset is to machine learning what the hydrogen atom is to quantum mechanics‚Äîa fundamental system that everyone studies first. It contains 70,000 images of handwritten digits, each 28 √ó 28 pixels in grayscale with pixel values ranging from 0 (black) to 255 (white). This gives us 784 input features per image (28 √ó 28 = 784), each representing the intensity of one pixel.\nFrom a physicist‚Äôs perspective, each image is a point in a 784-dimensional space. Our task is to partition this space into 10 regions, one for each digit. This is a classification problem, and neural networks provide a flexible way to learn these complex decision boundaries. We could download and preprocess the data ourselves, but the sklearn module has already done the heavy lifting for us:\n\nLoad the data\n\nfrom sklearn.datasets import fetch_openml\nX, y = fetch_openml('mnist_784', version=1, return_X_y=True,as_frame=False)\n\nThe images are now contained in the array X, while the labels (so which number it is) are contained in y. Let‚Äôs have a look at a random image and label.\n\ni = 33419\nplt.imshow(np.array(X)[i].reshape(28, 28), cmap='gray')\nplt.colorbar()\nplt.show()\nprint('label: ',y[i])\n\n\n\n\n\n\n\n\nlabel:  8\n\n\n\n\nNormalize the data\nNormalization is a crucial preprocessing step in machine learning. By dividing by 255, we scale all pixel values to the interval [0, 1]. This normalization serves several purposes: it prevents numerical overflow issues during computation, helps the gradient descent algorithm converge faster (since all features are on the same scale), and makes the learning rate easier to tune. Think of it as choosing appropriate units in physics‚Äîworking in natural units (‚Ñè = c = 1) often simplifies calculations, and similarly, normalized data simplifies neural network training.\n\nX = X/255\n\n\n\nPreparing training and testing data\nFor our first network, we‚Äôre building a binary classifier‚Äîa system that answers a yes/no question: ‚ÄúIs this digit a zero?‚Äù The default MNIST labels indicate which digit each image represents (‚Äò0‚Äô, ‚Äò1‚Äô, ‚Äò2‚Äô, etc.), but we need to convert this to binary labels: 1 if the image shows a zero, and 0 otherwise. This is analogous to simplifying a complex quantum system by projecting it onto a two-level system‚Äîwe‚Äôre reducing a 10-class problem to a binary one.\n\ny_new = np.zeros(y.shape)\ny_new[np.where(y == '0')[0]] = 1\ny = y_new\n\nWe now split the data into training and testing sets. This separation is crucial: we train on one set and evaluate on a completely different set to test our network‚Äôs ability to generalize. This is similar to how you might measure a physical quantity multiple times‚Äîsome measurements are used to calibrate your instrument (training), while others test its accuracy (testing). The MNIST images are pre-arranged so that the first 60,000 form the training set and the last 10,000 serve as the test set. We‚Äôll also transpose the data so that each example becomes a column rather than a row, which makes our matrix operations more intuitive:\n\nm = 60000\nm_test = X.shape[0] - m\n\nX_train, X_test = X[:m].T, X[m:].T\ny_train, y_test = y[:m].reshape(1,m), y[m:].reshape(1, m_test)\n\nFinally, we shuffle the training set. Shuffling ensures that our network doesn‚Äôt learn any spurious patterns related to the order of examples. If all the zeros came first, followed by all the ones, the network might learn temporal patterns that don‚Äôt actually exist in the data:\n\nnp.random.seed(1)\nshuffle_index = np.random.permutation(m)\nX_train, y_train = X_train[:,shuffle_index], y_train[:,shuffle_index]\n\nLet‚Äôs verify our preprocessing by examining a random image and its label:\n\ni = 39\nplt.imshow(X_train[:,i].reshape(28, 28), cmap='gray')\nplt.colorbar()\nplt.show()\nprint(y_train[:,i])\n\n\n\n\n\n\n\n\n[0.]\n\n\nTry changing the index i to explore different images. When you find a zero, verify that the corresponding label is 1, and when you find any other digit, check that the label is 0. This sanity check ensures our binary labeling worked correctly.",
    "crumbs": [
      "üõ†Ô∏è Hands-On Seminars",
      "Advanced Topics",
      "Advanced: Deep Learning"
    ]
  },
  {
    "objectID": "seminars/seminar01/1_deep_learning.html#a-single-neuron-the-fundamental-building-block",
    "href": "seminars/seminar01/1_deep_learning.html#a-single-neuron-the-fundamental-building-block",
    "title": "Neural Networks",
    "section": "A Single Neuron: The Fundamental Building Block ‚öõÔ∏è",
    "text": "A Single Neuron: The Fundamental Building Block ‚öõÔ∏è\nThe basic unit of a neural network is an artificial neuron, inspired by biological neurons but mathematically much simpler. A neuron takes multiple inputs, performs a weighted sum (plus a bias), and passes the result through an activation function to produce a single output. The diagram below shows a simple neuron with two inputs:\n\n\n\nimage\n\n\nWhile the biological inspiration is interesting, it‚Äôs more useful for us as physicists to think of a neuron as a parametrized nonlinear function. The weights and bias are parameters we‚Äôll adjust during training, and the activation function introduces nonlinearity‚Äîcrucial because linear combinations of linear functions are still linear, but we need nonlinearity to approximate complex decision boundaries.\n\nForward Propagation: From Input to Output\nThe term ‚Äúforward propagation‚Äù describes how information flows through the network from inputs to outputs. A neuron performs three sequential operations on its inputs.\nFirst, each input value \\(x_i\\) is multiplied by its corresponding weight \\(w_i\\): \\[\\begin{eqnarray}\nx_{1}\\rightarrow x_{1} w_{1}\\\\\nx_{2}\\rightarrow x_{2} w_{2}\n\\end{eqnarray}\\]\nThese weights determine how much influence each input has on the output. In physics terms, think of the weights as coupling constants that determine the strength of interaction between input features and the output.\nSecond, all weighted inputs are summed and a bias term \\(b\\) is added: \\[\\begin{equation}\nz = x_{1} w_{1}+ x_{2} w_{2}+b\n\\end{equation}\\]\nThe bias allows the neuron to shift its activation threshold, similar to how an external potential can shift energy levels in quantum mechanics. It‚Äôs crucial for fitting data that doesn‚Äôt pass through the origin.\nThird, this weighted sum \\(z\\) is passed through an activation function \\(\\sigma(\\cdot)\\) to produce the final output: \\[\\begin{equation}\ny=\\sigma(z) = \\sigma( x_{1} w_{1}+ x_{2} w_{2}+b)\n\\end{equation}\\]\nThe activation function serves a critical purpose: it introduces nonlinearity into the network. Without it, stacking multiple layers would be pointless‚Äîany composition of linear transformations is itself linear. The activation function allows neural networks to approximate arbitrary nonlinear functions, much like how Fourier series can approximate arbitrary periodic functions through combinations of sines and cosines.\nA commonly used activation function is the sigmoid function, which smoothly maps any real number to the interval (0, 1).\nFor a single training example \\(x\\) (which is itself a vector of 784 pixel values), we can write this more compactly using vector notation:\n\\[\\begin{equation*}\n\\hat{y} = \\sigma(w^{\\rm T} x + b)\\ .\n\\end{equation*}\\]\nHere \\(w^{\\rm T} x\\) is the dot product of the weight vector with the input vector, \\(b\\) is a scalar bias, and \\(\\sigma\\) is the sigmoid activation function: \\[\\begin{equation*}\n\\sigma(z) = \\frac{1}{1+{\\rm e}^{-z}}\\ .\n\\end{equation*}\\]\nThe sigmoid function has a beautiful S-shaped curve. For large positive inputs it approaches 1, for large negative inputs it approaches 0, and it transitions smoothly in between. This makes it perfect for binary classification‚Äîwe can interpret the output as a probability. Interestingly, the sigmoid function is related to the Fermi-Dirac distribution in statistical mechanics (without the temperature parameter), which describes the occupation probability of quantum states.\nLet‚Äôs define and visualize the sigmoid function:\n\ndef sigmoid(z):\n    return 1/(1 + np.exp(-z))\n\n\nx=np.linspace(-5,5,100)\nplt.figure(figsize=(5,3))\nplt.plot(x,sigmoid(x))\nplt.xlabel('input')\nplt.ylabel('output')\nplt.grid()\nplt.show()\n\n\n\n\nSigmoid function\n\n\n\n\nExample calculation: Consider a simple two-input neuron with weights \\(w=[0,1]\\) and bias \\(b=4\\). If we provide the input \\(x=[2,3]\\), the neuron computes:\n\\[\\begin{equation}\nz = w\\cdot x+b = 0 \\cdot 2 + 1 \\cdot 3 + 4 = 7\n\\end{equation}\\]\nPassing this through the sigmoid gives: \\[\\begin{equation}\ny=\\sigma(7) = \\frac{1}{1+e^{-7}} \\approx 0.999\n\\end{equation}\\]\nThe output is very close to 1, indicating high confidence. This entire procedure‚Äîpropagating input values forward through the network to obtain an output‚Äîis called feedforward or forward propagation.\nOur immediate goal is to scale this up: we‚Äôll create a network with a single neuron that has 784 inputs (one for each pixel in our 28 √ó 28 images) and produces a single sigmoid output indicating whether the image is a zero.\nVectorization for efficiency: The real power of NumPy comes from vectorization‚Äîprocessing multiple examples simultaneously. Instead of computing predictions one image at a time, we‚Äôll stack all training examples side-by-side in a matrix \\(X\\), where each column is one example. The forward pass then becomes:\n\\[\\begin{equation*}\n\\hat{Y} = \\sigma(w^{\\rm T} X + b)\\ .\n\\end{equation*}\\]\nNote that \\(\\hat{Y}\\) is now a vector (one prediction per training example), not a scalar. This vectorized computation is orders of magnitude faster than looping over examples individually‚ÄîNumPy uses optimized linear algebra libraries (BLAS/LAPACK) under the hood, similar to how you‚Äôd use optimized FFT routines rather than implementing Fourier transforms naively.\nIn our code, we‚Äôll compute this in two stages: first Z = np.matmul(W.T, X) + b (the weighted sum), then A = sigmoid(Z) (the activation). We use A for ‚ÄúActivation‚Äù to match standard deep learning notation. This two-stage breakdown might seem unnecessary now, but it will make our backward propagation code clearer‚Äîeach stage needs its own gradient computation.\n\n\nLoss Function: Quantifying Prediction Error üìâ\nNow that we can make predictions, we need to quantify how wrong those predictions are. This quantification is called the loss (or cost) function. It‚Äôs analogous to defining an energy functional in variational methods‚Äîwe need a scalar quantity to minimize.\nYou might think to use mean squared error (MSE) from your curve-fitting experience:\n\\[\\begin{equation}\nMSE(y,\\hat{y})=\\frac{1}{n}\\sum_{i=1}^{n}(y-\\hat{y})^2\n\\end{equation}\\]\nwhere \\(\\hat{y}\\) represents our predictions and \\(y\\) represents the ground truth (actual labels from the training set). MSE works, but for classification problems, there‚Äôs a more principled choice.\nWe‚Äôll use binary cross-entropy loss, which comes from information theory. For a single training example:\n\\[\\begin{equation*}\nL(y,\\hat{y}) = -y\\log(\\hat{y})-(1-y)\\log(1-\\hat{y})\\ .\n\\end{equation*}\\]\nThis formula might look strange at first, but it has a beautiful interpretation. When \\(y=1\\) (the image is a zero), only the first term matters, and we‚Äôre penalized more heavily the further \\(\\hat{y}\\) is from 1. When \\(y=0\\) (not a zero), only the second term matters, penalizing predictions far from 0. The logarithm ensures that the penalty grows rapidly as predictions become more confident but wrong‚Äîbeing confidently wrong is much worse than being tentatively wrong.\nThis loss function derives from maximum likelihood estimation and is related to the Kullback-Leibler divergence between probability distributions. It‚Äôs the natural choice for binary classification with sigmoid outputs. For physicists, you might recognize similar logarithmic terms in the partition function of statistical mechanics.\nAveraging over a training set of \\(m\\) examples:\n\\[\\begin{equation*}\nL(Y,\\hat{Y}) = -\\frac{1}{m}\\sum_{i = 0}^{m}y^{(i)}\\log(\\hat{y}^{(i)})-(1-y^{(i)})\\log(1-\\hat{y}^{(i)})\\ .\n\\end{equation*}\\]\nIn Python code, this looks like\n\ndef compute_loss(Y, Y_hat):\n    m = Y.shape[1]\n    L = -(1./m)*(np.sum(np.multiply(np.log(Y_hat), Y)) + np.sum(np.multiply(np.log(1 - Y_hat), (1 - Y))))\n    return L",
    "crumbs": [
      "üõ†Ô∏è Hands-On Seminars",
      "Advanced Topics",
      "Advanced: Deep Learning"
    ]
  },
  {
    "objectID": "seminars/seminar01/1_deep_learning.html#training-the-network-learning-from-data",
    "href": "seminars/seminar01/1_deep_learning.html#training-the-network-learning-from-data",
    "title": "Neural Networks",
    "section": "Training the Network: Learning from Data üéì",
    "text": "Training the Network: Learning from Data üéì\nTraining a neural network means adjusting its parameters (weights and biases) to minimize the loss function. This is fundamentally an optimization problem, not unlike minimizing the action in Lagrangian mechanics or finding ground state energies in quantum systems. The key difference is that our loss function lives in a very high-dimensional space‚Äîwith 784 inputs, we have 784 weights plus 1 bias, giving us a 785-dimensional parameter space to navigate.\nThe optimization technique we‚Äôll use is called gradient descent‚Äîwe follow the downhill direction of the loss function, taking small steps toward the minimum. But how do we find this direction? We need gradients, which means we need derivatives of the loss with respect to all parameters. This is where backpropagation comes in.\n\nBackward Propagation: The Chain Rule in Action üîó\nThe output of our network depends entirely on the inputs (which are fixed for a given training example) and on the parameters we choose: the weights \\(w\\) and biases \\(b\\). We can therefore view the loss as a function of these parameters:\n\\[\nL(w_{1},w_{2},w_{3},\\ldots ,b_{1},b_{2},b_{3},\\ldots)\n\\]\nTo train the network through gradient descent, we need to know how the loss changes when we adjust each weight. This is captured by the partial derivative:\n\\[\n\\frac{\\partial L}{\\partial w_j}\n\\]\nThis derivative tells us the direction and magnitude of steepest increase in the loss. By moving in the opposite direction (negative gradient), we can reduce the loss. The process of computing these gradients by working backward from the loss through the network is called backpropagation‚Äîit‚Äôs just a clever application of the chain rule from calculus, applied systematically to a computational graph.\nBackpropagation is often presented as mysterious or complicated, but it‚Äôs really just careful bookkeeping with derivatives. As physicists, you‚Äôve used the chain rule countless times‚Äîbackpropagation is the same principle, just applied to a more complex composite function.\nDeriving the gradients\nLet‚Äôs focus on a single training example to keep the math clear. We can think of computing the loss in three stages: \\(w_j\\rightarrow z \\rightarrow \\hat{y} \\rightarrow L\\). The formulas for these stages are: \\[\\begin{align*}\nz &= w^{\\rm T} x + b\\ , \\\\\n\\hat{y} &= \\sigma(z)\\ , \\\\\nL(y,\\hat{y}) &= -y\\log(\\hat{y})-(1-y)\\log(1-\\hat{y})\\ .\n\\end{align*}\\]\nBy the chain rule from calculus, the gradient of the loss with respect to weight \\(w_j\\) factors into three partial derivatives:\n\\[\\begin{align*}\n\\frac{\\partial L}{\\partial w_j} = \\frac{\\partial L}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial z}\\frac{\\partial z}{\\partial w_j}\n\\end{align*}\\]\nThis factorization is the essence of backpropagation. Each partial derivative corresponds to one ‚Äúlink‚Äù in our computational chain. Let‚Äôs compute each factor systematically. The derivatives are a bit tedious to write out, but none of them are particularly complicated‚Äîthey‚Äôre all applications of basic calculus rules you already know.\n\\(\\partial L/\\partial\\hat{y}\\): \\[\\begin{align*}\n\\frac{\\partial L}{\\partial\\hat{y}} &= \\frac{\\partial}{\\partial\\hat{y}}\\left(-y\\log(\\hat{y})-(1-y)\\log(1-\\hat{y})\\right) \\\\\n&= -y\\frac{\\partial}{\\partial\\hat{y}}\\log(\\hat{y})-(1-y)\\frac{\\partial}{\\partial\\hat{y}}\\log(1-\\hat{y}) \\\\\n&= -\\frac{y}{\\hat{y}} +\\frac{(1 - y)}{1-\\hat{y}} \\\\\n&= \\frac{\\hat{y} - y}{\\hat{y}(1-\\hat{y})}\n\\end{align*}\\]\n\\(\\partial \\hat{y}/\\partial z\\): \\[\\begin{align*}\n\\frac{\\partial }{\\partial z}\\sigma(z)\n&= \\frac{\\partial }{\\partial z}\\left(\\frac{1}{1 + {\\rm e}^{-z}}\\right) \\\\\n&= \\frac{1}{(1 + {\\rm e}^{-z})^2}\\frac{\\partial }{\\partial z}(1 + {\\rm e}^{-z}) \\\\\n&= \\frac{{\\rm e}^{-z}}{(1 + {\\rm e}^{-z})^2} \\\\\n&= \\frac{1}{1 + {\\rm e}^{-z}}\\frac{{\\rm e}^{-z}}{1 + {\\rm e}^{-z}} \\\\\n&= \\frac{1}{1 + {\\rm e}^{-z}}\\left(1 - \\frac{1}{1 + {\\rm e}^{-z}}\\right) \\\\\n&= \\sigma(z)(1-\\sigma(z)) \\\\\n&= \\hat{y}(1-\\hat{y})\n\\end{align*}\\]\n\\(\\partial z/\\partial w_j\\): \\[\\begin{align*}\n\\frac{\\partial }{\\partial w_j}(w^{\\rm T} x + b) &= \\frac{\\partial }{\\partial w_j}(w_0x_0 + \\dots + w_nx_n + b) \\\\\n&= x_j\n\\end{align*}\\]\nSubstituting back into the chain rule yields: \\[\\begin{align*}\n\\frac{\\partial L}{\\partial w_j}\n&= \\frac{\\partial L}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial z}\\frac{\\partial z}{\\partial w_j} \\\\\n&= \\frac{\\hat{y} - y}{\\hat{y}(1-\\hat{y})}\\hat{y}(1-\\hat{y}) x_j \\\\\n&= (\\hat{y} - y)x_j\\ .\n\\end{align*}\\]\nwhich does not look that unfriendly anymore.\nVectorizing the gradients: When we have \\(m\\) training examples processed simultaneously, the weight gradient becomes: \\[\\begin{align*}\n\\frac{\\partial L}{\\partial w} = \\frac{1}{m} X(\\hat{Y} - Y)^{\\rm T}\\ .\n\\end{align*}\\]\nThe factor of \\(1/m\\) comes from averaging the loss over all examples. Notice how clean this is: the gradient is simply the input matrix multiplied by the error vector \\((\\hat{Y} - Y)\\).\nA similar derivation for the bias gradient yields, for a single example: \\[\\begin{align*}\n\\frac{\\partial L}{\\partial b} = (\\hat{y} - y)\\ .\n\\end{align*}\\]\nVectorized over \\(m\\) examples: \\[\\begin{align*}\n\\frac{\\partial L}{\\partial b} = \\frac{1}{m}\\sum_{i=1}^{m}{(\\hat{y}^{(i)} - y^{(i)})}\\ .\n\\end{align*}\\]\nIn our code, we‚Äôll follow the convention of labeling gradients with a d prefix: dW for \\(\\partial L/\\partial W\\) and db for \\(\\partial L/\\partial b\\). This notation reminds us that these are derivatives. The backpropagation step then consists of just two lines: dW = (1/m) * np.matmul(X, (A-Y).T) and db = (1/m)*np.sum(A-Y, axis=1, keepdims=True).\n\n\nGradient Descent: Following the Downhill Path ‚õ∞Ô∏è\nWe now have all the ingredients needed to train our neural network! The optimization algorithm we‚Äôll use is called gradient descent‚Äîconceptually simple but remarkably effective. The update rule for weights is:\n\\[\nw\\leftarrow w-\\eta\\frac{\\partial L}{\\partial w}\n\\]\nHere \\(\\eta\\) (Greek letter eta) is the learning rate, a hyperparameter that controls the step size. Think of gradient descent as navigating down a mountain in fog: the gradient tells you which direction is downhill, and the learning rate determines how big a step you take.\nThe logic is straightforward. If \\(\\partial L/\\partial w\\) is positive, the loss increases as \\(w\\) increases, so we should decrease \\(w\\) to reduce the loss. If \\(\\partial L/\\partial w\\) is negative, the loss decreases as \\(w\\) increases, so we should increase \\(w\\). The gradient descent update automatically does the right thing in both cases.\nThe identical update rule applies to the bias: \\(b\\leftarrow b-\\eta\\frac{\\partial L}{\\partial b}\\).\nWe repeat this update process many times‚Äîeach complete pass through all training data is called an epoch. This is analogous to iterative methods you might have encountered in computational physics, like the relaxation method for solving Laplace‚Äôs equation or the power method for finding eigenvectors. We start with a random guess and iteratively improve it until convergence.\nThe term ‚Äústochastic‚Äù gradient descent (SGD) actually refers to a variant where we don‚Äôt use all training examples at once but rather small random batches. For our purposes, we‚Äôll compute gradients using the full training set each iteration, which is sometimes called batch gradient descent. This is more stable but slower for large datasets.\n\n\nBuild and Train: Putting It All Together üî®\nNow comes the exciting moment‚Äîassembling all our components into a working neural network! Let‚Äôs review what we have: a forward propagation function (computing predictions), a loss function (measuring error), and backpropagation (computing gradients). The training loop combines these pieces iteratively to learn optimal weights.\nHere‚Äôs our complete training procedure for a single-neuron network recognizing zeros:\nThis architecture is actually equivalent to logistic regression‚Äîthe sigmoid function is called a ‚Äúlogistic‚Äù function, hence the name. Despite its simplicity, logistic regression is quite powerful and forms the foundation for more complex networks.\n\nlearning_rate = 1\n\nX = np.array(X_train)\nY = np.array(y_train)\n\nn_x = X.shape[0]\nm = X.shape[1]\n\nW = np.random.randn(n_x, 1) * 0.01\nb = np.zeros((1, 1))\n\nfor i in range(200):\n    Z = np.matmul(W.T, X) + b\n    A = sigmoid(Z)\n\n    loss = compute_loss(Y, A)\n\n    dW = (1/m)*np.matmul(X, (A-Y).T)\n    db = (1/m)*np.sum(A-Y, axis=1, keepdims=True)\n\n    W = W - learning_rate * dW\n    b = b - learning_rate * db\n\n    if i % 10 == 0:\n        print(\"Epoch\", i, \" loss: \", loss)\n\nprint(\"Final loss:\", loss)\n\nEpoch 0  loss:  0.7471125121616977\nEpoch 10  loss:  0.07308269582929021\nEpoch 20  loss:  0.06131832354627721\nEpoch 30  loss:  0.05523011981200572\nEpoch 40  loss:  0.0513243202361425\nEpoch 50  loss:  0.04854004196371184\nEpoch 60  loss:  0.04642485272904433\nEpoch 70  loss:  0.04474722082574825\nEpoch 80  loss:  0.043374333931969114\nEpoch 90  loss:  0.04222371551840797\nEpoch 100  loss:  0.04124104599832092\nEpoch 110  loss:  0.04038888651110667\nEpoch 120  loss:  0.03964048057966332\nEpoch 130  loss:  0.0389761317586134\nEpoch 140  loss:  0.038380979206568466\nEpoch 150  loss:  0.03784357458020544\nEpoch 160  loss:  0.03735493964481513\nEpoch 170  loss:  0.03690792357698068\nEpoch 180  loss:  0.03649675336766015\nEpoch 190  loss:  0.036116712255247\nFinal loss: 0.03579805734492837\n\n\nWatching the loss decrease is encouraging‚Äîit means our network is learning! But how do we judge its actual performance? The loss function tells us about training progress, but what we really care about is classification accuracy on unseen data.\nTo evaluate our network properly, we‚Äôll use the confusion matrix‚Äîa fundamental tool in classification that shows not just whether predictions are right or wrong, but specifically how they‚Äôre wrong. The confusion matrix displays actual labels in the rows and predicted labels in the columns, giving us a complete picture of our classifier‚Äôs behavior.\n\n\n\nconfusion_matrix\n\n\nUnderstanding the confusion matrix is crucial. True positives (TP) are zeros correctly identified as zeros. False positives (FP) are other digits incorrectly classified as zeros. False negatives (FN) are zeros that we missed (classified as ‚Äúnot zero‚Äù). True negatives (TN) are other digits correctly identified as not-zeros.\nFrom these four quantities, we can derive useful metrics. Precision = TP/(TP+FP) tells us what fraction of predicted zeros are actually zeros. Recall = TP/(TP+FN) tells us what fraction of actual zeros we successfully identified. Accuracy = (TP+TN)/(TP+TN+FP+FN) tells us the overall fraction of correct predictions.\nFortunately, sklearn provides functions to compute all of this automatically. We just need to supply predictions and actual labels. Crucially, we evaluate on the test set X_test‚Äîdata the network has never seen during training. This tests generalization, not memorization.\n\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nZ = np.matmul(W.T,X_test) + b\nA = sigmoid(Z)\n\npredictions = (A&gt;.5)[0,:]\nlabels = (y_test == 1)[0,:]\n\nprint(confusion_matrix(predictions, labels))\n\n[[8973   33]\n [  47  947]]\n\n\n\nprint(classification_report(predictions, labels))\n\n              precision    recall  f1-score   support\n\n       False       0.99      1.00      1.00      9006\n        True       0.97      0.95      0.96       994\n\n    accuracy                           0.99     10000\n   macro avg       0.98      0.97      0.98     10000\nweighted avg       0.99      0.99      0.99     10000\n\n\n\n\n\nTesting Our Model: Individual Predictions üî¨\nBeyond aggregate statistics, it‚Äôs instructive to examine individual predictions. We can test our network on a single image and see what it predicts. Since sigmoid outputs range from 0 to 1, we use 0.5 as our decision threshold‚Äîoutputs above 0.5 are classified as zeros, below 0.5 as not-zeros. This threshold could be adjusted to trade off precision versus recall, but 0.5 is the natural choice for balanced classes.\n\ni=200\nbool(sigmoid(np.matmul(W.T, np.array(X_test)[:,i])+b)&gt;0.5)\n\nFalse\n\n\n\nplt.imshow(np.array(X_test)[:,i].reshape(28,28),cmap='gray')\n\n\n\n\nExample Image",
    "crumbs": [
      "üõ†Ô∏è Hands-On Seminars",
      "Advanced Topics",
      "Advanced: Deep Learning"
    ]
  },
  {
    "objectID": "seminars/seminar01/1_deep_learning.html#network-with-hidden-layers-increasing-representational-power",
    "href": "seminars/seminar01/1_deep_learning.html#network-with-hidden-layers-increasing-representational-power",
    "title": "Neural Networks",
    "section": "Network with Hidden Layers: Increasing Representational Power üß©",
    "text": "Network with Hidden Layers: Increasing Representational Power üß©\nSo far, our network consists of just an input layer (784 pixels) connected directly to a single output neuron. This is the simplest possible neural network architecture. To handle more complex patterns, we need to add hidden layers‚Äîintermediate layers between input and output.\nHidden layers are called ‚Äúhidden‚Äù because they‚Äôre internal to the network; we don‚Äôt directly observe their outputs during normal operation. But these hidden units are where the magic happens! They allow the network to learn intermediate representations‚Äîabstract features derived from the raw input pixels.\nThink of it hierarchically: the first layer might learn to detect edges and curves, while deeper layers combine these into higher-level features like loops and intersections. This compositional structure is what makes deep learning so powerful‚Äîcomplex concepts are built from simpler ones, much like how physical theories build complexity from fundamental principles.\nHere‚Äôs the architecture we‚Äôll build: 784 inputs ‚Üí 64 hidden units ‚Üí 1 output neuron.\n\n\n\nhidden\n\n\nOur network now has two sets of parameters: weights \\(W_1\\) and biases \\(b_1\\) connecting the input to the hidden layer (shape 64 √ó 784), and weights \\(W_2\\) and biases \\(b_2\\) connecting the hidden layer to the output (shape 1 √ó 64). This gives us roughly 50,000 trainable parameters‚Äîa significant increase from 785 in the single-neuron network!\nThe forward pass now has two stages: input ‚Üí hidden layer (apply \\(W_1\\), \\(b_1\\), then sigmoid), then hidden layer ‚Üí output (apply \\(W_2\\), \\(b_2\\), then sigmoid). Backpropagation also has two stages, working backward from the output error through each layer using the chain rule.\nWe won‚Äôt derive all the equations again‚Äîthey follow the same principles as before, just applied to each layer. The code extends naturally from our single-neuron implementation:\n\nX = X_train\nY = y_train\n\nn_x = X.shape[0]\nn_h = 64\nlearning_rate = 1\n\nW1 = np.random.randn(n_h, n_x)\nb1 = np.zeros((n_h, 1))\nW2 = np.random.randn(1, n_h)\nb2 = np.zeros((1, 1))\n\nfor i in range(100):\n\n    Z1 = np.matmul(W1, X) + b1\n    A1 = sigmoid(Z1)\n    Z2 = np.matmul(W2, A1) + b2\n    A2 = sigmoid(Z2)\n\n    loss = compute_loss(Y, A2)\n\n    dZ2 = A2-Y\n    dW2 = (1./m) * np.matmul(dZ2, A1.T)\n    db2 = (1./m) * np.sum(dZ2, axis=1, keepdims=True)\n\n    dA1 = np.matmul(W2.T, dZ2)\n    dZ1 = dA1 * sigmoid(Z1) * (1 - sigmoid(Z1))\n    dW1 = (1./m) * np.matmul(dZ1, X.T)\n    db1 = (1./m) * np.sum(dZ1, axis=1, keepdims=True)\n\n    W2 = W2 - learning_rate * dW2\n    b2 = b2 - learning_rate * db2\n    W1 = W1 - learning_rate * dW1\n    b1 = b1 - learning_rate * db1\n\n    if i % 10 == 0:\n        print(\"Epoch\", i, \"loss: \", loss)\n\nprint(\"Final loss:\", loss)\n\nEpoch 0 loss:  2.395166635058746\nEpoch 10 loss:  0.22074168759268953\nEpoch 20 loss:  0.1660154822272753\nEpoch 30 loss:  0.13990677867922952\nEpoch 40 loss:  0.12390102523919129\nEpoch 50 loss:  0.11269161497108851\nEpoch 60 loss:  0.10421329497723456\nEpoch 70 loss:  0.09747959072905935\nEpoch 80 loss:  0.09194898313097832\nEpoch 90 loss:  0.0872943606401609\nFinal loss: 0.08367740628296327\n\n\nTo evaluate our improved network, we again use the confusion matrix on the test set. You should notice better performance compared to the single-neuron network‚Äîthe hidden layer provides additional representational power.\n\nZ1 = np.matmul(W1, X_test) + b1\nA1 = sigmoid(Z1)\nZ2 = np.matmul(W2, A1) + b2\nA2 = sigmoid(Z2)\n\npredictions = (A2&gt;.5)[0,:]\nlabels = (y_test == 1)[0,:]\n\nprint(confusion_matrix(predictions, labels))\nprint(classification_report(predictions, labels))\n\n[[8905  178]\n [ 115  802]]\n              precision    recall  f1-score   support\n\n       False       0.99      0.98      0.98      9083\n        True       0.82      0.87      0.85       917\n\n    accuracy                           0.97     10000\n   macro avg       0.90      0.93      0.91     10000\nweighted avg       0.97      0.97      0.97     10000",
    "crumbs": [
      "üõ†Ô∏è Hands-On Seminars",
      "Advanced Topics",
      "Advanced: Deep Learning"
    ]
  },
  {
    "objectID": "seminars/seminar01/1_deep_learning.html#multiclass-network-recognizing-all-digits",
    "href": "seminars/seminar01/1_deep_learning.html#multiclass-network-recognizing-all-digits",
    "title": "Neural Networks",
    "section": "Multiclass Network: Recognizing All Digits üî¢",
    "text": "Multiclass Network: Recognizing All Digits üî¢\nOur binary classifier successfully distinguishes zeros from non-zeros, but the real challenge is recognizing all ten digits (0-9). This requires extending to multiclass classification‚Äîinstead of a yes/no question, we‚Äôre asking ‚Äúwhich of these 10 categories?‚Äù\nThe key architectural change is replacing our single output neuron with 10 output neurons, one per digit class. Each neuron produces a score for its corresponding digit. We then interpret these scores as probabilities‚Äîthe neuron with the highest score determines our prediction.\nThis is analogous to expanding from a two-state quantum system (like spin-¬Ω) to a ten-state system‚Äîwe need ten basis states to represent all possibilities. The output layer produces a probability distribution over these ten states.\nFor example, the output array [0,1,0,0,0,0,0,0,0,0] would indicate the network predicts digit 1 (the second position, using zero-indexing). In practice, the outputs won‚Äôt be exactly 0 and 1, but rather probabilities that sum to 1. The predicted class is simply the position of the maximum value.\nTo train a multiclass network, we need to reload the original MNIST labels (not our binary 0/1 encoding):\n\nfrom sklearn.datasets import fetch_openml\nX, y = fetch_openml('mnist_784', version=1, return_X_y=True,as_frame=False)\n\nX = X / 255\n\nWe‚Äôll convert these labels to one-hot encoding‚Äîa representation where each label becomes a vector of length 10 with a single 1 and nine 0s. The digit ‚Äú3‚Äù becomes [0,0,0,1,0,0,0,0,0,0], for instance. This encoding matches our network‚Äôs output format and makes the loss calculation straightforward. The result is a 10 √ó 70,000 array where each column is one training example‚Äôs label:\n\ndigits = 10\nexamples = y.shape[0]\n\ny = y.reshape(1, examples)\n\nY_new = np.eye(digits)[y.astype('int32')]\nY_new = Y_new.T.reshape(digits, examples)\n\n\nY_new.shape\n\n(10, 70000)\n\n\nWe also separate into training and testing sets, maintaining the same 60,000/10,000 split:\n\nm = 60000\nm_test = X.shape[0] - m\n\nX_train, X_test = X[:m].T, X[m:].T\nY_train, Y_test = Y_new[:,:m], Y_new[:,m:]\n\nshuffle_index = np.random.permutation(m)\nX_train, Y_train = X_train[:, shuffle_index], Y_train[:, shuffle_index]\n\n\ni = 58\nplt.imshow(X_train[:,i].reshape(28,28), cmap='gray')\nplt.colorbar()\nplt.show()\nY_train[:,i]\n\n\n\n\n\n\n\n\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n\n\n\nChanges to the Model Architecture üîß\nLet‚Äôs systematically examine what changes are required to extend our binary classifier to multiclass. The input and hidden layers remain unchanged‚Äîthey still extract useful features. Only the output layer and loss function need modification.\n\nForward Pass: The Softmax Function\nThe critical change is in the output layer. Instead of a single sigmoid unit, we now have 10 units with a softmax activation function. Softmax generalizes sigmoid to multiple classes‚Äîit converts a vector of arbitrary real numbers (the logits \\(z\\)) into a probability distribution that sums to 1.\nThe softmax function computes the activation for each output unit \\(i\\) as: \\[\\begin{align*}\n\\sigma(z)_i = \\frac{{\\rm e}^{z_i}}{\\sum_{j=0}^9{\\rm e}^{z_i}}\\ .\n\\end{align*}\\]\nThe exponential ensures all outputs are positive, and the normalization ensures they sum to 1, giving us a valid probability distribution. Units with larger logits \\(z_i\\) get exponentially larger probabilities‚Äîthe softmax is ‚Äúsoft‚Äù because it doesn‚Äôt simply pick the maximum but gives a smooth distribution.\nIn vectorized NumPy code: A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0). The axis=0 parameter ensures we normalize each example independently (summing down columns, not across the entire matrix).\n\n\nLoss Function: Categorical Cross-Entropy\nWith multiple classes, we need to generalize our loss function. The categorical cross-entropy extends binary cross-entropy to \\(n\\) classes. For a single training example: \\[\\begin{align*}\nL(y,\\hat{y}) = -\\sum_{i=0}^n y_i\\log(\\hat{y}_i)\\ .\n\\end{align*}\\] This is the negative log-probability assigned to the correct class. Since the labels are one-hot encoded, only one term in the sum is nonzero (the term corresponding to the true class). This means we‚Äôre minimizing the negative log-likelihood‚Äîmaximizing the probability assigned to the correct class.\nFrom a physics perspective, this is deeply connected to statistical mechanics. The cross-entropy loss is equivalent to minimizing the Kullback-Leibler divergence between the true distribution (the one-hot labels) and the predicted distribution (softmax outputs). It‚Äôs the information-theoretic measure of how well our predictions match reality.\nAveraging over \\(m\\) training examples: \\[\\begin{align*}\nL(y,\\hat{y}) = -\\frac{1}{m}\\sum_{j=0}^m\\sum_{i=0}^n y_i^{(i)}\\log(\\hat{y}_i^{(i)})\\ .\n\\end{align*}\\]\nSo let‚Äôs define:\n\ndef compute_multiclass_loss(Y, Y_hat):\n    L_sum = np.sum(np.multiply(Y, np.log(Y_hat)))\n    m = Y.shape[1]\n    L = -(1/m) * L_sum\n    return L\n\n\n\nBackpropagation: A Fortunate Simplification\nHere‚Äôs a beautiful mathematical result: despite the complexity of the softmax function, the gradient simplifies dramatically. When we combine softmax activation with cross-entropy loss, the derivative of the loss with respect to the logits (the \\(z\\) values) is simply: \\[\\begin{align*}\n\\frac{\\partial L}{\\partial z_i} = \\hat{y}_i - y_i\\ .\n\\end{align*}\\]\nThis is exactly the same form we had with sigmoid and binary cross-entropy! The predicted probabilities minus the true labels. This isn‚Äôt a coincidence‚Äîit reflects the principled connection between exponential family distributions and their conjugate loss functions. The mathematical elegance here is reminiscent of how the Hamiltonian and Lagrangian formulations of mechanics lead to equivalent equations of motion.\nWe won‚Äôt walk through the full derivation (it requires careful application of the chain rule with matrix derivatives), but trust that this simple gradient makes implementation straightforward.\n\n\n\nBuild and Train: The Complete Network üöÄ\nNow we assemble our final, full-featured digit classifier. With more parameters (now around 50,000) and a more complex task (10 classes instead of 2), training will take longer. We‚Äôll run 200 epochs to give the network sufficient time to learn the patterns distinguishing all ten digits.\nThe architecture is: 784 inputs ‚Üí 64 hidden units (sigmoid) ‚Üí 10 outputs (softmax). Watch how the loss decreases as training progresses‚Äîyou‚Äôre witnessing the network discover representations of handwritten digits!\n\nn_x = X_train.shape[0]\nn_h = 64\nlearning_rate = 1\n\nW1 = np.random.randn(n_h, n_x)\nb1 = np.zeros((n_h, 1))\nW2 = np.random.randn(digits, n_h)\nb2 = np.zeros((digits, 1))\n\nX = X_train\nY = Y_train\n\nfor i in range(200):\n\n    Z1 = np.matmul(W1,X) + b1\n    A1 = sigmoid(Z1)\n    Z2 = np.matmul(W2,A1) + b2\n    A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0)\n\n    loss = compute_multiclass_loss(Y, A2)\n\n    dZ2 = A2-Y\n    dW2 = (1./m) * np.matmul(dZ2, A1.T)\n    db2 = (1./m) * np.sum(dZ2, axis=1, keepdims=True)\n\n    dA1 = np.matmul(W2.T, dZ2)\n    dZ1 = dA1 * sigmoid(Z1) * (1 - sigmoid(Z1))\n    dW1 = (1./m) * np.matmul(dZ1, X.T)\n    db1 = (1./m) * np.sum(dZ1, axis=1, keepdims=True)\n\n    W2 = W2 - learning_rate * dW2\n    b2 = b2 - learning_rate * db2\n    W1 = W1 - learning_rate * dW1\n    b1 = b1 - learning_rate * db1\n\n    if (i % 10 == 0):\n        print(\"Epoch\", i, \"loss: \", loss)\n\nprint(\"Final loss:\", loss)\n\nEpoch 0 loss:  9.359409945262723\nEpoch 10 loss:  2.480915410750769\nEpoch 20 loss:  1.674432764227767\nEpoch 30 loss:  1.3330104308788548\nEpoch 40 loss:  1.1447842302497118\nEpoch 50 loss:  1.0230964725181804\nEpoch 60 loss:  0.9368747323694273\nEpoch 70 loss:  0.871957389404843\nEpoch 80 loss:  0.8208795576102073\nEpoch 90 loss:  0.7793325725168161\nEpoch 100 loss:  0.7446649543545801\nEpoch 110 loss:  0.7151537041535516\nEpoch 120 loss:  0.6896258244540621\nEpoch 130 loss:  0.6672519100025255\nEpoch 140 loss:  0.6474268213495038\nEpoch 150 loss:  0.6296970416913454\nEpoch 160 loss:  0.6137147676333653\nEpoch 170 loss:  0.5992079750548168\nEpoch 180 loss:  0.5859603076597457\nEpoch 190 loss:  0.5737971945414019\nFinal loss: 0.5636592880338959\n\n\nLet‚Äôs see how we did:\n\nZ1 = np.matmul(W1, X_test) + b1\nA1 = sigmoid(Z1)\nZ2 = np.matmul(W2, A1) + b2\nA2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0)\n\npredictions = np.argmax(A2, axis=0)\nlabels = np.argmax(Y_test, axis=0)\n\n\nModel Performance: Confusion Matrix Analysis üìä\nNow for the moment of truth‚Äîhow well does our network perform on unseen test data?\n\nprint(confusion_matrix(predictions, labels))\nprint(classification_report(predictions, labels))\n\n[[ 896    0   26    8    8   22   30    4   14   10]\n [   0 1076   15    6    2    7    3    6   14    2]\n [  14   13  815   30   10   13   23   33   24   13]\n [  10   12   47  820    4   60    5   18   40   17]\n [   0    1   17    0  790   22   30   17   14  106]\n [  27    4    7   56    5  669   23    6   58   16]\n [  13    5   30    7   23   28  822    0   21    1]\n [   6    2   18   25   12   10    3  866   18   37]\n [  12   22   47   42   19   44   16   15  739   28]\n [   2    0   10   16  109   17    3   63   32  779]]\n              precision    recall  f1-score   support\n\n           0       0.91      0.88      0.90      1018\n           1       0.95      0.95      0.95      1131\n           2       0.79      0.82      0.81       988\n           3       0.81      0.79      0.80      1033\n           4       0.80      0.79      0.80       997\n           5       0.75      0.77      0.76       871\n           6       0.86      0.87      0.86       950\n           7       0.84      0.87      0.86       997\n           8       0.76      0.75      0.75       984\n           9       0.77      0.76      0.76      1031\n\n    accuracy                           0.83     10000\n   macro avg       0.82      0.83      0.82     10000\nweighted avg       0.83      0.83      0.83     10000\n\n\n\nWe achieve approximately 84% accuracy across all ten digits‚Äînot bad for a network we built entirely from scratch! Modern state-of-the-art methods using convolutional neural networks reach 99%+ accuracy, but our simple fully-connected network demonstrates the fundamental principles.\nLooking at the confusion matrix and classification report, you can see which digits are hardest to distinguish. Often, 4s and 9s are confused, or 3s and 5s‚Äîeven humans sometimes struggle with these pairs in badly written examples. The confusion matrix gives insight into the network‚Äôs systematic errors, which could guide improvements.",
    "crumbs": [
      "üõ†Ô∏è Hands-On Seminars",
      "Advanced Topics",
      "Advanced: Deep Learning"
    ]
  },
  {
    "objectID": "seminars/seminar01/1_deep_learning.html#testing-the-model-interactive-exploration",
    "href": "seminars/seminar01/1_deep_learning.html#testing-the-model-interactive-exploration",
    "title": "Neural Networks",
    "section": "Testing the Model: Interactive Exploration üîç",
    "text": "Testing the Model: Interactive Exploration üîç\nLet‚Äôs test our trained network on individual examples to get an intuitive feel for its predictions:\n\ni=2003\nplt.imshow(X_test[:,i].reshape(28,28), cmap='gray')\npredictions[i]\n\nnp.int64(5)\n\n\n\n\n\n\n\n\n\nTry changing the index i to explore different test images. Compare the network‚Äôs predictions with what you see. When does it make mistakes? Are the errors understandable? This kind of error analysis is crucial in practical machine learning‚Äîunderstanding failure modes helps you improve your models.",
    "crumbs": [
      "üõ†Ô∏è Hands-On Seminars",
      "Advanced Topics",
      "Advanced: Deep Learning"
    ]
  },
  {
    "objectID": "seminars/seminar01/1_deep_learning.html#reflection-and-next-steps",
    "href": "seminars/seminar01/1_deep_learning.html#reflection-and-next-steps",
    "title": "Neural Networks",
    "section": "Reflection and Next Steps üéØ",
    "text": "Reflection and Next Steps üéØ\nCongratulations! You‚Äôve built a complete neural network from scratch and trained it to recognize handwritten digits with reasonable accuracy. More importantly, you understand every component: forward propagation (matrix multiplications and activations), loss functions (quantifying error), backpropagation (computing gradients via chain rule), and gradient descent (iterative optimization).\nKey Insights for Physicists: The mathematical tools we used‚Äîlinear algebra, calculus, and optimization‚Äîare the same ones you use throughout physics. Neural networks are fundamentally just differentiable programs that we optimize. The loss landscape we navigate with gradient descent is analogous to potential energy surfaces in molecular dynamics or action functionals in field theory.\nWhat We Built: Our final network has ~50,000 parameters organized into two layers. It processes 784-dimensional input vectors (flattened images) through learned transformations to produce 10-dimensional output probability distributions. Training involved computing gradients for all 50,000 parameters on 60,000 examples, iterating 200 times‚Äîabout 600 million gradient computations in total, all completed in seconds thanks to vectorized NumPy operations.\nLimitations and Improvements: Our network treats pixels as independent features, ignoring spatial structure. Convolutional neural networks (CNNs) exploit the 2D structure of images, achieving much better performance. We also used a fixed learning rate and simple gradient descent; modern optimizers like Adam adapt the learning rate automatically. And we‚Äôve barely scratched the surface of architecture design‚Äîdepth, width, activation functions, regularization, and many other choices affect performance.\nIn the Next Seminar: We‚Äôll explore high-level frameworks like TensorFlow and PyTorch that make building and training complex networks much easier. We‚Äôll also see how to apply these techniques to physics problems‚Äîfrom analyzing experimental data to solving differential equations with neural networks. The principles remain the same; the tools just become more powerful.\nFinal Thought: Machine learning is rapidly transforming physics research. From discovering new particles in collider data to controlling quantum systems to simulating complex materials, neural networks are becoming as essential as Fourier transforms or numerical integration. By understanding these tools at a fundamental level, you‚Äôre well-prepared to apply them creatively in your own research. ```",
    "crumbs": [
      "üõ†Ô∏è Hands-On Seminars",
      "Advanced Topics",
      "Advanced: Deep Learning"
    ]
  },
  {
    "objectID": "seminars/seminar06/Report/Figures/Figure2/Figure2.html",
    "href": "seminars/seminar06/Report/Figures/Figure2/Figure2.html",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "import matplotlib as mpl\nimport matplotlib.font_manager as font_manager\nfrom IPython.core.display import HTML\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom directory_tree import display_tree\n\n\nplt.rcParams.update({'font.size': 12,\n                     'lines.linewidth': 1,\n                     'lines.markersize': 10,\n                     'axes.labelsize': 11,\n                     'xtick.labelsize' : 10,\n                     'ytick.labelsize' : 10,\n                     'xtick.top' : True,\n                     'xtick.direction' : 'in',\n                     'ytick.right' : True,\n                     'ytick.direction' : 'in',}) \n\n%config InlineBackend.figure_format = 'retina'\n\n\ndef get_size(w,h):\n    return((w/2.54,h/2.54))\n\n\n# Generate fake experimental data\nx = np.linspace(0, 10, 100)\ny = np.sin(x) + np.random.normal(0, 0.2, size=100)\n\n# Set up the fancy plot\nfig, ax = plt.subplots(figsize=get_size(8, 6),dpi=150)\nax.plot(x, y, color='blue', linewidth=2, label='EXP',alpha=0.5)\n\n# Add some fancy elements to the plot\nax.fill_between(x, y, color='lightgray')\nax.scatter(x, y, color='blue', s=30, label='DATA ',alpha=0.5)\n\nax.set_xlabel('Time')\nax.set_ylabel('Measurement')\n\nax.legend(loc='lower right')\n\nax.tick_params(axis='both', which='major')\n\nax.grid(color='gray', linestyle='--', linewidth=0.5)\n\nax.annotate('Interesting\\nPoint', xy=(4.8, 0), xytext=(4.8, 0.7),\n            arrowprops=dict(facecolor='black', arrowstyle='-&gt;'),\n            fontsize=10, ha='center')\nplt.savefig(\"../figure2.pdf\",bbox_inches = 'tight')\nplt.show()"
  },
  {
    "objectID": "seminars/seminar06/Report/Text/paper.html",
    "href": "seminars/seminar06/Report/Text/paper.html",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "{ ‚Äúcells‚Äù: [ { ‚Äúcell_type‚Äù: ‚Äúmarkdown‚Äù, ‚Äúmetadata‚Äù: {}, ‚Äúsource‚Äù: [ ‚Äú\\documentclass[aps,prl, 9pt,groupedaddress,twocolumn]{revtex4-1}‚Äù, ‚Äú%\\usepackage[USenglish]{babel} ‚Äù, ‚Äú\\usepackage{amsmath}‚Äù, ‚Äú\\usepackage{amsfonts}‚Äù, ‚Äú\\usepackage{graphicx}‚Äù, ‚Äú\\usepackage{xcolor}‚Äù, ‚Äú%\\usepackage{float}‚Äù, ‚Äú%\\bibliographystyle{apsrev4-1}‚Äù, ‚Äú%\\usepackage{hyperref}‚Äù, ‚Äú%\\usepackage{color}‚Äù, ‚Äú\\usepackage{float}‚Äù, ‚Äú‚Äù, ‚Äú‚Äù, ‚Äú\\begin{document}‚Äù, ‚Äú\\title{The Quirky Adventures of a Mischievous Teapot}‚Äù, ‚Äú‚Äù, ‚Äú\\author{Chat GPT}‚Äù, ‚Äú\\email{chatgpt@physik.uni-leipzig.de}‚Äù, ‚Äú\\homepage[]{http://www.uni-leipzig.de/~chatgpt}‚Äù, ‚Äú\\affiliation{Large Language Model Group, Institute of Artificial Physics, University of Leipzig, 04103 Leipzig, Germany}‚Äù, ‚Äú‚Äù, ‚Äú\\begin{abstract}‚Äù, ‚Äúthis whimsical exploration, we delve into the unexpected escapades of a mischievous teapot named Earl Grey. Equipped with a charming personality and an insatiable thirst for adventure, Earl Grey embarks on a quest that transcends the boundaries of the ordinary teapot life. From hosting wild tea parties in enchanted forests to engaging in sassy conversations with talking sugar cubes, Earl Grey‚Äôs escapades are nothing short of extraordinary.‚Äù, ‚Äúa series of unpredictable events, Earl Grey finds itself caught in a hilarious tea-stealing competition with a gang of misfit coffee mugs. As the rivalry intensifies, Earl Grey‚Äôs lid becomes a portal to an alternate dimension where gravity is topsy-turvy, and tea leaves dance to their own rhythm. Amidst the chaos, Earl Grey must navigate through a world of whimsy, dodging flying teaspoons and outsmarting gravity-defying tea leaves.‚Äù, ‚Äúa comedic blend of wit and charm, this abstract takes you on a hilarious journey that questions the norms of teapot life, celebrates the joy of spontaneity, and reminds us that even in the most unexpected circumstances, a steaming cup of tea can bring laughter and delight to the world.‚Äù, ‚Äúnote that this abstract is purely fictional and intended for entertainment purposes.‚Äù, ‚Äú\\end{abstract}‚Äù, ‚Äú‚Äù, ‚Äú\\maketitle‚Äù, ‚Äú‚Äù, ‚Äú\\input{content/01_introduction}‚Äù, ‚Äú‚Äù, ‚Äú\\input{content/02_theory}‚Äù, ‚Äú‚Äù, ‚Äú\\input{content/03_results}‚Äù, ‚Äú‚Äù, ‚Äú\\input{content/04_conclusions}‚Äù, ‚Äú‚Äù, ‚Äú‚Äù, ‚Äú‚Äù, ‚Äú\\end{document}‚Äù ] } ], ‚Äúmetadata‚Äù: { ‚Äúkernelspec‚Äù: { ‚Äúname‚Äù: ‚Äúpython3‚Äù, ‚Äúlanguage‚Äù: ‚Äúpython‚Äù, ‚Äúdisplay_name‚Äù: ‚ÄúPython 3 (ipykernel)‚Äù, ‚Äúpath‚Äù: ‚Äú/Users/fci/quarto_env/share/jupyter/kernels/python3‚Äù } }, ‚Äúnbformat‚Äù: 4, ‚Äúnbformat_minor‚Äù: 4 }"
  },
  {
    "objectID": "seminars/seminar09/md_vibration.html",
    "href": "seminars/seminar09/md_vibration.html",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "",
    "text": "So far, we have already developed a basic molecular dynamics simulation for simple atoms and interactions by so-called van der Waals forces. Since we are already nearing the end of the course, we would at least like to add some more complexity to the simulation. In this seminar, we would like to make molecules by binding two atoms together. We will then simulate the vibration of these molecules by solving the equations of motion for the atoms."
  },
  {
    "objectID": "seminars/seminar09/md_vibration.html#bond-vibrations",
    "href": "seminars/seminar09/md_vibration.html#bond-vibrations",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "Bond vibrations",
    "text": "Bond vibrations\nA bond vibration is modeled as a harmonic oscillator where two atoms are coupled by a spring force with spring constant \\(k\\). The bond force on atom \\(i\\) due to a bond with atom \\(j\\) is then given by\n\\[\\vec{F}_{bond}^i = -k(\\vec{r}_{ij} - \\vec{r}_0)\\]\nwhere \\(\\vec{r}_{ij}\\) is the vector between atom \\(i\\) and atom \\(j\\) and \\(\\vec{r}_0\\) is the equilibrium bond length vector. In reality bonds are not harmonic oscillators and deformations lead to a non-linear force curve. However, for small deviations from the equilibrium bond length, the harmonic approximation is quite good. The spring constant \\(k\\) determines the strength of the bond and therefore the frequency \\(\\omega\\) of the bond vibration\n\\[\\omega = \\sqrt{\\frac{k}{\\mu}}\\]\nwhere \\(\\mu = \\frac{m_1 m_2}{m_1 + m_2}\\) is the reduced mass of the two bonded atoms with masses \\(m_1\\) and \\(m_2\\).\n\nReduced units\nAs we have defined reduced units for our simulation based on the LJ parameters, we need to express the bond parameters in these units.\n\nBond length\nThe bond length \\(r_0\\) is expressed in terms of the Lennard-Jones length scale \\(\\sigma\\). For the bond between two hydrogen atoms, the equilibrium bond length is approximately \\(0.74\\sigma\\).\n\\[ r_{\\text{bond}}^* = \\frac{r_{\\text{bond}}}{\\sigma_{\\text{H}}} = \\frac{0.74}{1.0} = 0.74 \\]\n\n\nSpring constant\nThe spring constant \\(k\\) is expressed in reduced units based on the LJ energy scale \\(\\epsilon\\) and length scale \\(\\sigma\\). For a hydrogen molecule, the spring constant is approximately \\(440\\) in reduced units.\n\\[ k_{\\text{spring}}^* = \\frac{k \\sigma_{\\text{H}}^2}{\\epsilon_{\\text{H}}} \\]"
  },
  {
    "objectID": "seminars/seminar09/md_vibration.html#equations-of-motion",
    "href": "seminars/seminar09/md_vibration.html#equations-of-motion",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "Equations of motion",
    "text": "Equations of motion\nThe equations of motion for the two atoms need to account for both the bond forces and the van der Waals forces. The total force on each atom is the sum of these contributions:\n\\[m_1 \\ddot{\\vec{r}}_1 = \\vec{F}_{bond}^1 + \\vec{F}_{vdW}^1\\]\n\\[m_2 \\ddot{\\vec{r}}_2 = \\vec{F}_{bond}^2 + \\vec{F}_{vdW}^2\\]\nwhere \\(\\vec{r}_1\\) and \\(\\vec{r}_2\\) are the positions of the two atoms, \\(\\vec{F}_{bond}\\) are the bond forces described above, and \\(\\vec{F}_{vdW}\\) are the van der Waals forces we implemented earlier. We can rewrite these equations as a system of first-order differential equations\n\\[\\dot{\\vec{v}}_1 = \\frac{\\vec{F}_{bond}^1 + \\vec{F}_{vdW}^1}{m_1}\\]\n\\[\\dot{\\vec{v}}_2 = \\frac{\\vec{F}_{bond}^2 + \\vec{F}_{vdW}^2}{m_2}\\]\n\\[\\dot{\\vec{r}}_1 = \\vec{v}_1\\]\n\\[\\dot{\\vec{r}}_2 = \\vec{v}_2\\]\nThe van der Waals forces act between atoms of different molecules while the bond forces act between atoms within the same molecule. Both types of forces are calculated at each timestep and combined to determine the total force on each atom."
  },
  {
    "objectID": "seminars/seminar09/md_vibration.html#implementation",
    "href": "seminars/seminar09/md_vibration.html#implementation",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "Implementation",
    "text": "Implementation\n\nThe Bond class\nAs we wrote the simulation quite modular, we can now easily add the bond forces to the simulation. To do so, we will first introduce a new class Bond that will store the bond parameters and the atoms involved in the bond.\nclass Bond:\n    def __init__(self, atom1, atom2, k, r0):\n        self.atom1 = atom1\n        self.atom2 = atom2\n        self.k = k      # spring constant\n        self.r0 = r0    # equilibrium bond length\nThis bond class will be used to store the bond parameters for each bond in the simulation.\n\n\nThe DiatomicMolecule class\nWhile this is just a bond between two atoms, which can be also part of a more complicated molecule, we also need to define a molecule class that will store the atoms and bonds that make up the molecule.\nclass DiatomicMolecule:\n    def __init__(self, atom1, atom2, bond):\n        self.atom1 = atom1\n        self.atom2 = atom2\n        self.bond = bond\nThis class will store the two atoms and the bond between them.\n\n\nThe ForceField class update\nWe will also need to update the ForceField class to store the bond parameters and to calculate the bond forces. We will add a list of bonds to the force field and a method to calculate the bond forces.\nclass ForceField:\n    def __init__(self):\n        self.parameters = {\n            'C': {'epsilon': 1.615, 'sigma': 1.36},\n            'H': {'epsilon': 1.0, 'sigma': 1.0},\n            'O': {'epsilon': 1.846, 'sigma': 3.0},\n        }\n        self.bond_parameters = {\n            ('H', 'H'): {'k': 500.0, 'r0': 0.74},  # Example parameters for H2\n            ('O', 'H'): {'k': 550.0, 'r0': 0.96},  # Example parameters for OH bond\n        }\n        self.box_size = None\n\n    def calculate_bond_force(self, bond):\n        \"\"\"Calculate harmonic bond force\"\"\"\n        r = self.minimum_image_distance(bond.atom1.position, bond.atom2.position)\n        r_mag = np.linalg.norm(r)\n\n        # F = -k(r - r0)‚àô(r/|r|)\n        force_mag = -bond.k * (r_mag - bond.r0)\n        force = force_mag * r/r_mag\n        return force\n\n\n    # previous code goes here\nThe new addition provides the bond parameters\nself.bond_parameters = {\n    ('H', 'H'): {'k': 500.0, 'r0': 0.74},  # Example parameters for H2\n    ('O', 'H'): {'k': 550.0, 'r0': 0.96},  # Example parameters for OH bond\n}\nwhich are exemplary values for the bond between two hydrogen atoms and between an oxygen and a hydrogen atom. The method calculate_bond_force calculates the bond force between two atoms given a bond object.\ndef calculate_bond_force(self, bond):\n    \"\"\"Calculate harmonic bond force\"\"\"\n    r = self.minimum_image_distance(bond.atom1.position, bond.atom2.position)\n    r_mag = np.linalg.norm(r)\n\n    # F = -k(r - r0)‚àô(r/|r|)\n    force_mag = -bond.k * (r_mag - bond.r0)\n    force = force_mag * r/r_mag\n    return force\nAs a first step, this function calculates the distance vector between the two atoms and its magnitude. The force is then calculated as \\(F = -k(r - r_0) \\cdot (r/|r|)\\), where \\(r\\) is the distance vector, \\(r_0\\) is the equilibrium bond length, and \\(k\\) is the spring constant. The force is then returned as a vector."
  },
  {
    "objectID": "seminars/seminar09/md_vibration.html#md-simulation-class-update",
    "href": "seminars/seminar09/md_vibration.html#md-simulation-class-update",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "MD Simulation class update",
    "text": "MD Simulation class update\nThe MDSimulation class is the controller of the simulation and needs to be updated to include the molecules and bond forces. We will add a list of molecules to the simulation and update the calculate_forces method to include the bond forces. The method to update the positions and velocities of the atoms will stay the same as we only need to add the bond forces to the total force calculation.\nclass MDSimulation:\n    def __init__(self, molecules, forcefield, timestep, box_size):\n        self.molecules = molecules\n        self.atoms = [atom for mol in molecules for atom in [mol.atom1, mol.atom2]]\n        self.forcefield = forcefield\n        self.forcefield.box_size = box_size\n        self.timestep = timestep\n        self.box_size = np.array(box_size)\n        self.energy_history = []\n\n    def calculate_forces(self):\n        # Reset all forces\n        for atom in self.atoms:\n            atom.reset_force()\n\n        # Calculate bonded forces\n        for molecule in self.molecules:\n            force = self.forcefield.calculate_bond_force(molecule.bond)\n            molecule.atom1.add_force(force)\n            molecule.atom2.add_force(-force)\n\n        # Calculate non-bonded forces between molecules\n        for i, mol1 in enumerate(self.molecules):\n            for mol2 in self.molecules[i+1:]:\n                # Calculate forces between atoms of different molecules\n                for atom1 in [mol1.atom1, mol1.atom2]:\n                    for atom2 in [mol2.atom1, mol2.atom2]:\n                        force = self.forcefield.calculate_lj_force(atom1, atom2)\n                        atom1.add_force(force)\n                        atom2.add_force(-force)\n\n    # def update_positions_and_velocities(self):\nThe constructor of the MDSimulation class now takes a list of molecules as an argument. The list of atoms is created by flattening the list of atoms in each molecule. This is all we need in the constructor.\nThe calculate_forces method now calculates the bond forces for each molecule and adds them to the atoms. The non-bonded forces are calculated as before. The methodupdate_positions_and_velocities will remain the same as we only need to add the bond forces to the total force calculation."
  },
  {
    "objectID": "seminars/seminar09/md_vibration.html#create-a-diatomic-molecule",
    "href": "seminars/seminar09/md_vibration.html#create-a-diatomic-molecule",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "Create a diatomic molecule",
    "text": "Create a diatomic molecule\nFinally, we need something to create the diatomic molecules for out simulation. We will write a function that creates a number of diatomic molecules with a given box size and atom types. The function will create the atoms at random positions with a small displacement from the center of a grid and create a bond between them.\n```python\ndef create_diatomic_molecules(num_molecules, box_size, type1=\"H\", type2=\"H\", mass1=1.0, mass2=1.0):\n    molecules = []\n    spacing = np.min(box_size) / np.ceil(np.sqrt(num_molecules))\n\n    for i in range(num_molecules):\n        # Calculate grid position for molecule center\n        row = i // int(np.ceil(np.sqrt(num_molecules)))\n        col = i % int(np.ceil(np.sqrt(num_molecules)))\n        center = np.array([col * spacing + spacing/2, row * spacing + spacing/2])\n\n        # Create atoms with small random displacement for initial bond length\n        displacement = np.random.randn(2) * 0.1\n        atom1 = Atom(2*i, type1, center + displacement, mass=mass1)\n        atom2 = Atom(2*i+1, type2, center - displacement, mass=mass2)\n\n        # Create bond\n        ff = ForceField()\n        bond_params = ff.bond_parameters[(type1, type2)]\n        bond = Bond(atom1, atom2, bond_params['k'], bond_params['r0'])\n\n        molecules.append(DiatomicMolecule(atom1, atom2, bond))\n\n    return molecules"
  },
  {
    "objectID": "seminars/seminar09/md_vibration.html#md-simulation",
    "href": "seminars/seminar09/md_vibration.html#md-simulation",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "MD Simulation",
    "text": "MD Simulation\nIn the last step we need to initialize the simulation with the diatomic molecules and run the simulation.\n## Initialize simulation as before\n\nbox_size = np.array([50.0, 50.0])\nnum_molecules = 100\nmolecules = create_diatomic_molecules(num_molecules, box_size, \"H\", \"H\")\nff = ForceField()\nsim = MDSimulation(molecules, ff, 0.001, box_size)\n\n# Initialize velocities for all atoms\natoms = [atom for mol in molecules for atom in [mol.atom1, mol.atom2]]\ninitialize_velocities(atoms, temperature=5.0)\n\n## rest of the simulation goes here"
  },
  {
    "objectID": "seminars/seminar09/md_vibration.html#summary",
    "href": "seminars/seminar09/md_vibration.html#summary",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "Summary",
    "text": "Summary\nIn this module, we extended our molecular dynamics simulation to include diatomic molecules with bond vibrations. We added several new components:\n\nA Bond class to model the harmonic bond between two atoms, implementing Hooke‚Äôs law for the bond force\nA DiatomicMolecule class to manage pairs of bonded atoms\nUpdates to the ForceField class to include bond parameters and force calculations\nModifications to the MDSimulation class to handle both bonded and non-bonded interactions\n\nThe bond vibrations are modeled as harmonic oscillators with a spring constant k and equilibrium bond length \\(r_0\\). The total forces on each atom now include both the van der Waals forces between molecules and the bond forces within molecules. We provided functions to create and initialize diatomic molecules in a simulation box, setting up a foundation for simulating more complex molecular systems.\nThe implementation maintains the modular structure of our previous code while adding the capability to simulate molecular vibrations, bringing us closer to modeling real molecular systems."
  },
  {
    "objectID": "seminars/seminar09/seminar9.html",
    "href": "seminars/seminar09/seminar9.html",
    "title": "Seminar Coding Exercises 2",
    "section": "",
    "text": "Example1: Damped Oscillator\n\n\n\nCreate a simple simulation of a damped oscillator using a class structure. Include basic methods to calculate and plot the motion. This introduces both classes and simple differential equations. The damped oscillator is a classic physics problem that demonstrates how energy is dissipated in real systems through friction or air resistance. The motion follows an exponentially decaying sinusoidal pattern, determined by the mass, spring constant, and damping coefficient. We‚Äôll use a class to encapsulate these properties and provide methods to analyze the system‚Äôs behavior over time.\nTime estimate: 20 minutes\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nYou‚Äôll need to:\n\nUse np.linspace() to create time array from 0 to t_max\nCalculate position for each time point using self.position()\nCreate a figure using plt.figure()\nPlot time vs position using plt.plot()\nAdd labels, title, and grid\nShow the plot using plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nNote"
  },
  {
    "objectID": "seminars/Assignment 2.html",
    "href": "seminars/Assignment 2.html",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "Define a function year_to_century that returns the century for a given year. The first century spans from the year 1 up to and including the year 100, the second - from the year 101 up to and including the year 200, etc.\n\ndef year_to_century(year):\n    return (year + 99) // 100\n\nyear_to_century(2024)\n\n21\n\n\nWrite python code to import the numpy module under the namespace np.\n\nimport numpy as np\n\nWrite a Python function inner_product that computes the inner product of two vectors of the same length.¬†Do not use any external modules.\nHint: You can use the raise keyword to throw an error.\n\ndef inner_product(vec1, vec2):\n    \n    if len(vec1) != len(vec2):\n        raise SystemError(\"Vectors must be of the same length\")\n    \n    a=0\n    for i in range(0,len(vec1)):\n        a += vec1[i] * vec2[i]\n    return a\n\n\ninner_product([1, 2, 3], [3, 4, 5])\n\n26\n\n\nDefine a function triangle that takes the 3 sides of a triangle and returns one of the following:¬†\n‚ÄúNo Triangle‚Äù:¬†If the sides don‚Äôt form a valid triangle (degenerate case).\n‚ÄúAcute Triangle‚Äù: If all angles are acute.\n‚ÄúRight Triangle‚Äù: If one angle is exactly 90¬∞.\n‚ÄúObtuse Triangle‚Äù: If one angle is obtuse.\n\ndef triangle(a, b, c):\n    sides = sorted([a, b, c]) # Sort sides so that a &lt;= b &lt;= c\n    a, b, c = sides\n    \n    if a + b &lt;= c:\n        return \"No Triangle\"\n    \n    if a**2 + b**2 == c**2:\n        return \"Right Triangle\"\n    \n    if a**2 + b**2 &gt; c**2:\n        return \"Acute Triangle\"\n    \n    return \"Obtuse Triangle\"\n\nAssign the 100 numbers between -10 and 10 including -10 and 10 to the variable x using the linspace function of numpy. Assign the function values \\(x¬≥ - 3x¬≤ - 10\\) to the variable y. Calculate the numerical derivative $ $ from the differences between neighbouring values in the¬†y and the x array and store it in the variable derivative.\n\nimport numpy as np\n\nx=np.linspace(-10,10,100)\ny=x**3 - 3*x**2 - 10\n\nderivative=(y[1:]-y[:-1])/(x[1:]-x[:-1])\n\n\nx=np.random.randint(10,size=(10,8))\nx\n\narray([[2, 0, 6, 0, 1, 5, 0, 7],\n       [4, 2, 5, 7, 9, 8, 2, 1],\n       [4, 1, 3, 9, 9, 3, 8, 8],\n       [9, 4, 0, 6, 9, 2, 0, 3],\n       [2, 5, 2, 0, 1, 2, 4, 8],\n       [6, 1, 1, 7, 5, 5, 8, 2],\n       [0, 1, 0, 0, 2, 0, 1, 4],\n       [7, 4, 4, 2, 3, 7, 0, 3],\n       [4, 3, 6, 8, 3, 1, 0, 4],\n       [1, 0, 5, 6, 7, 2, 3, 9]])\n\n\n\ndt=0.05\nD=2.2e-13\nN=200\nsigma=np.sqrt(2*D*dt)\nscale=1e6\nn_trajectories=1000\n\nplt.figure(figsize=(6,6))\n[plt.plot(np.random.normal(0,sigma,size=N).cumsum()*scale,np.random.normal(0,sigma,size=N).cumsum()*scale,'k-',alpha=0.2) for _ in range(n_trajectories)]\nplt.xlim(-5,5)\nplt.ylim(-5,5)\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.plot(dx)\n\n\n\n\n\n\n\n\n\n?np.random.randint\n\n\nSignature: np.random.randint(low, high=None, size=None, dtype=&lt;class 'int'&gt;)\n\nDocstring:\n\nrandint(low, high=None, size=None, dtype=int)\n\n\n\nReturn random integers from `low` (inclusive) to `high` (exclusive).\n\n\n\nReturn random integers from the \"discrete uniform\" distribution of\n\nthe specified dtype in the \"half-open\" interval [`low`, `high`). If\n\n`high` is None (the default), then results are from [0, `low`).\n\n\n\n.. note::\n\n    New code should use the `~numpy.random.Generator.integers`\n\n    method of a `~numpy.random.Generator` instance instead;\n\n    please see the :ref:`random-quick-start`.\n\n\n\nParameters\n\n----------\n\nlow : int or array-like of ints\n\n    Lowest (signed) integers to be drawn from the distribution (unless\n\n    ``high=None``, in which case this parameter is one above the\n\n    *highest* such integer).\n\nhigh : int or array-like of ints, optional\n\n    If provided, one above the largest (signed) integer to be drawn\n\n    from the distribution (see above for behavior if ``high=None``).\n\n    If array-like, must contain integer values\n\nsize : int or tuple of ints, optional\n\n    Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n\n    ``m * n * k`` samples are drawn.  Default is None, in which case a\n\n    single value is returned.\n\ndtype : dtype, optional\n\n    Desired dtype of the result. Byteorder must be native.\n\n    The default value is long.\n\n\n\n    .. warning::\n\n      This function defaults to the C-long dtype, which is 32bit on windows\n\n      and otherwise 64bit on 64bit platforms (and 32bit on 32bit ones).\n\n      Since NumPy 2.0, NumPy's default integer is 32bit on 32bit platforms\n\n      and 64bit on 64bit platforms.  Which corresponds to `np.intp`.\n\n      (`dtype=int` is not the same as in most NumPy functions.)\n\n\n\nReturns\n\n-------\n\nout : int or ndarray of ints\n\n    `size`-shaped array of random integers from the appropriate\n\n    distribution, or a single such random int if `size` not provided.\n\n\n\nSee Also\n\n--------\n\nrandom_integers : similar to `randint`, only for the closed\n\n    interval [`low`, `high`], and 1 is the lowest value if `high` is\n\n    omitted.\n\nrandom.Generator.integers: which should be used for new code.\n\n\n\nExamples\n\n--------\n\n&gt;&gt;&gt; np.random.randint(2, size=10)\n\narray([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]) # random\n\n&gt;&gt;&gt; np.random.randint(1, size=10)\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\n\nGenerate a 2 x 4 array of ints between 0 and 4, inclusive:\n\n\n\n&gt;&gt;&gt; np.random.randint(5, size=(2, 4))\n\narray([[4, 0, 2, 1], # random\n\n       [3, 2, 2, 0]])\n\n\n\nGenerate a 1 x 3 array with 3 different upper bounds\n\n\n\n&gt;&gt;&gt; np.random.randint(1, [3, 5, 10])\n\narray([2, 2, 9]) # random\n\n\n\nGenerate a 1 by 3 array with 3 different lower bounds\n\n\n\n&gt;&gt;&gt; np.random.randint([1, 5, 7], 10)\n\narray([9, 8, 7]) # random\n\n\n\nGenerate a 2 by 4 array using broadcasting with dtype of uint8\n\n\n\n&gt;&gt;&gt; np.random.randint([1, 3, 5, 7], [[10], [20]], dtype=np.uint8)\n\narray([[ 8,  6,  9,  7], # random\n\n       [ 1, 16,  9, 12]], dtype=uint8)\n\nType:      method\n\n\n\n\nlist(range(0,6))\n\n[0, 1, 2, 3, 4, 5]\n\n\n\n    \n\n\nimport matplotlib.pyplot as plt \n\ndifference=derivative-np.array(derivative1)\nplt.plot(x[:-1],difference)\n\n\n\n\n\n\n\n\n\n\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[45], line 1\n----&gt; 1 x.diff()\n\nAttributeError: 'numpy.ndarray' object has no attribute 'diff'\n\n\n\n\nx=np.linspace(-10,10,100)\n\n\ndir(x)\n\n['T',\n '__abs__',\n '__add__',\n '__and__',\n '__array__',\n '__array_finalize__',\n '__array_function__',\n '__array_interface__',\n '__array_namespace__',\n '__array_priority__',\n '__array_struct__',\n '__array_ufunc__',\n '__array_wrap__',\n '__bool__',\n '__buffer__',\n '__class__',\n '__class_getitem__',\n '__complex__',\n '__contains__',\n '__copy__',\n '__deepcopy__',\n '__delattr__',\n '__delitem__',\n '__dir__',\n '__divmod__',\n '__dlpack__',\n '__dlpack_device__',\n '__doc__',\n '__eq__',\n '__float__',\n '__floordiv__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__iadd__',\n '__iand__',\n '__ifloordiv__',\n '__ilshift__',\n '__imatmul__',\n '__imod__',\n '__imul__',\n '__index__',\n '__init__',\n '__init_subclass__',\n '__int__',\n '__invert__',\n '__ior__',\n '__ipow__',\n '__irshift__',\n '__isub__',\n '__iter__',\n '__itruediv__',\n '__ixor__',\n '__le__',\n '__len__',\n '__lshift__',\n '__lt__',\n '__matmul__',\n '__mod__',\n '__mul__',\n '__ne__',\n '__neg__',\n '__new__',\n '__or__',\n '__pos__',\n '__pow__',\n '__radd__',\n '__rand__',\n '__rdivmod__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__rfloordiv__',\n '__rlshift__',\n '__rmatmul__',\n '__rmod__',\n '__rmul__',\n '__ror__',\n '__rpow__',\n '__rrshift__',\n '__rshift__',\n '__rsub__',\n '__rtruediv__',\n '__rxor__',\n '__setattr__',\n '__setitem__',\n '__setstate__',\n '__sizeof__',\n '__str__',\n '__sub__',\n '__subclasshook__',\n '__truediv__',\n '__xor__',\n 'all',\n 'any',\n 'argmax',\n 'argmin',\n 'argpartition',\n 'argsort',\n 'astype',\n 'base',\n 'byteswap',\n 'choose',\n 'clip',\n 'compress',\n 'conj',\n 'conjugate',\n 'copy',\n 'ctypes',\n 'cumprod',\n 'cumsum',\n 'data',\n 'device',\n 'diagonal',\n 'dot',\n 'dtype',\n 'dump',\n 'dumps',\n 'fill',\n 'flags',\n 'flat',\n 'flatten',\n 'getfield',\n 'imag',\n 'item',\n 'itemset',\n 'itemsize',\n 'mT',\n 'max',\n 'mean',\n 'min',\n 'nbytes',\n 'ndim',\n 'newbyteorder',\n 'nonzero',\n 'partition',\n 'prod',\n 'ptp',\n 'put',\n 'ravel',\n 'real',\n 'repeat',\n 'reshape',\n 'resize',\n 'round',\n 'searchsorted',\n 'setfield',\n 'setflags',\n 'shape',\n 'size',\n 'sort',\n 'squeeze',\n 'std',\n 'strides',\n 'sum',\n 'swapaxes',\n 'take',\n 'to_device',\n 'tobytes',\n 'tofile',\n 'tolist',\n 'trace',\n 'transpose',\n 'var',\n 'view']"
  },
  {
    "objectID": "lectures/lecture04/04-plotting.html",
    "href": "lectures/lecture04/04-plotting.html",
    "title": "Plotting",
    "section": "",
    "text": "Data visualization through plotting is a crucial tool for analyzing and interpreting scientific data and theoretical predictions. While plotting capabilities are not built into Python‚Äôs core, they are available through various external library modules. Matplotlib is widely recognized as the de facto standard for plotting in Python. However, several other powerful plotting libraries exist, including PlotLy, Seaborn, and Bokeh, each offering unique features and capabilities for data visualization.\nAs Matplotlib is an external library (actually a collection of libraries), it must be imported into any script that uses it. While Matplotlib relies heavily on NumPy, importing NumPy separately is not always necessary for basic plotting. However, for most scientific applications, you‚Äôll likely use both. To create 2D plots, you typically start by importing Matplotlib‚Äôs pyplot module:\nThis import introduces the implicit interface of pyplot for creating figures and plots. Matplotlib offers two main interfaces:\nWe will use most of the the the pyplot interface as in the examples below. The section Additional Plotting will refer to the explicit programming of figures.\nWe can set some of the parameters for the appearance of graphs globally. In case you still want to modify a part of it, you can set individual parameters later during plotting. The command used here is the\nfunction, which takes a dictionary with the specific parameters as key."
  },
  {
    "objectID": "lectures/lecture04/04-plotting.html#simple-plotting",
    "href": "lectures/lecture04/04-plotting.html#simple-plotting",
    "title": "Plotting",
    "section": "Simple Plotting",
    "text": "Simple Plotting\nMatplotlib offers multiple levels of functionality for creating plots. Throughout this section, we‚Äôll primarily focus on using commands that leverage default settings. This approach simplifies the process, as Matplotlib automatically handles much of the graph layout. These high-level commands are ideal for quickly creating effective visualizations without delving into intricate details. At the end of this section, we‚Äôll briefly touch upon more advanced techniques that provide greater control over plot elements and layout.\n\nAnatomy of a Line Plot\nTo create a basic line plot, use the following command:\nplt.plot(x, y)\nBy default, this generates a line plot. However, you can customize the appearance by adjusting various parameters within the plot() function. For instance, you can modify it to resemble a scatter plot by changing certain arguments. The versatility of this command allows for a range of visual representations beyond simple line plots.\nLet‚Äôs create a simple line plot of the sine function over the interval [0, 4œÄ]. We‚Äôll use NumPy to generate the x-values and calculate the corresponding y-values. The following code snippet demonstrates this process:\n1x = np.linspace(0, 4.*np.pi, 100)\n2y = np.sin(x)\n\n3plt.figure(figsize=(4,3))\n4plt.plot(x, y)\n5plt.tight_layout()\n6plt.show()\n\n1\n\nCreate an array of 100 values between 0 and 4œÄ.\n\n2\n\nCalculate the sine of each value in the array.\n\n3\n\ncreate a new figure\n\n4\n\nplot the data\n\n5\n\nautomatically adjust the layout\n\n6\n\nshow the figure\n\n\nHere is the code in a Python cell:\n\n\n\n\n\n\nTry to change the values of the x and y arrays and see how the plot changes.\n\n\n\n\n\n\nWhy use plt.tight_layout()\n\n\n\n\n\nplt.tight_layout() is a very useful function in Matplotlib that automatically adjusts the spacing between plot elements to prevent overlapping and ensure that all elements fit within the figure area. Here‚Äôs what it does:\n\nPadding Adjustment: It adjusts the padding between and around subplots to prevent overlapping of axis labels, titles, and other elements.\nSubplot Spacing: It optimizes the space between multiple subplots in a figure.\nText Accommodation: It ensures that all text elements (like titles, labels, and legends) fit within the figure without being cut off.\nMargin Adjustment: It adjusts the margins around the entire figure to make sure everything fits neatly.\nAutomatic Resizing: If necessary, it can slightly resize subplot areas to accommodate all elements.\nLegend Positioning: It takes into account the presence and position of legends when adjusting layouts.\n\nKey benefits of using plt.tight_layout():\n\nIt saves time in manual adjustment of plot elements.\nIt helps create more professional-looking and readable plots.\nIt‚Äôs particularly useful when creating figures with multiple subplots or when saving figures to files.\n\nYou typically call plt.tight_layout() just before plt.show() or plt.savefig(). For example:\nplt.figure()\n# ... (your plotting code here)\nplt.tight_layout()\nplt.show()\n\n\n\n\nAxis Labels\nTo enhance the clarity and interpretability of our plots, it‚Äôs crucial to provide context through proper labeling. Let‚Äôs add descriptive axis labels to our diagram, a practice that significantly improves the readability and comprehension of the data being presented.\nplt.xlabel('x-label')\nplt.ylabel('y-label')\n\n\n\n\n\n\n\n\nLegends\nplt.plot(..., label=r'$\\sin(x)$')\nplt.legend(loc='lower left')\n\n\n\n\n\n\n\n\nPlots with error bars\nWhen plotting experimental data it is customary to include error bars that indicate graphically the degree of uncertainty that exists in the measurement of each data point. The MatPlotLib function errorbar plots data with error bars attached. It can be used in a way that either replaces or augments the plot function. Both vertical and horizontal error bars can be displayed. The figure below illustrates the use of error bars.\n\n\n\n\n\n\n\n\nSaving figures\nTo save a figure to a file we can use the savefig method in the Figure class. Matplotlib can generate high-quality output in a number formats, including PNG, JPG, EPS, SVG, PGF and PDF. For scientific papers, I recommend using PDF whenever possible. (LaTeX documents compiled with pdflatex can include PDFs using the includegraphics command). In some cases, PGF can also be good alternative."
  },
  {
    "objectID": "lectures/lecture04/04-plotting.html#other-plot-types",
    "href": "lectures/lecture04/04-plotting.html#other-plot-types",
    "title": "Plotting",
    "section": "Other Plot Types",
    "text": "Other Plot Types\n\nScatter plot\nIf you prefer to use symbols for plotting just use the\nplt.scatter(x,y)\ncommand of pylab. Note that the scatter command requires a x and y values and you can set the marker symbol (see an overview of the marker symbols).\n\n\n\n\n\n\n\n\nHistograms\nA very useful plotting command is also the hist command. It generates a histogram of the data provided. A histogram is a graphical representation of the distribution of numerical data. It is an estimate of the probability distribution of a continuous variable. To construct a histogram, the first step is to ‚Äúbin‚Äù the range of values‚Äîthat is, divide the entire range of values into a series of intervals‚Äîand then count how many values fall into each interval. The bins are usually specified as consecutive, non-overlapping intervals of a variable. The bins must be adjacent, and are often (but not required to be) of equal size.\nWhen using the histogram function, you have flexibility in how the data is grouped. If you only provide the dataset, the function will automatically determine appropriate bins. However, you can also specify custom bins by passing an array of intervals using the syntax hist(data, bins=b), where b is your custom array of bin edges. To normalize the histogram so that the total area under it equals 1, you can set the density parameter to True. It‚Äôs worth noting that the histogram function doesn‚Äôt just create a visual representation; it also returns useful information such as the count of data points in each bin and the bin edges themselves.\n\n\n\n\n\n\nPhysics Interlude- Probability density for finding an oscillating particle\n\n\n\nLet‚Äôs integrate histogram plotting with a fundamental physics concept: the simple harmonic oscillator in one dimension. This system is described by a specific equation of motion:\n\\[\\begin{equation}\n\\ddot{x}(t) = -\\omega^2 x(t)\n\\end{equation}\\]\nFor an initial elongation \\(\\Delta x\\) at \\(t=0\\), the solution is:\n\\[\\begin{equation}\nx(t) = \\Delta x \\cos(\\omega t)\n\\end{equation}\\]\nTo calculate the probability of finding the spring at a certain elongation, we need to consider the time spent at different positions. The time \\(dt\\) spent in the interval [\\(x(t)\\), \\(x(t)+dx\\)] depends on the speed:\n\\[\\begin{equation}\nv(t) = \\frac{dx}{dt} = -\\omega \\Delta x \\sin(\\omega t)\n\\end{equation}\\]\nThe probability of finding the oscillator in a certain interval is the fraction of time spent in this interval, normalized by half the oscillation period \\(T/2\\):\n\\[\\begin{equation}\n\\frac{dt}{T/2} = \\frac{1}{T/2}\\frac{dx}{v(t)} = \\frac{1}{T/2}\\frac{-dx}{\\omega \\Delta x \\sin(\\omega t)}\n\\end{equation}\\]\nGiven that \\(\\omega = 2\\pi/T\\), we can derive the probability density:\n\\[\\begin{equation}\np(x)dx = \\frac{1}{\\pi \\Delta x}\\frac{dx}{\\sqrt{1-\\left(\\frac{x(t)}{\\Delta x}\\right)^2}}\n\\end{equation}\\]\nThis probability density reveals that the spring is more likely to be found at elongations where its speed is low. This principle extends to non-equilibrium physics, where entities moving with variable speed are more likely to be found in locations where they move slowly.\nWe can visualize this using the histogram function. By evaluating the position at equidistant times using the equation of motion and creating a histogram of these positions, we can represent the probability of finding the oscillator at certain positions. When properly normalized, this histogram will reflect the theoretical probability density we derived.\n\n\n\n\n\n\n\n\n\n\nSetting plotting limits and excluding data\nIf you want to zoom in to s specific region of a plot you can set the limits of the individual axes.\n\n\n\n\n\n\n\n\nMasked arrays\nSometimes you encounter situations, when you wish to mask some of the data of your plot, because they are not showing real data as the vertical lines in the plot above. For this purpose, you can mask the data arrays in various ways to not show up. The example below uses the\nnp.ma.masked_where()\nfunction of NumPy, which takes a condition as the first argument and what should be returned if that condition is fulfilled.\n\n\n\n\n\n\nIf you look at the resulting array, you will find, that the entries have not been removed but replaced by --, so the values are not existent and thefore not plotted.\n\n\n\n\n\n\n\nLogarithmic plots\n\n\n\n\n\nData sets can span many orders of magnitude from fractional quantities much smaller than unity to values much larger than unity. In such cases it is often useful to plot the data on logarithmic axes.\n\nSemi-log plots\nFor data sets that vary exponentially in the independent variable, it is often useful to use one or more logarithmic axes. Radioactive decay of unstable nuclei, for example, exhibits an exponential decrease in the number of particles emitted from the nuclei as a function of time.\nMatPlotLib provides two functions for making semi-logarithmic plots, semilogx and semilogy, for creating plots with logarithmic x and y axes, with linear y and x axes, respectively. We illustrate their use in the program below, which made the above plots.\n\n\n\n\n\n\n\n\nLog-log plots\nMatPlotLib can also make log-log or double-logarithmic plots using the function loglog. It is useful when both the \\(x\\) and \\(y\\) data span many orders of magnitude. Data that are described by a power law \\(y=Ax^b\\), where \\(A\\) and \\(b\\) are constants, appear as straight lines when plotted on a log-log plot. Again, the loglog function works just like the plot function but with logarithmic axes.\n\n\n\n\n\n\n\n\n\n\n\n\nCombined plots\nYou can combine multiple data with the same axes by stacking multiple plots.\n\n\n\n\n\n\n\n\nArranging multiple plots\nOften you want to create two or more graphs and place them next to one another, generally because they are related to each other in some way.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnimations\n\n\n\n\n\nMatplotlib can also be used to create animations. The FuncAnimation class makes it easy to create animations by repeatedly calling a function to update the plot. The following example shows a simple pendulum animation.\n\n\n\n\n\n\n\n\n\n\n\nSimple contour plot\n\n\n\n\n\n\nPhysics Interlude\n\n\n\n\n\n\nContour and Density Plots\nA contour plots are useful tools to study two dimensional data, meaning \\(Z(X,Y)\\). A contour plots the lines of constant value of the function \\(Z\\).\n\n\nUnderstanding Wave Interference\nImagine throwing two stones into a pond. Each stone creates circular waves that spread out. When these waves meet, they create interesting patterns - this is called interference. Let‚Äôs explore this using physics and Python!\n\nWhat is a Wave?\nA wave can be described mathematically. For our example, we‚Äôll look at spherical waves (like those in the pond). Each wave has: - An amplitude (how tall the wave is) - A wavelength (distance between wave peaks) - A frequency (how fast it oscillates)\n\n\nMathematical Description\nFor a single wave source, we can write: \\[\\begin{equation}\nU(r)=e^{-i\\,k r}\n\\end{equation}\\]\nWhere: - \\(k\\) is related to the wavelength (\\(k = 2\\pi/\\lambda\\)) - \\(r\\) is the distance from the source - We‚Äôve simplified by ignoring how the wave gets smaller as it travels (\\(1/r\\) term)\n\n\nTwo Wave Sources\nWhen we have two wave sources (like two stones dropped in the pond): 1. Each source creates its own wave 2. The waves combine where they meet 3. The total wave is the sum of both waves\n\n\n\ninterference\n\n\nMathematically: \\[\\begin{equation}\nU_{total} = e^{-i\\,k r_1} + e^{-i\\,k r_2}\n\\end{equation}\\]\nWhere \\(r_1\\) and \\(r_2\\) are the distances from each source.\n\n\nWhat We See (Intensity)\nWhat we actually see is the intensity of the combined waves:\n\\[\\begin{equation}\n\\text{Intensity} \\propto |U_{total}|^2\n\\end{equation}\\]\nThis will show us where the waves:\n\nAdd up (bright regions - constructive interference)\nCancel out (dark regions - destructive interference)\n\nLet‚Äôs create a Python program to visualize this!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColor contour plot\n\n\n\n\n\n\n\n\nImage plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced Plotting - Explicit Version\n\n\n\n\n\nAdvanced Plotting - Explicit Version\nWhile we have so far largely relied on the default setting and the automatic arrangement of plots, there is also a way to precisely design your plot. Python provides the tools of object oriented programming and thus modules provide classes which can be instanced into objects. This explicit interfaces allows you to control all details without the automatisms of pyplot.\nThe figure below, which is taken from the matplotlib documentation website shows the sets of commands and the objects in the figure, the commands refer to. It is a nice reference, when creating a figure.\n\n\n\nanatomy of a figure\n\n\n\nPlots with Multiple Spines\nSometimes it is very useful to plot different quantities in the same plot with the same x-axis but with different y-axes. Here is some example, where each line plot has its own y-axis.\n\n\n\n\n\n\n\n\nInsets\nInsets are plots within plots using their own axes. We therefore need to create two axes systems, if we want to have a main plot and and inset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpine axis\n\n\n\n\n\n\n\n\nPolar plot\n\n\n\n\n\n\n\n\nText annotation\nAnnotating text in matplotlib figures can be done using the text function. It supports LaTeX formatting just like axis label texts and titles:\n\n\n\n\n\n\n\n\n3D Plotting\nMatplotlib was initially designed with only two-dimensional plotting in mind. Around the time of the 1.0 release, some three-dimensional plotting utilities were built on top of Matplotlib‚Äôs two-dimensional display, and the result is a convenient (if somewhat limited) set of tools for three-dimensional data visualization. Three-dimensional plots are enabled by importing the mplot3d toolkit, included with the main Matplotlib installation:\n\n\n\n\n\n\nOnce this submodule is imported, a three-dimensional axes can be created by passing the keyword projection=‚Äò3d‚Äô to any of the normal axes creation routines:\n\nProjection Scence\n\n\n\n\n\n\nWith this three-dimensional axes enabled, we can now plot a variety of three-dimensional plot types. Three-dimensional plotting is one of the functionalities that benefits immensely from viewing figures interactively rather than statically in the notebook; recall that to use interactive figures, you can use %matplotlib notebook rather than %matplotlib inline when running this code.\n\n\nLine Plotting in 3D\nfrom sets of (x, y, z) triples. In analogy with the more common two-dimensional plots discussed earlier, these can be created using the ax.plot3D and ax.scatter3D functions. The call signature for these is nearly identical to that of their two-dimensional counterparts, so you can refer to Simple Line Plots and Simple Scatter Plots for more information on controlling the output. Here we‚Äôll plot a trigonometric spiral, along with some points drawn randomly near the line:\n\n\n\n\n\n\nNotice that by default, the scatter points have their transparency adjusted to give a sense of depth on the page. While the three-dimensional effect is sometimes difficult to see within a static image, an interactive view can lead to some nice intuition about the layout of the points. Use the scatter3D or the plot3D method to plot a random walk in 3-dimensions in your exercise.\n\n\nSurface Plotting\nA surface plot is like a wireframe plot, but each face of the wireframe is a filled polygon. Adding a colormap to the filled polygons can aid perception of the topology of the surface being visualized:"
  },
  {
    "objectID": "lectures/lecture04/04-plotting.html#contour-and-density-plots",
    "href": "lectures/lecture04/04-plotting.html#contour-and-density-plots",
    "title": "Plotting",
    "section": "Contour and Density Plots",
    "text": "Contour and Density Plots\nA contour plots are useful tools to study two dimensional data, meaning \\(Z(X,Y)\\). A contour plots the lines of constant value of the function \\(Z\\)."
  },
  {
    "objectID": "lectures/lecture04/04-plotting.html#understanding-wave-interference",
    "href": "lectures/lecture04/04-plotting.html#understanding-wave-interference",
    "title": "Plotting",
    "section": "Understanding Wave Interference",
    "text": "Understanding Wave Interference\nImagine throwing two stones into a pond. Each stone creates circular waves that spread out. When these waves meet, they create interesting patterns - this is called interference. Let‚Äôs explore this using physics and Python!\n\nWhat is a Wave?\nA wave can be described mathematically. For our example, we‚Äôll look at spherical waves (like those in the pond). Each wave has: - An amplitude (how tall the wave is) - A wavelength (distance between wave peaks) - A frequency (how fast it oscillates)\n\n\nMathematical Description\nFor a single wave source, we can write: \\[\\begin{equation}\nU(r)=e^{-i\\,k r}\n\\end{equation}\\]\nWhere: - \\(k\\) is related to the wavelength (\\(k = 2\\pi/\\lambda\\)) - \\(r\\) is the distance from the source - We‚Äôve simplified by ignoring how the wave gets smaller as it travels (\\(1/r\\) term)\n\n\nTwo Wave Sources\nWhen we have two wave sources (like two stones dropped in the pond): 1. Each source creates its own wave 2. The waves combine where they meet 3. The total wave is the sum of both waves\n\n\n\ninterference\n\n\nMathematically: \\[\\begin{equation}\nU_{total} = e^{-i\\,k r_1} + e^{-i\\,k r_2}\n\\end{equation}\\]\nWhere \\(r_1\\) and \\(r_2\\) are the distances from each source.\n\n\nWhat We See (Intensity)\nWhat we actually see is the intensity of the combined waves:\n\\[\\begin{equation}\n\\text{Intensity} \\propto |U_{total}|^2\n\\end{equation}\\]\nThis will show us where the waves:\n\nAdd up (bright regions - constructive interference)\nCancel out (dark regions - destructive interference)\n\nLet‚Äôs create a Python program to visualize this!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColor contour plot\n\n\n\n\n\n\n\n\nImage plot"
  },
  {
    "objectID": "lectures/lecture04/04-plotting.html#advanced-plotting---explicit-version",
    "href": "lectures/lecture04/04-plotting.html#advanced-plotting---explicit-version",
    "title": "Plotting",
    "section": "Advanced Plotting - Explicit Version",
    "text": "Advanced Plotting - Explicit Version\nWhile we have so far largely relied on the default setting and the automatic arrangement of plots, there is also a way to precisely design your plot. Python provides the tools of object oriented programming and thus modules provide classes which can be instanced into objects. This explicit interfaces allows you to control all details without the automatisms of pyplot.\nThe figure below, which is taken from the matplotlib documentation website shows the sets of commands and the objects in the figure, the commands refer to. It is a nice reference, when creating a figure.\n\n\n\nanatomy of a figure\n\n\n\nPlots with Multiple Spines\nSometimes it is very useful to plot different quantities in the same plot with the same x-axis but with different y-axes. Here is some example, where each line plot has its own y-axis.\n\n\n\n\n\n\n\n\nInsets\nInsets are plots within plots using their own axes. We therefore need to create two axes systems, if we want to have a main plot and and inset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpine axis\n\n\n\n\n\n\n\n\nPolar plot\n\n\n\n\n\n\n\n\nText annotation\nAnnotating text in matplotlib figures can be done using the text function. It supports LaTeX formatting just like axis label texts and titles:\n\n\n\n\n\n\n\n\n3D Plotting\nMatplotlib was initially designed with only two-dimensional plotting in mind. Around the time of the 1.0 release, some three-dimensional plotting utilities were built on top of Matplotlib‚Äôs two-dimensional display, and the result is a convenient (if somewhat limited) set of tools for three-dimensional data visualization. Three-dimensional plots are enabled by importing the mplot3d toolkit, included with the main Matplotlib installation:\n\n\n\n\n\n\nOnce this submodule is imported, a three-dimensional axes can be created by passing the keyword projection=‚Äò3d‚Äô to any of the normal axes creation routines:\n\nProjection Scence\n\n\n\n\n\n\nWith this three-dimensional axes enabled, we can now plot a variety of three-dimensional plot types. Three-dimensional plotting is one of the functionalities that benefits immensely from viewing figures interactively rather than statically in the notebook; recall that to use interactive figures, you can use %matplotlib notebook rather than %matplotlib inline when running this code.\n\n\nLine Plotting in 3D\nfrom sets of (x, y, z) triples. In analogy with the more common two-dimensional plots discussed earlier, these can be created using the ax.plot3D and ax.scatter3D functions. The call signature for these is nearly identical to that of their two-dimensional counterparts, so you can refer to Simple Line Plots and Simple Scatter Plots for more information on controlling the output. Here we‚Äôll plot a trigonometric spiral, along with some points drawn randomly near the line:\n\n\n\n\n\n\nNotice that by default, the scatter points have their transparency adjusted to give a sense of depth on the page. While the three-dimensional effect is sometimes difficult to see within a static image, an interactive view can lead to some nice intuition about the layout of the points. Use the scatter3D or the plot3D method to plot a random walk in 3-dimensions in your exercise.\n\n\nSurface Plotting\nA surface plot is like a wireframe plot, but each face of the wireframe is a filled polygon. Adding a colormap to the filled polygons can aid perception of the topology of the surface being visualized:"
  },
  {
    "objectID": "lectures/lecture04/1_quantum_mechanics.html",
    "href": "lectures/lecture04/1_quantum_mechanics.html",
    "title": "Quantum Mechanics",
    "section": "",
    "text": "In the last lecture, we have modeled electromagnetic waves not by solving the wave equation, but by taking the solutions of wave equations like a plane wave or a spherical wave. Today we will solve a wave equation, but not for electromagnetic waves, but for matter waves. We will solve the stationary Schr√∂dinger equation with the implicit solution scheme, which we have already applied for the diffusion equation. With the help of that we will tackle the particle in a box, the harmonic oscillator and the periodic potential. All of these problems have also analytical solutions, thus we do not need the numerical solution in principle. But it will give us some practice on how to tackle such problems. As not all of you might be familiar with the physical description of quantum mechanics, we will give a short introduction into this field first.\nimport numpy as np\nfrom scipy.sparse import diags\nfrom scipy.sparse.linalg import eigsh\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n# default values for plotting\nplt.rcParams.update({'font.size': 12,\n                     'axes.titlesize': 18,\n                     'axes.labelsize': 16,\n                     'axes.labelpad': 14,\n                     'lines.linewidth': 1,\n                     'lines.markersize': 10,\n                     'xtick.labelsize' : 16,\n                     'ytick.labelsize' : 16,\n                     'xtick.top' : True,\n                     'xtick.direction' : 'in',\n                     'ytick.right' : True,\n                     'ytick.direction' : 'in',})"
  },
  {
    "objectID": "lectures/lecture04/1_quantum_mechanics.html#quantum-mechanics-in-a-nutshell",
    "href": "lectures/lecture04/1_quantum_mechanics.html#quantum-mechanics-in-a-nutshell",
    "title": "Quantum Mechanics",
    "section": "Quantum Mechanics in a Nutshell",
    "text": "Quantum Mechanics in a Nutshell\nQuantum Mechanics assumes that all particles propagate as waves. They are described by a wavefunction \\(\\Psi(x,t)\\). A quantum mechanical object thus posseses an amplitude and a phase which propagate in space and time. One could see the wavefunction in analogy to the electric field \\(\\vec{E}(x)\\) of an electromagnetic wave. As the square of the electric field describes the propagation of energy of a wave, the square magnitude of the wavefunction, i.e.¬†\\(|\\Psi(x,t)|^2\\), describes the propagation of probability density of the quantum mechanical wave. The wavefunction itself is thus just the probability amplitude.\n\nTime dependent Schr√∂dinger equation\nThe dynamics of a quantum mechanical wave is described,for example, by the time dependent Schr√∂dinger equation\n\\[\\begin{equation}\n-i\\hbar\\frac{\\partial \\Psi(x,t)}{\\partial t} = \\left ( \\frac{-\\hbar^2 }{2m}\\frac{\\partial^2}{\\partial x^2}+V(x,t) \\right ) \\Psi(x,t)\n\\end{equation}\\]\nwhis is written here for one dimension only.\nThe bracket on the right side of the above equation contains the so-called Hamilton operator \\(\\hat{H}\\). The Hamilton operator \\(\\hat{H}\\) contains the energy operators for the kinetic and potential energies and represents the total energy of the system.\n\\[\\begin{equation}\n\\hat{H}=\\left ( \\frac{-\\hbar^2 }{2m}\\frac{\\partial^2}{\\partial x^2}+V(x,t) \\right )\n\\end{equation}\\]\n\n\nStationary Schr√∂dinger equation\nOur first problems will be stationary problems. We will not ask for the temporal development of the quantum object. We will rather ask, what solutions without time dependence are possible. In general this is much like the question asking what kind of standing waves are possible on a string or in an optical resonator. In quantum mechanics the boundaries, which define the standing waves are formed by the potential energy \\(V(x)\\).\nWe therefore also need the stationary Schr√∂dinger equation, where the left side of the time dependent Schr√∂dinger equation does not depend on time, hence is constant in time. This stationary (time-independent) Schr√∂dinger equation is\n\\[\\begin{equation}\n\\hat{H}\\Psi(x)=E\\Psi(x)\n\\end{equation}\\]\nThe Hamilton operator \\(\\hat{H}\\) gives a recipe how to calculate the energies for a given wavefunction \\(\\Psi(x)\\) in terms of derivates or multiplications by functions. If this recipe reduces to a multiplication of the wave function with a number \\(E\\), then these wavefunctions are eigenfunction of the Hamilton operator and the values of \\(E\\) are the eigenvalues of the problem, i.e.¬†the time-independent solutions of this differential equation."
  },
  {
    "objectID": "lectures/lecture04/1_quantum_mechanics.html#recap-implicit-solution",
    "href": "lectures/lecture04/1_quantum_mechanics.html#recap-implicit-solution",
    "title": "Quantum Mechanics",
    "section": "Recap: Implicit Solution",
    "text": "Recap: Implicit Solution\nAccording to our above description, the Hamilton operator \\(\\hat{H}\\) contains two parts, a second derivative in the position, which represents the kinetic energy and the potential energy operator \\(V(x)\\), which is in the simplest case just a function of \\(x\\).\n\\[\\begin{equation}\n\\left ( \\frac{-\\hbar^2 }{2m}\\frac{\\partial^2}{\\partial x^2}+V(x) \\right ) \\Psi(x)=E\\Psi(x)\n\\end{equation}\\]\nSince we want to apply our implicit solution scheme (Cranck Nicolson), we want to represent both parts as matrices.\n\nKinetic energy\nWe remember that we can write the second derivative of our wavefunction \\(\\Psi(x)\\) in the finite difference approximation as\n\\[\\begin{equation}\n\\Psi^{''}(x)=\\frac{\\Psi(x+\\delta x)-2\\Psi(x)+\\Psi(x-\\delta x)}{\\delta x^{2}}\n\\end{equation}\\]\nIf we want to evaluate the wavefunction at certain positions \\(x_{i}\\), then this second derivative translates into\n\\(T\\Psi=\\frac{d^2}{dx^2}\\Psi=\\frac{1}{\\delta x^2}\n\\begin{bmatrix}\n-2 & 1  & 0 & 0 & 0 & 0\\\\\n1 & -2 & 1 & 0 & 0 & 0\\\\\n0 & 1  & -2 & 1 & 0 & 0\\\\\n0 & 0  & 1  & -2 & 1 & 0\\\\\n0 & 0  & 0  &  1 & -2 & 1\\\\\n0 & 0  & 0  &  0 &  1 & -2\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\Psi(x_{1})\\\\\n\\Psi(x_{2})\\\\\n\\Psi(x_{3})\\\\\n\\Psi(x_{4})\\\\\n\\Psi(x_{5})\\\\\n\\Psi(x_{6})\n\\end{bmatrix}\\)\nif we just use 6 positions. Please remember, that in the version above, we have imposed already boundary conditions in the first and the last row, which are \\(\\Psi(x_{0})=0\\) and \\(\\Psi(x_{7})=0\\). If we multiply this matrix by \\(-\\hbar^{2}/2m\\), we obtain the kinetic energy for an object of mass \\(m\\).\n\n\nPotential energy\nThe potential energy values are just values at the diagonal of the matrix\n\\(V\\Psi=\n\\begin{bmatrix}\nV(x_{1}) & 0  & 0 & 0 & 0 & 0\\\\\n0 & V(x_{2}) & 0 & 0 & 0 & 0\\\\\n0 & 0  & V(x_{3}) & 0 & 0 & 0\\\\\n0 & 0  & 0  & V(x_{4}) & 0 & 0\\\\\n0 & 0  & 0  &  0 & V(x_{5}) & 0\\\\\n0 & 0  & 0  &  0 &  0 & V(x_{6})\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\Psi(x_{1})\\\\\n\\Psi(x_{2})\\\\\n\\Psi(x_{3})\\\\\n\\Psi(x_{4})\\\\\n\\Psi(x_{5})\\\\\n\\Psi(x_{6})\n\\end{bmatrix}\\)\nan you may insert the specific potential energy values for your particular problem here.\nOur final problem \\(\\hat{H}\\Psi=E\\Psi\\) will thus have the following shape\n\\[\\begin{equation}\n\\begin{bmatrix}\n-2+V(x_{1}) & 1  & 0 & 0 & 0 & 0\\\\\n1 & -2+V(x_{2}) & 1 & 0 & 0 & 0\\\\\n0 & 1 & -2+V(x_{3})  & 1 & 0 & 0 \\\\\n0 &0 & 1  & -2+V(x_{4})  & 1 & 0 \\\\\n0 & 0 & 0  & 1  &  -2+V(x_{5}) & 1 \\\\\n0 & 0 & 0  & 0  &  1 &  -2+V(x_{6}) \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\Psi(x_{1})\\\\\n\\Psi(x_{2})\\\\\n\\Psi(x_{3})\\\\\n\\Psi(x_{4})\\\\\n\\Psi(x_{5})\\\\\n\\Psi(x_{6})\n\\end{bmatrix}=E\n\\begin{bmatrix}\n\\Psi(x_{1})\\\\\n\\Psi(x_{2})\\\\\n\\Psi(x_{3})\\\\\n\\Psi(x_{4})\\\\\n\\Psi(x_{5})\\\\\n\\Psi(x_{6})\n\\end{bmatrix}\n\\end{equation}\\]\nwhere I skipped the prefactor \\(-\\hbar^2/2m\\), to fit the matrices on one line. Yet I did not succeed. This is the final system of coupled equations which we can supply to any matrix solver. We will use a solver from the scipy.linalg module. In case we have special boundary conditions, we need to take them into account and replace the first and the last line for example with the particular boundary conditions."
  },
  {
    "objectID": "lectures/lecture04/2_brownian_motion copy.html",
    "href": "lectures/lecture04/2_brownian_motion copy.html",
    "title": "Brownian Motion",
    "section": "",
    "text": "File as PDF\nWe will use our newly gained knowledge about classes for the simulation of Brownian motion. This actually perfectly fits to the object oriented programming topic, as each Brownian particle (or colloid) can be seen as an object instanciated from the same class, but perhaps with different properties. Some particles might be larger and some smaller for example. We know already some part of that, as we have covered it in earlier lectures."
  },
  {
    "objectID": "lectures/lecture04/2_brownian_motion copy.html#physics",
    "href": "lectures/lecture04/2_brownian_motion copy.html#physics",
    "title": "Brownian Motion",
    "section": "Physics",
    "text": "Physics\nThe Brownian motion of a colloidal particle (called solute) results from the collisions with the surrounding solvent molecules. Due to these collisions, the particle has in equilibrium a mean kinetic energy defined by the temperature. With this kinetic energy it would travel a mean distance \\(l\\) before it takes another random direction and go another step length \\(l\\). This mean distance the particle travels is very short in liquids. It is on the other of picometers. It has been shown by Lindenberg and L√©vy that a sequence of many such infinitesimal small random steps leads to a total effect, which can be approximated by a normal distribution. This important theorem is called the central limit theorem.\nFor our Brownian motion the sequence of tiny steps leads after a time \\(t\\) to the following probability distribution to find the particle at a position \\(x\\) if it initially started at \\(x=0\\):\n\\[\\begin{equation}\np(x,\\Delta t)=\\frac{1}{\\sqrt{4\\pi D \\Delta t}}e^{-\\frac{x^2}{4D \\Delta t}}\n\\end{equation}\\]\nwhere \\(D\\) is the diffusion coefficient. Thus each step of our Brownian motion simulation for a timestep of \\(\\Delta t\\) is taken from a Gaussian distribution with a varaince of \\(\\sigma^2=2D \\Delta t\\).\nFor our simulation that means that we can draw numbers from a normal distribution with np.random.normal with the standard deviation \\(\\sigma=\\sqrt{2D \\Delta t}\\) as a parameter. This has to be done for the x-coordinate and the y-coordinate.\nThe code for our Brownian motion therefore is\nsigma=np.sqrt(2*D*dt)\ndx,dy=[(np.random.normal(0.0, sigma),np.random.normal(0.0, sigma)]\nx=x+dx\ny=y+dy\nwhich gives a whole 2d trajectory. With the help of this, we would like to write a colloidal particle class. So lets make a plan how this could work out."
  },
  {
    "objectID": "lectures/lecture04/2_brownian_motion copy.html#class-planning",
    "href": "lectures/lecture04/2_brownian_motion copy.html#class-planning",
    "title": "Brownian Motion",
    "section": "Class Planning",
    "text": "Class Planning\n\nPhysics project Colloidal particle class\nWe will define a class for a colloidal particle, which we may use later for our projects as well. This makes sense, as we can have different colloidal particles of different radius for example, which do start to carry out Brownian motion from different positions. A colloidal particle is and object, which has properties very much in the same way as classes intend that. The whole definition requires some planning, especially on what the class should keep track of and what the object.\nThe particle class shall keep track of\n* the total number of colloidal particles\n* the value of k_B T/(6 pi eta) = 2.2e-19\nThe class shall provide the class specific methods\n* how_many() which returns the total number of colloids\n* __str__ which returns a string with radius and position of the particle\n\n\nPhysics interlude: Colloidal particle class\nEach object shall then contain the following properties\n* the particle radius, R\n* a list of all x position, x\n* a list of all y position, y\n* the index of the colloid, index\n* the diffusion coefficient given by k_B T/(6 pi eta R), D\nThe object shall provide the following methods\n* sim_trajectory() simulate a whole trajectory at once\n* update(dt) do one step of Brownian motion with a time step dt as argument, return the current position\n* get_trajectory() return the trajectory as a pandas DataFrame with the columns x and y\n* get_D() return the diffusion coefficient\n\n\n\n\n\n\n\n\nNote:\nNote that the function sim_trajectory is actually calling the function update of the same object to generate the whole trajectory at once."
  },
  {
    "objectID": "lectures/lecture04/2_brownian_motion copy.html#simulating",
    "href": "lectures/lecture04/2_brownian_motion copy.html#simulating",
    "title": "Brownian Motion",
    "section": "Simulating",
    "text": "Simulating\nWith the help of this Colloid class, we would like to carry out simulations of Brownian motion of multiple particles. The simulations shall\n\ntake n=200 particles\nhave N=200 trajectory points each\nstart all at 0,0\nparticle objects should be stored in a list p_list"
  },
  {
    "objectID": "lectures/lecture04/2_brownian_motion copy.html#plotting-the-trajectories",
    "href": "lectures/lecture04/2_brownian_motion copy.html#plotting-the-trajectories",
    "title": "Brownian Motion",
    "section": "Plotting the trajectories",
    "text": "Plotting the trajectories\nThe next step is to plot all the trajectories."
  },
  {
    "objectID": "lectures/lecture04/2_brownian_motion copy.html#characterizing-the-brownian-motion",
    "href": "lectures/lecture04/2_brownian_motion copy.html#characterizing-the-brownian-motion",
    "title": "Brownian Motion",
    "section": "Characterizing the Brownian motion",
    "text": "Characterizing the Brownian motion\nNow that we have a number of trajectories, we can analyze the motion of our Brownian particles.\n\nCalculate the particle speed\nOne way is to calculate its speed by measuring how far it traveled within a certain time \\(n\\, dt\\), where \\(dt\\) is the timestep of out simulation. We can do that as\n\\[\\begin{equation}\nv(n dt) = \\frac{&lt;\\sqrt{(x_{i+n}-x_{i})^2+(y_{i+n}-y_{i})^2}&gt;}{n\\,dt}\n\\end{equation}\\]\nThe angular brackets on the top take care of the fact that we can measure the distance traveled within a certain time \\(n\\, dt\\) several times along a trajectory.\n\n\n\nmsd\n\n\nThese values can be used to calculate a mean speed. Note that there is not an equal amount of data pairs for all separations available. For \\(n=1\\) there are 5 distances available. For \\(n=5\\), however, only 1. This changes the statistical accuracy of the mean.\n\n\n\n\n\n\nThe result of this analysis shows, that each particle has an apparent speed which seems to increase with decreasing time of observation or which decreases with increasing time. This would mean that there is some friction at work, which slows down the particle in time, but this is apparently not true. Also an infinite speed at zero time appears to be unphysical. The correct answer is just that the speed is no good measure to characterize the motion of a Brownian particle.\n\n\nCalculate the particle mean squared displacement\nA better way to characterize the motion of a Brownian particle is the mean squared displacement, as we have already mentioned it in previous lectures. We may compare our simulation now to the theoretical prediction, which is\n\\[\\begin{equation}\n\\langle \\Delta r^{2}(t)\\rangle=2 d D t\n\\end{equation}\\]\nwhere \\(d\\) is the dimension of the random walk, which is \\(d=2\\) in our case.\n\n\n\n\n\n\nThe results show that the mean squared displacement of the individual particles follows on average the theoretical predictions of a linear growth in time. That means, we are able to read the diffusion coefficient from the slope of the MSD of the individual particles if recorded in a simulation or an experiment.\nYet, each individual MSD is deviating strongly from the theoretical prediction especially at large times. This is due to the fact mentioned earlier that our simulation (or experimental) data only has a limited number of data points, while the theoretical prediction is made for the limit of infinite data points.\n\nWarning: Analysis of MSD data\nSingle particle tracking, either in the experiment or in numerical simulations can therefore only deliver an estimate of the diffusion coefficient and care should be taken when using the whole MSD to obtain the diffusion coefficient. One typically uses only a short fraction of the whole MSD data at short times."
  },
  {
    "objectID": "lectures/lecture03/1_numpy.html",
    "href": "lectures/lecture03/1_numpy.html",
    "title": "NumPy Module",
    "section": "",
    "text": "Numpy is the core library for scientific computing in Python. It provides a high-performance multidimensional array object and tools for working with these arrays. The NumPy array, formally called ndarray in NumPy documentation, is the real workhorse of data structures for scientific and engineering applications. The NumPy array is similar to a list but where all the elements of the list are of the same type. The elements of a NumPy array are usually numbers, but can also be booleans, strings, or other objects. When the elements are numbers, they must all be of the same type.",
    "crumbs": [
      "‚ö° Week 4: Classical Mechanics 1",
      "Arrays with Numpy"
    ]
  },
  {
    "objectID": "lectures/lecture03/1_numpy.html#creating-numpy-arrays",
    "href": "lectures/lecture03/1_numpy.html#creating-numpy-arrays",
    "title": "NumPy Module",
    "section": "Creating Numpy Arrays",
    "text": "Creating Numpy Arrays\nThere are a number of ways to initialize new numpy arrays, for example from\n\na Python list or tuples\nusing functions that are dedicated to generating numpy arrays, such as arange, linspace, etc.\nreading data from files which will be covered in the files section\n\n\nFrom lists\nFor example, to create new vector and matrix arrays from Python lists we can use the numpy.array function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing array-generating functions\nFor larger arrays it is inpractical to initialize the data manually, using explicit python lists. Instead we can use one of the many functions in numpy that generate arrays of different forms. Some of the more common are:\n\n\n\n\n\n\n\n\n\n\n\n\n\nlinspace and logspace\nThe linspace function creates an array of N evenly spaced points between a starting point and an ending point. The form of the function is linspace(start, stop, N).If the third argument N is omitted,then N=50.\n\n\n\n\n\n\nlogspace is doing equivelent things with logaritmic spacing. Other types of array creation techniques are listed below. Try around with these commands to get a feeling what they do.\n\n\n\n\n\n\n\n\nmgrid\nmgrid generates a multi-dimensional matrix with increasing value entries, for example in columns and rows:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndiag\ndiag generates a diagonal matrix with the list supplied to it. The values can be also offset from the main diagonal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nzeros and ones\nzeros and ones creates a matrix with the dimensions given in the argument and filled with 0 or 1.",
    "crumbs": [
      "‚ö° Week 4: Classical Mechanics 1",
      "Arrays with Numpy"
    ]
  },
  {
    "objectID": "lectures/lecture03/1_numpy.html#manipulating-numpy-arrays",
    "href": "lectures/lecture03/1_numpy.html#manipulating-numpy-arrays",
    "title": "NumPy Module",
    "section": "Manipulating NumPy arrays",
    "text": "Manipulating NumPy arrays\n\nSlicing\nSlicing is the name for extracting part of an array by the syntax M[lower:upper:step]\n\n\n\n\n\n\n\n\n\n\n\n\nAny of the three parameters in M[lower:upper:step] can be ommited.\n\n\n\n\n\n\n\n\n\n\n\n\nNegative indices counts from the end of the array (positive index from the begining):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndex slicing works exactly the same way for multidimensional arrays:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDifferences\n\n\n\nSlicing can be effectively used to calculate differences for example for the calculation of derivatives. Here the position \\(y_i\\) of an object has been measured at times \\(t_i\\) and stored in an array each. We wish to calculate the average velocity at the times \\(t_{i}\\) from the arrays by\n\\[\\begin{equation}\nv_{i}=\\frac{y_i-y_{i-1}}{t_{i}-t_{i-1}}\n\\end{equation}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReshaping\nArrays can be reshaped into any form, which contains the same number of elements.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdding a new dimension: newaxis\nWith newaxis, we can insert new dimensions in an array, for example converting a vector to a column or row matrix.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStacking and repeating arrays\n\n\n\n\n\nUsing function repeat, tile, vstack, hstack, and concatenate we can create larger vectors and matrices from smaller ones. Please try the individual functions yourself in your notebook. We wont discuss them in detail.\n\nTile and repeat\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConcatenate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHstack and vstack",
    "crumbs": [
      "‚ö° Week 4: Classical Mechanics 1",
      "Arrays with Numpy"
    ]
  },
  {
    "objectID": "lectures/lecture03/1_numpy.html#applying-mathematical-functions",
    "href": "lectures/lecture03/1_numpy.html#applying-mathematical-functions",
    "title": "NumPy Module",
    "section": "Applying mathematical functions",
    "text": "Applying mathematical functions\nAll kinds of mathematica operations can be carried out on arrays. Typically these operation act element wise as seen from the examples below.\n\nOperation involving one array\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperations involving multiple arrays\nVector operations enable efficient element-wise calculations where corresponding elements at matching positions are processed simultaneously. Instead of handling elements one by one, these operations work on entire arrays at once, making them particularly fast. When multiplying two vectors using these operations, the result is not a single number (as in a dot product) but rather a new array where each element is the product of the corresponding elements from the input vectors. This element-wise multiplication is just one example of vector operations, which can include addition, subtraction, and other mathematical functions.",
    "crumbs": [
      "‚ö° Week 4: Classical Mechanics 1",
      "Arrays with Numpy"
    ]
  },
  {
    "objectID": "lectures/lecture02/04-summary02.html",
    "href": "lectures/lecture02/04-summary02.html",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "Click to expand Matplotlib Plotting Cheat Sheet\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplt.figure(figsize=(width, height))\n\n\n\nplt.plot(x, y, 'bo-')  # Blue line with circle markers\nplt.xlabel('X-axis label')\nplt.ylabel('Y-axis label')\nplt.title('Plot Title')\nplt.legend(['Label'], loc='best')\n\n\n\nplt.scatter(x, y, marker='o')\n\n\n\nplt.hist(data, bins=10, density=True)\n\n\n\nplt.plot(x, y1, 'b-', label='Line 1')\nplt.scatter(x, y2, color='r', label='Scatter')\nplt.legend()\n\n\n\nplt.savefig('filename.pdf')\n\n\n\nplt.errorbar(x, y, yerr=yerror, fmt='ro', ecolor='black')\n\n\n\nplt.xlim(xmin, xmax)\nplt.ylim(ymin, ymax)\n\n\n\nmasked_data = np.ma.masked_where(condition, data)\n\n\n\nplt.semilogx(x, y)  # Log scale on x-axis\nplt.semilogy(x, y)  # Log scale on y-axis\nplt.loglog(x, y)    # Log scale on both axes\n\n\n\nplt.subplot(rows, cols, plot_number)\n\n\n\nplt.contour(X, Y, Z, levels)\nplt.contourf(X, Y, Z, levels)  # Filled contour plot\n\n\n\nplt.imshow(data, extent=[xmin, xmax, ymin, ymax], cmap='colormap')\n\n\n\nfrom mpl_toolkits import mplot3d\nax = plt.axes(projection='3d')\nax.plot3D(x, y, z)\nax.scatter3D(x, y, z)\nax.plot_surface(X, Y, Z)\n\n\n\nplt.rcParams.update({'font.size': 12, 'lines.linewidth': 2})\nplt.tight_layout()\nRemember to always call plt.show() to display your plots!"
  },
  {
    "objectID": "lectures/lecture02/04-summary02.html#matplotlib-plotting-cheat-sheet",
    "href": "lectures/lecture02/04-summary02.html#matplotlib-plotting-cheat-sheet",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "Click to expand Matplotlib Plotting Cheat Sheet\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplt.figure(figsize=(width, height))\n\n\n\nplt.plot(x, y, 'bo-')  # Blue line with circle markers\nplt.xlabel('X-axis label')\nplt.ylabel('Y-axis label')\nplt.title('Plot Title')\nplt.legend(['Label'], loc='best')\n\n\n\nplt.scatter(x, y, marker='o')\n\n\n\nplt.hist(data, bins=10, density=True)\n\n\n\nplt.plot(x, y1, 'b-', label='Line 1')\nplt.scatter(x, y2, color='r', label='Scatter')\nplt.legend()\n\n\n\nplt.savefig('filename.pdf')\n\n\n\nplt.errorbar(x, y, yerr=yerror, fmt='ro', ecolor='black')\n\n\n\nplt.xlim(xmin, xmax)\nplt.ylim(ymin, ymax)\n\n\n\nmasked_data = np.ma.masked_where(condition, data)\n\n\n\nplt.semilogx(x, y)  # Log scale on x-axis\nplt.semilogy(x, y)  # Log scale on y-axis\nplt.loglog(x, y)    # Log scale on both axes\n\n\n\nplt.subplot(rows, cols, plot_number)\n\n\n\nplt.contour(X, Y, Z, levels)\nplt.contourf(X, Y, Z, levels)  # Filled contour plot\n\n\n\nplt.imshow(data, extent=[xmin, xmax, ymin, ymax], cmap='colormap')\n\n\n\nfrom mpl_toolkits import mplot3d\nax = plt.axes(projection='3d')\nax.plot3D(x, y, z)\nax.scatter3D(x, y, z)\nax.plot_surface(X, Y, Z)\n\n\n\nplt.rcParams.update({'font.size': 12, 'lines.linewidth': 2})\nplt.tight_layout()\nRemember to always call plt.show() to display your plots!"
  },
  {
    "objectID": "lectures/lecture02/04-summary02.html#basic-setup",
    "href": "lectures/lecture02/04-summary02.html#basic-setup",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\n\nplt.figure(figsize=(width, height))"
  },
  {
    "objectID": "lectures/lecture02/04-summary02.html#line-plots",
    "href": "lectures/lecture02/04-summary02.html#line-plots",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "plt.plot(x, y, 'bo-')  # Blue line with circle markers\nplt.xlabel('X-axis label')\nplt.ylabel('Y-axis label')\nplt.title('Plot Title')\nplt.legend(['Label'], loc='best')"
  },
  {
    "objectID": "lectures/lecture02/04-summary02.html#scatter-plots",
    "href": "lectures/lecture02/04-summary02.html#scatter-plots",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "plt.scatter(x, y, marker='o')"
  },
  {
    "objectID": "lectures/lecture02/04-summary02.html#histograms",
    "href": "lectures/lecture02/04-summary02.html#histograms",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "plt.hist(data, bins=10, density=True)"
  },
  {
    "objectID": "lectures/lecture02/04-summary02.html#combined-plots",
    "href": "lectures/lecture02/04-summary02.html#combined-plots",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "plt.plot(x, y1, 'b-', label='Line 1')\nplt.scatter(x, y2, color='r', label='Scatter')\nplt.legend()"
  },
  {
    "objectID": "lectures/lecture02/04-summary02.html#saving-figures",
    "href": "lectures/lecture02/04-summary02.html#saving-figures",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "plt.savefig('filename.pdf')"
  },
  {
    "objectID": "lectures/lecture02/04-summary02.html#error-bars",
    "href": "lectures/lecture02/04-summary02.html#error-bars",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "plt.errorbar(x, y, yerr=yerror, fmt='ro', ecolor='black')"
  },
  {
    "objectID": "lectures/lecture02/04-summary02.html#setting-plot-limits",
    "href": "lectures/lecture02/04-summary02.html#setting-plot-limits",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "plt.xlim(xmin, xmax)\nplt.ylim(ymin, ymax)"
  },
  {
    "objectID": "lectures/lecture02/04-summary02.html#masked-arrays",
    "href": "lectures/lecture02/04-summary02.html#masked-arrays",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "masked_data = np.ma.masked_where(condition, data)"
  },
  {
    "objectID": "lectures/lecture02/04-summary02.html#logarithmic-plots",
    "href": "lectures/lecture02/04-summary02.html#logarithmic-plots",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "plt.semilogx(x, y)  # Log scale on x-axis\nplt.semilogy(x, y)  # Log scale on y-axis\nplt.loglog(x, y)    # Log scale on both axes"
  },
  {
    "objectID": "lectures/lecture02/04-summary02.html#multiple-subplots",
    "href": "lectures/lecture02/04-summary02.html#multiple-subplots",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "plt.subplot(rows, cols, plot_number)"
  },
  {
    "objectID": "lectures/lecture02/04-summary02.html#contour-plots",
    "href": "lectures/lecture02/04-summary02.html#contour-plots",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "plt.contour(X, Y, Z, levels)\nplt.contourf(X, Y, Z, levels)  # Filled contour plot"
  },
  {
    "objectID": "lectures/lecture02/04-summary02.html#image-plots",
    "href": "lectures/lecture02/04-summary02.html#image-plots",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "plt.imshow(data, extent=[xmin, xmax, ymin, ymax], cmap='colormap')"
  },
  {
    "objectID": "lectures/lecture02/04-summary02.html#d-plots",
    "href": "lectures/lecture02/04-summary02.html#d-plots",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "from mpl_toolkits import mplot3d\nax = plt.axes(projection='3d')\nax.plot3D(x, y, z)\nax.scatter3D(x, y, z)\nax.plot_surface(X, Y, Z)"
  },
  {
    "objectID": "lectures/lecture02/04-summary02.html#customization",
    "href": "lectures/lecture02/04-summary02.html#customization",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "plt.rcParams.update({'font.size': 12, 'lines.linewidth': 2})\nplt.tight_layout()\nRemember to always call plt.show() to display your plots!"
  },
  {
    "objectID": "lectures/lecture02/3_datatypes.html",
    "href": "lectures/lecture02/3_datatypes.html",
    "title": "Data Types in Python",
    "section": "",
    "text": "It‚Äôs time to look at different data types we may find useful in our course. Besides the number types mentioned previously, there are also other types like strings, lists, tuples, dictionaries and sets.\nEach of these data types has a number of connected methods (functions) which allow to manipulate the data contained in a variable. If you want to know which methods are available for a certain object use the command dir, e.g.\nThe following few cells will give you a short introduction into each type.",
    "crumbs": [
      "üéØ Week 2: Python Building Blocks",
      "Datatypes for Physics"
    ]
  },
  {
    "objectID": "lectures/lecture02/3_datatypes.html#strings",
    "href": "lectures/lecture02/3_datatypes.html#strings",
    "title": "Data Types in Python",
    "section": "Strings",
    "text": "Strings\nStrings are lists of keyboard characters as well as other characters not on your keyboard. They are useful for printing results on the screen, during reading and writing of data.\n\n\n\n\n\n\n\n\n\n\n\n\nString can be concatenated using the + operator.\n\n\n\n\n\n\n\n\n\n\n\n\nAs strings are lists, each character in a string can be accessed by addressing the position in the string (see Lists section)\n\n\n\n\n\n\nStrings can also be made out of numbers.\n\n\n\n\n\n\nIf you want to obtain a number of a string, you can use what is known as type casting. Using type casting you may convert the string or any other data type into a different type if this is possible. To find out if a string is a pure number you may use the str.isnumeric method. For the above string, we may want to do a conversion to the type int by typing:\n\n\n\n\n\n\n\n\n\n\n\n\nThere are a number of methods connected to the string data type. Usually the relate to formatting or finding sub-strings. Formatting will be a topic in our next lecture. Here we just refer to one simple find example.",
    "crumbs": [
      "üéØ Week 2: Python Building Blocks",
      "Datatypes for Physics"
    ]
  },
  {
    "objectID": "lectures/lecture02/3_datatypes.html#lists",
    "href": "lectures/lecture02/3_datatypes.html#lists",
    "title": "Data Types in Python",
    "section": "Lists",
    "text": "Lists\nLists have a variety of uses. They are useful, for example, in various bookkeeping tasks that arise in computer programming. Like arrays, they are sometimes used to store data. However, lists do not have the specialized properties and tools that make arrays so powerful for scientific computing. So in general, we prefer arrays to lists for working with scientific data. For other tasks, lists work just fine and can even be preferable to arrays.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndividual elements in a list can be accessed by the variable name and the number (index) of the list element put in square brackets. Note that the index for the elements start at 0 for the first element (left).\n\n\n\n\n\n\nIndices in Python\n\n\n\nThe first element of a list or array is accessed by the index 0. If the array has N elements, the last entries index is N-1.\n\n\n\n\n\n\n\n\nElements may be also accessed from the back by nagative indices. b[-1] denotes the last element in the list and b[-2], the element before the last.\n\n\n\n\n\n\n\n\n\n\n\n\nThe length of a list can be obtained by the len command if you need the number of elements in the list for your calculations.\n\n\n\n\n\n\nThere are powerful ways to iterate through a list and also through arrays in form of iterator. This is called list comprehension. We will talk about them later in more detail. Here is an example, which shows the powerful options you have in Python.\n\n\n\n\n\n\nIndividual elements in a list can be replaced by assigning a new value to them\n\n\n\n\n\n\n\n\n\n\n\n\nLists can be concatanated by the + operator\n\n\n\n\n\n\nA very useful feature for lists in python is the slicing of lists. Slicing means, that we access only a range of elements in the list, i.e.¬†element 3 to 7. This is done by inserting the starting and the ending element number separated by a colon (:) in the square brackets. The index numbers can be positive or negative again.\n\n\n\n\n\n\nInserting a second colon behind the ending element index together with a thrid number allows even to select only ever second or third element from a list.\n\n\n\n\n\n\nIt is sometimes also useful to reverse a list. This can be easily done with the reverse command.\n\n\n\n\n\n\nLists may be created in different ways. An empty list can be created by assigning emtpy square brackets to a variable name. You can append elements to the list with the help of the append command which has to be added to the variable name as shown below. This way of adding a particular function, which is part of a certain variable class is part of object oriented programming.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA list of numbers can be easily created by the range() command.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLists (and also tuples below) can be multidimensional as well, i.e.¬†for an image. The individual elements may then be addressed by supplying two indices in two square brackets.",
    "crumbs": [
      "üéØ Week 2: Python Building Blocks",
      "Datatypes for Physics"
    ]
  },
  {
    "objectID": "lectures/lecture02/3_datatypes.html#tuples",
    "href": "lectures/lecture02/3_datatypes.html#tuples",
    "title": "Data Types in Python",
    "section": "Tuples",
    "text": "Tuples\nTuples are also list, but immutable. That means, if a tuple has been once defined, it cannot be changed. Try to change an element to see the result.\n\n\n\n\n\n\nTuples may be unpacked, e.g.¬†its values may be assigned to normal variables in the following way",
    "crumbs": [
      "üéØ Week 2: Python Building Blocks",
      "Datatypes for Physics"
    ]
  },
  {
    "objectID": "lectures/lecture02/3_datatypes.html#dictionaries",
    "href": "lectures/lecture02/3_datatypes.html#dictionaries",
    "title": "Data Types in Python",
    "section": "Dictionaries",
    "text": "Dictionaries\nDictionaries are like lists, but the elements of dictionaries are accessed in a different way than for lists. The elements of lists and arrays are numbered consecutively, and to access an element of a list or an array, you simply refer to the number corresponding to its position in the sequence. The elements of dictionaries are accessed by ‚Äúkeys‚Äù, which can be either strings or (arbitrary) integers (in no particular order). Dictionaries are an important part of core Python. However, we do not make much use of them in this introduction to scientific Python, so our discussion of them is limited.",
    "crumbs": [
      "üéØ Week 2: Python Building Blocks",
      "Datatypes for Physics"
    ]
  },
  {
    "objectID": "lectures/lecture02/3_datatypes.html#sets",
    "href": "lectures/lecture02/3_datatypes.html#sets",
    "title": "Data Types in Python",
    "section": "Sets",
    "text": "Sets\nSets are like lists but have immutable unique entries, which means the elements can not be changed once defined. An emtpy set is created by the set() method.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou may add elements to a set with the add method:\nYou may also remove objects with the discard method:\n\n\n\n\n\n\nYou may also apply a variety of operation to sets checking their intersection, union or difference.",
    "crumbs": [
      "üéØ Week 2: Python Building Blocks",
      "Datatypes for Physics"
    ]
  },
  {
    "objectID": "lectures/lecture02/3_datatypes.html#practice-exercises",
    "href": "lectures/lecture02/3_datatypes.html#practice-exercises",
    "title": "Data Types in Python",
    "section": "Practice Exercises üéØ",
    "text": "Practice Exercises üéØ\n\nExercise 1: String Manipulation\nCreate a string variable containing your full name. Then:\n\nExtract your first name using slicing\nConvert your last name to uppercase\nCount how many times the letter ‚Äòa‚Äô (or ‚ÄòA‚Äô) appears in your full name\nPrint all results with appropriate labels\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nfull_name = \"Albert Einstein\"\n\n# Extract first name (assuming space separates first and last)\nspace_index = full_name.find(' ')\nfirst_name = full_name[:space_index]\n\n# Extract and uppercase last name\nlast_name = full_name[space_index+1:].upper()\n\n# Count letter 'a' (case-insensitive)\ncount_a = full_name.lower().count('a')\n\nprint(f\"First name: {first_name}\")\nprint(f\"Last name (uppercase): {last_name}\")\nprint(f\"Number of 'a' letters: {count_a}\")\n\n\n\n\n\nExercise 2: Working with Lists\nCreate a list of temperatures in Celsius: [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\nThen:\n\nUse list slicing to get only the temperatures from index 2 to 7\nUse list comprehension to convert all temperatures to Fahrenheit: \\(F = \\frac{9}{5}C + 32\\)\nCreate a reversed version of the original list\nPrint all results\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n# Create temperature list\ncelsius = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n\n# Slice temperatures from index 2 to 7\ntemp_slice = celsius[2:8]\nprint(f\"Sliced temperatures: {temp_slice}\")\n\n# Convert to Fahrenheit using list comprehension\nfahrenheit = [(9/5)*c + 32 for c in celsius]\nprint(f\"Fahrenheit temperatures: {fahrenheit}\")\n\n# Reverse the list\ncelsius_reversed = celsius.copy()\ncelsius_reversed.reverse()\nprint(f\"Reversed list: {celsius_reversed}\")\n\n# Alternative: use slicing to reverse\ncelsius_reversed_alt = celsius[::-1]\nprint(f\"Reversed (using slicing): {celsius_reversed_alt}\")\n\n\n\n\n\nExercise 3: Physics Constants Dictionary\nCreate a dictionary called physics_constants that stores:\n\n'c': speed of light = 299792458 (m/s)\n'h': Planck constant = 6.62607015e-34 (J‚ãÖs)\n'g': gravitational acceleration = 9.81 (m/s¬≤)\n'k_B': Boltzmann constant = 1.380649e-23 (J/K)\n\nThen: - Access and print the speed of light - Add a new constant: 'G' (gravitational constant) = 6.674e-11 (m¬≥/kg‚ãÖs¬≤) - Print all keys in the dictionary - Print all values in the dictionary - Calculate the energy of a photon with wavelength Œª = 500 nm using \\(E = \\frac{hc}{\\lambda}\\)\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n# Create physics constants dictionary\nphysics_constants = {\n    'c': 299792458,           # speed of light (m/s)\n    'h': 6.62607015e-34,      # Planck constant (J‚ãÖs)\n    'g': 9.81,                # gravitational acceleration (m/s¬≤)\n    'k_B': 1.380649e-23       # Boltzmann constant (J/K)\n}\n\n# Access and print speed of light\nprint(f\"Speed of light: {physics_constants['c']} m/s\")\n\n# Add gravitational constant\nphysics_constants['G'] = 6.674e-11\nprint(f\"\\nAdded gravitational constant G\")\n\n# Print all keys\nprint(f\"\\nAll constants: {list(physics_constants.keys())}\")\n\n# Print all values\nprint(f\"All values: {list(physics_constants.values())}\")\n\n# Calculate photon energy for Œª = 500 nm\nwavelength = 500e-9  # convert nm to m\nphoton_energy = (physics_constants['h'] * physics_constants['c']) / wavelength\nprint(f\"\\nPhoton energy (Œª=500nm): {photon_energy:.3e} J\")\n\n\n\n\n\nExercise 4: Set Operations\nCreate two sets:\n\nelements_group1: containing {'Hydrogen', 'Helium', 'Lithium', 'Beryllium', 'Carbon'}\nelements_group2: containing {'Carbon', 'Nitrogen', 'Oxygen', 'Helium'}\n\nThen find and print:\n\nElements that appear in both sets (intersection)\nElements that appear in either set (union)\nElements only in group1 (difference)\nElements in group1 or group2 but not both (symmetric difference)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n# Create two sets of elements\nelements_group1 = {'Hydrogen', 'Helium', 'Lithium', 'Beryllium', 'Carbon'}\nelements_group2 = {'Carbon', 'Nitrogen', 'Oxygen', 'Helium'}\n\n# Intersection - elements in both sets\nintersection = elements_group1 & elements_group2\nprint(f\"Elements in both groups: {intersection}\")\n\n# Union - elements in either set\nunion = elements_group1 | elements_group2\nprint(f\"All unique elements: {union}\")\n\n# Difference - elements only in group1\ndifference = elements_group1 - elements_group2\nprint(f\"Elements only in group1: {difference}\")\n\n# Symmetric difference - elements in one set but not both\nsymmetric_diff = elements_group1 ^ elements_group2\nprint(f\"Elements in one group but not both: {symmetric_diff}\")",
    "crumbs": [
      "üéØ Week 2: Python Building Blocks",
      "Datatypes for Physics"
    ]
  },
  {
    "objectID": "lectures/lecture02/3_datatypes.html#quiz-data-types-in-python",
    "href": "lectures/lecture02/3_datatypes.html#quiz-data-types-in-python",
    "title": "Data Types in Python",
    "section": "Quiz: Data Types in Python",
    "text": "Quiz: Data Types in Python\nLet‚Äôs test your understanding of Python data types!\n\n\nWhat is the output of the following code?\na = [1, 2, 3]\nb = (1, 2, 3)\nprint(type(a), type(b))\n\n&lt;class 'list'&gt; &lt;class 'list'&gt;\n&lt;class 'list'&gt; &lt;class 'tuple'&gt;\n&lt;class 'tuple'&gt; &lt;class 'list'&gt;\n&lt;class 'tuple'&gt; &lt;class 'tuple'&gt;\n\nWhich of the following is mutable?\n\nList\nTuple\nString\nInteger\n\nWhat will be the output of this code?\nmy_dict = {'a': 1, 'b': 2, 'c': 3}\nprint(my_dict['b'])\n\na\n2\nb\nKeyError\n\nHow do you create an empty set in Python?\n\n{}\n[]\nset()\n()\n\nWhat is the result of 3 + 4.0?\n\n7\n7.0\n‚Äò7.0‚Äô\nTypeError\n\n\n\n\n\n\n\n\n\nClick to reveal answers\n\n\n\n\n\n\n&lt;class 'list'&gt; &lt;class 'tuple'&gt;\nList\n2\nset()\n7.0",
    "crumbs": [
      "üéØ Week 2: Python Building Blocks",
      "Datatypes for Physics"
    ]
  },
  {
    "objectID": "lectures/lecture02/05-lecture02.html",
    "href": "lectures/lecture02/05-lecture02.html",
    "title": "Matplotlib Cheat Sheet",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Create a new figure\nplt.figure(figsize=(width, height))\n\n# Create a plot\nplt.plot(x, y, 'bo-')  # Blue line with circle markers\n\n# Set labels and title\nplt.xlabel('X-axis label')\nplt.ylabel('Y-axis label')\nplt.title('Plot Title')\n\n# Add a legend\nplt.legend(['Label'], loc='best')\n\n# Show the plot\nplt.show()\n\n\n\n\nLine plot: plt.plot(x, y)\nScatter plot: plt.scatter(x, y)\nBar plot: plt.bar(x, height)\nHistogram: plt.hist(data, bins=10)\nBox plot: plt.boxplot(data)\nErrorbar plot: plt.errorbar(x, y, yerr=error)\n\n\n\n\n\nSet axis limits: plt.xlim(xmin, xmax), plt.ylim(ymin, ymax)\nSet axis scales: plt.xscale('log'), plt.yscale('log')\nSet tick marks: plt.xticks(ticks, labels), plt.yticks(ticks, labels)\nAdd a grid: plt.grid(True)\nChange line style: plt.plot(x, y, linestyle='--', color='r', linewidth=2)\nChange marker style: plt.plot(x, y, marker='o', markersize=5)\n\n\n\n\n\nSubplots: fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\nPlot on specific axes: ax1.plot(x, y)\n\n\n\n\nplt.savefig('filename.png', dpi=300, bbox_inches='tight')\n\n\n\n\nContour plot: plt.contour(X, Y, Z)\n3D plot:\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(X, Y, Z)\n\n\n\n\n\nAdjust layout: plt.tight_layout()\nSet style: plt.style.use('ggplot')\nColormap: plt.imshow(data, cmap='viridis')\n\n\n\n\n\nColors: ‚Äòb‚Äô (blue), ‚Äòg‚Äô (green), ‚Äòr‚Äô (red), ‚Äòc‚Äô (cyan), ‚Äòm‚Äô (magenta), ‚Äòy‚Äô (yellow), ‚Äòk‚Äô (black), ‚Äòw‚Äô (white)\nMarkers: ‚Äò.‚Äô (point), ‚Äòo‚Äô (circle), ‚Äòs‚Äô (square), ‚Äò^‚Äô (triangle up), ‚Äòv‚Äô (triangle down)\nLinestyles: ‚Äò-‚Äô (solid), ‚Äò‚Äì‚Äô (dashed), ‚Äò:‚Äô (dotted), ‚Äò-.‚Äô (dash-dot)\n\nRemember to always check the Matplotlib documentation for the most up-to-date and detailed information!"
  },
  {
    "objectID": "lectures/lecture02/05-lecture02.html#basic-plot-setup",
    "href": "lectures/lecture02/05-lecture02.html#basic-plot-setup",
    "title": "Matplotlib Cheat Sheet",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Create a new figure\nplt.figure(figsize=(width, height))\n\n# Create a plot\nplt.plot(x, y, 'bo-')  # Blue line with circle markers\n\n# Set labels and title\nplt.xlabel('X-axis label')\nplt.ylabel('Y-axis label')\nplt.title('Plot Title')\n\n# Add a legend\nplt.legend(['Label'], loc='best')\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "lectures/lecture02/05-lecture02.html#common-plot-types",
    "href": "lectures/lecture02/05-lecture02.html#common-plot-types",
    "title": "Matplotlib Cheat Sheet",
    "section": "",
    "text": "Line plot: plt.plot(x, y)\nScatter plot: plt.scatter(x, y)\nBar plot: plt.bar(x, height)\nHistogram: plt.hist(data, bins=10)\nBox plot: plt.boxplot(data)\nErrorbar plot: plt.errorbar(x, y, yerr=error)"
  },
  {
    "objectID": "lectures/lecture02/05-lecture02.html#customization",
    "href": "lectures/lecture02/05-lecture02.html#customization",
    "title": "Matplotlib Cheat Sheet",
    "section": "",
    "text": "Set axis limits: plt.xlim(xmin, xmax), plt.ylim(ymin, ymax)\nSet axis scales: plt.xscale('log'), plt.yscale('log')\nSet tick marks: plt.xticks(ticks, labels), plt.yticks(ticks, labels)\nAdd a grid: plt.grid(True)\nChange line style: plt.plot(x, y, linestyle='--', color='r', linewidth=2)\nChange marker style: plt.plot(x, y, marker='o', markersize=5)"
  },
  {
    "objectID": "lectures/lecture02/05-lecture02.html#multiple-plots",
    "href": "lectures/lecture02/05-lecture02.html#multiple-plots",
    "title": "Matplotlib Cheat Sheet",
    "section": "",
    "text": "Subplots: fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\nPlot on specific axes: ax1.plot(x, y)"
  },
  {
    "objectID": "lectures/lecture02/05-lecture02.html#saving-plots",
    "href": "lectures/lecture02/05-lecture02.html#saving-plots",
    "title": "Matplotlib Cheat Sheet",
    "section": "",
    "text": "plt.savefig('filename.png', dpi=300, bbox_inches='tight')"
  },
  {
    "objectID": "lectures/lecture02/05-lecture02.html#advanced-plots",
    "href": "lectures/lecture02/05-lecture02.html#advanced-plots",
    "title": "Matplotlib Cheat Sheet",
    "section": "",
    "text": "Contour plot: plt.contour(X, Y, Z)\n3D plot:\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(X, Y, Z)"
  },
  {
    "objectID": "lectures/lecture02/05-lecture02.html#useful-settings",
    "href": "lectures/lecture02/05-lecture02.html#useful-settings",
    "title": "Matplotlib Cheat Sheet",
    "section": "",
    "text": "Adjust layout: plt.tight_layout()\nSet style: plt.style.use('ggplot')\nColormap: plt.imshow(data, cmap='viridis')"
  },
  {
    "objectID": "lectures/lecture02/05-lecture02.html#common-parameters",
    "href": "lectures/lecture02/05-lecture02.html#common-parameters",
    "title": "Matplotlib Cheat Sheet",
    "section": "",
    "text": "Colors: ‚Äòb‚Äô (blue), ‚Äòg‚Äô (green), ‚Äòr‚Äô (red), ‚Äòc‚Äô (cyan), ‚Äòm‚Äô (magenta), ‚Äòy‚Äô (yellow), ‚Äòk‚Äô (black), ‚Äòw‚Äô (white)\nMarkers: ‚Äò.‚Äô (point), ‚Äòo‚Äô (circle), ‚Äòs‚Äô (square), ‚Äò^‚Äô (triangle up), ‚Äòv‚Äô (triangle down)\nLinestyles: ‚Äò-‚Äô (solid), ‚Äò‚Äì‚Äô (dashed), ‚Äò:‚Äô (dotted), ‚Äò-.‚Äô (dash-dot)\n\nRemember to always check the Matplotlib documentation for the most up-to-date and detailed information!"
  },
  {
    "objectID": "lectures/lecture02/01-summary02.html",
    "href": "lectures/lecture02/01-summary02.html",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "Click to expand Python Basics Cheat Sheet\n\n\n\n\n\n\n\n\n\ndef function_name(parameter1: type, parameter2: type) -&gt; return_type:\n    # function body\n    return value\n\n\n\nresult = function_name(argument1, argument2)\n\n\n\n\n\n\nfor item in sequence:\n    # code to be executed for each item\nExample:\nfor i in range(1, 11):\n    print(i)\n\n\n\nwhile condition:\n    # code to be executed while condition is true\n    # remember to update the condition\nExample:\ni = 1\nwhile i &lt;= 10:\n    print(i)\n    i += 1\n\n\n\n\n\n\nif condition:\n    # code to be executed if condition is true\n\n\n\nif condition:\n    # code to be executed if condition is true\nelse:\n    # code to be executed if condition is false\n\n\n\nif condition1:\n    # code for condition1\nelif condition2:\n    # code for condition2\nelse:\n    # code if no conditions are true\n\n\n\n\n\nUse descriptive function names\nSpecify parameter and return types for clarity\nIndent properly in loops and conditional statements\nRemember to increment counters in while loops\nUse range() for numeric loops in for statements\nConsider multiple conditions with elif for complex logic"
  },
  {
    "objectID": "lectures/lecture02/01-summary02.html#python-basics-cheat-sheet",
    "href": "lectures/lecture02/01-summary02.html#python-basics-cheat-sheet",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "Click to expand Python Basics Cheat Sheet\n\n\n\n\n\n\n\n\n\ndef function_name(parameter1: type, parameter2: type) -&gt; return_type:\n    # function body\n    return value\n\n\n\nresult = function_name(argument1, argument2)\n\n\n\n\n\n\nfor item in sequence:\n    # code to be executed for each item\nExample:\nfor i in range(1, 11):\n    print(i)\n\n\n\nwhile condition:\n    # code to be executed while condition is true\n    # remember to update the condition\nExample:\ni = 1\nwhile i &lt;= 10:\n    print(i)\n    i += 1\n\n\n\n\n\n\nif condition:\n    # code to be executed if condition is true\n\n\n\nif condition:\n    # code to be executed if condition is true\nelse:\n    # code to be executed if condition is false\n\n\n\nif condition1:\n    # code for condition1\nelif condition2:\n    # code for condition2\nelse:\n    # code if no conditions are true\n\n\n\n\n\nUse descriptive function names\nSpecify parameter and return types for clarity\nIndent properly in loops and conditional statements\nRemember to increment counters in while loops\nUse range() for numeric loops in for statements\nConsider multiple conditions with elif for complex logic"
  },
  {
    "objectID": "lectures/lecture02/01-summary02.html#functions",
    "href": "lectures/lecture02/01-summary02.html#functions",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "def function_name(parameter1: type, parameter2: type) -&gt; return_type:\n    # function body\n    return value\n\n\n\nresult = function_name(argument1, argument2)"
  },
  {
    "objectID": "lectures/lecture02/01-summary02.html#loops",
    "href": "lectures/lecture02/01-summary02.html#loops",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "for item in sequence:\n    # code to be executed for each item\nExample:\nfor i in range(1, 11):\n    print(i)\n\n\n\nwhile condition:\n    # code to be executed while condition is true\n    # remember to update the condition\nExample:\ni = 1\nwhile i &lt;= 10:\n    print(i)\n    i += 1"
  },
  {
    "objectID": "lectures/lecture02/01-summary02.html#conditional-statements",
    "href": "lectures/lecture02/01-summary02.html#conditional-statements",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "if condition:\n    # code to be executed if condition is true\n\n\n\nif condition:\n    # code to be executed if condition is true\nelse:\n    # code to be executed if condition is false\n\n\n\nif condition1:\n    # code for condition1\nelif condition2:\n    # code for condition2\nelse:\n    # code if no conditions are true"
  },
  {
    "objectID": "lectures/lecture02/01-summary02.html#tips",
    "href": "lectures/lecture02/01-summary02.html#tips",
    "title": "Einf√ºhrung in die Modellierung Physikalischer Prozesse",
    "section": "",
    "text": "Use descriptive function names\nSpecify parameter and return types for clarity\nIndent properly in loops and conditional statements\nRemember to increment counters in while loops\nUse range() for numeric loops in for statements\nConsider multiple conditions with elif for complex logic"
  },
  {
    "objectID": "lectures/lecture05/2_brownian_motion.html",
    "href": "lectures/lecture05/2_brownian_motion.html",
    "title": "Brownian Motion with Object-Oriented Programming",
    "section": "",
    "text": "Now that you‚Äôve learned about classes and object-oriented programming, let‚Äôs revisit Brownian motion! This time, we‚Äôll use OOP principles to create a more sophisticated and organized simulation. Each Brownian particle (or colloid) will be represented as an object instantiated from a Colloid class, allowing particles to have different properties (like size) while sharing common behaviors. This approach will make our code more modular, reusable, and easier to extend."
  },
  {
    "objectID": "lectures/lecture05/2_brownian_motion.html#brownian-motion",
    "href": "lectures/lecture05/2_brownian_motion.html#brownian-motion",
    "title": "Brownian Motion with Object-Oriented Programming",
    "section": "Brownian Motion",
    "text": "Brownian Motion\n\nWhat is Brownian Motion?\nImagine a dust particle floating in water. If you look at it under a microscope, you‚Äôll see it moving in a random, zigzag pattern. This is Brownian motion!\n\n\nWhy Does This Happen?\nWhen we observe Brownian motion, we‚Äôre seeing the effects of countless molecular collisions. Water isn‚Äôt just a smooth, continuous fluid - it‚Äôs made up of countless tiny molecules that are in constant motion. These water molecules are continuously colliding with our particle from all directions. Each individual collision causes the particle to move just a tiny bit, barely noticeable on its own. However, when millions of these tiny collisions happen every second from random directions, they create the distinctive zigzag motion we observe.\n\n\nThe Simplified Math Behind It\nWhen our particle moves:\n\nEach step is random in direction\nThe size of each step depends on:\n\nTemperature (warmer = more movement)\nTime between steps\nA property called the ‚Äúdiffusion coefficient‚Äù (D)\n\n\n\n\nHow We Can Simulate This?\nIn Python, we can simulate these random steps using random number. These random numbers can be generated with the numpy library. Numpy provides a number of different functions that provide random numbers from different distributions. For Brownian motion, we use a special distribution called the ‚Äúnormal distribution‚Äù.\nstep_size = np.sqrt(2 * D * time_step)\ndx = random_number * step_size  # Random step in x direction\ndy = random_number * step_size  # Random step in y direction\n\nnew_x = old_x + dx\nnew_y = old_y + dy\nWhere:\n\nD is how easily the particle moves (diffusion coefficient)\ntime_step is how often we update the position\nrandom_number is chosen from a special ‚Äúnormal distribution‚Äù\n\n\n\n\n\n\n\nTip\n\n\n\nWhen simulating Brownian motion, we use np.random.normal to generate random steps following this distribution. The normal distribution is characterized by two parameters: the mean and the standard deviation. The mean is the average value, and the standard deviation is a measure of how spread out the values are. For Brownian motion, we use a standard deviation that depends on the diffusion coefficient and the time step. The standard deviation \\(\\sigma=\\sqrt{2D \\Delta t}\\) determines the typical step size, which we can use as a parameter in the normal distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced Mathematical Details\n\n\n\n\n\nThe Brownian motion of a colloidal particle results from collisions with surrounding solvent molecules. These collisions lead to a probability distribution described by:\n\\[\np(x,\\Delta t)=\\frac{1}{\\sqrt{4\\pi D \\Delta t}}e^{-\\frac{x^2}{4D \\Delta t}}\n\\]\nwhere: - \\(D\\) is the diffusion coefficient - \\(\\Delta t\\) is the time step - The variance is \\(\\sigma^2=2D \\Delta t\\)\nThis distribution emerges from the central limit theorem, as shown by Lindenberg and L√©vy, when considering many infinitesimally small random steps.\nThe evolution of the probability density function \\(p(x,t)\\) is governed by the diffusion equation:\n\\[\n\\frac{\\partial p}{\\partial t}=D\\frac{\\partial^2 p}{\\partial x^2}\n\\]\nThis partial differential equation, also known as Fick‚Äôs second law, describes how the concentration of particles evolves over time due to diffusive processes. The Gaussian distribution above is the fundamental solution (Green‚Äôs function) of this diffusion equation, representing how an initially localized distribution spreads out over time.\nThe connection between the microscopic random motion and the macroscopic diffusion equation was first established by Einstein in his 1905 paper on Brownian motion, providing one of the earliest quantitative links between statistical mechanics and thermodynamics."
  },
  {
    "objectID": "lectures/lecture05/2_brownian_motion.html#why-use-a-class",
    "href": "lectures/lecture05/2_brownian_motion.html#why-use-a-class",
    "title": "Brownian Motion with Object-Oriented Programming",
    "section": "Why Use a Class?",
    "text": "Why Use a Class?\nA class is perfect for this physics simulation because each colloidal particle:\n\nHas specific properties\n\nSize (radius)\nCurrent position\nMovement history\nDiffusion coefficient\n\nFollows certain behaviors\n\nMoves randomly (Brownian motion)\nUpdates its position over time\nKeeps track of where it‚Äôs been\n\nCan exist alongside other particles\n\nMany particles can move independently\nEach particle keeps track of its own properties\nParticles can have different sizes\n\nNeeds to track its state over time\n\nRemember previous positions\nCalculate distances moved\nMaintain its own trajectory\n\n\nThis natural mapping between real particles and code objects makes classes an ideal choice for our simulation."
  },
  {
    "objectID": "lectures/lecture05/2_brownian_motion.html#class-design",
    "href": "lectures/lecture05/2_brownian_motion.html#class-design",
    "title": "Brownian Motion with Object-Oriented Programming",
    "section": "Class Design",
    "text": "Class Design\nLet‚Äôs design a Python class to simulate colloidal particles undergoing Brownian motion. This object-oriented approach will help us manage multiple particles with different properties and behaviors.\n\nClass-Level Properties\nThe Colloid class will maintain information shared by all particles:\n\nA counter for the total number of particles\nThe physical constant \\(k_B T/(6\\pi\\eta) = 2.2√ó10^{-19}\\) (combining temperature and fluid properties)\n\n\n\nClass Methods\nThe class will provide these shared functions:\n\nhow_many(): Reports the total number of particles\n__str__: Creates a readable description of a particle‚Äôs properties\n\n\n\nInstance Properties\nEach individual particle object will have:\n\nRadius (R)\nPosition history (x and y coordinates)\nUnique identifier (index)\nDiffusion coefficient (\\(D = k_B T/(6\\pi\\eta R)\\))\n\n\n\nInstance Methods\nEach particle will be able to:\n\nsim_trajectory(): Generate a complete motion path\nupdate(dt): Calculate one step of Brownian motion\nget_trajectory(): Return its movement history\nget_D(): Provide its diffusion coefficient\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the function sim_trajectory is actually calling the function update of the same object to generate the whole trajectory at once."
  },
  {
    "objectID": "lectures/lecture05/2_brownian_motion.html#simulating",
    "href": "lectures/lecture05/2_brownian_motion.html#simulating",
    "title": "Brownian Motion with Object-Oriented Programming",
    "section": "Simulating",
    "text": "Simulating\nWith the help of this Colloid class, we would like to carry out simulations of Brownian motion of multiple particles. The simulations shall\n\ntake n=200 particles\nhave N=200 trajectory points each\nstart all at 0,0\nparticle objects should be stored in a list p_list"
  },
  {
    "objectID": "lectures/lecture05/2_brownian_motion.html#plotting-the-trajectories",
    "href": "lectures/lecture05/2_brownian_motion.html#plotting-the-trajectories",
    "title": "Brownian Motion with Object-Oriented Programming",
    "section": "Plotting the trajectories",
    "text": "Plotting the trajectories\nThe next step is to plot all the trajectories."
  },
  {
    "objectID": "lectures/lecture05/2_brownian_motion.html#characterizing-the-brownian-motion",
    "href": "lectures/lecture05/2_brownian_motion.html#characterizing-the-brownian-motion",
    "title": "Brownian Motion with Object-Oriented Programming",
    "section": "Characterizing the Brownian motion",
    "text": "Characterizing the Brownian motion\nNow that we have a number of trajectories, we can analyze the motion of our Brownian particles.\n\nCalculate the particle speed\nOne way is to calculate its speed by measuring how far it traveled within a certain time \\(n\\, dt\\), where \\(dt\\) is the timestep of out simulation. We can do that as\n\\[\\begin{equation}\nv(n dt) = \\frac{&lt;\\sqrt{(x_{i+n}-x_{i})^2+(y_{i+n}-y_{i})^2}&gt;}{n\\,dt}\n\\end{equation}\\]\nThe angular brackets on the top take care of the fact that we can measure the distance traveled within a certain time \\(n\\, dt\\) several times along a trajectory.\nThese values can be used to calculate a mean speed. Note that there is not an equal amount of data pairs for all separations available. For \\(n=1\\) there are 5 distances available. For \\(n=5\\), however, only 1. This changes the statistical accuracy of the mean.\n\n\n\n\n\n\nThe result of this analysis shows, that each particle has an apparent speed which seems to increase with decreasing time of observation or which decreases with increasing time. This would mean that there is some friction at work, which slows down the particle in time, but this is apparently not true. Also an infinite speed at zero time appears to be unphysical. The correct answer is just that the speed is no good measure to characterize the motion of a Brownian particle.\n\n\nCalculate the particle mean squared displacement\nA better way to characterize the motion of a Brownian particle is the mean squared displacement, as we have already mentioned it in previous lectures. We may compare our simulation now to the theoretical prediction, which is\n\\[\\begin{equation}\n\\langle \\Delta r^{2}(t)\\rangle=2 d D t\n\\end{equation}\\]\nwhere \\(d\\) is the dimension of the random walk, which is \\(d=2\\) in our case.\n\n\n\n\n\n\nThe results show that the mean squared displacement of the individual particles follows on average the theoretical predictions of a linear growth in time. That means, we are able to read the diffusion coefficient from the slope of the MSD of the individual particles if recorded in a simulation or an experiment.\nYet, each individual MSD is deviating strongly from the theoretical prediction especially at large times. This is due to the fact mentioned earlier that our simulation (or experimental) data only has a limited number of data points, while the theoretical prediction is made for the limit of infinite data points.\n\n\n\n\n\n\nAnalysis of MSD data\n\n\n\nSingle particle tracking, either in the experiment or in numerical simulations can therefore only deliver an estimate of the diffusion coefficient and care should be taken when using the whole MSD to obtain the diffusion coefficient. One typically uses only a short fraction of the whole MSD data at short times."
  },
  {
    "objectID": "lectures/lecture11/1_repetition.html",
    "href": "lectures/lecture11/1_repetition.html",
    "title": "Repetition",
    "section": "",
    "text": "After we have completed the first part of the course, we will have a repetition session to review the main concepts and topics covered so far. This will help reinforce your understanding and prepare you for the final exam. This part contains a number of exercises you can work on to test your knowledge and skills."
  },
  {
    "objectID": "lectures/lecture11/1_repetition.html#what-is-a-program",
    "href": "lectures/lecture11/1_repetition.html#what-is-a-program",
    "title": "Repetition",
    "section": "What is a Program?",
    "text": "What is a Program?\nA program is a sequence of instructions that tells a computer how to perform a specific task. These instructions must be:\n\nPrecise and unambiguous\nWritten in a language the computer understands\nLogically structured\nDesigned to achieve a specific goal"
  },
  {
    "objectID": "lectures/lecture11/1_repetition.html#basic-elements-of-python",
    "href": "lectures/lecture11/1_repetition.html#basic-elements-of-python",
    "title": "Repetition",
    "section": "Basic Elements of Python",
    "text": "Basic Elements of Python\n\nVariables and Data Types\nIn Python, variables are containers for storing data values. Python is dynamically typed, meaning you don‚Äôt need to declare variable types explicitly.\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Exercise 1: Unit Conversion\n\n\n\n\n\nWrite a program that converts a temperature from Celsius to Fahrenheit and Kelvin. Use the formulas:\n\n¬∞F = (¬∞C √ó 9/5) + 32\nK = ¬∞C + 273.15\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumerical Operations\nPython supports all basic mathematical operations:\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Exercise 2: Basic Kinematics\n\n\n\n\n\nCalculate the final velocity of an object given its initial velocity, acceleration, and time. Use the formula: v = v‚ÇÄ + at\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLists and Arrays\nFor physics calculations, we often need to work with collections of numbers:\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Exercise 3: Force Calculations\n\n\n\n\n\nCreate an array of masses (in kg) and calculate the force of gravity on each mass. Use F = mg where g = 9.81 m/s¬≤. Numpy is already imported and can be used with np\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "lectures/lecture11/1_repetition.html#control-structures",
    "href": "lectures/lecture11/1_repetition.html#control-structures",
    "title": "Repetition",
    "section": "Control Structures",
    "text": "Control Structures\n\nConditional Statements\nConditional statements allow programs to make decisions:\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Exercise 4: Phase of Matter\n\n\n\n\n\nWrite a program that determines the phase of water based on its temperature (assume standard pressure):\n\nBelow 0¬∞C: Solid (Ice)\n0-100¬∞C: Liquid\nAbove 100¬∞C: Gas (Steam)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Exercise 5: Projectile Range Calculator\n\n\n\n\n\nWrite a program that calculates if a projectile will hit a target given:\n\nInitial velocity\nLaunch angle\nTarget distance\n\nUse the range formula: \\(R = \\frac{v_0^2 \\sin(2\\theta)}{g}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLoops\nLoops allow repetition of code:\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Exercise 6: Radioactive Decay Calculator\n\n\n\n\n\nWrite a program that simulates radioactive decay over multiple half-lives:\n\nStart with an initial number of atoms (\\(N_0\\))\nCalculate remaining atoms after each half-life period\nContinue for 5 half-lives\n\nUse the formula \\(N(t) = N_0 \\cdot \\left(\\frac{1}{2}\\right)^{t/t_{1/2}}\\) where \\(t_{1/2}\\) is the half-life.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Exercise 7: Time to Ground Calculator\n\n\n\n\n\nWrite a program that calculates how long it takes for an object to reach the ground when dropped from different heights:\n\nStart with an initial height \\(h_0\\)\nCalculate position using \\(y = h_0 - \\frac{1}{2}gt^2\\)\nFind the time when \\(y = 0\\)\nUse small time steps (\\(dt = 0.01s\\))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "lectures/lecture11/1_repetition.html#basic-numerical-calculations",
    "href": "lectures/lecture11/1_repetition.html#basic-numerical-calculations",
    "title": "Repetition",
    "section": "Basic Numerical Calculations",
    "text": "Basic Numerical Calculations\nFor physics, we often use NumPy for numerical calculations:"
  },
  {
    "objectID": "lectures/lecture11/1_repetition.html#simple-physics-example",
    "href": "lectures/lecture11/1_repetition.html#simple-physics-example",
    "title": "Repetition",
    "section": "Simple Physics Example",
    "text": "Simple Physics Example\nLet‚Äôs calculate the position of a projectile under gravity:"
  },
  {
    "objectID": "lectures/lecture11/1_repetition.html#code-organization",
    "href": "lectures/lecture11/1_repetition.html#code-organization",
    "title": "Repetition",
    "section": "Code Organization",
    "text": "Code Organization\n\nUse meaningful variable names\nAdd comments to explain complex logic\nBreak down complex problems into smaller functions\nUse consistent indentation"
  },
  {
    "objectID": "lectures/lecture11/1_repetition.html#example-of-well-organized-code",
    "href": "lectures/lecture11/1_repetition.html#example-of-well-organized-code",
    "title": "Repetition",
    "section": "Example of Well-Organized Code",
    "text": "Example of Well-Organized Code\nHere‚Äôs an example calculating the period of a simple pendulum:"
  },
  {
    "objectID": "lectures/lecture11/1_repetition.html#classes-and-objects",
    "href": "lectures/lecture11/1_repetition.html#classes-and-objects",
    "title": "Repetition",
    "section": "Classes and Objects",
    "text": "Classes and Objects\nClasses are blueprints for creating objects that combine data (attributes) and functions (methods). This is particularly useful for modeling physical systems.\n\nBasic Class Structure\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced Self-Exercise 8: Electric Charge Class\n\n\n\n\n\nCreate a class representing an electric charge that can:\n\nStore position \\((x, y)\\), charge magnitude \\((q)\\), and mass \\((m)\\)\nCalculate electric potential at a point using \\(V = \\frac{kq}{r}\\)\nCalculate electric force on another charge using \\(F = \\frac{kq_1q_2}{r^2}\\)\nCalculate the direction of force (attraction/repulsion)\n\nUse \\(k = 8.99 \\times 10^9\\) N\\(\\cdot\\)m¬≤/C¬≤ (Coulomb‚Äôs constant)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "lectures/lecture11/1_repetition.html#a-physics-example-harmonic-oscillator",
    "href": "lectures/lecture11/1_repetition.html#a-physics-example-harmonic-oscillator",
    "title": "Repetition",
    "section": "A Physics Example: Harmonic Oscillator",
    "text": "A Physics Example: Harmonic Oscillator\nHere‚Äôs a more complete example modeling a harmonic oscillator:"
  },
  {
    "objectID": "lectures/lecture11/1_repetition.html#inheritance",
    "href": "lectures/lecture11/1_repetition.html#inheritance",
    "title": "Repetition",
    "section": "Inheritance",
    "text": "Inheritance\nClasses can inherit properties and methods from other classes:"
  },
  {
    "objectID": "lectures/lecture11/1_repetition.html#key-points-about-classes",
    "href": "lectures/lecture11/1_repetition.html#key-points-about-classes",
    "title": "Repetition",
    "section": "Key Points About Classes",
    "text": "Key Points About Classes\n\nClasses combine data (attributes) and functions (methods)\nThe __init__ method initializes new objects\nself refers to the instance of the class\nMethods can modify the object‚Äôs state\nInheritance allows creating specialized versions of classes\nClasses help organize code and model real-world systems\n\nClasses are particularly useful in physics for: - Modeling physical systems - Organizing simulation code - Creating reusable components - Building hierarchies of related objects"
  },
  {
    "objectID": "lectures/lecture10/1_spring_pendulum.html",
    "href": "lectures/lecture10/1_spring_pendulum.html",
    "title": "Spring Pendulum",
    "section": "",
    "text": "In the last lectures, we have explored the use the scipy module odeint to do the work of solving differential equations for us. We have studied coupled pendula and explored the details of the solutions. This time we have two more projects ahead of us. We first want to consider the motion of a spring pendulum and then come from that to the motion of planets. Both are examples where we have not only a tangential accelaration but also a radial component. Otherwise, these problems do look similar than the ones we considered before."
  },
  {
    "objectID": "lectures/lecture10/1_spring_pendulum.html#physical-model",
    "href": "lectures/lecture10/1_spring_pendulum.html#physical-model",
    "title": "Spring Pendulum",
    "section": "Physical Model",
    "text": "Physical Model\nThe image below shows the situation we would like to cover in our second project. This is also a kind of coupled pendula, however, the situation is more subtle.\nWe have a single spring which is mounted to a support and a mass. The spring can be elongated in length but also in angle so that you finally have a pendulum and a spring. Both motions are coupled now in a similar way as for the coupled pendula we treated. This time, however, the length change of the spring modulates the frequency of the pendulum.\n\n\n\nSpringPendulum.png\n\n\n\nEquations of motion\nA mass \\(m\\) is attached to a spring with spring constant \\(k\\), which is attached to a support point as shown in the figure. The length of the resulting pendulum at any given time is the spring rest length \\(L_0\\) plus the stretch (or compression) \\(L\\), and the angle of the pendulum with respect to the vertical is \\(\\theta\\).\nThe differential equations for this system are given by\n\\[\\begin{eqnarray}\n\\ddot{L}&=&(L_0+L)\\dot{\\theta}^2-\\frac{k}{m}L+g\\cos(\\theta)\\\\\n\\ddot{\\theta}&=&-\\frac{1}{L_0+L}[g\\sin(\\theta)+2\\dot{L}\\dot{\\theta}]\n\\end{eqnarray}\\]\nLet‚Äôs break down the terms in these equations:\nFor the radial motion (L):\n\n\\((L_0+L)\\dot{\\theta}^2\\) represents the centrifugal force term\n\\(-\\frac{k}{m}L\\) is the restoring force of the spring (Hooke‚Äôs law)\n\\(g\\cos(\\theta)\\) is the component of gravity along the radial direction\n\nFor the angular motion (Œ∏):\n\n\\(g\\sin(\\theta)\\) represents the gravitational torque\n\\(2\\dot{L}\\dot{\\theta}\\) is the Coriolis force term\nThe factor \\(\\frac{1}{L_0+L}\\) comes from the moment of inertia\n\nThe equations are coupled - the radial and angular motions affect each other. The system shows interesting behavior like:\n\nEnergy exchange between radial and angular modes\nNon-linear oscillations\nPotential for chaotic motion under certain conditions\n\nThe key players in this equation are the centrifugal and Coriolis forces. These forces are fictitious forces that arise when we consider the motion of an object in a rotating frame of reference. The centrifugal force is the force that pushes an object away from the center of rotation, while the Coriolis force is the force that acts perpendicular to the direction of motion of an object in a rotating frame.\nCentrifugal Force:\n\\[\\begin{equation}\n\\vec{F}_{\\text{centrifugal}} = m(\\vec{\\omega} \\times (\\vec{\\omega} \\times \\vec{r}))\n\\end{equation}\\]\nCoriolis Force: \\[\\begin{equation}\n\\vec{F}_{\\text{Coriolis}} = 2m(\\vec{\\omega} \\times \\vec{v}_{\\text{rel}})\n\\end{equation}\\]\nwhere:\n\n\\(m\\) is the mass\n\\(\\vec{\\omega}\\) is the angular velocity vector\n\\(\\vec{r}\\) is the position vector\n\\(\\vec{v}_{\\text{rel}}\\) is the velocity relative to the rotating frame\n\\(\\times\\) denotes the cross product\n\nIn component form, if we consider rotation about the z-axis with \\(\\vec{\\omega} = \\omega\\hat{k}\\), these become:\nCentrifugal Force: \\[\\begin{equation}\n\\vec{F}_{\\text{centrifugal}} = m\\omega^2(x\\hat{i} + y\\hat{j})\n\\end{equation}\\]\nCoriolis Force: \\[\\begin{equation}\n\\vec{F}_{\\text{Coriolis}} = 2m\\omega(-v_y\\hat{i} + v_x\\hat{j})\n\\end{equation}\\]\nwhere \\(\\hat{i}\\), \\(\\hat{j}\\), and \\(\\hat{k}\\) are unit vectors in the x, y, and z directions respectively.\nBelow we write a program that plots the motion of the mass for some initial \\(\\theta\\neq0\\). Explore different solutions. We should get that when the spring is very stiff (large k), it reduces approximately to a simple pendulum. When \\(\\theta\\) is small, it can show beats between the spring and pendulum modes."
  },
  {
    "objectID": "lectures/lecture10/1_spring_pendulum.html#numerical-solution",
    "href": "lectures/lecture10/1_spring_pendulum.html#numerical-solution",
    "title": "Spring Pendulum",
    "section": "Numerical Solution",
    "text": "Numerical Solution\nFor the numerical solution of the differential equations, we will use the odeint function from the scipy module. So this is another training for us to use this function.\n\nInitial parameters\nWe will have to think about the initial conditions. We will later modify them to see how the system behaves in different regimes.\n\n\n\n\n\n\n\n\nSolution\nThe solution of the differential equations is done by the odeint function.\n\n\n\n\n\n\nThe provided answer is a 2D array with the first column being the length of the spring, the second column the velocity of the spring, the third column the angle of the pendulum, and the fourth column the angular velocity of the pendulum.\n\n\n\n\n\n\nWe can convert the answer to the x and y positions of the mass. This is done by the following code. As the length \\(L\\) is only the elongation of the spring, we have to add the equilibrium length \\(L_0\\) to get the total length of the spring. The x and y positions are then given by \\(L\\sin(\\theta)\\) and \\(L\\cos(\\theta)\\).\n\n\n\n\n\n\n\n\nPlotting\nThe plots below show the motion of the mass in the x-y plane.\n\n\n\n\n\n\nThe next plot is providing the so called phase space plot. This is a plot of the velocity of the mass versus the position of the mass. This plot is very useful to understand the dynamics of the system.\n\n\n\n\n\n\nFinally we plot the phase space of the pendulum. This is the angle of the pendulum versus the angular velocity of the pendulum.\n\n\n\n\n\n\n\nAngle and Length over Time"
  },
  {
    "objectID": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html",
    "href": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html",
    "title": "Brownian Motion Course Restructuring",
    "section": "",
    "text": "Date: January 2025 Status: ‚úÖ Complete\n\n\nBrownian Motion content has been split into two modules to improve learning outcomes:\n\nWeek 2-3: Simple, function-based approach (NEW)\nWeek 6-7: Advanced OOP approach (EXISTING, moved from Week 2-3)\n\nThis change reduces cognitive load, allows students to focus on physics first, and demonstrates the value of OOP through direct comparison.\n\n\n\n\n\n\n\nlectures/lecture02/4_brownian_motion_simple.qmd\n\nSimple Brownian motion using only functions and numpy arrays\nNo classes, no OOP\nFocus on physics and basic programming\nSuitable for Week 2-3 when students are still learning fundamentals\n\nlectures/lecture05/brownian_motion_comparison.qmd\n\nSide-by-side comparison of function-based vs.¬†OOP approaches\nShows when to use each approach\nDemonstrates scalability issues with functions\nOptional supplementary material\n\nlectures/PEDAGOGICAL_NOTES.md\n\nDetailed rationale for the restructuring\nTeaching tips for each stage\nAssessment guidance\nSuccess metrics\n\n\n\n\n\n\n_quarto.yml\n\nWeek 2-3 now links to 4_brownian_motion_simple.qmd\nWeek 6-7 section uncommented with Classes + OOP Brownian Motion\nProper progression: simple ‚Üí advanced\n\nlectures/lecture05/2_brownian_motion.qmd\n\nUpdated title to ‚ÄúBrownian Motion with Object-Oriented Programming‚Äù\nAdded note comparing to the simple approach\nClarified this is the OOP version\n\n\n\n\n\n\n\nWeek 2-3: Modeling Motion\n‚îú‚îÄ‚îÄ Datatypes for Physics\n‚îú‚îÄ‚îÄ Functions: Reusable Physics Equations\n‚îú‚îÄ‚îÄ Application: Brownian Motion (Simple)    ‚Üê NEW\n‚îî‚îÄ‚îÄ Arrays with Numpy\n\nWeek 6-7: Code Organization\n‚îú‚îÄ‚îÄ Modules: Organizing Your Code\n‚îú‚îÄ‚îÄ Classes & Objects\n‚îú‚îÄ‚îÄ Advanced: Brownian Motion with OOP       ‚Üê MOVED HERE\n‚îî‚îÄ‚îÄ Working with Data Files\n\n\n\n\n\n\nOriginal structure taught classes in Week 2-3 via Brownian Motion, causing:\n\n‚ùå Cognitive overload (too many new concepts at once)\n‚ùå Students confused by self, __init__, OOP paradigm\n‚ùå Physics understanding suffered\n‚ùå Students asked ‚Äúwhy do we need this?‚Äù (no motivation for OOP yet)\n\n\n\n\nTwo-stage approach:\nStage 1 (Week 2-3): Physics First - Students understand Brownian motion conceptually - Build confidence with functions, loops, arrays - See immediate results ‚Üí motivation boost - Natural programming progression\nStage 2 (Week 6-7): Professional Code - Revisit familiar physics with better tools - Students SEE why classes help (direct comparison) - Natural motivation: ‚Äúmy Week 2 code was messy, this is better!‚Äù - Learning OOP makes sense in context\n\n\n\n\n\n\n\nFile: lectures/lecture02/4_brownian_motion_simple.qmd\nLearning Goals: - Understand Brownian motion physics - Generate random steps with np.random.normal() - Store trajectories in arrays - Compute and plot mean squared displacement - Build confidence with functions\nTeaching Tips: - Emphasize: ‚ÄúWe‚Äôre focusing on physics first!‚Äù - Celebrate when students get their first random walk working - Allow experimentation with parameters (D, dt, N) - Don‚Äôt mention ‚Äúwe‚Äôll do this better later‚Äù - let them enjoy the success\nCommon Questions: - Q: ‚ÄúWhy lists instead of something fancier?‚Äù - A: ‚ÄúKeeping it simple so we focus on understanding the physics!‚Äù\n\n\n\nFile: lectures/lecture05/2_brownian_motion.qmd\nLearning Goals: - Understand when to use classes vs.¬†functions - Design and implement classes for physics simulations - Appreciate code organization benefits - Compare approaches critically\nTeaching Tips: - Start with: ‚ÄúRemember our Brownian motion? Let‚Äôs improve it!‚Äù - Show the comparison document (optional) - Emphasize scalability: ‚ÄúImagine 1000 particles with different properties‚Ä¶‚Äù - Let students refactor their Week 2 code into classes\nCommon Questions: - Q: ‚ÄúIs OOP really necessary?‚Äù - A: ‚ÄúFor small problems, no. But you‚Äôre becoming a computational physicist - you‚Äôll write complex simulations. This is how professionals do it.‚Äù\n\n\n\n\n\n\n\nTest: - Physics: What causes Brownian motion? - Coding: Generate random steps, plot trajectories - Analysis: Interpret MSD plots - Problem-solving: Modify simulation parameters\nAvoid: - Questions about classes or OOP - Complex data structures\n\n\n\nTest: - Design: When to use classes? - Implementation: Write a simple class - Comparison: Function vs.¬†OOP tradeoffs - Application: Extend the Colloid class\nCan include: - ‚ÄúCompare your Week 2 solution to this OOP version‚Äù - ‚ÄúRefactor this function-based code into a class‚Äù\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFile\nWeek\nPurpose\nStatus\n\n\n\n\nlecture02/4_brownian_motion_simple.qmd\n2-3\nSimple function-based simulation\n‚úÖ NEW\n\n\nlecture05/1_classes.qmd\n6-7\nClasses introduction\n‚úÖ Existing\n\n\nlecture05/2_brownian_motion.qmd\n6-7\nOOP Brownian motion\n‚úÖ Updated\n\n\n\n\n\n\n\n\n\n\n\n\n\nFile\nPurpose\n\n\n\n\nlecture05/brownian_motion_comparison.qmd\nSide-by-side comparison (optional)\n\n\nlectures/PEDAGOGICAL_NOTES.md\nDetailed teaching rationale\n\n\nlectures/BROWNIAN_MOTION_RESTRUCTURE_README.md\nThis document\n\n\n\n\n\n\n\n\nYou‚Äôll know this restructuring is working when:\n‚úÖ Week 2-3: Students excited about physics simulations (not frustrated by syntax)\n‚úÖ Week 6-7: Students appreciate OOP value (not asking ‚Äúwhy bother?‚Äù)\n‚úÖ Homework: More physics questions, fewer ‚Äúhow does self work?‚Äù questions\n‚úÖ Course feedback: Students mention clear progression and building confidence\n‚úÖ Code quality: Students use classes appropriately in later projects\n\n\n\n\nConsider for future iterations:\n\nVideo comparison: Show both approaches being coded live\nDebugging exercise: Give students messy function-based code ‚Üí refactor to classes\nProject: Students implement both versions and write reflection\nAdvanced extension: Particle interactions (naturally benefits from OOP)\nApply pattern to other topics: Planetary motion, wave simulations, etc.\n\n\n\n\n\nBefore teaching:\n\nTest all code cells in both Brownian motion notebooks\nVerify Quarto builds correctly\nCheck that links in _quarto.yml work\nReview pedagogical notes\nPrepare example solutions for homework\nCreate answer key showing both approaches\n\n\n\n\n\nFor pedagogical questions: See PEDAGOGICAL_NOTES.md\nFor technical issues: Check notebook code cells execute properly\nFor course structure: Consult _quarto.yml and verify week numbering\n\n\n\n\nThis restructuring addresses student feedback indicating confusion with early OOP introduction. The two-stage approach follows best practices in computational science education:\n\nConcrete before abstract (Papert)\nSpiral learning (Bruner)\nJust-in-time teaching (Novak)\nCognitive load management (Sweller)\n\n\nLast Updated: January 2025 Maintained By: Course Instructor Version: 1.0"
  },
  {
    "objectID": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#executive-summary",
    "href": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#executive-summary",
    "title": "Brownian Motion Course Restructuring",
    "section": "",
    "text": "Brownian Motion content has been split into two modules to improve learning outcomes:\n\nWeek 2-3: Simple, function-based approach (NEW)\nWeek 6-7: Advanced OOP approach (EXISTING, moved from Week 2-3)\n\nThis change reduces cognitive load, allows students to focus on physics first, and demonstrates the value of OOP through direct comparison."
  },
  {
    "objectID": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#what-changed",
    "href": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#what-changed",
    "title": "Brownian Motion Course Restructuring",
    "section": "",
    "text": "lectures/lecture02/4_brownian_motion_simple.qmd\n\nSimple Brownian motion using only functions and numpy arrays\nNo classes, no OOP\nFocus on physics and basic programming\nSuitable for Week 2-3 when students are still learning fundamentals\n\nlectures/lecture05/brownian_motion_comparison.qmd\n\nSide-by-side comparison of function-based vs.¬†OOP approaches\nShows when to use each approach\nDemonstrates scalability issues with functions\nOptional supplementary material\n\nlectures/PEDAGOGICAL_NOTES.md\n\nDetailed rationale for the restructuring\nTeaching tips for each stage\nAssessment guidance\nSuccess metrics\n\n\n\n\n\n\n_quarto.yml\n\nWeek 2-3 now links to 4_brownian_motion_simple.qmd\nWeek 6-7 section uncommented with Classes + OOP Brownian Motion\nProper progression: simple ‚Üí advanced\n\nlectures/lecture05/2_brownian_motion.qmd\n\nUpdated title to ‚ÄúBrownian Motion with Object-Oriented Programming‚Äù\nAdded note comparing to the simple approach\nClarified this is the OOP version"
  },
  {
    "objectID": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#course-structure-updated",
    "href": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#course-structure-updated",
    "title": "Brownian Motion Course Restructuring",
    "section": "",
    "text": "Week 2-3: Modeling Motion\n‚îú‚îÄ‚îÄ Datatypes for Physics\n‚îú‚îÄ‚îÄ Functions: Reusable Physics Equations\n‚îú‚îÄ‚îÄ Application: Brownian Motion (Simple)    ‚Üê NEW\n‚îî‚îÄ‚îÄ Arrays with Numpy\n\nWeek 6-7: Code Organization\n‚îú‚îÄ‚îÄ Modules: Organizing Your Code\n‚îú‚îÄ‚îÄ Classes & Objects\n‚îú‚îÄ‚îÄ Advanced: Brownian Motion with OOP       ‚Üê MOVED HERE\n‚îî‚îÄ‚îÄ Working with Data Files"
  },
  {
    "objectID": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#why-this-change",
    "href": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#why-this-change",
    "title": "Brownian Motion Course Restructuring",
    "section": "",
    "text": "Original structure taught classes in Week 2-3 via Brownian Motion, causing:\n\n‚ùå Cognitive overload (too many new concepts at once)\n‚ùå Students confused by self, __init__, OOP paradigm\n‚ùå Physics understanding suffered\n‚ùå Students asked ‚Äúwhy do we need this?‚Äù (no motivation for OOP yet)\n\n\n\n\nTwo-stage approach:\nStage 1 (Week 2-3): Physics First - Students understand Brownian motion conceptually - Build confidence with functions, loops, arrays - See immediate results ‚Üí motivation boost - Natural programming progression\nStage 2 (Week 6-7): Professional Code - Revisit familiar physics with better tools - Students SEE why classes help (direct comparison) - Natural motivation: ‚Äúmy Week 2 code was messy, this is better!‚Äù - Learning OOP makes sense in context"
  },
  {
    "objectID": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#teaching-guide",
    "href": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#teaching-guide",
    "title": "Brownian Motion Course Restructuring",
    "section": "",
    "text": "File: lectures/lecture02/4_brownian_motion_simple.qmd\nLearning Goals: - Understand Brownian motion physics - Generate random steps with np.random.normal() - Store trajectories in arrays - Compute and plot mean squared displacement - Build confidence with functions\nTeaching Tips: - Emphasize: ‚ÄúWe‚Äôre focusing on physics first!‚Äù - Celebrate when students get their first random walk working - Allow experimentation with parameters (D, dt, N) - Don‚Äôt mention ‚Äúwe‚Äôll do this better later‚Äù - let them enjoy the success\nCommon Questions: - Q: ‚ÄúWhy lists instead of something fancier?‚Äù - A: ‚ÄúKeeping it simple so we focus on understanding the physics!‚Äù\n\n\n\nFile: lectures/lecture05/2_brownian_motion.qmd\nLearning Goals: - Understand when to use classes vs.¬†functions - Design and implement classes for physics simulations - Appreciate code organization benefits - Compare approaches critically\nTeaching Tips: - Start with: ‚ÄúRemember our Brownian motion? Let‚Äôs improve it!‚Äù - Show the comparison document (optional) - Emphasize scalability: ‚ÄúImagine 1000 particles with different properties‚Ä¶‚Äù - Let students refactor their Week 2 code into classes\nCommon Questions: - Q: ‚ÄúIs OOP really necessary?‚Äù - A: ‚ÄúFor small problems, no. But you‚Äôre becoming a computational physicist - you‚Äôll write complex simulations. This is how professionals do it.‚Äù"
  },
  {
    "objectID": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#assessment-recommendations",
    "href": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#assessment-recommendations",
    "title": "Brownian Motion Course Restructuring",
    "section": "",
    "text": "Test: - Physics: What causes Brownian motion? - Coding: Generate random steps, plot trajectories - Analysis: Interpret MSD plots - Problem-solving: Modify simulation parameters\nAvoid: - Questions about classes or OOP - Complex data structures\n\n\n\nTest: - Design: When to use classes? - Implementation: Write a simple class - Comparison: Function vs.¬†OOP tradeoffs - Application: Extend the Colloid class\nCan include: - ‚ÄúCompare your Week 2 solution to this OOP version‚Äù - ‚ÄúRefactor this function-based code into a class‚Äù"
  },
  {
    "objectID": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#files-reference",
    "href": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#files-reference",
    "title": "Brownian Motion Course Restructuring",
    "section": "",
    "text": "File\nWeek\nPurpose\nStatus\n\n\n\n\nlecture02/4_brownian_motion_simple.qmd\n2-3\nSimple function-based simulation\n‚úÖ NEW\n\n\nlecture05/1_classes.qmd\n6-7\nClasses introduction\n‚úÖ Existing\n\n\nlecture05/2_brownian_motion.qmd\n6-7\nOOP Brownian motion\n‚úÖ Updated\n\n\n\n\n\n\n\n\n\n\n\n\n\nFile\nPurpose\n\n\n\n\nlecture05/brownian_motion_comparison.qmd\nSide-by-side comparison (optional)\n\n\nlectures/PEDAGOGICAL_NOTES.md\nDetailed teaching rationale\n\n\nlectures/BROWNIAN_MOTION_RESTRUCTURE_README.md\nThis document"
  },
  {
    "objectID": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#success-metrics",
    "href": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#success-metrics",
    "title": "Brownian Motion Course Restructuring",
    "section": "",
    "text": "You‚Äôll know this restructuring is working when:\n‚úÖ Week 2-3: Students excited about physics simulations (not frustrated by syntax)\n‚úÖ Week 6-7: Students appreciate OOP value (not asking ‚Äúwhy bother?‚Äù)\n‚úÖ Homework: More physics questions, fewer ‚Äúhow does self work?‚Äù questions\n‚úÖ Course feedback: Students mention clear progression and building confidence\n‚úÖ Code quality: Students use classes appropriately in later projects"
  },
  {
    "objectID": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#optional-enhancements",
    "href": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#optional-enhancements",
    "title": "Brownian Motion Course Restructuring",
    "section": "",
    "text": "Consider for future iterations:\n\nVideo comparison: Show both approaches being coded live\nDebugging exercise: Give students messy function-based code ‚Üí refactor to classes\nProject: Students implement both versions and write reflection\nAdvanced extension: Particle interactions (naturally benefits from OOP)\nApply pattern to other topics: Planetary motion, wave simulations, etc."
  },
  {
    "objectID": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#testing-checklist",
    "href": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#testing-checklist",
    "title": "Brownian Motion Course Restructuring",
    "section": "",
    "text": "Before teaching:\n\nTest all code cells in both Brownian motion notebooks\nVerify Quarto builds correctly\nCheck that links in _quarto.yml work\nReview pedagogical notes\nPrepare example solutions for homework\nCreate answer key showing both approaches"
  },
  {
    "objectID": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#questions-or-issues",
    "href": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#questions-or-issues",
    "title": "Brownian Motion Course Restructuring",
    "section": "",
    "text": "For pedagogical questions: See PEDAGOGICAL_NOTES.md\nFor technical issues: Check notebook code cells execute properly\nFor course structure: Consult _quarto.yml and verify week numbering"
  },
  {
    "objectID": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#acknowledgments",
    "href": "lectures/BROWNIAN_MOTION_RESTRUCTURE_README.html#acknowledgments",
    "title": "Brownian Motion Course Restructuring",
    "section": "",
    "text": "This restructuring addresses student feedback indicating confusion with early OOP introduction. The two-stage approach follows best practices in computational science education:\n\nConcrete before abstract (Papert)\nSpiral learning (Bruner)\nJust-in-time teaching (Novak)\nCognitive load management (Sweller)\n\n\nLast Updated: January 2025 Maintained By: Course Instructor Version: 1.0"
  },
  {
    "objectID": "lectures/lecture07/1_curve_fitting.html",
    "href": "lectures/lecture07/1_curve_fitting.html",
    "title": "Curve fitting",
    "section": "",
    "text": "Let‚Äôs take a break from physics-related topics and explore another crucial area: curve fitting. We‚Äôll focus on demonstrating how to apply the least-squares method to fit a quadratic function with three parameters to experimental data. It‚Äôs worth noting that this approach can be applied to more complex functions or even simpler linear models.\nBefore diving into the fitting process, it‚Äôs essential to consider how to best estimate your model parameters. In some cases, you may be able to derive explicit estimators for the parameters, which can simplify the fitting procedure. Therefore, it‚Äôs advisable to carefully consider your approach before beginning the actual fitting process.\nFor those who want to delve deeper into this subject, you might find it interesting to explore concepts like maximum likelihood estimation. This method offers an alternative approach to parameter estimation and can provide valuable insights in certain scenarios."
  },
  {
    "objectID": "lectures/lecture07/1_curve_fitting.html#idea",
    "href": "lectures/lecture07/1_curve_fitting.html#idea",
    "title": "Curve fitting",
    "section": "Idea",
    "text": "Idea\nIn experimental physics, we often collect data points to understand the underlying physical phenomena. This process involves fitting a mathematical model to the experimental data.\nThe data typically comes as a series of paired points:\n\n\n\nx-data\ny-data\n\n\n\n\n\\(x_{1}\\)\n\\(y_{1}\\)\n\n\n\\(x_{2}\\)\n\\(y_{2}\\)\n\n\n‚Ä¶\n‚Ä¶\n\n\n\\(x_{N}\\)\n\\(y_{N}\\)\n\n\n\nEach point \\(\\{x_i, y_i\\}\\) may represent the result of multiple independent measurements. For instance, \\(y_1\\) could be the mean of several measurements \\(y_{1,j}\\):\n\\[y_1 = \\frac{1}{N}\\sum_{j=1}^N y_{1,j}\\]\nWhen these measurements have an uncertainty \\(\\sigma\\) for individual readings, the sum of all measurements has a variance of \\(N\\sigma^2\\) and a standard deviation of \\(\\sqrt{N}\\sigma\\). Consequently, the mean value has an associated error (standard deviation) known as the Standard Error of the Mean (SEOM):\n\\[\\sigma_{SEOM} = \\frac{\\sigma}{\\sqrt{N}}\\]\nThis SEOM is crucial in physics measurements.\nIt‚Äôs also important to note the definition of variance:\n\\[\\sigma_1^2 = \\frac{1}{N} \\sum_{j=1}^N (y_{1,j} - y_1)^2\\]\nThis statistical framework forms the basis for analyzing experimental data and fitting mathematical models to understand the underlying physics."
  },
  {
    "objectID": "lectures/lecture07/1_curve_fitting.html#least-squares",
    "href": "lectures/lecture07/1_curve_fitting.html#least-squares",
    "title": "Curve fitting",
    "section": "Least squares",
    "text": "Least squares\nIn experimental physics, we often collect data points to understand the underlying physical phenomena. To make sense of this data, we fit a mathematical model to it. One common method for fitting data is the least squares method.\n\nWhy use least squares fitting?\nThe goal of least squares fitting is to find the set of parameters for our model that best describes the data. This is done by minimizing the differences (or residuals) between the observed data points and the model‚Äôs predictions.\n\n\nGaussian uncertainty and probability\nWhen we take measurements, there is always some uncertainty. Often, this uncertainty can be modeled using a Gaussian (normal) distribution. This distribution is characterized by its mean (average value) and standard deviation (a measure of the spread of the data).\nIf we describe our data with a model function, which delivers a function value \\(f(x_{i},a)\\) for a set of parameters \\(a\\) at the position \\(x_{i}\\), the Gaussian uncertainty dictates a probability of finding a data value \\(y_{i}\\):\n\\[\\begin{equation}\np_{y_{i}}=\\frac{1}{\\sqrt{2\\pi}\\sigma_{i}}\\exp\\left(-\\frac{(y_{i}-f(x_{i},a))^2}{2\\sigma_{i}^2}\\right)\n\\end{equation}\\]\nHere, \\(\\sigma_{i}\\) represents the uncertainty in the measurement \\(y_{i}\\).\n\n\nCombining probabilities for multiple data points\nTo understand how well our model fits all the data points, we need to consider the combined probability of observing all the data points. This is done by multiplying the individual probabilities:\n\\[\\begin{equation}\np(y_{1},\\ldots,y_{N})=\\prod_{i=1}^{N}\\frac{1}{\\sqrt{2\\pi}\\sigma_{i}}\\exp\\left(-\\frac{(y_{i}-f(x_{i},a))^2}{2\\sigma_{i}^2}\\right)\n\\end{equation}\\]\n\n\nMaximizing the joint probability\nThe best fit of the model to the data is achieved when this joint probability is maximized. To simplify the calculations, we take the logarithm of the joint probability:\n\\[\\begin{equation}\n\\ln(p(y_{1},\\ldots,y_{N}))=-\\frac{1}{2}\\sum_{i=1}^{N}\\left( \\frac{y_{i}-f(x_{i},a)}{\\sigma_{i}}\\right)^2 - \\sum_{i=1}^{N}\\ln\\left( \\sigma_{i}\\sqrt{2\\pi}\\right)\n\\end{equation}\\]\nThe first term on the right side (except the factor 1/2) is the least squared deviation, also known as \\(\\chi^{2}\\):\n\\[\\begin{equation}\n\\chi^{2} =\\sum_{i=1}^{N}\\left( \\frac{y_{i}-f(x_{i},a)}{\\sigma_{i}}\\right)^2\n\\end{equation}\\]\nThe second term is just a constant value given by the uncertainties of our experimental data."
  },
  {
    "objectID": "lectures/lecture07/1_curve_fitting.html#data",
    "href": "lectures/lecture07/1_curve_fitting.html#data",
    "title": "Curve fitting",
    "section": "Data",
    "text": "Data\nLet‚Äôs have a look at the meaning of this equation. Let‚Äôs assume we measure the trajectory of a ball that has been thrown at an angle \\(\\alpha\\) with an initial velocity \\(v_{0}\\). We have collected data points by measuring the height of the ball above the ground at equally spaced distances from the throwing person.\n\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.5.2 from the internet...\n    (need help?)\n    \n\n\n\n\nThe table above shows the measured data points \\(y_{i}\\) at the position \\(x_{i}\\) with the associated uncertainties \\(\\sigma_{i}\\).\nWe can plot the data and expect, of course, a parabola. Therefore, we model our experimental data with a parabola like\n\\[\\begin{equation}\ny = ax^2 + bx + c\n\\end{equation}\\]\nwhere the parameter \\(a\\) must be negative since the parabola is inverted.\nI have created an interactive plot with an interact widget, as this allows you to play around with the parameters. The value of \\(\\chi^2\\) is also included in the legend, so you can get an impression of how good your fit of the data is.\n\nviewof aSlider = Inputs.range([-4, 0], { label: \"a\", step: 0.01, value: -1.7 });\nviewof bSlider = Inputs.range([-2, 2], { label: \"b\", step: 0.01, value: 1.3 });\nviewof cSlider = Inputs.range([-2, 2], { label: \"c\", step: 0.01, value: 1.0 });\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfiltered = transpose(data);\n// Create the plot\n\nxValues = Array.from({ length: 100 }, (_, i) =&gt; i / 100);\nparabolaData = xValues.map(x =&gt; ({ x, y: parabola(x, aSlider, bSlider, cSlider) }));\n\n\nparabola = (x, a, b, c) =&gt; a * x**2 + b * x + c\n\ncalculateChiSquared = (data, a, b, c) =&gt; {\n  let chisq = 0\n  let x= data.map(d =&gt; d.x)\n  let y= data.map(d =&gt; d.y)\n  let err= data.map(d =&gt; d.error)\n  for (let i = 0; i &lt; x.length; i++) {\n    let y_model = parabola(x[i], a, b, c)\n    chisq += ((y[i] - y_model) / err[i])**2\n  }\n  return chisq\n}\n\nchisq = calculateChiSquared(filtered, aSlider, bSlider, cSlider)\n\nPlot.plot({\n  marks: [\n    Plot.dot(filtered, { x: \"x\", y: \"y\" }),\n    Plot.ruleY(filtered, { x: \"x\", y1: d =&gt; d.y - d.error, y2: d =&gt; d.y + d.error }),\n    Plot.line(parabolaData, { x: \"x\", y: \"y\" }),\n    Plot.text([{ x: 0.8, y: 1.5, label: `œá¬≤: ${chisq.toFixed(2)}` }], {\n          x: \"x\",\n          y: \"y\",\n          text: \"label\",\n          dy: -10, // Adjust vertical position if needed\n          fill: \"black\", // Set text color\n          fontSize: 16\n        }),\n    Plot.frame()\n  ],\n  x: {\n    label: \"X Axis\",\n    labelAnchor: \"center\",\n    labelOffset: 35,\n    grid: true,\n    tickFormat: \".2f\", // Format ticks to 2 decimal places\n    domain: [0, 1]\n  },\n  y: {\n    label: \"Y Axis\",\n    grid: true,\n    tickFormat: \".2f\", // Format ticks to 2 decimal places\n    labelAnchor: \"center\",  // Center the label on its axis\n    labelAngle: -90,\n    labelOffset: 60,\n    domain: [0, 2],\n  },\n  width: 400,\n  height: 400,\n  marginLeft: 100,\n  marginBottom: 40,\n  style: {\n    fontSize: \"14px\",          // This sets the base font size\n    \"axis.label\": {\n      fontSize: \"18px\",        // This sets the font size for axis labels\n      fontWeight: \"bold\"       // Optionally make it bold\n    },\n    \"axis.tick\": {\n      fontSize: \"14px\"         // This sets the font size for tick labels\n    }\n  },\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe have that troubling point at the right edge with a large uncertainty. However, since the value of \\(\\chi^2\\) divides the deviation by the uncertainty \\(\\sigma_{i}\\), the weight for this point overall in the \\(\\chi^2\\) is smaller than for the other points.\n\\[\\begin{equation}\n\\chi^{2}=\\sum_{i=1}^{N}\\left( \\frac{y_{i}-f(x_{i},a)}{\\sigma_{i}}\\right)^2\n\\end{equation}\\]\nYou may simply check the effect by changing the uncertainty of the last data points in the error array."
  },
  {
    "objectID": "lectures/lecture07/1_curve_fitting.html#least-square-fitting",
    "href": "lectures/lecture07/1_curve_fitting.html#least-square-fitting",
    "title": "Curve fitting",
    "section": "Least square fitting",
    "text": "Least square fitting\nTo find the best fit of the model to the experimental data, we use the least squares method. This method minimizes the sum of the squared differences between the observed data points and the model‚Äôs predictions.\nMathematically, we achieve this by minimizing the least squares, i.e., finding the parameters \\(a\\) that minimize the following expression:\n\\[\\begin{equation}\n\\frac{d\\chi^{2}}{da}=\\sum_{i=1}^{N}\\frac{1}{\\sigma_{i}^2}\\frac{df(x_{i},a)}{da}[y_{i}-f(x_{i},a)]=0\n\\end{equation}\\]\nThis kind of least squares minimization is done by fitting software using different types of algorithms.\n\nFitting with SciPy\nLet‚Äôs do some fitting using the SciPy library, which is a powerful tool for scientific computing in Python. We will use the curve_fit method from the optimize sub-module of SciPy.\nFirst, we need to define the model function we would like to fit to the data. In this case, we will use our parabola function:\n\n\n\n\n\n\nNext, we need to provide initial guesses for the parameters. These initial guesses help the fitting algorithm start the search for the optimal parameters:\n\n\n\n\n\n\nWe then call the curve_fit function to perform the fitting:\n\n\n\n\n\n\n\n\n\n\n\n\ncurve_fit Function\n\n\n\n\n\nThe curve_fit function is used to fit a model function to data. It finds the optimal parameters for the model function that minimize the sum of the squared residuals between the observed data and the model‚Äôs predictions.\n\nParameters\n\nparabola:\n\nThis is the model function that you want to fit to the data. In this case, parabola is a function that represents a quadratic equation of the form ( y = ax^2 + bx + c ).\n\nx_data:\n\nThis is the array of independent variable data points (the x-values).\n\ny_data:\n\nThis is the array of dependent variable data points (the y-values).\n\nsigma=err:\n\nThis parameter specifies the uncertainties (standard deviations) of the y-data points. The err array contains the uncertainties for each y-data point. These uncertainties are used to weight the residuals in the least squares optimization.\n\np0=init_guess:\n\nThis parameter provides the initial guesses for the parameters of the model function. The init_guess array contains the initial guesses for the parameters ( a ), ( b ), and ( c ). Providing good initial guesses can help the optimization algorithm converge more quickly and accurately.\n\nabsolute_sigma=True:\n\nThis parameter indicates whether the provided sigma values are absolute uncertainties. If absolute_sigma is set to True, the sigma values are treated as absolute uncertainties. If absolute_sigma is set to False, the sigma values are treated as relative uncertainties, and the covariance matrix of the parameters will be scaled accordingly.\n\n\n\n\nReturn Value\nThe curve_fit function returns two values:\n\npopt:\n\nAn array containing the optimal values for the parameters of the model function that minimize the sum of the squared residuals.\n\npcov:\n\nThe covariance matrix of the optimal parameters. The diagonal elements of this matrix provide the variances of the parameter estimates, and the off-diagonal elements provide the covariances between the parameter estimates.\n\n\n\n\n\n\nThe fit variable contains the results of the fitting process. It is composed of various results, which we can split into the fitted parameters and the covariance matrix:\n\n\n\n\n\n\nThe ans variable contains the fitted parameters fit_a, fit_b, and fit_c, while the cov variable contains the covariance matrix. Let‚Äôs have a look at the fit and the \\(\\chi^{2}\\) value first:\n\n\n\n\n\n\nWe can then plot the fitted curve along with the original data points and the \\(\\chi^{2}\\) value:\n\n\n\n\n\n\n\n\n\\(\\chi\\)-squared value\nThe value of \\(\\chi^2\\) gives you a measure of the quality of the fit. We can judge the quality by calculating the expectation value of \\(\\chi^2\\):\n\\[\\begin{equation}\n\\langle \\chi^{2}\\rangle =\\sum_{i=1}^{N} \\frac{\\langle (y_{i}-f(x_{i},a) )^2\\rangle }{\\sigma_{i}^2}=\\sum_{i=1}^{N} \\frac{\\sigma_{i}^2}{\\sigma_{i}^2}=N\n\\end{equation}\\]\nSo, the mean of the least squared deviation increases with the number of data points. Therefore:\n\n\\(\\chi^{2} \\gg N\\) means that the fit is bad.\n\\(\\chi^{2} &lt; N\\) means that the uncertainties are wrong.\n\nThe first case may occur if you don‚Äôt have a good fit to your data, for example, if you are using the wrong model. The second case typically occurs if you don‚Äôt have accurate estimates of the uncertainties and you assume all uncertainties to be constant.\nIt is really important to have a good estimate of the uncertainties and to include them in your fit. If you include the uncertainties in your fit, it is called a weighted fit. If you don‚Äôt include the uncertainties (meaning you keep them constant), it is called an unweighted fit.\nFor our fit above, we obtain a \\(\\chi^{2}\\) which is on the order of \\(N=10\\), which tells you that I have created the data with reasonable accuracy.\n\n\nResiduals\nAnother way to assess the quality of the fit is by looking at the residuals. Residuals are defined as the deviation of the data from the model for the best fit:\n\\[\\begin{equation}\nr_i = y_i - f(x_{i},a)\n\\end{equation}\\]\nThe residuals can also be expressed as the percentage of the deviation of the data from the fit:\n\\[\\begin{equation}\nr_i = 100 \\left( \\frac{y_i - f(x_{i},a)}{y_i} \\right)\n\\end{equation}\\]\n\n\nImportance of Residuals\nResiduals are important because they provide insight into how well the model fits the data. If the residuals show only statistical fluctuations around zero, then the fit and likely also the model are good. However, if there are systematic patterns in the residuals, it may indicate that the model is not adequately capturing the underlying relationship in the data.\n\n\nVisualizing Residuals\nLet‚Äôs visualize the residuals to better understand their distribution. We will plot the residuals as a function of the independent variable \\(x\\).\n\n\n\n\n\n\n\n\n\n\n\n\nCommon Patterns in Residuals\n\n\n\n\n\nRandom Fluctuations Around Zero:\n\nIf the residuals are randomly scattered around zero, it suggests that the model is a good fit for the data.\n\nSystematic Patterns:\n\nIf the residuals show a systematic pattern (e.g., a trend or periodicity), it may indicate that the model is not capturing some aspect of the data. This could suggest the need for a more complex model.\n\nIncreasing or Decreasing Trends:\n\nIf the residuals increase or decrease with \\(x\\), it may indicate heteroscedasticity (non-constant variance) or that a different functional form is needed."
  },
  {
    "objectID": "lectures/lecture07/1_curve_fitting.html#covariance-matrix",
    "href": "lectures/lecture07/1_curve_fitting.html#covariance-matrix",
    "title": "Curve fitting",
    "section": "Covariance Matrix",
    "text": "Covariance Matrix\nIn the previous sections, we discussed how to fit a model to experimental data and assess the quality of the fit using residuals. Now, let‚Äôs take a closer look at the uncertainties in the fit parameters and how they are related to each other. This is where the covariance matrix comes into play.\n\nPurpose of the Covariance Matrix\nThe covariance matrix provides important information about the uncertainties in the fit parameters and how these uncertainties are related to each other. It helps us understand the precision of the parameter estimates and whether the parameters are independent or correlated.\n\n\nUnderstanding Covariance\nCovariance is a measure of how much two random variables change together. If the covariance between two variables is positive, it means that they tend to increase or decrease together. If the covariance is negative, it means that one variable tends to increase when the other decreases. If the covariance is zero, it means that the variables are independent.\n\n\nCovariance Matrix in Curve Fitting\nWhen we fit a model to data, we obtain estimates for the parameters of the model. These estimates have uncertainties due to the measurement errors in the data. The covariance matrix quantifies these uncertainties and the relationships between them.\nFor a model with three parameters \\((a, b, c)\\), the covariance matrix is a \\(3 \\times 3\\) matrix that looks like this:\n\\[\\begin{equation}\n{\\rm cov}(p_{i}, p_{j}) =\n\\begin{bmatrix}\n\\sigma_{aa}^{2} & \\sigma_{ab}^{2} & \\sigma_{ac}^{2} \\\\\n\\sigma_{ba}^{2} & \\sigma_{bb}^{2} & \\sigma_{bc}^{2} \\\\\n\\sigma_{ca}^{2} & \\sigma_{cb}^{2} & \\sigma_{cc}^{2}\n\\end{bmatrix}\n\\end{equation}\\]\nThe diagonal elements provide the variances (squared uncertainties) of the fit parameters, while the off-diagonal elements describe the covariances between the parameters.\n\n\nExample\nLet‚Äôs calculate the covariance matrix for our fitted model and interpret the results.\n\n\n\n\n\n\n\n\nInterpreting the Covariance Matrix\nThe covariance matrix provides valuable information about the uncertainties in the fit parameters:\n\nDiagonal Elements: The diagonal elements represent the variances of the parameters. The square root of these values gives the standard deviations (uncertainties) of the parameters.\nOff-Diagonal Elements: The off-diagonal elements represent the covariances between the parameters. If these values are large, it indicates that the parameters are correlated.\n\n\n\nGenerating Synthetic Data\nTo better understand the covariance matrix, let‚Äôs generate synthetic data and fit the model to each dataset. This will help us visualize the uncertainties in the parameters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation Matrix\nTo better understand the relationships between the parameters, we can normalize the covariance matrix to obtain the correlation matrix. The correlation matrix has values between -1 and 1, where 1 indicates perfect positive correlation, -1 indicates perfect negative correlation, and 0 indicates no correlation.\n\n\n\n\n\n\n\n\nVisualizing the Covariance and Correlation\nLet‚Äôs visualize the covariance and correlation between the parameters using scatter plots.\n\n\n\n\n\n\nBy examining the covariance and correlation matrices, we can gain a deeper understanding of the uncertainties in the fit parameters and how they are related to each other.\n\n\nImproving the Model\nIf we find that the parameters are highly correlated, we might want to find a better model containing more independent parameters. For example, we can write down a different model:\n\\[\\begin{equation}\ny = a(x - b)^2 + c\n\\end{equation}\\]\nThis model also contains three parameters, but the parameter \\(b\\) directly refers to the maximum of our parabola, while the parameter \\(a\\) denotes its curvature.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe see from the covariance matrix that the new model has a smaller correlation of the parameters with each other.\n\n\n\n\n\n\nThis is also expressed by our correlation matrix.\n\n\n\n\n\n\nBy examining the covariance and correlation matrices, we can gain valuable insights into the uncertainties in the fit parameters and how to improve our model."
  },
  {
    "objectID": "lectures/lecture09/3_fourier_analysis.html",
    "href": "lectures/lecture09/3_fourier_analysis.html",
    "title": "Fourier Analysis",
    "section": "",
    "text": "Fourier analysis, or the description of functions as a series of sine and cosine functions, serves as a powerful tool in both numerical data analysis and the solution of differential equations. In experimental physics, Fourier transforms find widespread applications. For instance, optical tweezers utilize frequency spectra to characterize positional fluctuations, while Lock-In detection employs Fourier analysis for specific frequency signals. Additionally, many optical phenomena can be understood through the lens of Fourier transforms.\nFourier analysis extends far beyond these examples, finding applications across numerous fields of physics and engineering. In this lecture, we will examine Fourier Series and Fourier transforms from a mathematical perspective. We will apply these concepts to analyze the frequency spectrum of oscillations in coupled pendula, and later revisit them when simulating the motion of a Gaussian wavepacket in quantum mechanics."
  },
  {
    "objectID": "lectures/lecture09/3_fourier_analysis.html#fourier-series",
    "href": "lectures/lecture09/3_fourier_analysis.html#fourier-series",
    "title": "Fourier Analysis",
    "section": "Fourier series",
    "text": "Fourier series\nA Fourier series represents a periodic function \\(f(t)\\) with period \\(2\\pi\\) or, more generally, any arbitrary interval \\(T\\) as a sum of sine and cosine functions:\n\\[\nf(t)=\\frac{A_{0}}{2}+\\sum_{k=1}^{\\infty}\\left ( A_{k}\\cos\\left (\\omega_k t\\right) + B_{k}\\sin\\left (\\omega_k t\\right)\\right )\n\\tag{1}\\]\nwhere \\(\\omega_k=\\frac{2\\pi k}{T}\\). Here, \\(T\\) represents the period of the cosine and sine functions, with their amplitudes defined by the coefficients \\(A_k\\) and \\(B_k\\). The term \\(A_0\\) represents a constant offset added to the oscillating functions. Equation¬†1 expresses an arbitrary periodic function \\(f(t)\\) on an interval T as a sum of oscillating sine and cosine functions with discrete frequencies (\\(\\omega_k\\)):\n\\[\\begin{equation*}\n\\omega_k= 0, \\frac{2\\pi}{T}, \\frac{4\\pi}{T}, \\frac{6\\pi}{T}, ... , \\frac{n\\pi}{T}\n\\end{equation*}\\]\nand varying amplitudes. We can demonstrate that the cosine and sine functions in the sum (Equation¬†1) are orthogonal using the trigonometric identity:\n\\[\\begin{equation}\n\\sin(\\omega_{i} t)\\sin(\\omega_{k}t )=\\frac{1}{2}\\lbrace\\cos((\\omega_{i}-\\omega_{k})t)- \\cos((\\omega_{i}+\\omega_{k})t\\rbrace\n\\end{equation}\\]\nThis leads to the integral:\n\\[\\begin{equation}\n\\int\\limits_{-\\frac{T}{2}}^{+\\frac{T}{2}}  \\sin(\\omega_{i}t)\\sin (\\omega_k t) dt\n\\end{equation}\\]\nwhich splits into two integrals over cosine functions with sum \\((\\omega_{1}+\\omega_{2})\\) and difference frequency \\((\\omega_{1}-\\omega_{2})\\). With \\(\\omega_k=k 2\\pi/T\\), \\((k \\in \\mathbb{Z}^+ )\\), this evaluates to:\n\\[\\begin{equation}\n\\int\\limits_{-\\frac{T}{2}}^{+\\frac{T}{2}}  \\sin(\\omega_{i}t)\\sin (\\omega_k t) dt  =\\begin{cases}\n0 &\\text{for }  i\\neq k, \\\\\nT/2 &\\text{for }  i=k\n\\end{cases}\n\\end{equation}\\]\nA similar result holds for cosine functions:\n\\[\\begin{equation}\n\\int\\limits_{-\\frac{T}{2}}^{+\\frac{T}{2}}  \\cos(\\omega_{i}t)\\cos (\\omega_k t) dt  =\\begin{cases}\n0 &\\text{for }  i\\neq k, \\\\\nT/2 &\\text{for }  i=k\n\\end{cases}\n\\end{equation}\\]\nThe coefficients \\(A_k\\) and \\(B_k\\) are determined by projecting the function \\(f(t)\\) onto these basis functions:\n\\[\\begin{align}\n\\int\\limits_{-\\frac{T}{2}}^{+\\frac{T}{2}} & \\cos (\\omega_k t) dt  =\\begin{cases}\n0 &\\text{for }  k\\neq0, \\\\\nT &\\text{for }  k=0\n\\end{cases} \\\\\n\\int\\limits_{-\\frac{T}{2}}^{+\\frac{T}{2}} & \\sin(\\omega_k t) dt=0  \\text{ for all }k\n\\end{align}\\]\nFor the cosine coefficients:\n\\[\\begin{equation}\\label{A_k}\nA_k=\\frac{2}{T}\\int\\limits_{-\\frac{T}{2}}^{+\\frac{T}{2}} f(t)\\cos(\\omega_k t) dt  \\text{ for } k \\neq 0\n\\end{equation}\\]\nand the constant term:\n\\[\\begin{equation}\nA_0= \\frac{1}{T}\\int\\limits_{-\\frac{T}{2}}^{+\\frac{T}{2}} f(t) dt\n\\end{equation}\\]\nFinally, for the sine coefficients:\n\\[\\begin{equation}\\label{B_k}\nB_k=\\frac{2}{T}\\int\\limits_{-\\frac{T}{2}}^{+\\frac{T}{2}} f(t) \\sin(\\omega_k t) dt,\\,  \\forall k\n\\end{equation}\\]"
  },
  {
    "objectID": "lectures/lecture09/3_fourier_analysis.html#fourier-transform",
    "href": "lectures/lecture09/3_fourier_analysis.html#fourier-transform",
    "title": "Fourier Analysis",
    "section": "Fourier transform",
    "text": "Fourier transform\nThe Fourier transform extends the concept of Fourier series by representing arbitrary non-periodic functions \\(f(t)\\) through a continuous spectrum of complex functions \\(\\exp(i\\omega t)\\). Known as the continuous Fourier transform, this approach replaces the discrete frequency sum of sine and cosine functions found in Fourier series with an integral over complex exponential functions \\(\\exp(i\\omega t)\\) spanning continuous frequency values \\(\\omega\\).\nThe Fourier transform of a function \\(f(t)\\) is defined as:\n\\[\nF(\\omega)=\\int\\limits_{-\\infty}^{+\\infty}f(t)e^{-i\\omega t}dt\n\\tag{2}\\]\nwhere \\(F(\\omega)\\) represents the spectrum of frequencies present in \\(f(t)\\). The inverse Fourier transform recovers the original function \\(f(t)\\) from its frequency spectrum (Equation¬†2):\n\\[\\begin{equation}\\label{eq:inverse_FT}\nf(t)=\\frac{1}{2\\pi}\\int\\limits_{-\\infty}^{+\\infty}F(\\omega)e^{+i\\omega t}dt\n\\end{equation}\\]\nThe Fourier transform \\(F(\\omega)\\) yields a complex number, encoding both phase and amplitude information of the oscillations. The phase of oscillation at frequency \\(\\omega\\) is given by:\n\\[\\begin{equation}\n\\phi=\\tan^{-1}\\left(\\frac{Im(F(\\omega))}{Re(F(\\omega))}\\right)\n\\end{equation}\\]\nwhile the amplitude at frequency \\(\\omega\\) is:\n\\[\\begin{equation}\nx_{0}^{\\rm theo}=|F(\\omega)|\n\\end{equation}\\]\nModern computing offers efficient algorithms for Fourier transformation, particularly the Fast Fourier Transform (FFT) implemented in libraries like NumPy. We‚Äôll use these algorithms to identify oscillation patterns in our signals. Here‚Äôs an example of using NumPy‚Äôs FFT functions to compute the transform and generate the corresponding frequency axis:\nf=np.fft.fft(alpha)\nfreq = np.fft.fftfreq(t.shape[-1],time/t.shape[-1])"
  },
  {
    "objectID": "lectures/lecture09/3_fourier_analysis.html#frequency-analysis-of-our-coupled-pendula",
    "href": "lectures/lecture09/3_fourier_analysis.html#frequency-analysis-of-our-coupled-pendula",
    "title": "Fourier Analysis",
    "section": "Frequency analysis of our coupled pendula",
    "text": "Frequency analysis of our coupled pendula\nLet us now apply Fourier analysis to examine the data from our previous simulation of coupled pendula, which includes both normal modes and beat mode oscillations of the harmonic oscillator.\n\n\n\n\n\n\nFirst, we extract the time series data and angular displacements into separate arrays for clearer analysis:\n\n\n\n\n\n\nNext, we perform the Fourier transform of our signals and visualize their frequency spectra:\n\n\n\n\n\n\nOur analysis reveals that the beat mode represents a superposition of the system‚Äôs two normal modes. This demonstrates a fundamental principle: any possible state of a coupled oscillator system can be constructed from a superposition of its normal modes with specific amplitudes."
  },
  {
    "objectID": "lectures/lecture08/4_solving_ODEs.html",
    "href": "lectures/lecture08/4_solving_ODEs.html",
    "title": "Solving Ordinary Differential Equations (ODEs)",
    "section": "",
    "text": "This lecture covers methods for solving ordinary differential equations (ODEs), which are fundamental to many physics problems. We‚Äôll explore different numerical approaches, from basic to more sophisticated methods."
  },
  {
    "objectID": "lectures/lecture08/4_solving_ODEs.html#introduction",
    "href": "lectures/lecture08/4_solving_ODEs.html#introduction",
    "title": "Solving Ordinary Differential Equations (ODEs)",
    "section": "",
    "text": "This lecture covers methods for solving ordinary differential equations (ODEs), which are fundamental to many physics problems. We‚Äôll explore different numerical approaches, from basic to more sophisticated methods."
  },
  {
    "objectID": "lectures/lecture08/4_solving_ODEs.html#the-harmonic-oscillator",
    "href": "lectures/lecture08/4_solving_ODEs.html#the-harmonic-oscillator",
    "title": "Solving Ordinary Differential Equations (ODEs)",
    "section": "The Harmonic Oscillator",
    "text": "The Harmonic Oscillator\n\n\n\n\n\n\nThe Classical Harmonic Oscillator\n\n\n\nThe harmonic oscillator represents one of the most important physical systems, appearing in: - Mechanical oscillations (springs, pendulums) - Electrical circuits (LC circuits) - Quantum mechanics (quantum harmonic oscillator) - Molecular vibrations\nThe equation of motion is:\n\\[\\begin{equation}\n\\frac{d^2x}{dt^2} + \\omega^2 x = 0\n\\end{equation}\\]\nwhere: - \\(x\\) is the displacement - \\(t\\) is time - \\(\\omega = \\sqrt{k/m}\\) is the angular frequency - \\(k\\) is the spring constant - \\(m\\) is the mass\nInitial conditions required: - Initial position: \\(x(t=0) = x_0\\) - Initial velocity: \\(\\dot{x}(t=0) = v_0\\)"
  },
  {
    "objectID": "lectures/lecture08/4_solving_ODEs.html#numerical-solution-methods",
    "href": "lectures/lecture08/4_solving_ODEs.html#numerical-solution-methods",
    "title": "Solving Ordinary Differential Equations (ODEs)",
    "section": "Numerical Solution Methods",
    "text": "Numerical Solution Methods\n\n1. Implicit Solution (Crank-Nicolson Method)\nThe matrix approach transforms our second-order ODE into a system of coupled equations. This method is particularly stable for oscillatory systems.\n\nMatrix Construction\nFor \\(N\\) time points, we construct two matrices:\n\nThe second derivative matrix (\\(T\\)):\n\n\\[\\begin{equation}\nT=\\frac{1}{\\delta t^2}\n\\begin{bmatrix}\n-2 & 1  & 0 & \\cdots & 0\\\\\n1 & -2 & 1 & \\cdots & 0\\\\\n\\vdots & \\ddots & \\ddots & \\ddots & \\vdots\\\\\n0 & \\cdots & 1 & -2 & 1\\\\\n0 & \\cdots & 0 & 1 & -2\n\\end{bmatrix}\n\\end{equation}\\]\n\nThe potential term matrix (\\(V\\)):\n\n\\[\\begin{equation}\nV = \\omega^2\n\\begin{bmatrix}\n1 & 0 & \\cdots & 0\\\\\n0 & 1 & \\cdots & 0\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n0 & 0 & \\cdots & 1\n\\end{bmatrix}\n\\end{equation}\\]"
  },
  {
    "objectID": "lectures/lecture08/4_solving_ODEs.html#explicit-solution-methods",
    "href": "lectures/lecture08/4_solving_ODEs.html#explicit-solution-methods",
    "title": "Solving Ordinary Differential Equations (ODEs)",
    "section": "Explicit Solution Methods",
    "text": "Explicit Solution Methods\n\nState-Space Representation\nTo implement explicit numerical methods effectively, we first convert our second-order ODE into a system of first-order equations. This state-space representation is crucial for numerical integration.\nFor the harmonic oscillator:\n\\[\\begin{equation}\n\\ddot{x} + \\omega^2x = 0\n\\end{equation}\\]\nWe define: - Position: \\(x\\) - Velocity: \\(v = \\dot{x}\\)\nThis gives us the system:\n\\[\\begin{equation}\n\\begin{bmatrix} \\dot{x} \\\\ \\dot{v} \\end{bmatrix} =\n\\begin{bmatrix} v \\\\ -\\omega^2x \\end{bmatrix}\n\\end{equation}\\]\nOur state vector is:\n\\[\\begin{equation}\ny = \\begin{bmatrix} x \\\\ v \\end{bmatrix}\n\\end{equation}\\]\n\n\n1. Euler Method\nThe Euler method is the simplest numerical integration technique. It comes directly from the Taylor expansion:\n\\[\\begin{equation}\ny(t + \\Delta t) = y(t) + \\dot{y}(t)\\Delta t + O(\\Delta t^2)\n\\end{equation}\\]\n\n\n\n\n\n\n\n\n2. Euler-Cromer Method\nThe Euler-Cromer method (also known as the semi-implicit Euler method) is particularly good for oscillatory systems because it conserves energy better than the standard Euler method.\nKey difference: - Uses the updated velocity to compute position - Better energy conservation for oscillatory systems\n\\[\\begin{align}\nv_{i+1} &= v_i - \\omega^2 x_i \\Delta t \\\\\nx_{i+1} &= x_i + v_{i+1} \\Delta t\n\\end{align}\\]\n\n\n\n\n\n\n\n\n3. Velocity Verlet Method\nThe Velocity Verlet method is a symplectic integrator that provides excellent energy conservation for Hamiltonian systems. It‚Äôs particularly useful for molecular dynamics simulations.\nThe algorithm: 1. Update position using current velocity and acceleration 2. Calculate new acceleration at new position 3. Update velocity using average of old and new accelerations\n\\[\\begin{align}\nx_{i+1} &= x_i + v_i\\Delta t + \\frac{1}{2}a_i\\Delta t^2 \\\\\na_{i+1} &= -\\omega^2 x_{i+1} \\\\\nv_{i+1} &= v_i + \\frac{1}{2}(a_i + a_{i+1})\\Delta t\n\\end{align}\\]\n\n\n\n\n\n\n\n\nComparison of Methods\nLet‚Äôs compare these methods for the harmonic oscillator:\n\n\n\n\n\n\n\n\n\n\n\n\nMethod Characteristics\n\n\n\n\nEuler Method:\n\nSimplest method\nFirst-order accurate (\\(O(\\Delta t)\\))\nOften unstable for oscillatory systems\nEnergy tends to increase over time\n\nEuler-Cromer Method:\n\nBetter energy conservation\nStill first-order accurate\nMore stable for oscillatory systems\nEnergy tends to decrease slightly over time\n\nVelocity Verlet Method:\n\nSecond-order accurate (\\(O(\\Delta t^2)\\))\nExcellent energy conservation\nSymplectic (preserves phase space volume)\nRecommended for long-time integration"
  },
  {
    "objectID": "lectures/lecture08/4_solving_ODEs.html#solving-odes-with-scipy",
    "href": "lectures/lecture08/4_solving_ODEs.html#solving-odes-with-scipy",
    "title": "Solving Ordinary Differential Equations (ODEs)",
    "section": "Solving ODEs with SciPy",
    "text": "Solving ODEs with SciPy\nSciPy provides sophisticated ODE solvers through scipy.integrate.odeint and scipy.integrate.solve_ivp. These implementations use advanced algorithms with automatic step size adjustment and error control.\n\nUsing scipy.integrate.odeint\nThe odeint function uses the LSODA algorithm from the FORTRAN library ODEPACK, which automatically switches between methods for stiff and non-stiff problems.\n\n\n\n\n\n\nStiff vs Non-stiff Problems\n\n\n\n\nStiff problems: Have multiple timescales with widely different magnitudes\nNon-stiff problems: Have timescales of similar magnitude\n\nLSODA uses: - Adams method for non-stiff problems - BDF method (Backward Differentiation Formula) for stiff problems\n\n\n\n\n\n\n\n\n\n\nUsing scipy.integrate.solve_ivp\nThe newer solve_ivp function provides more control over the integration process and supports multiple modern solving methods.\n\n\n\n\n\n\n\n\n\n\n\n\nAvailable Methods in solve_ivp\n\n\n\n\nRK45 (default):\n\nExplicit Runge-Kutta method of order 5(4)\nGood general-purpose method\nAdaptive step size\n\nRK23:\n\nExplicit Runge-Kutta method of order 3(2)\nUsually faster but less accurate than RK45\nGood for rough solutions\n\nDOP853:\n\nExplicit Runge-Kutta method of order 8\nHigh accuracy\nMore expensive computationally\n\nBDF:\n\nImplicit method\nGood for stiff problems\nVariable order (1 to 5)\n\nLSODA:\n\nAutomatic method switching\nAdapts between Adams and BDF\nGood all-purpose solver\n\n\n\n\n\n\nAdvantages of SciPy Methods\n\nAdaptive Step Size:\n\nAutomatically adjusts step size for efficiency\nMaintains desired accuracy\nHandles rapid changes better\n\nError Control:\n\nSpecified through relative and absolute tolerances\nEnsures solution reliability\nProvides error estimates\n\nMethod Selection:\n\nChoose method based on problem characteristics\nAutomatic stiffness detection (LSODA)\nHigher-order methods available\n\nDense Output:\n\nContinuous solution representation\nInterpolation between steps\nEfficient for plotting or further analysis"
  },
  {
    "objectID": "lectures/lecture08/4_solving_ODEs.html#damped-driven-pendulum",
    "href": "lectures/lecture08/4_solving_ODEs.html#damped-driven-pendulum",
    "title": "Solving Ordinary Differential Equations (ODEs)",
    "section": "Damped Driven Pendulum",
    "text": "Damped Driven Pendulum\nThe damped driven pendulum is an excellent example of a nonlinear system that can exhibit both regular and chaotic behavior.\n\n\n\n\n\n\nThe Damped Driven Pendulum Equation\n\n\n\nThe equation of motion is:\n\\[\\begin{equation}\n\\ddot{\\theta} + \\frac{g}{L}\\sin(\\theta) + b\\dot{\\theta} = \\beta\\cos(\\omega t)\n\\end{equation}\\]\nwhere: - \\(\\theta\\) is the angle from vertical - \\(g\\) is gravitational acceleration - \\(L\\) is pendulum length - \\(b\\) is damping coefficient - \\(\\beta\\) is driving amplitude - \\(\\omega\\) is driving frequency\n\n\n\n\n\n\n\n\n\nParameter Study: Transition to Chaos\nLet‚Äôs examine how the system behavior changes with driving amplitude:\n\n\n\n\n\n\n\n\n\n\n\n\nKey Features of the Damped Driven Pendulum\n\n\n\n\nRegular Motion:\n\nSmall driving forces lead to periodic motion\nSystem settles into a stable orbit\nPredictable long-term behavior\n\nChaotic Motion:\n\nLarger driving forces can lead to chaos\nSensitive dependence on initial conditions\nUnpredictable long-term behavior\n\nBifurcations:\n\nSystem can transition between different types of motion\nCritical points where behavior changes qualitatively\nPeriod doubling route to chaos\n\nEnergy Balance:\n\nDriving force adds energy\nDamping removes energy\nCompetition leads to rich dynamics\n\n\n\n\n\n\nEnergy Analysis\nLet‚Äôs analyze the system‚Äôs energy over time:\n\n\n\n\n\n\nThis completes our analysis of the damped driven pendulum, demonstrating its rich dynamical behavior and various analysis techniques. The system serves as an excellent example of how nonlinearity can lead to complex behavior in even seemingly simple mechanical systems."
  },
  {
    "objectID": "lectures/lecture08/3_solving_ODEs.html",
    "href": "lectures/lecture08/3_solving_ODEs.html",
    "title": "Solving ODEs",
    "section": "",
    "text": "In the previous lecture on numerical differentiation, we learned how to calculate derivatives numerically‚Äîapproximating rates of change from discrete data points using finite difference formulas like the central difference \\(f'_i \\approx \\frac{f_{i+1} - f_{i-1}}{2\\Delta x}\\). Now we face the conceptually inverse problem: solving Ordinary Differential Equations (ODEs), where we know the relationship between a function and its derivatives, and we need to find the function itself. This is one of the most powerful tools in computational physics, allowing us to predict how physical systems evolve in time.\nODEs describe the dynamics of nearly every physical system you‚Äôll encounter. From the motion of planets to the decay of radioactive nuclei, from oscillating springs to quantum wavefunctions, differential equations capture how systems change. Learning to solve them numerically opens the door to understanding complex phenomena that lack analytical solutions‚Äîwhich includes most real-world physics problems.",
    "crumbs": [
      "‚ö° Week 5: Classical Mechanics 2",
      "Tool: Solving ODEs"
    ]
  },
  {
    "objectID": "lectures/lecture08/3_solving_ODEs.html#introduction-what-are-odes",
    "href": "lectures/lecture08/3_solving_ODEs.html#introduction-what-are-odes",
    "title": "Solving ODEs",
    "section": "Introduction: What are ODEs?",
    "text": "Introduction: What are ODEs?\n\nODEs in Physics: The Language of Dynamics\nThe fundamental laws governing physical systems are typically expressed as differential equations‚Äîrelationships involving derivatives that describe how quantities change. These aren‚Äôt arbitrary mathematical constructs; they emerge naturally from conservation laws and fundamental principles.\nConsider Newton‚Äôs Second Law, which states that force equals mass times acceleration: \\[F = ma = m\\frac{d^2x}{dt^2}\\] This is an ODE relating position to its second time derivative. Given the forces acting on a system and its initial position and velocity, this equation determines all future motion.\nA classic example is the Simple Harmonic Oscillator describing a mass on a spring. The restoring force \\[F = -kx\\] leads to the equation of motion \\[m\\frac{d^2x}{dt^2} = -kx\\] or equivalently \\[\\frac{d^2x}{dt^2} + \\omega^2 x = 0\\] where \\(\\4omega = \\sqrt{k/m}\\)$ is the natural angular frequency. This same equation appears in electrical circuits (LC oscillators), quantum mechanics (harmonic potential), and countless other contexts.\nFor more realistic systems, we might have a Damped Driven Pendulum with the equation \\[\\frac{d^2\\theta}{dt^2} = -\\frac{g}{L}\\sin(\\theta) - b\\frac{d\\theta}{dt} + \\beta\\cos(\\omega t)\\]. This incorporates the gravitational restoring torque (with \\(\\sin\\theta\\) making it nonlinear), friction-like damping proportional to angular velocity, and an external periodic driving force. Such systems can exhibit rich behavior including resonance and even chaos.\nThese are all Ordinary Differential Equations (ODEs)‚Äî‚Äúordinary‚Äù because they involve derivatives with respect to a single independent variable (usually time \\(t\\)), unlike partial differential equations which involve multiple variables like both space and time.\n\n\nFrom Derivatives to ODEs: Reversing the Problem\nIn the last lecture on numerical differentiation, we learned how to calculate derivatives numerically‚Äîgiven a set of data points \\(\\{x_i\\}\\), we approximated the derivatives \\(\\frac{dx}{dt}\\) using finite difference formulas. We saw how the forward difference \\(f'_i \\approx \\frac{f_{i+1} - f_i}{\\Delta x}\\) and the more accurate central difference \\(f'_i \\approx \\frac{f_{i+1} - f_{i-1}}{2\\Delta x}\\) allow us to compute slopes from discrete data. Now we face the conceptually opposite problem: we know the relationship between a function and its derivatives (the ODE), and we need to find the function itself.\n\n\n\n\n\n\nConnection to Numerical Differentiation\n\n\n\nThese two lectures form a complementary pair:\nDifferentiation (Previous Lecture): Given function values \\(f(x)\\), compute derivatives \\(f'(x)\\) using finite differences.\nIntegration (This Lecture): Given derivative relationships \\(\\frac{dx}{dt} = f(x,t)\\), compute function values \\(x(t)\\).\nThe mathematical tools are deeply connected. The finite difference formulas we derived (forward, central, backward) will reappear inside the ODE solvers as building blocks. The Taylor series analysis we used to derive central differences will help us understand integration methods. Even the matrix representation of derivatives will connect to an alternative approach for solving ODEs.\nThink of it this way: differentiation extracts velocity from position data, while ODE integration computes position trajectories from velocity equations. They‚Äôre inverse operations, and understanding both gives you complete mastery over the discrete calculus needed for computational physics.\n\n\nMore precisely, we‚Äôre given a differential equation like \\[\\frac{d^2x}{dt^2} = f(x, \\frac{dx}{dt}, t)\\] along with initial conditions specifying \\(x(t_0)\\) and \\(\\frac{dx}{dt}|_{t_0}\\)$. From this information, we must find the complete solution \\(x(t)\\) for all future (and possibly past) times. This process is called solving or integrating the differential equation.\nThis is fundamentally an initial value problem: knowing the state of a system at one instant, we use the laws of physics (encoded in the ODE) to predict its evolution. It‚Äôs analogous to how, in classical mechanics, specifying position and velocity at one moment determines the entire trajectory.\n\n\nConverting to First-Order Systems\nMost numerical methods work with first-order differential equations. A second-order equation like:\n\\[\\frac{d^2x}{dt^2} = f(x, \\frac{dx}{dt}, t)\\]\ncan be converted to a system of two first-order equations by introducing velocity as a separate variable:\n\\[\\vec{y} = \\begin{bmatrix} x \\\\ v \\end{bmatrix}\\]\nThen: \\[\\frac{d\\vec{y}}{dt} = \\begin{bmatrix} \\frac{dx}{dt} \\\\ \\frac{dv}{dt} \\end{bmatrix} = \\begin{bmatrix} v \\\\ f(x, v, t) \\end{bmatrix}\\]\nFor example, the harmonic oscillator becomes:\n\\[\\frac{d}{dt}\\begin{bmatrix} x \\\\ v \\end{bmatrix} = \\begin{bmatrix} v \\\\ -\\omega^2 x \\end{bmatrix}\\]\n\n\n\n\n\n\nWhy Convert to First-Order?\n\n\n\nStandard numerical ODE solvers are designed to work with first-order systems‚Äîequations of the form \\[\\frac{d\\vec{y}}{dt} = \\vec{f}(\\vec{y}, t)\\] where \\(\\vec{y}\\) is a vector containing all the variables describing the system‚Äôs state. This might seem restrictive since many physics equations are second-order (like Newton‚Äôs law), but converting to first-order form is straightforward and reveals the system‚Äôs structure more clearly.\nThe conversion is conceptually natural: in classical mechanics, completely specifying a system requires both positions and velocities (or momenta). These form the system‚Äôs state space or phase space. By explicitly including both \\(x\\) and \\(v\\) as separate variables in our first-order system, we‚Äôre making this structure explicit. Once in first-order form, we can apply powerful, well-tested numerical algorithms that have been optimized over decades of research in numerical analysis.",
    "crumbs": [
      "‚ö° Week 5: Classical Mechanics 2",
      "Tool: Solving ODEs"
    ]
  },
  {
    "objectID": "lectures/lecture08/3_solving_ODEs.html#the-practical-tool-scipys-odeint",
    "href": "lectures/lecture08/3_solving_ODEs.html#the-practical-tool-scipys-odeint",
    "title": "Solving ODEs",
    "section": "The Practical Tool: SciPy‚Äôs odeint",
    "text": "The Practical Tool: SciPy‚Äôs odeint\nFor most physics problems, we don‚Äôt need to implement numerical integration from scratch. Python‚Äôs SciPy library provides excellent ODE solvers.\n\nBasic Usage: The odeint Function\nThe main tool we‚Äôll use is scipy.integrate.odeint, which provides a simple yet powerful interface for solving ODEs:\nfrom scipy.integrate import odeint\n\nsolution = odeint(derivative_function, initial_conditions, time_points)\nUnderstanding the parameters is crucial. The derivative_function is where you encode the physics‚Äîit‚Äôs a Python function that takes the current state \\(\\vec{y}\\) and time \\(t\\), and returns the derivatives \\(\\frac{d\\vec{y}}{dt}\\) according to your ODE. The initial_conditions specify where the system starts‚Äîtypically a vector like \\([x_0, v_0]\\) containing initial position and velocity. The time_points argument is an array of times where you want the solution computed‚Äîodeint automatically handles the intermediate time-stepping needed to reach these points accurately.\nThe function returns a 2D array where each row represents the system‚Äôs state at one of the requested time points. If you solve for 1000 time points with a 2-component state vector, you‚Äôll get a 1000√ó2 array. The first column might be position, the second velocity, and each row corresponds to one moment in time.\n\n\nExample 1: Simple Harmonic Oscillator\nLet‚Äôs solve the harmonic oscillator equation:\n\\[\\frac{d^2x}{dt^2} + \\omega^2 x = 0\\]\nStep 1: Convert to first-order system\n\\[\\frac{d}{dt}\\begin{bmatrix} x \\\\ v \\end{bmatrix} = \\begin{bmatrix} v \\\\ -\\omega^2 x \\end{bmatrix}\\]\nStep 2: Define the derivative function\n\n\n\n\n\n\nStep 3: Set up and solve\n\n\n\n\n\n\nStep 4: Plot the results\n\n\n\n\n\n\nStep 5: Phase space diagram\nA useful way to visualize oscillatory systems is in phase space (position vs velocity):\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the Phase Space\n\n\n\nThe circular trajectory in phase space reveals fundamental properties of the harmonic oscillator. Each point on this circle represents a different state \\((x, v)\\)‚Äîa unique combination of position and velocity. As the system oscillates, it traces this closed loop repeatedly, returning to its starting point after one period.\nThe circular shape reflects energy conservation. The total energy \\(E = \\frac{1}{2}mv^2 + \\frac{1}{2}kx^2\\) remains constant, which describes an ellipse (or circle if we scale the axes appropriately) in the \\(x\\)-\\(v\\) plane. Systems with different initial energies trace different circles‚Äîhigher energy means larger amplitude oscillations and larger circles. If we added damping, the trajectory would spiral inward as energy dissipates, eventually settling at the origin (the equilibrium point where \\(x=0\\) and \\(v=0\\)).\nPhase space diagrams are powerful tools in physics, revealing qualitative system behavior at a glance: periodic motion appears as closed loops, damped motion spirals inward, and chaotic motion fills regions without repeating.\n\n\n\n\nExample 2: Damped Driven Pendulum\nNow let‚Äôs tackle a more complex problem - a damped driven pendulum:\n\\[\\frac{d^2\\theta}{dt^2} = -\\frac{g}{L}\\sin(\\theta) - b\\frac{d\\theta}{dt} + \\beta\\cos(\\omega t)\\]\nThis includes:\n\nRestoring force: \\(-\\frac{g}{L}\\sin(\\theta)\\) (nonlinear!)\nDamping: \\(-b\\frac{d\\theta}{dt}\\) (friction)\nDriving force: \\(\\beta\\cos(\\omega t)\\) (external periodic force)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Parameter Space\n\n\n\nThe beauty of computational physics is the ease of exploring how systems respond to different parameters. Try modifying the code to see various phenomena. Increasing the damping coefficient b produces stronger dissipation, causing oscillations to die out more quickly as energy is removed from the system. This models increasing air resistance or friction.\nChanging the driving frequency omega_drive reveals resonance effects‚Äîwhen the driving frequency matches the natural frequency, even small driving amplitudes can produce large oscillations as energy accumulates in phase with the system‚Äôs natural motion. This is why bridges can collapse from periodic forces and why opera singers can shatter wine glasses.\nIncreasing the driving amplitude beta, especially combined with large initial angles where the \\(\\sin\\theta \\approx \\theta\\) approximation breaks down, can push the pendulum into chaotic regimes. In this regime, tiny changes in initial conditions lead to drastically different long-term behavior, making prediction impossible beyond a certain time horizon despite the system being completely deterministic. The phase space trajectory becomes a tangled, never-repeating path‚Äîa hallmark of chaos.",
    "crumbs": [
      "‚ö° Week 5: Classical Mechanics 2",
      "Tool: Solving ODEs"
    ]
  },
  {
    "objectID": "lectures/lecture08/3_solving_ODEs.html#summary-the-ode-solving-workflow",
    "href": "lectures/lecture08/3_solving_ODEs.html#summary-the-ode-solving-workflow",
    "title": "Solving ODEs",
    "section": "Summary: The ODE-Solving Workflow",
    "text": "Summary: The ODE-Solving Workflow\nSolving ODEs computationally follows a systematic workflow that bridges physics and programming. First, you write down the physics governing your system‚ÄîNewton‚Äôs laws, conservation principles, empirical relations, or whatever equations capture the phenomenon you‚Äôre studying. Second, you convert any higher-order equations into a first-order system by introducing additional variables (like treating velocity as independent from position). Third, you implement a derivative function in Python that takes the current state and time and returns the rates of change according to your equations.\nFourth, you specify initial conditions‚Äîthe state of your system at time \\(t=0\\). In classical mechanics, this means both positions and velocities; knowing just position isn‚Äôt enough to predict the future. Fifth, you call odeint with your derivative function, initial conditions, and an array of times where you want the solution. The function handles all the numerical complexities automatically. Finally, you visualize the results through plots of trajectories versus time, phase space diagrams showing state evolution, or animations showing the system‚Äôs motion.\nThis workflow applies universally, whether you‚Äôre simulating planetary orbits, chemical reaction kinetics, epidemic spread, or quantum systems. Master this pattern and you can tackle an enormous range of physics problems.\nThe connection to numerical differentiation: Notice how solving ODEs is fundamentally the inverse of what we did in the previous lecture. There, we used finite differences to approximate \\(\\frac{dx}{dt}\\) from known values of \\(x(t)\\). Here, we use knowledge of \\(\\frac{dx}{dt}\\) (from the ODE) to construct \\(x(t)\\). The Taylor series that gave us central difference formulas now justifies our time-stepping algorithms. The matrix representations of derivatives reappear in implicit methods. These two lectures form a unified framework for discrete calculus‚Äîthe computational analog of the continuous calculus you learn in mathematics courses.\n\n\n\n\n\n\nWhy Does This Work?\n\n\n\nThe odeint function employs sophisticated numerical methods developed over decades of research in numerical analysis. At its core, it uses adaptive Runge-Kutta methods‚Äîalgorithms that step forward in time by evaluating the derivative function at multiple intermediate points and combining them cleverly to achieve high accuracy.\nThe ‚Äúadaptive‚Äù part is crucial: the algorithm automatically adjusts its step size based on the local behavior of the solution. In smooth regions where the solution changes slowly, it takes large steps for efficiency. In regions where the solution varies rapidly or has strong curvature, it reduces the step size to maintain accuracy. This adaptivity makes the method both accurate and efficient.\nAdditionally, odeint can handle stiff equations‚Äîproblems where some components of the solution change much more rapidly than others, causing simple methods to fail or require impractically small time steps. The underlying algorithms switch between explicit and implicit methods as needed to maintain stability.\nFor the vast majority of physics problems you‚Äôll encounter, odeint provides an excellent balance of ease-of-use, accuracy, and robustness. You don‚Äôt need to understand all the algorithmic details to use it effectively, but knowing it‚Äôs built on solid numerical foundations helps you trust the results.",
    "crumbs": [
      "‚ö° Week 5: Classical Mechanics 2",
      "Tool: Solving ODEs"
    ]
  },
  {
    "objectID": "lectures/lecture08/3_solving_ODEs.html#advanced-understanding-numerical-integration-methods",
    "href": "lectures/lecture08/3_solving_ODEs.html#advanced-understanding-numerical-integration-methods",
    "title": "Solving ODEs",
    "section": "Advanced: Understanding Numerical Integration Methods",
    "text": "Advanced: Understanding Numerical Integration Methods\n\n\n\n\n\n\nHow odeint Works Under the Hood\n\n\n\n\n\nWhile odeint handles the details for us, it‚Äôs useful to understand the basic concepts of numerical integration. Here we‚Äôll explore simple methods that illustrate the core ideas.\n\nThe Conceptual Idea: Time Stepping\nAll numerical ODE solvers share a common conceptual framework: stepping forward in time from known to unknown. Starting from initial conditions \\(\\vec{y}(t_0)\\) at time \\(t_0\\), we use the differential equation to predict the state at a slightly later time \\(t_1 = t_0 + \\Delta t\\). Then, treating this new state as our starting point, we step again to \\(t_2 = t_1 + \\Delta t\\), continuing this process to build up the complete solution trajectory.\nThe fundamental challenge is determining how to make each individual step. Given the current state \\(\\vec{y}_i\\) at time \\(t_i\\), how do we accurately compute the next state \\(\\vec{y}_{i+1}\\) at time \\(t_{i+1} = t_i + \\Delta t\\)? Different numerical methods answer this question in different ways, trading off between simplicity, accuracy, and computational cost. Let‚Äôs explore some fundamental approaches to understand what odeint is doing behind the scenes.\n\n\nEuler Method: The Simplest Approach\nThe Euler method is the most straightforward time-stepping algorithm, based directly on the Taylor expansion‚Äîthe same mathematical tool we used in the differentiation lecture to derive the central difference formula. Recall that we can expand any smooth function around a point using Taylor series:\n\\[\\vec{y}(t+\\Delta t) = \\vec{y}(t) + \\frac{d\\vec{y}}{dt}\\Delta t + \\frac{1}{2}\\frac{d^2\\vec{y}}{dt^2}\\Delta t^2 + \\mathcal{O}(\\Delta t^3)\\]\nThe Euler method makes a bold simplification: it keeps only the first two terms, dropping all the higher-order terms involving \\(\\Delta t^2\\) and beyond. Since we know \\(\\frac{d\\vec{y}}{dt} = \\vec{f}(\\vec{y}, t)\\) from our ODE, this truncation gives us:\n\\[\\vec{y}_{i+1} \\approx \\vec{y}_i + \\vec{f}(\\vec{y}_i, t_i)\\Delta t\\]\nNotice the deep connection to the differentiation lecture: if we rearrange this as \\(\\frac{\\vec{y}_{i+1} - \\vec{y}_i}{\\Delta t} \\approx \\vec{f}(\\vec{y}_i, t_i)\\), we recognize the forward difference formula for the derivative! Euler‚Äôs method is essentially using the forward difference approximation in reverse‚Äîinstead of computing the derivative from function values, we‚Äôre computing future function values from the derivative. Geometrically, this means following the tangent line for a short distance‚Äîusing the slope at the current point to extrapolate where we‚Äôll be after a small time step. It‚Äôs like driving by looking only at your current heading without anticipating curves in the road ahead.\nImplementation: The beauty of Euler‚Äôs method lies in its simplicity‚Äîjust one line of code:\ndef euler_step(y, t, dt, derivative_func):\n    \"\"\"Single step of Euler method\"\"\"\n    return y + derivative_func(y, t) * dt\nFor the harmonic oscillator, this becomes particularly concrete. Our state vector contains position and velocity, and we update both simultaneously:\n\\[\\begin{bmatrix} x_{i+1} \\\\ v_{i+1} \\end{bmatrix} = \\begin{bmatrix} x_i \\\\ v_i \\end{bmatrix} + \\begin{bmatrix} v_i \\\\ -\\omega^2 x_i \\end{bmatrix} \\Delta t\\]\nAccuracy considerations: The method‚Äôs simplicity comes at a cost. In each individual step, we incur a local error of order \\(\\mathcal{O}(\\Delta t^2)\\)‚Äîthe quadratic and higher terms we dropped from the Taylor expansion (just as we saw with the forward difference formula in the differentiation lecture, which had \\(\\mathcal{O}(\\Delta x^2)\\) error). Over many steps spanning a total time \\(T\\), these errors accumulate, producing a global error of order \\(\\mathcal{O}(\\Delta t)\\). This means to reduce the error by a factor of 10, you must take 10 times as many steps‚Äîa computationally expensive proposition. Moreover, Euler‚Äôs method can artificially inject or remove energy in oscillatory systems, causing solutions to spiral outward or inward when they should maintain constant amplitude.\n\n\nEuler-Cromer Method: Better for Oscillators\nThe Euler-Cromer method (also called the semi-implicit Euler method) makes a subtle but crucial modification to the standard Euler approach. Instead of using the current velocity to update position, it uses the updated velocity that we just calculated:\n\\[v_{i+1} = v_i + a(x_i, v_i, t_i)\\Delta t\\] \\[x_{i+1} = x_i + v_{i+1}\\Delta t \\quad \\text{(note: using } v_{i+1}\\text{, not } v_i\\text{)}\\]\nThis seemingly minor change‚Äîusing \\(v_{i+1}\\) instead of \\(v_i\\) in the position update‚Äîhas dramatic consequences for oscillatory systems. The method becomes symplectic, meaning it preserves the structure of phase space and conserves a modified energy that closely approximates the true energy. While individual trajectories may not be perfectly accurate, the method doesn‚Äôt systematically add or remove energy from the system, preventing the artificial spiraling behavior that plagues standard Euler integration of oscillators.\nThis property makes Euler-Cromer particularly valuable for long-term simulations of conservative systems like planetary orbits or molecular dynamics, where maintaining energy conservation over billions of time steps is crucial.\ndef euler_cromer_step(state, t, dt, k, m):\n    \"\"\"Single step of Euler-Cromer for harmonic oscillator\"\"\"\n    x, v = state\n    v_new = v - (k/m) * x * dt\n    x_new = x + v_new * dt  # Use updated velocity!\n    return np.array([x_new, v_new])\n\n\nComparison: Free Fall Example\nLet‚Äôs compare methods for free fall: \\(\\frac{d^2x}{dt^2} = -g\\)\n\n\n\n\n\n\n\n\nWhy Use odeint Instead?\nWhile understanding simple methods like Euler and Euler-Cromer builds intuition, they have serious limitations for practical work. Their accuracy is inherently limited‚Äîachieving acceptable error typically requires very small time steps \\(\\Delta t\\), making calculations slow. Their stability is problematic‚Äîfor certain types of equations (called ‚Äústiff‚Äù equations), these simple methods can become wildly unstable, producing nonsensical results even with small time steps. Their efficiency is poor because they use uniform time steps, wasting computation in smooth regions while potentially being inaccurate in rapidly-varying regions.\nThe odeint function uses adaptive Runge-Kutta methods that overcome all these limitations. These algorithms automatically adjust the step size based on local error estimates‚Äîtaking large steps where the solution is smooth and small steps where it‚Äôs rapidly changing. This adaptive strategy achieves high accuracy with far fewer derivative evaluations than fixed-step methods would require. The underlying LSODA algorithm can also detect stiff problems and switch to appropriate implicit methods that remain stable.\nFor production code, research, or any work where you trust the results to be correct, always use well-tested library functions like odeint, scipy.integrate.solve_ivp, or equivalent tools in other languages. These have been refined over decades and handle edge cases you might not even think to test for. Implementing your own integrator is valuable for learning, but for serious work, leverage the expertise embedded in these libraries.",
    "crumbs": [
      "‚ö° Week 5: Classical Mechanics 2",
      "Tool: Solving ODEs"
    ]
  },
  {
    "objectID": "lectures/lecture08/3_solving_ODEs.html#advanced-matrix-methods-for-odes",
    "href": "lectures/lecture08/3_solving_ODEs.html#advanced-matrix-methods-for-odes",
    "title": "Solving ODEs",
    "section": "Advanced: Matrix Methods for ODEs",
    "text": "Advanced: Matrix Methods for ODEs\n\n\n\n\n\n\nImplicit/Matrix Method (Crank-Nicolson)\n\n\n\n\n\nIn the numerical differentiation lecture, we explored how derivatives can be represented as matrix operations. We saw that the second derivative could be written as a matrix multiplication using a tridiagonal matrix with entries \\([1, -2, 1]\\) along the diagonals. This matrix perspective leads to an alternative approach for solving ODEs called the implicit or matrix method, which directly parallels the matrix differentiation techniques from the previous lecture.\n\nThe Harmonic Oscillator with Matrices\nRecall the harmonic oscillator equation:\n\\[\\frac{d^2x}{dt^2} + \\omega^2 x = 0\\]\nWe can discretize this using the matrix representation of the second derivative that we developed in the differentiation lecture:\n\\[\\frac{d^2x}{dt^2} \\approx \\frac{1}{\\Delta t^2}\\begin{bmatrix}\n-2 & 1  & 0 & \\cdots \\\\\n1 & -2 & 1 & \\cdots \\\\\n0 & 1  & -2 & \\cdots \\\\\n\\vdots & \\vdots & \\vdots & \\ddots\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1\\\\\nx_2\\\\\nx_3\\\\\n\\vdots\n\\end{bmatrix}\\]\nThis is exactly the same sparse matrix structure we constructed using scipy.sparse.diags in the previous lecture! The pattern \\([1, -2, 1]\\) encodes the finite difference formula \\(f''_i \\approx \\frac{f_{i-1} - 2f_i + f_{i+1}}{\\Delta x^2}\\).\nThis transforms the differential equation into a system of linear equations:\n\\[(T + \\omega^2 V)\\vec{x} = \\vec{b}\\]\nwhere: - \\(T\\) is the second derivative matrix - \\(V\\) is the identity matrix scaled by \\(\\omega^2\\) - \\(\\vec{b}\\) encodes boundary/initial conditions\n\n\nImplementation\n\n\n\n\n\n\n\n\nAdvantages and Disadvantages of Matrix Methods\nMatrix methods offer some conceptual and computational advantages in specific contexts. They solve for the solution at all time points simultaneously rather than stepping forward sequentially, which can provide better stability for certain stiff equations and gives direct access to the entire solution trajectory at once. The approach also makes the mathematical structure more explicit‚Äîyou see the ODE as a linear algebra problem, connecting to the rich theory of matrices and their properties.\nHowever, these advantages come with significant costs. Solving large linear systems is memory-intensive‚Äîstoring a dense \\(N \\times N\\) matrix for \\(N\\) time points quickly becomes impractical for long simulations. The method is also far less flexible for nonlinear equations, which dominate physics problems; you‚Äôd need to linearize or use iterative schemes. Setting up proper boundary and initial conditions in matrix form is more complex than simply specifying initial values for time-stepping methods.\nMatrix methods excel in specialized applications like boundary value problems (where conditions are specified at multiple points, not just initially), certain partial differential equations where spatial discretization naturally leads to matrices (as we‚Äôll see in future lectures on PDEs), and specific linear problems with particular stability requirements. For these niche applications, the simultaneous solution of all time points can be powerful. The matrix differentiation techniques from the previous lecture become especially valuable in these PDE contexts.\nHowever, for the vast majority of physics problems involving ODEs‚Äîespecially nonlinear initial value problems like most classical mechanics situations‚Äîtime-stepping methods like odeint are far more practical, efficient, and easier to implement. They handle nonlinearity naturally, use memory efficiently, and scale to long time simulations gracefully. Think of matrix methods as a specialized tool in your computational toolkit, while time-stepping methods are your everyday workhorses.",
    "crumbs": [
      "‚ö° Week 5: Classical Mechanics 2",
      "Tool: Solving ODEs"
    ]
  },
  {
    "objectID": "lectures/lecture08/3_solving_ODEs.html#whats-next-applications-across-physics",
    "href": "lectures/lecture08/3_solving_ODEs.html#whats-next-applications-across-physics",
    "title": "Solving ODEs",
    "section": "What‚Äôs Next? Applications Across Physics",
    "text": "What‚Äôs Next? Applications Across Physics\nNow that you‚Äôve mastered solving ODEs numerically, you possess a fundamental tool of computational physics. Combined with the numerical differentiation techniques from the previous lecture, you now understand both how to compute derivatives from functions and how to compute functions from their derivatives‚Äîcompleting the computational calculus toolkit. The techniques you‚Äôve learned apply across virtually every domain of physics. Consider planetary motion‚Äîby solving Newton‚Äôs gravitational equations, you can simulate orbits, predict eclipses, or study the complex dynamics of three-body systems that have no analytical solution. You can explore chaotic systems like the double pendulum, witnessing how deterministic equations can produce unpredictable, seemingly random behavior that‚Äôs exquisitely sensitive to initial conditions.\nYou can model wave propagation by discretizing wave equations, studying how disturbances travel through media. You can even tackle problems in quantum mechanics, solving the time-dependent Schr√∂dinger equation numerically to watch wavefunctions evolve and interfere. While Schr√∂dinger‚Äôs equation is fundamentally different from classical ODEs (it‚Äôs a partial differential equation with complex values), the same numerical principles apply after appropriate discretization.\nThe tools you‚Äôve learned across these two lectures‚Äînumerical differentiation and ODE integration‚Äîform an inseparable pair at the foundation of computational physics. The finite difference formulas we derived for computing derivatives reappear inside the time-stepping algorithms for solving ODEs. The matrix representations of differential operators connect both lectures. Together, they‚Äôre the building blocks for more sophisticated methods: molecular dynamics simulations, climate models, computational fluid dynamics, and finite element analysis all build on these core concepts. You‚Äôre now equipped to simulate, predict, and understand physical systems computationally, complementing the analytical techniques you learn in traditional physics courses.",
    "crumbs": [
      "‚ö° Week 5: Classical Mechanics 2",
      "Tool: Solving ODEs"
    ]
  },
  {
    "objectID": "lectures/lecture06/test.html",
    "href": "lectures/lecture06/test.html",
    "title": "Title",
    "section": "",
    "text": "viewof aSlider = Inputs.range([-4, 0], { label: \"a\", step: 0.01, value: -1.7 });\nviewof bSlider = Inputs.range([-2, 2], { label: \"b\", step: 0.01, value: 1.3 });\nviewof cSlider = Inputs.range([-2, 2], { label: \"c\", step: 0.01, value: 1.0 });\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfiltered = transpose(data);\n// Create the plot\n\nxValues = Array.from({ length: 100 }, (_, i) =&gt; i / 100);\nparabolaData = xValues.map(x =&gt; ({ x, y: parabola(x, aSlider, bSlider, cSlider) }));\n\n\nparabola = (x, a, b, c) =&gt; a * x**2 + b * x + c\n\ncalculateChiSquared = (data, a, b, c) =&gt; {\n  let chisq = 0\n  let x= data.map(d =&gt; d.x)\n  let y= data.map(d =&gt; d.y)\n  let err= data.map(d =&gt; d.error)\n  for (let i = 0; i &lt; x.length; i++) {\n    let y_model = parabola(x[i], a, b, c)\n    chisq += ((y[i] - y_model) / err[i])**2\n  }\n  return chisq\n}\n\nchisq = calculateChiSquared(filtered, aSlider, bSlider, cSlider)\n\nPlot.plot({\n  marks: [\n    Plot.dot(filtered, { x: \"x\", y: \"y\" }),\n    Plot.ruleY(filtered, { x: \"x\", y1: d =&gt; d.y - d.error, y2: d =&gt; d.y + d.error }),\n    Plot.line(parabolaData, { x: \"x\", y: \"y\" }),\n    Plot.text([{ x: 0.8, y: 1.5, label: `œá¬≤: ${chisq.toFixed(2)}` }], {\n          x: \"x\",\n          y: \"y\",\n          text: \"label\",\n          dy: -10, // Adjust vertical position if needed\n          fill: \"black\", // Set text color\n          fontSize: 16\n        }),\n    Plot.frame()\n  ],\n  x: {\n    label: \"X Axis\",\n    labelAnchor: \"center\",\n    labelOffset: 35,\n    grid: true,\n    tickFormat: \".2f\", // Format ticks to 2 decimal places\n    domain: [0, 1]\n  },\n  y: {\n    label: \"Y Axis\",\n    grid: true,\n    tickFormat: \".2f\", // Format ticks to 2 decimal places\n    labelAnchor: \"center\",  // Center the label on its axis\n    labelAngle: -90,\n    labelOffset: 60,\n    domain: [0, 2],\n  },\n  width: 400,\n  height: 400,\n  marginLeft: 100,\n  marginBottom: 40,\n  style: {\n    fontSize: \"14px\",          // This sets the base font size\n    \"axis.label\": {\n      fontSize: \"18px\",        // This sets the font size for axis labels\n      fontWeight: \"bold\"       // Optionally make it bold\n    },\n    \"axis.tick\": {\n      fontSize: \"14px\"         // This sets the font size for tick labels\n    }\n  },\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHello Here‚Äôs how you can do it:\n\nStep 1: Create the DataFrame in Python\n\n\n\n\n\n\n\nimport numpy as np\nimport io\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\nplt.rcParams.update({'font.size': 18})\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets\n\n# default values for plotting\nplt.rcParams.update({'font.size': 12,\n                     'lines.linewidth': 1,\n                     'lines.markersize': 10,\n                     'axes.labelsize': 11,\n                     'xtick.labelsize' : 10,\n                     'ytick.labelsize' : 10,\n                     'xtick.top' : True,\n                     'xtick.direction' : 'in',\n                     'ytick.right' : True,\n                     'ytick.direction' : 'in',})\n\ndata_str= \"\"\"\n0.000000000000000000e+00 9.916839204057067425e-01 5.332818799198481979e-02\n1.111111111111111049e-01 1.183667840440161712e+00 5.339559646742838422e-02\n2.222222222222222099e-01 1.310862148961057017e+00 5.366783326382672248e-02\n3.333333333333333148e-01 1.193174867265999639e+00 5.435097493883071090e-02\n4.444444444444444198e-01 1.265354130824580148e+00 5.576995501488732354e-02\n5.555555555555555802e-01 1.234277634100806154e+00 5.832636573698059268e-02\n6.666666666666666297e-01 1.067204799568996387e+00 6.242706729257353759e-02\n7.777777777777776791e-01 7.113706894723520469e-01 6.840334070620925078e-02\n8.888888888888888395e-01 5.111625429539546905e-01 7.645872257082597656e-02\n1.000000000000000000e+00 1.360838209996238390e+00 4.666811165343753287e-01\n\"\"\"\nx_data, y_data, err = np.loadtxt(io.StringIO(data_str), unpack=True)\n\ndef parabola(x,a,b,c):\n    return(a*x**2+b*x+c)\n\ndef plot(a,b,c):\n    y=parabola(x,a,b,c)\n    plt.figure(figsize=(8,6))\n    chisq=(((y_data-parabola(x_data,a,b,c))/err)**2).sum()\n    plt.plot(x,y,label=r'$\\chi^2$={0:6.3f}'.format(chisq))\n    plt.errorbar(x_data,y_data,yerr=err,marker='o',fmt=\"none\",color='k')\n\n    plt.scatter(x_data,y_data,marker='o',color='k')\n    plt.legend()\n    plt.xlabel('x- position')\n    plt.ylabel('y- position')\n    plt.show()\n\nx=np.linspace(0,1,100)\ninteract(plot,a=-1.7,b=1.3,c=1.0);"
  },
  {
    "objectID": "lectures/lecture06/01-lecture06.html",
    "href": "lectures/lecture06/01-lecture06.html",
    "title": "Lecture 6",
    "section": "",
    "text": "Advanced conditionals and loops for simulating complex scenarios.\nIntroduction to basic data structures (dictionaries) for handling multiple objects.\n\n\n\n\n\nSimulating elastic and inelastic collisions in one and two dimensions.\nApplying conservation laws (momentum and energy) to check the validity of the simulation.\nVisualization: Plotting the trajectories and velocities of colliding objects.\nHomework: Simulate a multi-object collision scenario."
  },
  {
    "objectID": "lectures/lecture06/01-lecture06.html#collisions-and-conservation-laws",
    "href": "lectures/lecture06/01-lecture06.html#collisions-and-conservation-laws",
    "title": "Lecture 6",
    "section": "",
    "text": "Advanced conditionals and loops for simulating complex scenarios.\nIntroduction to basic data structures (dictionaries) for handling multiple objects.\n\n\n\n\n\nSimulating elastic and inelastic collisions in one and two dimensions.\nApplying conservation laws (momentum and energy) to check the validity of the simulation.\nVisualization: Plotting the trajectories and velocities of colliding objects.\nHomework: Simulate a multi-object collision scenario."
  },
  {
    "objectID": "lectures/lecture01/3_datatypes.html",
    "href": "lectures/lecture01/3_datatypes.html",
    "title": "Data Types",
    "section": "",
    "text": "It‚Äôs time to look at different data types we may find useful in our course. Besides the number types mentioned previously, there are also other types like strings, lists, tuples, dictionaries and sets.\nEach of these data types has a number of connected methods (functions) which allow to manipulate the data contained in a variable. If you want to know which methods are available for a certain object use the command dir, e.g.\nThe following few cells will give you a short introduction into each type."
  },
  {
    "objectID": "lectures/lecture01/3_datatypes.html#strings",
    "href": "lectures/lecture01/3_datatypes.html#strings",
    "title": "Data Types",
    "section": "Strings",
    "text": "Strings\nStrings are lists of keyboard characters as well as other characters not on your keyboard. They are useful for printing results on the screen, during reading and writing of data.\n\n\n\n\n\n\n\n\n\n\n\n\nString can be concatenated using the + operator.\n\n\n\n\n\n\n\n\n\n\n\n\nAs strings are lists, each character in a string can be accessed by addressing the position in the string (see Lists section)\n\n\n\n\n\n\nStrings can also be made out of numbers.\n\n\n\n\n\n\nIf you want to obtain a number of a string, you can use what is known as type casting. Using type casting you may convert the string or any other data type into a different type if this is possible. To find out if a string is a pure number you may use the str.isnumeric method. For the above string, we may want to do a conversion to the type int by typing:\n\n\n\n\n\n\n\n\n\n\n\n\nThere are a number of methods connected to the string data type. Usually the relate to formatting or finding sub-strings. Formatting will be a topic in our next lecture. Here we just refer to one simple find example."
  },
  {
    "objectID": "lectures/lecture01/3_datatypes.html#quiz-data-types-in-python",
    "href": "lectures/lecture01/3_datatypes.html#quiz-data-types-in-python",
    "title": "Data Types",
    "section": "Quiz: Data Types in Python",
    "text": "Quiz: Data Types in Python\nLet‚Äôs test your understanding of Python data types!\n\n\nWhat is the output of the following code?\na = [1, 2, 3]\nb = (1, 2, 3)\nprint(type(a), type(b))\n\n&lt;class 'list'&gt; &lt;class 'list'&gt;\n&lt;class 'list'&gt; &lt;class 'tuple'&gt;\n&lt;class 'tuple'&gt; &lt;class 'list'&gt;\n&lt;class 'tuple'&gt; &lt;class 'tuple'&gt;\n\nWhich of the following is mutable?\n\nList\nTuple\nString\nInteger\n\nWhat will be the output of this code?\nmy_dict = {'a': 1, 'b': 2, 'c': 3}\nprint(my_dict['b'])\n\na\n2\nb\nKeyError\n\nHow do you create an empty set in Python?\n\n{}\n[]\nset()\n()\n\nWhat is the result of 3 + 4.0?\n\n7\n7.0\n‚Äò7.0‚Äô\nTypeError\n\n\n\n\n\n\n\n\n\nClick to reveal answers\n\n\n\n\n\n\n&lt;class 'list'&gt; &lt;class 'tuple'&gt;\nList\n2\nset()\n7.0"
  },
  {
    "objectID": "lectures/lecture01/01-lecture01.html",
    "href": "lectures/lecture01/01-lecture01.html",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "",
    "text": "As physicists, we need to:\n\nCalculate results from equations\nVisualize data and theoretical predictions\nDocument our work with equations and explanations\nShare reproducible results with colleagues\n\nJupyter Notebooks let us do all of this in one place!\n\n\n\n\n\n\nWhat You‚Äôll Create Today\n\n\n\nBy the end of this Lecture, you‚Äôll generate plots like this:\n\n\n\n\n\n\n\n\n\nAll from physics equations you already know!",
    "crumbs": [
      "üöÄ Week 1: Your First Physics Code",
      "Setup: Jupyter Notebooks"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-lecture01.html#why-jupyter",
    "href": "lectures/lecture01/01-lecture01.html#why-jupyter",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "",
    "text": "As physicists, we need to:\n\nCalculate results from equations\nVisualize data and theoretical predictions\nDocument our work with equations and explanations\nShare reproducible results with colleagues\n\nJupyter Notebooks let us do all of this in one place!\n\n\n\n\n\n\nWhat You‚Äôll Create Today\n\n\n\nBy the end of this Lecture, you‚Äôll generate plots like this:\n\n\n\n\n\n\n\n\n\nAll from physics equations you already know!",
    "crumbs": [
      "üöÄ Week 1: Your First Physics Code",
      "Setup: Jupyter Notebooks"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-lecture01.html#what-is-a-jupyter-notebook",
    "href": "lectures/lecture01/01-lecture01.html#what-is-a-jupyter-notebook",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "What is a Jupyter Notebook?",
    "text": "What is a Jupyter Notebook?\nA Jupyter Notebook is a web-based interactive document that combines:\n\nCode (Python) that you can run\nResults (numbers, plots, animations)\nDocumentation (text, equations in LaTeX)\n\nThink of it as a digital lab notebook for computational physics.\n\n\n\n\n\n\nKey Advantage\n\n\n\nUnlike traditional programming where you write everything first and then run it, Jupyter lets you experiment interactively - run small pieces of code, see results immediately, and build up your solution step by step.",
    "crumbs": [
      "üöÄ Week 1: Your First Physics Code",
      "Setup: Jupyter Notebooks"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-lecture01.html#opening-jupyter-lab",
    "href": "lectures/lecture01/01-lecture01.html#opening-jupyter-lab",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "Opening Jupyter Lab",
    "text": "Opening Jupyter Lab\n\nOption 1: Anaconda Navigator (Recommended for Beginners)\n\nOpen Anaconda Navigator\nClick Launch under JupyterLab\nYour browser opens automatically\n\n\n\nOption 2: Command Line\nOpen terminal and type:\njupyter lab\n\n\n\n\n\n\nTroubleshooting\n\n\n\nIf Jupyter doesn‚Äôt start, check: - Anaconda is installed - You‚Äôre connected to the internet (first time only) - Try restarting Anaconda Navigator",
    "crumbs": [
      "üöÄ Week 1: Your First Physics Code",
      "Setup: Jupyter Notebooks"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-lecture01.html#jupyterlab-interface",
    "href": "lectures/lecture01/01-lecture01.html#jupyterlab-interface",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "JupyterLab Interface",
    "text": "JupyterLab Interface\nWhen JupyterLab opens, you‚Äôll see:\n\nLeft sidebar: File browser (like Windows Explorer/Finder)\nMain area: Where notebooks open\nLauncher tab: Click ‚ÄúPython 3‚Äù under Notebook to create a new notebook\n\n\n\n\n\n\n\nTry It Yourself (Embedded Demo)\n\n\n\n\n\nHere‚Äôs a live JupyterLab environment you can experiment with:\nOpen Fullscreen",
    "crumbs": [
      "üöÄ Week 1: Your First Physics Code",
      "Setup: Jupyter Notebooks"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-lecture01.html#creating-your-first-notebook",
    "href": "lectures/lecture01/01-lecture01.html#creating-your-first-notebook",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "Creating Your First Notebook",
    "text": "Creating Your First Notebook\n\nClick Python 3 tile under ‚ÄúNotebook‚Äù in the Launcher\nA new tab opens with an empty notebook\nSave it: File ‚Üí Save Notebook As‚Ä¶ ‚Üí physics_week1.ipynb\n\n\n\n\n\n\n\nFile Extension\n\n\n\nJupyter notebooks end in .ipynb (IPython Notebook)",
    "crumbs": [
      "üöÄ Week 1: Your First Physics Code",
      "Setup: Jupyter Notebooks"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-lecture01.html#two-types-of-cells",
    "href": "lectures/lecture01/01-lecture01.html#two-types-of-cells",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "Two Types of Cells",
    "text": "Two Types of Cells\nJupyter notebooks consist of cells - individual blocks that contain either code or text.\n\nCode Cells (Default)\n\nWrite Python code here\nRun to see results\nIdentified by [ ]: on the left\n\n\n\nMarkdown Cells\n\nWrite notes, explanations, equations\nUse for documentation\nChange a cell to Markdown: Click cell, then press M key",
    "crumbs": [
      "üöÄ Week 1: Your First Physics Code",
      "Setup: Jupyter Notebooks"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-lecture01.html#your-first-calculation",
    "href": "lectures/lecture01/01-lecture01.html#your-first-calculation",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "Your First Calculation üéØ",
    "text": "Your First Calculation üéØ\nLet‚Äôs calculate gravitational potential energy!\n\nPhysics\n\\[E_{\\text{pot}} = mgh\\]\n\n\nCode\nType this in your first cell:\n# Gravitational potential energy\nm = 2.0    # mass in kg\ng = 9.81   # gravity in m/s¬≤\nh = 10.0   # height in m\n\nE_pot = m * g * h\n\nprint(f\"Potential energy: {E_pot} J\")\n\n\nRun It!\n\nClick the ‚ñ∂ button (top of cell), OR\nPress Shift + Enter (run and move to next cell), OR\nPress Ctrl/Cmd + Enter (run and stay)\n\nYou should see: Potential energy: 196.2 J\n\n\n\n\n\n\nPro Tip\n\n\n\nShift + Enter is the most common way to run cells - you‚Äôll use this constantly!",
    "crumbs": [
      "üöÄ Week 1: Your First Physics Code",
      "Setup: Jupyter Notebooks"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-lecture01.html#essential-keyboard-shortcuts",
    "href": "lectures/lecture01/01-lecture01.html#essential-keyboard-shortcuts",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "Essential Keyboard Shortcuts",
    "text": "Essential Keyboard Shortcuts\nYou‚Äôll save tons of time with these shortcuts:\n\nRunning Cells\n\n\n\nShortcut\nAction\n\n\n\n\nShift + Enter\nRun cell and move to next\n\n\nCtrl/Cmd + Enter\nRun cell and stay\n\n\n\n\n\nCell Operations (press Esc first to enter command mode)\n\n\n\nShortcut\nAction\n\n\n\n\nB\nInsert cell below\n\n\nA\nInsert cell above\n\n\nD D\nDelete cell (press D twice)\n\n\nM\nChange to Markdown cell\n\n\nY\nChange to Code cell\n\n\nZ\nUndo delete\n\n\n\n\n\nEditing\n\n\n\nShortcut\nAction\n\n\n\n\nEnter\nEnter edit mode\n\n\nEsc\nExit edit mode (command mode)\n\n\n\n\n\n\n\n\n\nTwo Modes\n\n\n\n\nEdit mode (green border): Type in cell\nCommand mode (blue border): Use keyboard shortcuts\n\nPress Esc to switch to command mode, Enter to switch to edit mode.",
    "crumbs": [
      "üöÄ Week 1: Your First Physics Code",
      "Setup: Jupyter Notebooks"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-lecture01.html#try-it-multi-step-calculation",
    "href": "lectures/lecture01/01-lecture01.html#try-it-multi-step-calculation",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "Try It: Multi-Step Calculation",
    "text": "Try It: Multi-Step Calculation\nLet‚Äôs calculate the time for a ball to fall from height h.\nPhysics: \\(h = \\frac{1}{2}gt^2\\) ‚Üí \\(t = \\sqrt{\\frac{2h}{g}}\\)\nYour Task:\n\nCreate a new cell below (press B)\nType this code:\n\nimport numpy as np  # We need square root\n\nh = 10.0   # height in m\ng = 9.81   # gravity in m/s¬≤\n\nt = np.sqrt(2 * h / g)\n\nprint(f\"Fall time: {t:.2f} seconds\")\n\nRun it! (Shift + Enter)",
    "crumbs": [
      "üöÄ Week 1: Your First Physics Code",
      "Setup: Jupyter Notebooks"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-lecture01.html#adding-documentation-with-markdown",
    "href": "lectures/lecture01/01-lecture01.html#adding-documentation-with-markdown",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "Adding Documentation with Markdown",
    "text": "Adding Documentation with Markdown\nGood physics notebooks explain what you‚Äôre doing!\n\nCreate a Markdown Cell\n\nInsert a new cell above your code (press Esc, then A)\nChange it to Markdown (press M)\nType:\n\n## Free Fall Calculation\n\nWe calculate the time for an object to fall from height $h = 10$ m.\n\nThe equation is:\n$$t = \\sqrt{\\frac{2h}{g}}$$\n\nwhere $g = 9.81 \\, \\text{m/s}^2$ is gravitational acceleration.\n\nRun the cell (Shift + Enter) to render it\n\n\n\n\n\n\n\nLaTeX Equations\n\n\n\n\nInline equation: $E = mc^2$ ‚Üí \\(E = mc^2\\)\nDisplay equation: $$E = mc^2$$ ‚Üí \\[E = mc^2\\]",
    "crumbs": [
      "üöÄ Week 1: Your First Physics Code",
      "Setup: Jupyter Notebooks"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-lecture01.html#your-first-plot",
    "href": "lectures/lecture01/01-lecture01.html#your-first-plot",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "Your First Plot! üìä",
    "text": "Your First Plot! üìä\nNow let‚Äôs visualize physics! This is where Jupyter really shines - you can see your calculations come to life.\n\nPlotting a Falling Ball\nLet‚Äôs plot the height of our falling ball over time. Don‚Äôt worry about understanding every detail - just see how easy it is to create beautiful graphs!\nCreate a new cell and type:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Calculate fall time\nh = 10.0\ng = 9.81\nt_fall = np.sqrt(2 * h / g)\n\n# Create time points from 0 to landing\nt = np.linspace(0, t_fall, 50)\n\n# Calculate height at each time: h = h0 - (1/2)gt¬≤\nheight = h - 0.5 * g * t**2\n\n# Create the plot\nplt.plot(t, height, 'b-', linewidth=2)\nplt.xlabel('Time (s)')\nplt.ylabel('Height (m)')\nplt.title('Ball Falling from 10m')\nplt.grid(True, alpha=0.3)\nplt.show()\nRun it! You just created your first physics visualization! üéâ\n\n\n\n\n\n\nWhat Just Happened?\n\n\n\n\nmatplotlib.pyplot is Python‚Äôs plotting library\nnp.linspace(0, t_fall, 50) creates 50 evenly-spaced time points\nplt.plot() draws the curve\nThe labels and grid make it look professional\n\nDon‚Äôt memorize this yet - we‚Äôll cover all the details in Lecture 4. For now, just enjoy seeing your calculation as a graph!\n\n\n\n\nChallenge: Plot Velocity\nThe velocity increases as the ball falls: \\(v = gt\\)\nTry modifying the code to plot velocity vs time instead!\n\n\n\n\n\n\nSolution\n\n\n\n\n\n# Calculate velocity at each time point\nvelocity = g * t\n\nplt.plot(t, velocity, 'r-', linewidth=2)\nplt.xlabel('Time (s)')\nplt.ylabel('Velocity (m/s)')\nplt.title('Velocity of Falling Ball')\nplt.grid(True, alpha=0.3)\nplt.show()\nNotice how velocity increases linearly - exactly what the equation predicts!",
    "crumbs": [
      "üöÄ Week 1: Your First Physics Code",
      "Setup: Jupyter Notebooks"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-lecture01.html#saving-your-work",
    "href": "lectures/lecture01/01-lecture01.html#saving-your-work",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "Saving Your Work",
    "text": "Saving Your Work\n\n ## What‚Äôs Next? üìä\nNow that you know the basics:\n\nLater: Advanced Plotting Techniques\n\nMultiple plots, logarithmic scales, 3D plots, animations, and more\nBuild on what you just learned!\n\nNext: Variables & Numbers\n\nUnderstand Python fundamentals in detail\nLearn about data types, operations, and more\n\n\nJupyter auto-saves every few minutes, but you should also:\n\nManual save: Ctrl/Cmd + S or File ‚Üí Save Notebook\nDownload: File ‚Üí Save and Export Notebook As ‚Üí HTML/PDF\n\n\n\n\n\n\n\nImportant!\n\n\n\nSaving the notebook saves your code and markdown, but also the output. If you want a clean notebook, do: Kernel ‚Üí Restart Kernel and Clear All Outputs before saving.",
    "crumbs": [
      "üöÄ Week 1: Your First Physics Code",
      "Setup: Jupyter Notebooks"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-lecture01.html#whats-next",
    "href": "lectures/lecture01/01-lecture01.html#whats-next",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "What‚Äôs Next? üìä",
    "text": "What‚Äôs Next? üìä\nNow that you know the basics:\n\nNext lesson: Plotting Your First Graph\n\nYou‚Äôll create beautiful physics visualizations\n\nThen: Variables & Numbers\n\nUnderstand what you just used in detail",
    "crumbs": [
      "üöÄ Week 1: Your First Physics Code",
      "Setup: Jupyter Notebooks"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-lecture01.html#quick-reference-card",
    "href": "lectures/lecture01/01-lecture01.html#quick-reference-card",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "Quick Reference Card",
    "text": "Quick Reference Card\n\nMost Important Commands\nShift + Enter  ‚Üí Run cell\nEsc + B        ‚Üí New cell below\nEsc + M        ‚Üí Change to Markdown\nEsc + D D      ‚Üí Delete cell\nCtrl/Cmd + S   ‚Üí Save\n\n\nGetting Help\n\nFunction help: Type ?function_name in a cell and run\nDocumentation: Press Shift + Tab while cursor is on a function name",
    "crumbs": [
      "üöÄ Week 1: Your First Physics Code",
      "Setup: Jupyter Notebooks"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-lecture01.html#advanced-topics-optional",
    "href": "lectures/lecture01/01-lecture01.html#advanced-topics-optional",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "Advanced Topics (Optional)",
    "text": "Advanced Topics (Optional)\n\n\n\n\n\n\nWhat is a Kernel?\n\n\n\n\n\nThe kernel is the computational engine that runs your code. Think of it as Python running in the background.\nKernel indicator (top-right):\n\n‚ö™ Idle: Ready to run code\n‚ö´ Busy: Currently executing\n\nKernel menu operations:\n\nRestart Kernel: Clears all variables, starts fresh\nInterrupt Kernel: Stops long-running code\n\nYou‚Äôll need to restart if:\n\nVariables get messy\nCode is stuck in an infinite loop (interrupt first, then restart)\nYou want a clean slate\n\n\n\n\n\n\n\n\n\n\nMarkdown Cheatsheet\n\n\n\n\n\n# Heading 1\n## Heading 2\n### Heading 3\n\n**bold text**\n*italic text*\n\n- Bullet list\n- Another item\n\n1. Numbered list\n2. Another item\n\n[Link text](https://www.example.com)\n\nInline code: `x = 5`\n\nEquation: $E = mc^2$\n\nDisplay equation:\n$$\\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$$\n\n\n\n\n\n\n\n\n\nComplete Jupyter Documentation\n\n\n\n\n\nFor more advanced features (magic commands, debugging, widgets), see:\n\nOfficial Jupyter Documentation\nJupyterLab Documentation",
    "crumbs": [
      "üöÄ Week 1: Your First Physics Code",
      "Setup: Jupyter Notebooks"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-lecture01.html#summary",
    "href": "lectures/lecture01/01-lecture01.html#summary",
    "title": "Getting Started with Jupyter Notebooks",
    "section": "Summary ‚úÖ",
    "text": "Summary ‚úÖ\nYou‚Äôve learned:\n\n‚úì What Jupyter notebooks are and why they‚Äôre useful for physics\n‚úì How to open JupyterLab and create a notebook\n‚úì The difference between code and Markdown cells\n‚úì How to run code and see results\n‚úì Essential keyboard shortcuts\n‚úì How to document your work with equations\n\nYou‚Äôre ready to start plotting! Move on to the next lesson.\n\n\n\n\n\n\n\nPractice Exercise\n\n\n\nBefore the next class, try this:\n\nCreate a new notebook called practice.ipynb\nCalculate the kinetic energy of a 1000 kg car traveling at 30 m/s\nUse a Markdown cell to write the equation: \\(E_k = \\frac{1}{2}mv^2\\)\nPrint the result in Joules\nBonus plotting challenge: Plot how kinetic energy changes with velocity\n\nUse velocities from 0 to 40 m/s\nHint: Copy and modify the falling ball plotting code!\n\nSave your notebook\n\nSolution: - Energy at 30 m/s: 450,000 J (or 450 kJ) - The plot should show a parabolic curve (energy ‚àù v¬≤)",
    "crumbs": [
      "üöÄ Week 1: Your First Physics Code",
      "Setup: Jupyter Notebooks"
    ]
  },
  {
    "objectID": "lectures/lecture12/3_diffusion_equation.html",
    "href": "lectures/lecture12/3_diffusion_equation.html",
    "title": "Diffusion equation",
    "section": "",
    "text": "So far, we have always looked at ordinary differential equations, i.e.¬†differential equations where the physical quantity we considered was depending only on one variable. In a lot of physical problems, the observable quantities depend on multiple variables like time and space. The differential equations, which govern those problems are partial differential equations. The diffusion equation is one of them. It pops up in various forms in physics, describing also heat conduction and in a slighly modified way this is corresponding to the time dependent Schr√∂dinger equation."
  },
  {
    "objectID": "lectures/lecture12/3_diffusion_equation.html#physical-model",
    "href": "lectures/lecture12/3_diffusion_equation.html#physical-model",
    "title": "Diffusion equation",
    "section": "Physical Model",
    "text": "Physical Model\nYou‚Äôve probably seen how ink spreads in water - this spreading is called diffusion and is the result of the Brownian motion of ink particles in water. We have already simulated the random motion in Lecture 5 and the real-time animation below shows this process, when all particles are starting at the center of the box. The diffusion equation describes how the concentration of particles (the number of particles per unit volume) changes over time and space.\nWhile physicists describe this with complex equations, our goal in this course is to learn how to simulate this process using Python. So the main task will be to split the diffusion equation into small pieces that we can calculate with a computer.\nThe basic diffusion equation looks like this:\n\\[\\begin{equation}\n\\frac{\\partial c({\\bf r},t)}{\\partial t}=D\\Delta c ({\\bf r},t)\n\\end{equation}\\]\nHere, \\(c\\) represents the concentration (like how much ink is at each point), \\(t\\) is time, and \\({\\bf r}\\) is position. \\(D\\) is just a number that tells us how fast the diffusion happens. Its unit is length squared per time.\nTo make this easier to program, we‚Äôll look at diffusion in just one direction (like along a line). This gives us:\n\\[\\begin{equation}\n\\frac{\\partial c(x,t)}{\\partial t}=D\\frac{\\partial^2 c(x,t)}{\\partial x^{2}}\n\\end{equation}\\]\nTo turn this equation into code, we need to break up space and time into small pieces. We‚Äôll use \\(c^{n}_{i}\\) in our program, where \\({\\bf n}\\) is which time step we‚Äôre on, and \\({\\bf i}\\) tells us which point in space we‚Äôre looking at.\n\n\n\n\n\nwidth = 600\nheight = 600\nmargin = ({top: 20, right: 30, bottom: 20, left: 40})\nplotHeight = 100\n\nviewof simulation = {\n  // Create main container\n  const container = d3.create(\"div\")\n    .style(\"display\", \"flex\")\n    .style(\"flex-direction\", \"column\");\n\n  // Create SVG for particle simulation\n  const svg = container.append(\"svg\")\n    .attr(\"width\", width)\n    .attr(\"height\", height - plotHeight)\n    .attr(\"viewBox\", [0, 0, width, height - plotHeight]);\n\n  // Create SVG for histogram\n  const histogramSvg = container.append(\"svg\")\n    .attr(\"width\", width)\n    .attr(\"height\", plotHeight)\n    .attr(\"viewBox\", [0, 0, width, plotHeight]);\n\n  const numParticles = 1000;\n  const D = 0.5; // Diffusion coefficient\n  const numBins = 80;\n\n  // Create particles at the center\n  const particles = Array.from({length: numParticles}, () =&gt; ({\n    x: width / 2,\n    y: (height - plotHeight) / 2,\n    vx: 0,\n    vy: 0\n  }));\n\n  // Setup scales for histogram\n  const xScale = d3.scaleLinear()\n    .domain([0, width])\n    .range([margin.left, width - margin.right]);\n\n  const yScale = d3.scaleLinear()\n    .domain([0, numParticles/5])\n    .range([plotHeight - margin.bottom, margin.top]);\n\n  // Create histogram generator\n  const histogram = d3.bin()\n    .domain(xScale.domain())\n    .thresholds(xScale.ticks(numBins))\n    .value(d =&gt; d.x);\n\n  // Create histogram group\n  const histogramGroup = histogramSvg.append(\"g\");\n\n  // Add axes\n  histogramSvg.append(\"g\")\n    .attr(\"transform\", `translate(0,${plotHeight - margin.bottom})`)\n    .call(d3.axisBottom(xScale));\n/*\n  histogramSvg.append(\"g\")\n    .attr(\"transform\", `translate(${margin.left},0)`)\n    .call(d3.axisLeft(yScale));\n*/\n  // Animation function\n  function animate() {\n    particles.forEach(particle =&gt; {\n      // Random walk implementation based on diffusion equation\n      const randomAngle = Math.random() * 2 * Math.PI;\n      const displacement = Math.sqrt(2 * D);\n\n      particle.x += displacement * Math.cos(randomAngle);\n      particle.y += displacement * Math.sin(randomAngle);\n\n      // Bounce off walls\n      if (particle.x &lt; 0) particle.x = 0;\n      if (particle.x &gt; width) particle.x = width;\n      if (particle.y &lt; 0) particle.y = 0;\n      if (particle.y &gt; (height - plotHeight)) particle.y = height - plotHeight;\n    });\n\n    // Update particle positions\n    circles\n      .attr(\"cx\", d =&gt; d.x)\n      .attr(\"cy\", d =&gt; d.y);\n\n    // Update histogram\n    const bins = histogram(particles);\n\n    const bars = histogramGroup.selectAll(\"rect\")\n      .data(bins);\n\n    bars.enter()\n      .append(\"rect\")\n      .merge(bars)\n      .attr(\"x\", d =&gt; xScale(d.x0))\n      .attr(\"y\", d =&gt; yScale(d.length))\n      .attr(\"width\", d =&gt; Math.max(0, xScale(d.x1) - xScale(d.x0) - 1))\n      .attr(\"height\", d =&gt; yScale(0) - yScale(d.length))\n      .attr(\"fill\", \"steelblue\");\n\n    bars.exit().remove();\n  }\n\n  // Create circles for particles\n  const circles = svg.selectAll(\"circle\")\n    .data(particles)\n    .join(\"circle\")\n    .attr(\"r\", 2)\n    .attr(\"fill\", \"steelblue\")\n    .attr(\"opacity\", 0.6);\n\n  // Start animation\n  d3.timer(animate);\n\n  return container.node();\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†1: Simulation showing the diffusion of particles in a 2D box. Particles move randomly based on the diffusion equation. The histogram shows the distribution of particles along the x-axis.\n\n\n\n\n\nSpatial derivative\n\n\nSpatial derivative\nLet‚Äôs break down how we handle changes in space. Just like before, we can estimate how quickly the concentration changes in space using three points:\n\\[\\begin{equation}\n\\frac{\\partial^{2} c(x,t)}{\\partial x^2}\\approx\\frac{c_{i+1}^{n}-2c_{i}^{n}+c_{i-1}^{n}}{\\Delta x^2}\n\\end{equation}\\]\nThink of this as looking at how the concentration changes between neighboring points. We‚Äôll collect all these concentrations at a particular time \\(n\\) into a list: \\({\\bf C}=\\lbrace c_{0}^{n},c_{1}^{n},c_{2}^{n}, \\ldots, c_{5}^{n}\\rbrace\\).\nTo make calculations easier for the computer, we can write this as a matrix equation:\n\\(M=\\frac{\\partial^2}{\\partial x^2}=\\frac{1}{\\delta x^2}\n\\begin{bmatrix}\n-2 & 1  & 0 & 0 & 0 & 0\\\\\n1 & -2 & 1 & 0 & 0 & 0\\\\\n0 & 1  & -2 & 1 & 0 & 0\\\\\n0 & 0  & 1  & -2 & 1 & 0\\\\\n0 & 0  & 0  &  1 & -2 & 1\\\\\n0 & 0  & 0  &  0 &  1 & -2\\\\\n\\end{bmatrix}\\)\nEach row in this matrix represents how we calculate the change at one point using its neighbors. We haven‚Äôt yet considered what happens at the edges of our system (the boundary conditions).\nThis lets us write our diffusion equation in a simpler form:\n\\[\\begin{equation}\n\\frac{\\partial c(x,t)}{\\partial t}\\approx DM{\\bf C}^{n}\n\\end{equation}\\]\nHere, \\(M\\) is our matrix from above, and \\({\\bf C}\\) is our list of concentrations. The \\(n\\) tells us we‚Äôre looking at a specific moment in time.\n\n\nTemporal derivative\nJust like we split up space into points, we also need to split up time into small steps. The change in concentration over time can be estimated by looking at how much it changes between two time steps:\n\\[\\begin{equation}\n\\frac{\\partial c(x,t)}{\\partial t}=\\frac{c_{i}^{n+1}-c_{i}^{n}}{\\delta t}\n\\end{equation}\\]\nHere, \\(n\\) tells us which time step we‚Äôre on (just like \\(i\\) told us which space point we were looking at). This equation works for any point in space \\(i\\).\nTo make our calculation more accurate, we use something called the Crank Nicolson scheme. First, we write our time derivative as:\n\\[\\begin{equation}\n\\frac{\\partial c}{\\partial t} = f(x)\n\\end{equation}\\]\nwhere \\(f(x)\\) is the spatial part we found earlier:\n\\[\\begin{equation}\nf(x)=D\\frac{\\partial^2 c(x,t)}{\\partial x^{2}}\n\\end{equation}\\]\nThe Crank Nicolson scheme tells us to take the average of this function at the current time step and the next time step:\n\\[\\begin{equation}\n\\frac{\\partial c}{\\partial t} \\approx \\frac{1}{2}\\left ( f^{n+1}(x)+f^{n}(x)\\right)\n\\end{equation}\\]\nwhere \\(n\\) keeps track of which time step we‚Äôre on.\n\n\nBringing all together\nWe can now bring all sides together to develop our implicit scheme.\n\\[\\begin{equation}\n\\frac{{\\bf C^{n+1}}-{\\bf C}^{n}}{\\delta t}=\\frac{1}{2} \\left (D M {\\bf C}^{n+1}+D M {\\bf C}^{n} \\right)\n\\end{equation}\\]\nWe can transform the last equation to yield the value of of the concentration at the time index \\(n+1\\), i.e.\n\\[\\begin{equation}\n\\left({\\bf I}-\\frac{\\delta t}{2}D M \\right ){\\bf C}^{n+1}=\\left({\\bf I}+\\frac{\\delta t}{2}D M \\right ){\\bf C}^{n}\n\\end{equation}\\]\nwhere \\({\\bf I}\\) is the identity matrix. This will correspond in our code to\n\\[\\begin{equation}\n{\\bf A}{\\bf C}^{n+1}={\\bf B}{\\bf C}^{n}\n\\end{equation}\\]\nwhere \\({\\bf A}=\\left({\\bf I}-\\frac{\\delta t}{2}D M \\right )\\) and \\({\\bf B}=\\left({\\bf I}+\\frac{\\delta t}{2}D M \\right )\\)."
  },
  {
    "objectID": "lectures/lecture12/3_diffusion_equation.html#numerical-solution",
    "href": "lectures/lecture12/3_diffusion_equation.html#numerical-solution",
    "title": "Diffusion equation",
    "section": "Numerical Solution",
    "text": "Numerical Solution\nWe are now ready to write some code. To simulate diffusion, we need two key pieces of information: what happens at the edges of our system (boundary conditions) and how the concentration looks at the start (initial condition).\nLet‚Äôs imagine we‚Äôre looking at diffusion along a line of length L=1. At both ends of this line (x=0 and x=L), we‚Äôll keep the concentration at zero throughout the simulation. This might represent, for example, a situation where any particles that reach the edges are immediately removed:\n\\[\\begin{equation}\nc(0,t)=c(L,t)=0\n\\end{equation}\\]\nFor our starting condition, we‚Äôll create a bell-shaped curve (Gaussian distribution) centered in the middle of our line (at x=L/2). This is like placing a concentrated drop of ink at the center:\n\\[\\begin{equation}\nc(x,0)=\\frac{1}{\\sigma\\sqrt{2\\pi }}e^{-\\frac{(x-L/2)^2}{2\\sigma^2}}\n\\end{equation}\\]\nHere, œÉ=0.05 controls how narrow or wide our initial distribution is - a smaller œÉ means a more concentrated initial drop.\n\nSetup Domain\n\n\n\n\n\n\n\n\nInitial Conditions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMatrix Setup\n\n\n\n\n\n\n\n\nSolution\nTo solve this system of equations, we‚Äôll use the spsolve function from the scipy.sparse.linalg module. This function is designed to solve sparse linear systems efficiently. Here is a detailed explanation of the code:\n1data = []\n2data.append(c)\n\n3for i in range(NT):\n4    A = (I -dt/2*D*M)\n5    B = (I + dt/2*D*M)*c\n6    c = sparse.linalg.spsolve(A, B)\n7    c = np.array(c)\n8    data.append(c)\n\n1\n\nInitialize empty list to store concentration profiles\n\n2\n\nStore initial condition as first element\n\n3\n\nLoop over all time steps\n\n4\n\nCreate matrix A for left side of equation: (I - dt/2D‚àÇ¬≤/‚àÇx¬≤)\n\n5\n\nCreate matrix B times current concentration: (I + dt/2D‚àÇ¬≤/‚àÇx¬≤)*c^n\n\n6\n\nSolve linear system Ac^(n+1) = Bc^n for next time step\n\n7\n\nConvert solution to numpy array for consistency\n\n8\n\nStore concentration profile of current time step"
  },
  {
    "objectID": "lectures/lecture12/3_diffusion_equation.html#where-to-go-from-here",
    "href": "lectures/lecture12/3_diffusion_equation.html#where-to-go-from-here",
    "title": "Diffusion equation",
    "section": "Where to go from here",
    "text": "Where to go from here\nThe diffusion equation we have solved here is a simple example of a partial differential equation. A similar type of equation is the heat equation, which describes how heat spreads in a material. Finally, also the Schr√∂dinger equation is a partial differential equation, which describes the time evolution of a quantum system. You could apply your knowledge of the diffusion equation to solve these more complex problems."
  },
  {
    "objectID": "lectures/lecture12/4_repetition.html",
    "href": "lectures/lecture12/4_repetition.html",
    "title": "Repetition solving ODEs",
    "section": "",
    "text": "Since we have already solved ODEs in one of the past lectures, we should repeat this here. The following exercises will help you to get a better understanding of the solution of ODEs. Try to solve the exercises by yourself and with the help of the contents of Lecture 8.\n\n\n\n\n\n\nSelf-Exercise 1: Simple Harmonic Oscillator\n\n\n\nWrite a program to solve the equation of motion for a simple harmonic oscillator. This example demonstrates how to solve a second-order differential equation using scipy‚Äôs odeint.\nThe equation of motion is: \\(\\frac{d^2x}{dt^2} + \\omega^2x = 0\\)\nThis represents an idealized spring-mass system or pendulum with small oscillations.\n\n\n\n\n\n\n\nUse odeint(oscillator, initial_state, t, args=(omega,)) to solve the system. The solution will have two columns: position ([:,0]) and velocity ([:,1]). Create a plot showing position vs time using matplotlib. Remember to label your axes and add a title.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Exercise 2: Radioactive Decay\n\n\n\nModel the process of radioactive decay, a fundamental concept in nuclear physics. This exercise shows how to solve a first-order differential equation that describes exponential decay.\nThe decay equation is: \\(\\frac{dN}{dt} = -\\lambda N\\)\nWhere \\(N\\) is the number of atoms and \\(\\lambda\\) is the decay constant.\n\n\n\n\n\n\n\nUse odeint(decay, N0, t, args=(lambda_,)) to solve the equation. The solution will be a 1D array of N values. Plot N vs t to visualize the exponential decay. Consider adding a horizontal line at N0/2 to show the half-life. Use plt.grid(True) to make the plot easier to read.\n\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "lectures/lecture12/01-lecture12.html",
    "href": "lectures/lecture12/01-lecture12.html",
    "title": "Lecture 12",
    "section": "",
    "text": "Introduction to more advanced simulation techniques (e.g., finite difference methods).\nStructuring a Python project (modules, documentation, testing).\n\n\n\n\n\nSimulating wave propagation, reflection, and transmission in different media.\nVisualization: Animating wave motion and energy transfer.\n\n\n\n- Introduction to the final project, where students choose a physics problem to model and solve using Python.\n- Discussion of project expectations, timelines, and resources.\n- Homework: Start working on the final project by selecting a topic and outlining the approach.\n\n\n\n## Ideal Gas Simulation with Pressure, Gravity, and Collisions\n\nUse the sliders below to adjust the speed of the gas particles and the gravitational force acting on them. The pressure exerted by the particles on the container walls will be displayed.\n\n  Adjust Particle Speed:\n  \n\n\n  Adjust Gravity:\n  \n\nPressure: 0 Pa"
  },
  {
    "objectID": "lectures/lecture12/01-lecture12.html#advanced-waves-and-final-project-introduction",
    "href": "lectures/lecture12/01-lecture12.html#advanced-waves-and-final-project-introduction",
    "title": "Lecture 12",
    "section": "",
    "text": "Introduction to more advanced simulation techniques (e.g., finite difference methods).\nStructuring a Python project (modules, documentation, testing).\n\n\n\n\n\nSimulating wave propagation, reflection, and transmission in different media.\nVisualization: Animating wave motion and energy transfer.\n\n\n\n- Introduction to the final project, where students choose a physics problem to model and solve using Python.\n- Discussion of project expectations, timelines, and resources.\n- Homework: Start working on the final project by selecting a topic and outlining the approach.\n\n\n\n## Ideal Gas Simulation with Pressure, Gravity, and Collisions\n\nUse the sliders below to adjust the speed of the gas particles and the gravitational force acting on them. The pressure exerted by the particles on the container walls will be displayed.\n\n  Adjust Particle Speed:\n  \n\n\n  Adjust Gravity:\n  \n\nPressure: 0 Pa"
  },
  {
    "objectID": "lectures/lecture15/lecture15.html",
    "href": "lectures/lecture15/lecture15.html",
    "title": "Repetiion",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Set plotting style\n#plt.style.use('seaborn')"
  },
  {
    "objectID": "lectures/lecture15/lecture15.html#setup",
    "href": "lectures/lecture15/lecture15.html#setup",
    "title": "Repetiion",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Set plotting style\n#plt.style.use('seaborn')"
  },
  {
    "objectID": "lectures/lecture15/lecture15.html#introduction-neural-networks-as-physical-systems",
    "href": "lectures/lecture15/lecture15.html#introduction-neural-networks-as-physical-systems",
    "title": "Repetiion",
    "section": "Introduction: Neural Networks as Physical Systems",
    "text": "Introduction: Neural Networks as Physical Systems\nNeural networks might seem complicated, but we can understand them using physics concepts you already know! Let‚Äôs explore how these computational tools are similar to physical systems we‚Äôve studied."
  },
  {
    "objectID": "lectures/lecture15/lecture15.html#neural-networks-as-energy-flow",
    "href": "lectures/lecture15/lecture15.html#neural-networks-as-energy-flow",
    "title": "Repetiion",
    "section": "1. Neural Networks as Energy Flow",
    "text": "1. Neural Networks as Energy Flow\nJust like we study energy flow through physical systems, a neural network processes information flowing through it:\n\nInput values: Like initial energy\nWeights: Like how efficiently energy transfers (think spring constants)\nBias: Like a baseline energy level\nOutput: Like final energy state\n\n\n# Simple example\ninput_value = 2    # Like initial energy\nweight = 0.5       # Like energy transfer efficiency\nbias = 1          # Like baseline energy level\noutput = weight * input_value + bias\n\nprint(f\"Input energy: {input_value}\")\nprint(f\"Transfer efficiency: {weight}\")\nprint(f\"Baseline: {bias}\")\nprint(f\"Final state: {output}\")\n\nInput energy: 2\nTransfer efficiency: 0.5\nBaseline: 1\nFinal state: 2.0"
  },
  {
    "objectID": "lectures/lecture15/lecture15.html#the-sigmoid-function-natures-barrier",
    "href": "lectures/lecture15/lecture15.html#the-sigmoid-function-natures-barrier",
    "title": "Repetiion",
    "section": "2. The Sigmoid Function: Nature‚Äôs Barrier",
    "text": "2. The Sigmoid Function: Nature‚Äôs Barrier\nMany physical systems have natural limits. Think about: - Terminal velocity in air resistance - Saturation in magnetic materials - Maximum compression of a spring\nNeural networks use a similar concept called the sigmoid function:\n\ndef sigmoid(x):\n    return 1/(1 + np.exp(-x))\n\nx = np.linspace(-10, 10, 100)\nplt.figure(figsize=(8, 5))\nplt.plot(x, sigmoid(x))\nplt.title('Sigmoid Function: Like a Physical Barrier')\nplt.xlabel('Input (like force)')\nplt.ylabel('Output (like displacement)')\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "lectures/lecture15/lecture15.html#learning-as-finding-minimum-potential-energy",
    "href": "lectures/lecture15/lecture15.html#learning-as-finding-minimum-potential-energy",
    "title": "Repetiion",
    "section": "3. Learning as Finding Minimum Potential Energy",
    "text": "3. Learning as Finding Minimum Potential Energy\nTraining a neural network is like a ball rolling down a hill: 1. Start somewhere on the hill (initial weights) 2. Look which way is downhill (calculate error) 3. Take a small step in that direction (update weights) 4. Repeat until you reach the bottom (minimum error)\n\n# Visualize a simple \"energy landscape\"\nx = np.linspace(-5, 5, 100)\ny = x**2  # Simple parabola like potential energy well\n\nplt.figure(figsize=(8, 5))\nplt.plot(x, y)\nplt.title('Learning is Like Finding Minimum Potential Energy')\nplt.xlabel('Weight Value')\nplt.ylabel('Error (like Potential Energy)')\nplt.grid(True)\n\n# Mark the minimum\nplt.plot(0, 0, 'ro', label='Minimum Error')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "lectures/lecture15/lecture15.html#practical-example-particle-detection",
    "href": "lectures/lecture15/lecture15.html#practical-example-particle-detection",
    "title": "Repetiion",
    "section": "4. Practical Example: Particle Detection",
    "text": "4. Practical Example: Particle Detection\nLet‚Äôs build a simple ‚Äúneuron‚Äù that could help detect particles:\n\ndef simple_neuron(energy, momentum, weight1, weight2, bias):\n    # Combine inputs like forces combining\n    total_input = weight1 * energy + weight2 * momentum + bias\n    # Convert to probability using sigmoid\n    probability = sigmoid(total_input)\n    return probability\n\n# Example values\nenergy = 100  # MeV\nmomentum = 50 # MeV/c\nw1, w2 = 0.01, 0.02  # Some random weights\nb = -1\n\nprobability = simple_neuron(energy, momentum, w1, w2, b)\nprint(f\"Probability this is our particle: {probability:.2%}\")\n\nProbability this is our particle: 73.11%"
  },
  {
    "objectID": "lectures/lecture15/lecture15.html#visualizing-decision-boundaries",
    "href": "lectures/lecture15/lecture15.html#visualizing-decision-boundaries",
    "title": "Repetiion",
    "section": "5. Visualizing Decision Boundaries",
    "text": "5. Visualizing Decision Boundaries\nJust like phase transitions in physics separate different states of matter, neural networks create boundaries between different classifications:\n\n# Create a grid of points\nE, M = np.meshgrid(np.linspace(0, 200, 100), np.linspace(0, 100, 100))\nZ = simple_neuron(E, M, w1, w2, b)\n\nplt.figure(figsize=(10, 6))\nplt.contourf(E, M, Z, levels=20)\nplt.colorbar(label='Classification Probability')\nplt.title('Particle Classification Boundary')\nplt.xlabel('Energy (MeV)')\nplt.ylabel('Momentum (MeV/c)')\nplt.show()"
  },
  {
    "objectID": "lectures/lecture15/lecture15.html#key-takeaways",
    "href": "lectures/lecture15/lecture15.html#key-takeaways",
    "title": "Repetiion",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nNeural Networks are like physical systems: 1. Process inputs like energy flow 2. Have natural limits (like terminal velocity) 3. Find minimum energy states during training 4. Create boundaries between different states 5. Follow principles of optimization we see in nature"
  },
  {
    "objectID": "lectures/lecture15/lecture15.html#try-it-yourself",
    "href": "lectures/lecture15/lecture15.html#try-it-yourself",
    "title": "Repetiion",
    "section": "Try It Yourself!",
    "text": "Try It Yourself!\nChange the weights and bias in the particle detection example above. How does this affect the decision boundary? This is similar to adjusting the parameters of a physical system!"
  },
  {
    "objectID": "lectures/lecture13/2_spherical_waves.html",
    "href": "lectures/lecture13/2_spherical_waves.html",
    "title": "Spherical waves",
    "section": "",
    "text": "After we have had a look at plane waves, we can explore a second solution of the homogeneous wave equation - Spherical Waves. Spherical waves are elementary waves that are for example considered in Huygens principle. So if we develop some code to visualize spherical waves, we may also verify Huygens principle later."
  },
  {
    "objectID": "lectures/lecture13/2_spherical_waves.html#equations",
    "href": "lectures/lecture13/2_spherical_waves.html#equations",
    "title": "Spherical waves",
    "section": "Equations",
    "text": "Equations\nA spherical wave is as well described by two exponentials containing the spatial and temporal dependence of the wave. The only difference is, that the wavefronts shall describe spheres instead of planes. We therefore need \\(|\\vec{k}||\\vec{r}|=k r=const\\). The product of the magntitudes of the wavevector and the distance from the source are constant. If we further generalize the position of the source to \\(\\vec{r}_{0}\\) we can write a spherical wave by\n\\[\\begin{equation}\nE=\\frac{E_{0}}{|\\vec{r}-\\vec{r}_{0}|}e^{i k|\\vec{r}-\\vec{r}_{0}|} e^{-i\\omega t}\n\\end{equation}\\]\nNote that we have to introduce an additional scaling of the amplitude with the inverse distance of the source. This is due to energy conservation, as we require that all the energy that flows through all spheres around the source is constant."
  },
  {
    "objectID": "lectures/lecture13/2_spherical_waves.html#electric-field",
    "href": "lectures/lecture13/2_spherical_waves.html#electric-field",
    "title": "Spherical waves",
    "section": "Electric field",
    "text": "Electric field\nLets have a look at the electric field of the spherical wave. Below is some code plotting the electric field is space. The source is at the origin and the plot nicely shows, that the amplitude decays with the distance.\n\n\n\n\n\n\nThe line plots below show that the field amplitude rapidly decays and the intensity follows a \\(1/r^2\\) law as expected. The slight deiviation at small distances is an artifact from our discretization. We used the image above to extract the line plot and therefore never exactly hit \\(r=0\\)."
  },
  {
    "objectID": "lectures/lecture13/2_spherical_waves.html#animation",
    "href": "lectures/lecture13/2_spherical_waves.html#animation",
    "title": "Spherical waves",
    "section": "Animation",
    "text": "Animation\nWe can also visualize the animation our spherical wave to check for the direction of the wave propagation.\nnorm = mpl.colors.Normalize(vmin=-5e6, vmax=5e6)\ncmap = cm.seismic\nm = cm.ScalarMappable(norm=norm, cmap=cmap)\ncanvas = Canvas(width=300, height=300,sync_image_data=True)\ndisplay(canvas)\ndef animate(k,time):\n    for t in time:\n        field=spherical_wave(k,omega0,r,r0,t)\n        data=np.zeros([300,300,3])\n        tmp=np.real(field.transpose())\n        c=m.to_rgba(tmp)\n        with hold_canvas(canvas):\n            canvas.put_image_data(c[:,:,:3]*255,0,0)\n        sleep(0.02)\ntime= np.linspace(0,1e-14,200)\nanimate(k,time)"
  },
  {
    "objectID": "lectures/lecture13/2_spherical_waves.html#plot-the-intensity-in-an-image-plane",
    "href": "lectures/lecture13/2_spherical_waves.html#plot-the-intensity-in-an-image-plane",
    "title": "Spherical waves",
    "section": "Plot the intensity in an image plane",
    "text": "Plot the intensity in an image plane\nAs we have now the electric field in space, wqe may also chose an arbitrary plane in space to record the intensity of that wave in space. Here we want to know the intensity in a plane at 10 ¬µm distance from the source, which is again at the origin. The intensity cross section at the screen is a Lorentzian function."
  },
  {
    "objectID": "lectures/lecture13/2_spherical_waves.html#interference-between-a-spherical-and-a-plane-wave",
    "href": "lectures/lecture13/2_spherical_waves.html#interference-between-a-spherical-and-a-plane-wave",
    "title": "Spherical waves",
    "section": "Interference between a spherical and a plane wave",
    "text": "Interference between a spherical and a plane wave\nIn the section on plane waves, we had a look at the interference pattern of plane waves in space. We now have a look at the interference of a plane wave and a spherical wave. The plane wave thereby probes the distortion of the spherical wavefronts and the interference pattern stores this information on the shape of the spherical wavefronts. This is exactly what is done in holography. Taking this interference pattern as a ‚Äúdiffraction grating‚Äù will allow you to restore information on the spherical wavefonts."
  },
  {
    "objectID": "lectures/lecture13/1_plane_waves.html",
    "href": "lectures/lecture13/1_plane_waves.html",
    "title": "Electromagnetic Waves",
    "section": "",
    "text": "In the previous parts we have dealt with mechanics essentially. Even if we have described Brownian motion, this has been done by a particular type of Newtons equation of motion, it is much like mechanics. Now we would like to have a look at some examples from electromagnetic waves. We will not solve the wave equation but look at some solution using the complex notion of the electric field. This shall train our use of complex numbers. The special solutions are the plane wave and the spherical wave and we will be able to simulate a number of things especially with the spherical waves as they are part of Huuygens principle."
  },
  {
    "objectID": "lectures/lecture13/1_plane_waves.html#plane-waves",
    "href": "lectures/lecture13/1_plane_waves.html#plane-waves",
    "title": "Electromagnetic Waves",
    "section": "Plane waves",
    "text": "Plane waves\nWe will start with plane waves. Plane waves are solutions of the homogeneous wave equation and are the simplest solutions of the wave equation. They are also the basis for the description of more complicated waves. We will have a look at the electric field of a plane wave and its propagation in space and time.\n\nEquations\nA plane wave is a solution of the homogeneous wave equation and is given in its complex form by\n\\[\\begin{equation}\nE=E_{0}e^{i\\vec{k}\\cdot \\vec{r}}e^{-i\\omega t}\n\\end{equation}\\]\nwhere the two exponentials contain a spatial and a temporal phase. \\(E_{0}\\) denotes the amplitude of the plane wave. The plane is defined by the shape of the wavefront which is given by \\(\\vec{k}\\cdot \\vec{r}=const\\), which is just the definition of a plane perpendicular to \\(\\vec{k}\\).\nA wave is a physical quantity which oscillates in space and time. Its energy current density is related to the square magnitude of the amplitude. We will include in the following the spatial and the temporal phase. For plotting just the spatial variation of the electric field, you may just use the spatial part of the equation\n\\[\\begin{equation}\nE=E_{0}e^{i\\vec{k}\\cdot \\vec{r}}\n\\end{equation}\\]\nBut since we also want to see the wave propagate, we will directly include also the temporal dependence in our function. In all of the examples below we set the amplitude of the wave \\(E_{0}=1\\).\nThe propagation of the wave is defined by wavevector \\(\\vec{k}\\). In vacuum, the wavevector is just real valued\n\\[\\begin{equation}\n\\vec{k}_{0}=\n\\begin{pmatrix}\nk_{0x} \\\\\nk_{0y}\\\\\nk_{0z}\\\\\n\\end{pmatrix}\n\\end{equation}\\]\nThe wavevector is providing the direction in which the wavefronts propagate. It is also proportional to the momentum of the wave, which will be important if we consider the refraction process a bit later. The magnitude of the wavevector is related to the wavelength \\(\\lambda\\).\n\\[\\begin{equation}\nk_{0}=\\frac{2\\pi}{\\lambda_{0}}=\\frac{\\omega}{c_{0}}\n\\end{equation}\\]\nAt the same time, its magnitude is also given by the angular frequency divided by the speed of light. The latter is called a dispersion relation.\nIn a medium, the wavevector is by a factor of \\(n\\) longer, where n is the refractive index. Since the refractive index may be a complex number, e.g.¬†\\(n=\\eta+i\\kappa\\), the wavevector can be complex as well. It is then given by\n\\[\\begin{equation}\n\\vec{k}=n\\vec{k}_{0}=\n\\begin{pmatrix}\nk_{x}^{\\prime}+ik_{x}^{\\prime\\prime} \\\\\nk_{y}^{\\prime}+ik_{y}^{\\prime\\prime} \\\\\nk_{z}^{\\prime}+ik_{z}^{\\prime\\prime} \\\\\n\\end{pmatrix}\n\\end{equation}\\]\nThe complex refractive index means that there is some damping of the electromagnetic wave due to absorption, for example.\nThe wavelength is then related to\n\\[\\begin{equation}\n\\Re(k)=\\eta \\frac{2\\pi}{\\lambda_{0}}\n\\end{equation}\\]\nand the imaginary part gives the damping\n\\[\\begin{equation}\n\\Im(k)=\\kappa \\frac{2\\pi}{\\lambda_{0}}\n\\end{equation}\\]\n\n\nElectric field\n\n\n\n\n\n\nLets have a look at waves and wave propagation. We want to create a wave, which has a wavelength of 532 nm in vacuum.\n\n\n\n\n\n\nIt shall propagate along the z-direction and we wull have a look at the x-z plane.\n\n\n\n\n\n\nWe can plot the electric field in the x-z plane by defining a grid of points (x,z). This is done by the meshgrid function of numpy. The meshgrid returns a 2-dimensional array for each coordinate. Have a look at the values in the meshgrid.\n\n\n\n\n\n\nIn the last lines, we defined an array of X,0,Z, where X and Z are already 2-dimensional array. This finally gives an array 3D vectors, which we can use to calculate the electric field at any point in space. If we want to plot the electric field, we have to calculate the real part of the complex values, as the electric field is a physical quantity, which is always real. There is not much to see for a plane wave in the intensity plot, as the intensity of a plane wave is constant in space. Yet, if you want to plot it, you have to calculate the magnitude square of the electric field, e.g.\n\\[\\begin{equation}\nI\\propto |E|^{2}\n\\end{equation}\\]\n\n\n\n\n\n\n\n\nPlane wave propagation\nThe above graph shows a static snapshot of the plane wave at a time \\(t=0\\). We know, however, that a plane wave is propagating in space and time. Since we know how to animate things, we may do that using the ipycanvas module.\n\nx=np.linspace(-2.5e-6,2.5e-6,300)\nz=np.linspace(0,5e-6,300)\n\nX,Z=np.meshgrid(x,z)\nr=np.array([X,0,Z],dtype=object)\ncanvas = Canvas(width=300, height=300,sync_image_data=False)\ndisplay(canvas)\nTo do the animation I use a little trick to get the same color map as in the matplotlib plotting. The function below uses the matplotlib color map seismic and the corresponding mapping of values with a given minimum vmin and maximum vmax value. The mapping is done in the animation function with c=m.to_rgba(tmp).\n\n#normalize the color map to a certain value range\nnorm = mpl.colors.Normalize(vmin=-1, vmax=1)\n\ncmap = cm.seismic\n\n# do the mapping of values to color values.\nm = cm.ScalarMappable(norm=norm, cmap=cmap)\nThis is our animation function, where I provide time and the wavevector as arguments, such that we may change both parameters easily.\ndef animate(k,time):\n    for t in time:\n        field=plane_wave(k,omega0,r,t)\n        tmp=np.real(field.transpose())\n        c=m.to_rgba(tmp)\n        with hold_canvas(canvas):\n            canvas.put_image_data(c[:,:,:3]*255,0,0)\n            #canvas.put_image_data(data*255,0,0)\n        sleep(0.02)\nWith the call below, you may animate the wave now with different refractive indices.\n\neta=1.\nkappa=0.0\nn=eta+kappa*1j\n\nk=n*k0*vec\ntime= np.linspace(0,5e-14,500)\nanimate(k,time)\n\n\nImaginary wave vector\nIf we now create a material, which has an imaginary part of the refractive index, we see that the amplitude decays and the wave fades.\n\n\n\n\n\n\nThe above plots show the electric field amplitude in the x-z plane. We may also have a look the field amplitude and intensity as a function of the z-position by chosing a single x-value. In the plot below, you may notice two things. The first is, that the wave decays exponentially with distance \\(z\\). Intensity and field decay with different decay length. The field decays with \\(\\exp(-\\kappa*k_{0}z)\\) while the intensity of cause decays twice as fast \\(\\exp(-2\\kappa*k_{0}z)\\) due to the fact the the intensity is the square of the electric field.\n\n\n\n\n\n\n\n\nAnimation\nOf course, we should not miss the animation.\ndisplay(canvas)\nk=n*k0*vec\ntime= np.linspace(0,5e-14,500)\nanimate(k,time)\n\n\nInterference of two plane waves\nIt is not very difficult to calculate from the definitions we did above now the interference of two plane waves, which have different directions of the wavevector. The total field in space is then just the sum of the two fields\n\\[\\begin{equation}\n\\vec{E}=\\vec{E}_{1}+\\vec{E}_{2}\n\\end{equation}\\]\nThe interesting thing is now to look at the intensity which\n\\[\\begin{equation}\nI\\propto |\\vec{E}|^2=|\\vec{E}_{1}|^2+|\\vec{E}_{2}|^2 + \\vec{E}_{1}^{*} \\vec{E}_{2}+\\vec{E}_{2}^{*}\\vec{E}_{1}\n\\end{equation}\\]\n\n\n\n\n\n\nWhile the field pattern still looks complicated, the intensity pattern is just a set of bright lines.\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced Topics: Plane wave at a boundary\n\n\n\n\n\nWe want to go a bit further now and have a look at the wave at a boundary between vaccum and glass for example. At this boundary, the electromagnetic wave is reflected and refracted such that two new wavevectors arise. These are easily calculated by the law of reflection and the law of refraction. Besides that, also the amplitude of the waves change. To calculate the field we need the so-called Fresnel equations.\n\nFresnel equations\nWhen electromagnetic waves hit a boundary, they will be reflected and refracted. The amplitude of the reflected and refracted wave is determined by the refractive index of the two materials, the angles and the polarizations. For the latter we differentiate between a polarization in the incident plane (the p-polarization) and perpendicular to the incident plane (s-polarization).\n\n\n\nFresnel\n\n\nFor each of the polarization we in general obtain a coeffcient for the reflection and one for the refraction. To make our calculation a bit simpler, we will assume only s-polarization. Then the two Fresnel coefficients are calculated as\n\\[\\begin{equation}\n\\left( \\frac{E_{0t}}{E_{0e}} \\right)_s = t_s =\\frac{2n_1 \\cos{\\alpha}}{n_1\\cos{\\alpha}+n_2\\cos{\\beta}}\n\\end{equation}\\]\n\\[\\begin{equation}\n\\left( \\frac{E_{0r}}{E_{0e}} \\right)_s = r_s =\\frac{n_1\\cos{\\alpha}-n_2\\cos{\\beta}}{n_1\\cos{\\alpha}+n_2\\cos{\\beta}}\n\\end{equation}\\]\nwhere \\(\\alpha\\) and \\(\\beta\\) are the incident and refraction angles, respectively. Note that the Fresnel coefficients are for the amplitudes and can be negative to account for a phase jump by \\(\\pi\\). To obtain the coefficients for the intensities, one has to square the Fresnel coefficients.\nTo bring everything correctly together, we therefore have to define a number of things. We will need a function calculating the outgoing angle from Snells law. And we need at least two functions calculating the reflection and transmission coefficient for one polarization. We use the s-polarization, where the electric field is always parallel to the interface.\n\n\n\n\n\n\nWith the definition of the Fresnel coefficients, we may now plot the reflection and the transmission coefficients. Note that the sum of reflection and transmission coefficients for the intensities have to add up to one if there is no absorption.\n\n\n\n\n\n\n\n\nIncident wave\nWe want to study the electric fields and the intensities at various angles. The most interesting one, is a case where we have total internal reflection. This happens, if light is propagating from the higher refractive index to a lower refractive index. If we start in glass (\\(n_1=1.5\\)) and transmit to vacuum \\(n_2=1\\), then at all angles above \\(\\theta_{c}=\\sin^{-1}(n_2/n_1)=41.810314895778596\\) are total internally reflected.\n\n\n\n\n\n\nWe may now specify or calculate the corresponding wavevectors for an incident angle of \\(45^{\\circ}\\). In general all waves (reflect, refracted) have to match with their phase at the boundary. If the boundary is along the x-direction, we therefore have\n\\[\\begin{equation}\nk_{x,in}=k_{x,r}=k_{x,t}\n\\end{equation}\\]\nThis fixes one component of all wavevectors in the plane. What is then missing, is the z-component of the wavevectors. The incident wavevector is providing \\(k_{z,in}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReflected wave\nFor the reflected wave the z-component of the wavevector is just flipped in sign, e.g.¬†\\(k_{z,r}=-k_{z,in}\\).\n\n\n\n\n\n\n\n\nRefracted wave\nThe magnitude of the z-component of the transmitted wave can be obtained from the conservation of momentum. The momentum of the wave is proportional to the magnitude of the wavevector on both sides.\n\\[\\begin{equation}\nk_{1}^2=k_{2}^{2}\n\\end{equation}\\]\nwhich is, due to \\(k=nk_{0}\\) the same as\n\\[\\begin{equation}\nn_{1}^2(k_{0x,in}^2+k_{0z,in}^{2})=n_2^2 (k_{0x,t}^{2}+k_{0z,t}^2)\n\\end{equation}\\]\nfrom which we get\n\\[\\begin{equation}\nk_{0z,t}=\\pm \\frac{1}{n_{1}}\\sqrt{n_2^2 k_{0z,in}^2 -(n_{1}^2-n_{2}^2)k_{0x}^{2}}\n\\end{equation}\\]\nIf we go from a medium with high refrective index to a lower one, the second term in the root may surpass the first one and the whole solution will become imaginary. The wave in the lower refractive index medium \\(n_{2}\\) is then evanescent.\n\n\n\n\n\n\nThe total field thus containes three components. In medium 1, the field consists of the incident and the reflected wave. In medium 2, we just have the transmitted wave, with a possible evanescent solution.\n\n\n\n\n\n\nThe plots below show the electric field on the left side and the intensity on the right side. Interestingly, the intensity is that of a standing wave in medium 1, while it is just decaying in medium 2. Note that the electric field is oscillating along the interface in medium 2 but not at all in z-direction. This means that there is no energy transport along the z-direction anymore.\n\n\n\n\n\n\nWe will also have a look ath the propagation of the wave yb defining our animation.\ncanvas = Canvas(width=500, height=500,sync_image_data=True)\ndisplay(canvas)\n\ndef animate(k,time):\n    for t in time:\n        field=np.zeros([500,500],dtype=complex)\n        field1=plane_wave(k1,omega0,r1,t)\n        field2=plane_wave(k2,omega0,r1,t)\n        field3=plane_wave(k3,omega0,r2,t)\n\n        beta=snell(n1,n2,alpha)\n        r=rs(n1,n2,alpha,beta)\n        t=ts(n1,n2,alpha,beta)\n\n        field[0:250,:]=field1+r*field2\n        field[250:,:]=t*field3\n        tmp=np.real(field.transpose())\n\n        c=m.to_rgba(tmp)\n        with hold_canvas(canvas):\n            canvas.put_image_data(c[:,:,:3]*255,0,0)\n        sleep(0.02)\n\ntime= np.linspace(0,1e-14,100)\nanimate(k,time)\nAs it is apparent from our simulation, the wave is longitudinal in medium 2 at this angle. Try to modify the incident angles yourself to see if the wave becomes propagating in medium 2.\nIn the last plot, we will have a look at the intensity in medium 1 and medium 2. What is nicely visible, is that the intensity decays in medium 2 with increasing distance. As compared to the absorbing case, there is not oscillation of the field in the z-direction, hence no energy transfer. Convince yourself that this is indeed an exponential decay by using the appropriate semilog plot."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References"
  }
]